Incidents.count
Incident.count
Product.first
Product.first.entity.name
EntityType.all
Entity.last
Cloud.current
Entity.find_by(name: 'RUVIXXCOM')
EntitiesIndex.by_name('RUVIXXCOM')
ent = Entity.find_by(name: 'RUVIXXCOM')
ent.entity_type_ids
EntityType(y)
EntityType(7)
EntityType.find(7)
ent.incidents
Incident.last
Incident.last.related_entities
Incident.last.entity
Incident.last.incident_entities
ent = Entity.find_by(name: 'SONY')
ent.entity_type_ids
Entity.find_by(name: 'GLOBALFOUNDRIES')
gst
Date.current.year
Date.current.year - 5
Date.current.year - 5.years
Date.current.year - 5.5
Date.current.year.class
reload
reload!
ReportingPeriodService.YEARS_TO_FIRST_REPORTS
ReportingPeriodService::YEARS_TO_FIRST_REPORTS
reload!
ReportingPeriodService.reporting_period_years
Product.includes(royalty_report_items: { :product_id })
Product.includes(royalty_report_items: :product_id )
Product.includes(royalty_report_items: :product_id).query
Product.includes(royalty_report_items: :product_id).to_s
Product.includes(royalty_report_items: :product_id).build
Product.includes(royalty_report_items: :product_id, :royalty_report_id).includes(royalty_reports: :id)
Product.includes(:royalty_report_items, :royalty_reports).where(royalty_report_items: {product_id: products.id, report_id: royalty_reports.id}, royalty_report: {start_date: Date.current})
Product.includes(:royalty_report_items, :royalty_reports).where(royalty_report_items: {product_id: products.id, report_id: royalty_reports.id}, royalty_report: {start_date: Date.current}).references(:products)
Product.joins('LEFT OUTER JOIN royalty_report_items ON products.id = royalty_report_items.product_id')
.joins('LEFT OUTER JOIN royalty_reports ON royalty_reports.id = royalty_report_items.royalty_report_id')
.where('royalty_reports.start_date is NULL')
Product.joins('LEFT OUTER JOIN royalty_report_items ON products.id = royalty_report_items.product_id') \
.joins('LEFT OUTER JOIN royalty_reports ON royalty_reports.id = royalty_report_items.royalty_report_id') \
Product.joins('LEFT OUTER JOIN royalty_report_items ON products.id = royalty_report_items.product_id').joins('LEFT OUTER JOIN royalty_reports ON royalty_reports.id = royalty_report_items.royalty_report_id').where('royalty_reports.start_date is NULL')
Product.joins('LEFT OUTER JOIN royalty_report_items ON products.id = royalty_report_items.product_id').joins('LEFT OUTER JOIN royalty_reports ON royalty_reports.id = royalty_report_items.royalty_report_id').where('royalty_reports.start_date is NULL').count
never_reported = Product.where <<-SQL
          products.id IN (
            SELECT p.id
            FROM products p
            LEFT OUTER JOIN royalty_report_items rri ON p.id=rri.product_id
            LEFT OUTER JOIN royalty_reports rr ON rr.id=rri.royalty_report_id
            WHERE rr.start_date IS NULL
          )
        SQL
_.count
Project.includes(:royalty_report_items).where('royalty_report_items.project.id = projects.id')
Project.includes(:royalty_report_items, :royalty_reports).where('royalty_report_items.project.id = projects.id').references(:royalty_report_items)
Project.includes(:royalty_report_items, :royalty_reports).where('royalty_report_items.project.id = projects.id').references(:royalty_report_items).count
Project.includes(:royalty_report_items, :royalty_reports).where('royalty_report_items.project_id = projects.id').references(:royalty_report_items).count
Project.includes(:royalty_report_items, :royalty_reports).where('royalty_report_items.project_id = projects.id').references(:royalty_report_items)
Project.includes(:royalty_report_items, :royalty_reports).where('royalty_report_items.project_id = ?', id).references(:royalty_report_items).where
Project.includes(:royalty_report_items, :royalty_reports).where('royalty_report_items.project_id = ?', :id).references(:royalty_report_items).where
Project.includes(:royalty_report_items, :royalty_reports).where('royalty_report_items.project_id = ?', :id).references(:royalty_report_items)
Project.includes(:royalty_report_items, :royalty_reports).where('royalty_report_items.project_id = ?', :id).references(:royalty_report_items).count
Product.includes(:royalty_report_items, :royalty_reports).where('royalty_report_items.project_id = ?', :id).references(:royalty_report_items).count
Product.joins(:royalty_report_items, :royalty_reports).where('royalty_report_items.project_id = ?', :id).references(:royalty_report_items).count
Product.joins(:royalty_report_items, :royalty_reports).where('royalty_report_items.project_id = ?', :id).references(:royalty_report_items).to_sql
Product.includes(:royalty_report_items, :royalty_reports).where('royalty_report_items.project_id = ?', :id).references(:royalty_report_items).to_sql
Product.includes(:royalty_report_items, :royalty_reports).where('royalty_report_items.product_id = products.id').references(:royalty_report_items).to_sql
Product.includes(:royalty_report_items, :royalty_reports).where('royalty_report_items.product_id = products.id').references(:royalty_report_items).count
Product.includes(:royalty_report_items, :royalty_reports).where('royalty_report_items.product_id = products.id').references(:royalty_report_items).size
Product.includes(royalty_report_items: :royalty_reports).where('royalty_report_items.product_id = products.id').references(:royalty_report_items).size
Product.includes(royalty_report_items: :royalty_reports).where('royalty_report_items.product_id = products.id').references(:royalty_report_items)
Product.includes(:royalty_report_items).where(royalty_report_items: {product_id: products.id})
Product.includes(:royalty_report_items).where(royalty_report_items: {product_id: id})
Product.includes(:royalty_report_items).where(royalty_report_items: {product_id: :id})
Product.includes(:royalty_report_items).where(royalty_report_items: {product_id: :id}).to_sql
Product.joinss(:royalty_report_items).where(royalty_report_items: {product_id: :id})
Product.joins(:royalty_report_items).where(royalty_report_items: {product_id: :id})
Product.joins(:royalty_report_items).where(royalty_report_items: {product_id: :id}).count
Product.joins('LEFT OUTER JOIN royalty_report_items ON products.id = royalty_report_items.product_id')
.joins('LEFT OUTER JOIN royalty_reports ON royalty_reports.id = royalty_report_items.royalty_report_id')
.where('royalty_reports.start_date is NULL')
Product.joins('LEFT OUTER JOIN royalty_report_items ON products.id = royalty_report_items.product_id').joins('LEFT OUTER JOIN royalty_reports ON royalty_reports.id = royalty_report_items.royalty_report_id').where('royalty_reports.start_date is NULL').to_sql
Product.where <<-SQL
          products.id IN (
            SELECT p.id
            FROM products p
            LEFT OUTER JOIN royalty_report_items rri ON p.id=rri.product_id
            LEFT OUTER JOIN royalty_reports rr ON rr.id=rri.royalty_report_id
            WHERE rr.start_date IS NULL
          )
        SQL.to_sql
Product.where <<-SQL
            SELECT p.id
Product.where <<-SQL
          products.id IN (
            SELECT p.id
            FROM products p
            LEFT OUTER JOIN royalty_report_items rri ON p.id=rri.product_id
            LEFT OUTER JOIN royalty_reports rr ON rr.id=rri.royalty_report_id
            WHERE rr.start_date IS NULL
          )
        SQL
_.to_sql
Product.joins('LEFT OUTER JOIN royalty_report_items ON products.id = royalty_report_items.product_id').joins('LEFT OUTER JOIN royalty_reports ON royalty_reports.id = royalty_report_items.royalty_report_id').where('royalty_reports.start_date is NULL').to_sql
Product.joins('LEFT OUTER JOIN royalty_report_items ON products.id = royalty_report_items.product_id').joins('LEFT OUTER JOIN royalty_reports ON royalty_reports.id = royalty_report_items.royalty_report_id').where('royalty_reports.start_date is NULL')
subq = Product.joins('LEFT OUTER JOIN royalty_report_items ON products.id = royalty_report_items.product_id').joins('LEFT OUTER JOIN royalty_reports ON royalty_reports.id = royalty_report_items.royalty_report_id').where('royalty_reports.start_date is NULL').select('products.id')
Product.where(id: Product).joins('LEFT OUTER JOIN royalty_report_items ON products.id = royalty_report_items.product_id').joins('LEFT OUTER JOIN royalty_reports ON royalty_reports.id = royalty_report_items.royalty_report_id').where('royalty_reports.start_date is NULL').select('products.id')
Product.where(id: Product('id')).joins('LEFT OUTER JOIN royalty_report_items ON products.id = royalty_report_items.product_id').joins('LEFT OUTER JOIN royalty_reports ON royalty_reports.id = royalty_report_items.royalty_report_id').where('royalty_reports.start_date is NULL').select('products.id')
Product.where(id: 'products.id').joins('LEFT OUTER JOIN royalty_report_items ON products.id = royalty_report_items.product_id').joins('LEFT OUTER JOIN royalty_reports ON royalty_reports.id = royalty_report_items.royalty_report_id').where('royalty_reports.start_date is NULL').select('products.id')
Product.includes(royalty_report_items: :royalty_reports).where('royalty_report_items.product_id = products.id').references(:royalty_report_items)
Product.includes(royalty_report_items: :royalty_reports).where('royalty_report_items.product_id = products.id').references(:royalty_report_items).count
Product.includes(royalty_report_items: :royalty_report).where('royalty_report_items.product_id = products.id').references(:royalty_report_items).count
Product.includes(royalty_report_items: :royalty_report).where('royalty_report_items.product_id = products.id').references(:royalty_report_items).where('royalty_report_items.report_id = royalty_report.id')
Product.includes(royalty_report_items: :royalty_report).where('royalty_report_items.product_id = products.id').references(:royalty_report_items).where('royalty_report_items.report_id = royalty_report.id').count
Product.includes(royalty_report_items: :royalty_report).where('royalty_report_items.product_id = products.id').references(:royalty_report_items).where('royalty_report_items.royalty_report_id = royalty_report.id').count
Product.includes(royalty_report_items: :royalty_report).where('royalty_report_items.product_id = products.id').references(:royalty_report_items).where('royalty_report_items.royalty_report_id = royalty_report.id').references(:royalty_report).count
Product.includes(royalty_report_items: :royalty_report).where('royalty_report_items.product_id = products.id').references(:royalty_report_items).where('royalty_report_items.royalty_report_id = royalty_reports.id').references(:royalty_report).count
Product.includes(royalty_report_items: :royalty_report).where('royalty_report_items.product_id = products.id').references(:royalty_report_items).where('royalty_report_items.royalty_report_id = royalty_reports.id').references(:royalty_report).where(royalty_report: {start_date: nil}).count
Product.includes(royalty_report_items: :royalty_report).where('royalty_report_items.product_id = products.id').references(:royalty_report_items).where('royalty_report_items.royalty_report_id = royalty_reports.id').references(:royalty_report).where(royalty_reports: {start_date: nil}).count
Product.includes(royalty_report_items: :royalty_report).where('royalty_report_items.product_id = products.id').references(:royalty_report_items).where('royalty_report_items.royalty_report_id = royalty_reports.id').references(:royalty_report).where('royalty_reports.start_date is NULL')
Product.includes(royalty_report_items: :royalty_report).where('royalty_report_items.product_id = products.id').references(:royalty_report_items).where('royalty_report_items.royalty_report_id = royalty_reports.id').references(:royalty_report).where('royalty_reports.start_date is NULL').to_sql
Product.where <<-SQL
          products.id IN (
            SELECT p.id
            FROM products p
            LEFT OUTER JOIN royalty_report_items rri ON p.id=rri.product_id
            LEFT OUTER JOIN royalty_reports rr ON rr.id=rri.royalty_report_id
            WHERE rr.start_date = '#{period.start_date.to_date}'
          )
        SQL
Product.where <<-SQL
          products.id IN (
            SELECT p.id
            FROM products p
            LEFT OUTER JOIN royalty_report_items rri ON p.id=rri.product_id
            LEFT OUTER JOIN royalty_reports rr ON rr.id=rri.royalty_report_id
            WHERE rr.start_date IS NULL
          )
        SQL
_.count
Product.joins(royalty_report_items: :royalty_report).where('royalty_report_items.product_id = products.id').references(:royalty_report_items).where('royalty_report_items.royalty_report_id = royalty_reports.id').references(:royalty_report).where('royalty_reports.start_date is NULL')
Product.joins('LEFT JOIN royalty_report_items').where('royalty_report_items.product_id = products.id').references(:royalty_report_items).joins('LEFT JOIN royalty_reports').where('royalty_report_items.royalty_report_id = royalty_reports.id').references(:royalty_report).where('royalty_reports.start_date is NULL')
Product.joins('LEFT JOIN royalty_report_items').where('royalty_report_items.product_id = products.id').references(:royalty_report_items).joins('LEFT JOIN royalty_reports').where('royalty_report_items.royalty_report_id = royalty_reports.id').references(:royalty_report).where('royalty_reports.start_date is NULL').count
Product.joins('LEFT OUTER JOIN royalty_report_items').where('royalty_report_items.product_id = products.id').references(:royalty_report_items).joins('LEFT OUTER JOIN royalty_reports').where('royalty_report_items.royalty_report_id = royalty_reports.id').references(:royalty_report).where('royalty_reports.start_date is NULL').count
Product.joins(royalty_report_items: :royalty_reports)
Product.joins(royalty_report_items: :royalty_reports).to_sql
Product.joins(royalty_report_items: :royalty_report).to_sql
Product.includes(royalty_report_items: :royalty_report).to_sql
Product.includes(royalty_report_items: :royalty_report)
Product.includes(:royalty_report_items, :royalty_reports)
Product.includes(:royalty_report_items, :royalty_reports).where('royalty_report_items.royalty_report_id = royalty_reports.id')
Product.includes(:royalty_report_items, :royalty_reports).where('royalty_report_items.royalty_report_id = royalty_reports.id').count
Product.includes(:royalty_report_items, :royalty_reports).where('royalty_report_items.royalty_report_id = royalty_reports.id').references(:royalty_report_items)
Product.includes(:royalty_report_items, :royalty_reports).where('royalty_report_items.royalty_report_id = royalty_reports.id').references(:royalty_report_items).count
Product.includes(:royalty_report_items, :royalty_report).where('royalty_report_items.royalty_report_id = royalty_reports.id').references(:royalty_report_items).count
Product.includes(royalty_report_items: :royalty_report).where('royalty_report_items.royalty_report_id = royalty_reports.id').references(:royalty_report_items).count
Product.includes(royalty_report_items: :royalty_report).where('royalty_report_items.royalty_report_id = royalty_reports.id').references(:royalty_report_items).where('royalty_reports.start_date is NULL').count
Product.includes(royalty_report_items: :royalty_report).where('royalty_report_items.royalty_report_id = royalty_reports.id').references(:royalty_report_items).where('royalty_reports.start_date is NULL').references(:royalty_report).count
Product.includes(royalty_report_items: :royalty_report).where('royalty_report_items.royalty_report_id = royalty_reports.id').references(:royalty_report_items).where('royalty_reports.start_date is NULL').references(:royalty_reports).count
Product.includes(royalty_report_items: :royalty_report).where('royalty_report_items.royalty_report_id = royalty_reports.id').references(:royalty_report_items).where('royalty_reports.start_date is NULL')
Product.includes(royalty_report_items: :royalty_report).where('royalty_report_items.royalty_report_id = royalty_reports.id').references(:royalty_report_items).where('royalty_reports.start_date is NULL').count
Product.includes(royalty_report_items: :royalty_report).select('product.id').where('royalty_report_items.royalty_report_id = royalty_reports.id').references(:royalty_report_items).where('royalty_reports.start_date is NULL').count
Product.includes(royalty_report_items: :royalty_report).where('royalty_report_items.royalty_report_id = royalty_reports.id').references(:royalty_report_items).where('royalty_reports.start_date is NULL').count
Product.includes(royalty_report_items: :royalty_report).where('royalty_report_items.royalty_report_id = royalty_reports.id').references(:royalty_report_items).where('royalty_reports.start_date is NULL').references(:royalty_report)
Product.includes(royalty_report_items: :royalty_report).where('royalty_report_items.royalty_report_id = royalty_reports.id').references(:royalty_report_items).where('royalty_reports.start_date is NULL').references(:royalty_report).count
Product.where <<-SQL
          products.id IN (
            SELECT p.id
            FROM products p
            LEFT OUTER JOIN royalty_report_items rri ON p.id=rri.product_id
            LEFT OUTER JOIN royalty_reports rr ON rr.id=rri.royalty_report_id
            WHERE rr.start_date IS NULL
          )
        SQL
Product.includes(royalty_report_items: :royalty_report).where('royalty_report_items.royalty_report_id = royalty_reports.id').references(:royalty_report_items).where('royalty_reports.start_date is NULL').references(:royalty_report).count
Product.includes(royalty_report_items: :royalty_report).on('royalty_report_items.royalty_report_id = royalty_reports.id').references(:royalty_report_items).where('royalty_reports.start_date is NULL').references(:royalty_report).count
Product.joins(royalty_report_items: :royalty_report)
OAOA
xalkjjfl;kjfashqpoierupoqwieutopqiwegoahdskjvbaskjdg
a;sdlfkj
helo
Product.where <<-SQL
          products.id IN (
            SELECT p.id
            FROM products p
            LEFT OUTER JOIN royalty_report_items rri ON p.id=rri.product_id
            LEFT OUTER JOIN royalty_reports rr ON rr.id=rri.royalty_report_id
            WHERE rr.start_date IS NULL
          )
Product.joins(royalty_report_items: :royalty_report).to_sql
Product.joins(royalty_report_items: :royalty_report).where('royalty_reports.start_date is NULL')
Product.joins(royalty_report_items: :royalty_report).where('royalty_reports.start_date is NULL').count
exit
Product.where <<-SQL
          products.id IN (
            SELECT p.id
            FROM products p
            LEFT OUTER JOIN royalty_report_items rri ON p.id=rri.product_id
            LEFT OUTER JOIN royalty_reports rr ON rr.id=rri.royalty_report_id
            WHERE rr.start_date IS NULL
          )
SQL
_.count
exit
Entity.find_by(name: '68836')
exit
Date.new
Date.current
reload!
FactoryGirl.build(:royalty_report, :invoiced)
exit
start_date = Date.current.beginning_of_year
start_date = Date.current.end_of_year
RoyaltyReport.all
RoyaltyReport.all.count
RoyaltyReport.where('royalty_reports.accepted at BETWEEN CAST(? AS DATE) AND CAST(? AS DATE)', start_date, end_date)
start_date = Date.current.beginning_of_year
end_date = Date.current.end_of_year
RoyaltyReport.where('royalty_reports.accepted at BETWEEN CAST(? AS DATE) AND CAST(? AS DATE)', start_date, end_date)
RoyaltyReport.where('royalty_reports.accepted at BETWEEN CAST(? AS DATE) AND CAST(? AS DATE)', start_date, end_date).count
RoyaltyReport.where('royalty_reports.accepted at BETWEEN CAST(? AS DATE) AND CAST(? AS DATE)', [start_date, end_date]).count
RoyaltyReport.where('royalty_reports.accepted at BETWEEN CAST("?" AS DATE) AND CAST("?" AS DATE)', start_date, end_date).count
RoyaltyReport.where('royalty_reports.accepted at BETWEEN CAST("#{start_date}" AS DATE) AND CAST("#{end_date}" AS DATE)').count
RoyaltyReport.where('royalty_reports.accepted_at at BETWEEN CAST("?" AS DATE) AND CAST("?" AS DATE)', start_date, end_date).count
RoyaltyReport.where('royalty_reports.accepted at BETWEEN CAST(? AS DATE) AND CAST(? AS DATE)', start_date, end_date).count
end_date = Date.current.end_of_year.end_of_day
start_date = Date.current.beginning_of_year.beginning_of_day
RoyaltyReport.where('royalty_reports.accepted at BETWEEN CAST(? AS DATE) AND CAST(? AS DATE)', start_date, end_date).count
RoyaltyReport.where('royalty_reports.accepted_at BETWEEN CAST(? AS DATE) AND CAST(? AS DATE)', start_date, end_date).count
@royalty_reports = RoyaltyReport.where('royalty_reports.accepted_at BETWEEN CAST(? AS DATE) AND CAST(? AS DATE)', start_date, end_date).includes(:entity)
@royalty_reports.count
@entities = Entity.includes(:contracts, :contract_technologies).where('(contract_technologies.reporting_type_id IS NOT NULL AND contract_technologies.reporting_type_id != ?)', REPORTING_TYPE_NOT).contract_reporting_range(start_date, end_date)
(@entities - @royalty_reports).uniq.count
exit
RoyaltyReport.
where("(accepted_at >= :from AND accepted_at <= :to)", {
    from: from.to_s, to: to.to_s
}).
select(:entity_id).uniq(&:entity_id)
RoyaltyReport.
where("(accepted_at >= :from AND accepted_at <= :to)", {
    from: Date.current.beginning_of_year, to: Date.current.end_of_year
}).
select(:entity_id).uniq(&:entity_id)
RoyaltyReport.
where("(accepted_at >= :from AND accepted_at <= :to)", {
    from: Date.current.beginning_of_year, to: Date.current.end_of_year
}).uniq(&:entity_id)
})select(:entity_id).uniq(&:entity_id)
RoyaltyReport.
where("(accepted_at >= :from AND accepted_at <= :to)", {
    from: Date.current.beginning_of_year, to: Date.current.end_of_year
}).select(:entity_id).uniq(&:entity_id)
RoyaltyReport.
where("(accepted_at >= :from AND accepted_at <= :to)", {
    from: Date.current.beginning_of_year, to: Date.current.end_of_year
}).select(:entity_id).uniq
Entity.includes(:contracts, :contract_technologies).where('contract_technologies.reporting_type_id IS NOT NULL AND contract_technologies.reporting_type_id != ?', REPORTING_TYPE_NOT).contract_reporting_range(Date.current.beginning_of_year, Date.current.end_of_year)
Entity.includes(:contracts, :contract_technologies).where('contract_technologies.reporting_type_id IS NOT NULL AND contract_technologies.reporting_type_id != ?', REPORTING_TYPE_NOT).contract_reporting_range(Date.current.beginning_of_year, Date.current.end_of_year).to_sql
Entity.includes(:contracts, :contract_technologies).where('contract_technologies.reporting_type_id IS NOT NULL').where('contract_technologies.reporting_type_id != ?', REPORTING_TYPE_NOT)
Entity.includes(:contracts, :contract_technologies).where('contract_technologies.reporting_type_id IS NOT NULL').where('contract_technologies.reporting_type_id != ?', REPORTING_TYPE_NOT).to_sql
exit
Entity.find(387)
max = _
max.entity_type
max.entity_type_id
max.entity_type_ids
max.contract_technologies
max.contract_technologies.first
max.contract_technologies.first.contract
max.contract_technologies.first.contract.contract_type
Entity.contract_reporting_range(Date.current.beginning_of_year, Date.current.end_of_year)
Entity.contract_reporting_range(Date.current.beginning_of_year, Date.current.end_of_year).count
Entity.all.contract_reporting_range(Date.current.beginning_of_year, Date.current.end_of_year).count
Entity.includes(:contracts).contract_reporting_range(Date.current.beginning_of_year, Date.current.end_of_year).count
Entity.includes(:contracts).contract_reporting_range(Date.current.beginning_of_year, Date.current.end_of_year).pluck(:id)
Entity.includes(:contracts).contract_reporting_range(Date.current.beginning_of_year, Date.current.end_of_year).pluck(:id).include?(max.id)
max.royalty_report_ids.count
max.royalty_report_ids
max.royalty_reports
am = Entity.find(39)
am.contract_technologies
max.contract_technologies
max.contract_technologies.contracts
max.contracts
max.contract_technologies
max.contract_technologies.first
max.contract_technologies.first.entity
max.contract_technologies.first.reporting
max.contract_technologies.first.contract_technology_status
max.contract_technologies.first.contract_technology_status(Date.current + 3.months)
max.contract_technologies.first.current_prepay_balance
max.contract_technologies.first.contract.royalty_report_items
max.contract_technologies.first.contract.royalty_report_items.count
max.contract_technologies.first.contract.contract_type
am.contract_technologies
am.contract_technologies.first.contract.contract_type
exit
3.month
3.months
ReportingPeriodService.quarters_from_range(DateTime.now - 3.month, DateTime.now)
contract = ContractTechnology.reporting.first
RegionsCountries
Region.ids
start_date = Date.current.beginning_of_year
end_date = Date.current.end_of_year
all_region_ids = Region.ids
ReportingPeriodService.quarters_from_range(
  royalty_reports.map(&:start_date).min,
  royalty_reports.map(&:end_date).max
)
ContractTechnology.reporting
.joins(:entity)
.joins('JOIN regions_countries ON regions_countries.country_id = entities.country_id')
.where('regions_countries.region_id IN (?)', all_region_ids)
.where(contract_id: Contract.reporting_range(period.start_date, period.end_date).ids).uniq.pluck(:id)
all_region_ids
start_date
end_date
total_techs = ContractTechnology.reporting.joins(:entity).joins('JOIN regions_countries ON regions_countries.country_id = entities.country_id').where('regions_countries.region_id IN (?)', all_region_ids).where(contract_id: Contract.reporting_range(period.start_date, period.end_date).ids).uniq.pluck(:id)
RoyaltyReport.accessible_by(ability)
.where(accepted_at >= start_date, accepted_at <= end_date)
reports = RoyaltyReport.accessible_by(ability)
.where(accepted_at >= start_date, accepted_at <= end_date)
reports = RoyaltyReport.where(accepted_at >= start_date)
reports = RoyaltyReport.where('royalty_reports.accepted_at >= ? AND royalty_reports.accepted_at <= ?', start_date, end_date)
reports = RoyaltyReport.where('royalty_reports.accepted_at >= ? AND royalty_reports.accepted_at <= ?', start_date, end_date).includes(:entity, royalty_report_items: [:contract])
periods = RoyaltyPeriodService.quarters_from_range(reports.uniq.pluck(:start_date).min, reports.uniq.pluck(:end_date).min)
periods = ReportingPeriodService.quarters_from_range(reports.uniq.pluck(:start_date).min, reports.uniq.pluck(:end_date).min)
periods = ReportingPeriodService.quarters_from_range(reports.uniq.pluck(:start_date).min, reports.uniq.pluck(:end_date).max)
period = periods.first
total_techs = ContractTechnology.reporting.joins(:entity).joins('JOIN regions_countries ON regions_countries.country_id = entities.country_id').where('regions_countries.region_id IN (?)', all_region_ids).where(contract_id: Contract.reporting_range(period.start_date, period.end_date).ids).uniq.pluck(:id)
total_techs = ContractTechnology.reporting.joins(:entity).joins('JOIN regions_countries ON regions_countries.country_id = entities.country_id').where('regions_countries.region_id IN (?)', all_region_ids).where(contract_id: Contract.reporting_range(period.start_date, period.end_date).ids).uniq.pluck(:id).count
period = periods.last
total_techs = ContractTechnology.reporting.joins(:entity).joins('JOIN regions_countries ON regions_countries.country_id = entities.country_id').where('regions_countries.region_id IN (?)', all_region_ids).where(contract_id: Contract.reporting_range(period.start_date, period.end_date).ids).uniq.pluck(:id).count
total_techs = ContractTechnology.reporting.joins(:entity).joins('JOIN regions_countries ON regions_countries.country_id = entities.country_id').where('regions_countries.region_id IN (?)', all_region_ids).where(contract_id: Contract.reporting_range(period.start_date, period.end_date).ids).uniq.pluck(:id)
reported = ContractTechnology.reporting.joins(:entity).joins('JOIN regions_countries ON regions_countries.country_id = entities.country_id').where('regions_countries.region_id IN (?)', all_region_ids).where(id: total_techs)
reported = ContractTechnology.reporting.joins(:entity).joins('JOIN regions_countries ON regions_countries.country_id = entities.country_id').where('regions_countries.region_id IN (?)', all_region_ids).where(id: total_techs).count
reported = ContractTechnology.reporting.joins(:entity).joins('JOIN royalty_reports rr ON contract_technologies.entity_id = rr.entity_id').joins('JOIN royalty_report_items rri ON rri.royalty_report_id = rr.id AND rri.technology_id = contract_technologies.technology_id').joins('JOIN regions_countries ON regions_countries.country_id = entities.country_id').where('regions_countries.region_id IN (?)', all_region_ids).where(id: total_techs)
reported.count
total_reported = reported.uniq.pluck(:id)
period
period_reports = reports.select{|r|r.start_date >= period.start_date.to_date && r.end_date <= period.end_date.to_date}
period_repoted = reported.where('rr.id IN (?)', period_reports.uniq.pluck(:id)).uniq.pluck(:id)
period_reported = reported.where('rr.id IN (?)', period_reports.uniq.pluck(:id)).uniq.map(&:id)
period_reported = reported.where('rr.id IN (?)', period_reports.uniq.map(&:id)).uniq.map(&:id)
never_reported = reported
never_reported = reported.where('rr.id NOT IN (?)', period_reports.uniq.map(&:id)).uniq.map(&:id)
period_reported.count
never_reported.count
total_reported.count
periods
period = periods[3]
period_reports = reports.select{|r|r.start_date >= period.start_date.to_date && r.end_date <= period.end_date.to_date}
total_reported.count
total.count
total_techs.count
ContractTechnolog.count
ContractTechnology.count
cotract_tech_ids = ContractTechnology.pluck(:id)
contract_tech_count = contract_tech_ids.count
contract_tech_ids = ContractTechnology.pluck(:id)
contract_tech_count = contract_tech_ids.count
ContractTechnology.first
Technology.count
contract_tech_count = contract_tech_ids.count
RoyaltyReportItem.count
RoyaltyReportItem.first
ContractTechnology.reporting.count
ContractTechnology.count
ContractTechnology.includes(:entity)
ContractTechnology.includes(:entity).count
ContractTechnology.joins(:entity).count
ContractTechnology.reporting.joins(:entity).count
ContractTechnology.reporting.includes(:entity).count
ContractTechnology.reporting.includes(:entity).where(contract_id: Contract.reporting_range(Date.current.beginning_of_year - 2.years, Date.current.end_of_year).ids).count
ContractTechnology.reporting.includes(:entity).where(contract_id: Contract.reporting_range(Date.current.beginning_of_year - 2.years, Date.current.end_of_year).ids).uniq.count
Entity.first.contract_id
RoyaltyReport.first
ContractTechnology
ContractTechnology.first.contract
ContractTechnology
exit
RoyaltyReport.last
RoyaltyReport.last.amount
RoyaltyReport.last.amount.to_f
NumberFormatter.number_to_short_currency(RoyaltyReport.last.amount, 3)
exit
ContractType.count
Entity.joins(:contracts, :contract_technologies).to_sql
@region_ids = Region.ids
@region_ids.map {|r| '&region_ids=' r.to_s}.join()
@region_ids.map {|r| '&region_ids=' + r.to_s}.join()
Entity.businesses
Entity.businesses.size
Entity.includes(:contracts, :contract_technologies)
.where('contract_technologies.reporting_type_id IS NOT NULL')
.where('contract_technologies.reporting_type_id != ?', REPORTING_TYPE_NOT)
Entity.includes(:contracts, :contract_technologies).
where('contract_technologies.reporting_type_id IS NOT NULL').
where('contract_technologies.reporting_type_id != ?', REPORTING_TYPE_NOT).
.contract_reporting_range(Date.parse('2013-01-01'), Date.parse('2013-03-31')).uniq.pluck(:id)
Entity.includes(:contracts, :contract_technologies).
where('contract_technologies.reporting_type_id IS NOT NULL').
where('contract_technologies.reporting_type_id != ?', REPORTING_TYPE_NOT).
.contract_reporting_range(Date.new('2013-01-01'), Date.new('2013-03-31')).uniq.pluck(:id)
exit
Sidekiq::Cron::Job.find('PendingReportApprovals::CronWorker').destroy
ApprovalTypesMap = {
  'project approval' => 'pending',
  'final approval'   => 'pending_final_approval',
}
Project.find_by(name: '2014 - DELL INC. AUDIT')
p = _
p.cost_reports
p.cost_reports.where(approved_at: nil)
p.cost_reports.where(approved_at: nil).first
p.cost_reports.joins(:cost_report_items).to_sql
p.cost_reports.joins(:cost_report_items).where('cost_report_items.finally_approved_at is NULL')
p.cost_reports.joins(:cost_report_items).where('cost_report_items.finally_approved_at is NULL').size
CostReport.find(15)
cr = _
cr.cost_report_items
cr.created_by_user
ar = AccrualReport.last
ar.start_date
ar.end_date
cr.cost_report_items.first.invoiced_at
cr.invoiced_at
cr.invoiced_at - 1.hour
CostReport.invoiced.size
CostReportItem.joins(:cost_reports).where('cost_reports.invoiced_at is NOT NULL').size
CostReportItem.joins(:cost_report).where('cost_reports.invoiced_at is NOT NULL').size
CostReportItem.joins(:cost_report).where('cost_reports.invoiced_at is NOT NULL AND cost_report_item.finally_approved_at IS NULL').size
CostReportItem.joins(:cost_report).where('cost_reports.invoiced_at is NOT NULL AND cost_report_items.finally_approved_at IS NULL').size
CostReportItem.joins(:cost_report).where('cost_reports.invoiced_at is NOT NULL AND (cost_report_items.finally_approved_at IS NULL OR cost_report_items.approved_at is NULL)').size
CostReportItem.joins(:cost_report).where('cost_reports.invoiced_at is NOT NULL AND (cost_report_items.finally_approved_at IS NULL OR cost_report_items.approved_at is NULL)')
CostReportItem.joins(:cost_report).where('cost_reports.invoiced_at is NOT NULL AND (cost_report_items.finally_approved_at IS NULL OR cost_report_items.approved_at is NULL)').each {|cri| cri.update_attributes(approved_at: cri.cost_report.invoiced_at - 2.hours, finally_approved_at: cri.cost_report.invoiced_at - 1.hour)}
CostReport.find(14)
CostReport.find(14).cost_report_items.first
Sidekiq::Cron::Job.find('PendingReportApprovals::CronWorker').destroy
CostReportItem.includes(:cost_report).where('cost_reports.invoiced_at is NOT NULL AND (cost_report_items.finally_approved_at IS NULL OR cost_report_items.approved_at is NULL)').each {|cri| cri.update_attributes(approved_at: cri.cost_report.invoiced_at - 2.hours, finally_approved_at: cri.cost_report.invoiced_at - 1.hour)}
CostReport.find(14)
cr = _
cr.cost_report_items.first
cr.cost_report_items.update_all(finally_approved_at: cr.invoiced_at)
CostReport.invoiced.each do |cr|
  cr.cost_report_items.each do |cri|
    if cri.approved_at.nil? || cri.finally_approved_at.nil?
      cri.update_attributes(approved_at: cr.invoiced_at, finally_approved_at: cr.invoiced_at)
    end
  end
end
p = Project.find_by(name: '2014 - DELL INC. AUDIT')
p.cost_report_items
AccrualReport.archived
AccrualReport.size
AccrualReport.count
AccrualReport.all
CostReportItem.includes(:cost_report).where('cost_reports.invoiced_at is NOT NULL AND (cost_report_items.finally_approved_at IS NULL OR cost_report_items.approved_at is NULL)').size
CostReportItem.includes(:cost_report).where('cost_reports.invoiced_at is NOT NULL').where('(cost_report_items.finally_approved_at IS NULL OR cost_report_items.approved_at is NULL)').size
CostReportItem.includes(:cost_report).where('cost_reports.invoiced_at is NOT NULL').references(:cost_reports).where('(cost_report_items.finally_approved_at IS NULL OR cost_report_items.approved_at is NULL)').size
CostReportItem.includes(:cost_report).where('cost_reports.invoiced_at is NOT NULL').references(:cost_reports).where('(cost_report_items.finally_approved_at IS NULL OR cost_report_items.approved_at is NULL)')
CostReportItem.includes(:cost_report).where('cost_reports.invoiced_at is NOT NULL').references(:cost_reports).where('(cost_report_items.approved_at is NULL').size
CostReportItem.includes(:cost_report).where('cost_reports.invoiced_at is NOT NULL').references(:cost_reports).where('cost_report_items.approved_at is NULL').size
CostReportItem.includes(:cost_report).where('cost_reports.invoiced_at is NOT NULL').references(:cost_reports).where('cost_report_items.finally_approved_at is NULL').size
CostReportItem.includes(:cost_report).where('cost_reports.invoiced_at is NOT NULL').references(:cost_reports).where('cost_report_items.finally_approved_at is NULL').update_all(finally_approved_at: Date.current)
CostReportItem.includes(:cost_report).where('cost_report.invoiced_at is NOT NULL').references(:cost_reports).where('cost_report_items.finally_approved_at is NULL').update_all(finally_approved_at: Date.current)
CostReportItem.joins(:cost_report).where('cost_report.invoiced_at is NOT NULL').references(:cost_reports).where('cost_report_items.finally_approved_at is NULL').update_all(finally_approved_at: Date.current)
cri_ids = CostReport.invoiced.joins(:cost_reports_items).uniq.select('cost_report_items.id')
cri.ids
cri_ids
cri_ids.size
cri_ids = CostReport.invoiced.joins(:cost_report_items).uniq.select('cost_report_items.id')
cri_ids = CostReport.invoiced.joins(:cost_report_items).uniq.select('cost_report_items.id').map(:id)
cri_ids = CostReport.invoiced.joins(:cost_report_items).uniq.select('cost_report_items.id').map(&:id)
CostReport.find(892)
CostReportItem.find(892)
CostReport.find(7)
CostReportItems.where(id: cri_ids).update(finally_approved_at: Date.current)
CostReportItem.where(id: cri_ids).update(finally_approved_at: Date.current)
CostReportItem.where(id: cri_ids).update_all(finally_approved_at: Date.current)
Sidekiq::Cron::Job.find('PendingReportApprovals::CronWorker').destroy
Project.find_by(name: '2014 - DELL INC. AUDIT')
p = _
p.cost_reports
p.cost_reports.count
p.cost_reports.invoiced
p.cost_reports.invoiced.count
p.cost_reports.uniq.invoiced.count
p.cost_reports.uniq
p.cost_reports.uniq.count
p.cost_reports.map(&:id)
p.cost_reports.map(&:id).count
p.cost_reports.map(&:id).uniq.count
p.cost_reports.map(&:id).uniq
CostReport.where(id: _).map(&:invoiced_at)
Sidekiq::Cron::Job.find('PendingReportApprovals::CronWorker').destroy
CostReport.pending
CostReportItem.pending
CostReportItem.pending.joins(:"cost_report")
CostReportItem.pending.joins(:"cost_report").to_sql
DAYS = 2
CostReportItem.pending.joins(:"cost_report").where('mod(datediff(now(), cost_reports.accepted_at), ?) = 0', DAYS).size
report_items = _
cost_items = CostReportItem.pending.joins(:"cost_report").where('mod(datediff(now(), cost_reports.accepted_at), ?) = 0', DAYS)
pending_approval = Project.joins(cost_report_items: :cost_report).merge(cost_items)
pending_approval = Project.joins(cost_report_items: :cost_report).merge(cost_items).count
pending_approval = Project.joins(cost_report_items: :cost_report).merge(cost_items).to_sql
pending_approval = Project.joins(cost_report_items: :cost_report).merge(cost_items).group('projects.id', 'r_id').pluck('projects.id', "cost_reports.id as r_id")
CostReport.find(57)
cr = _
cr.amount
cr.amount.to_f
pending_approval
project_reports = Hash[CostReport.preload()
project_reports = Hash[CostReport.preload(nil).find(pending_approval
pending_approval.map(&:last)
project_reports = Hash[CostReport.preload(nil).find(pending_approval.map(&:last)).group_by { |pr| pr.id }.map {|id, p| [id, p.first] }]
projects = Hash[Project
  .preload(:project_mgr, project_users: :user).
projects = Hash[Project
  .preload(:project_mgr, project_users: :user)
projects = Hash[Project
projects = Hash[Project.
  preload(:project_mgr, project_users: :user).
  find(pending_approval.map(&:first)).
  group_by { |p| p.id }.
map { |id, p| [id, p.first] } ]
pending_approval
pending_approval.each_with_object({}) do |row, hash|
  puts pr_id
  puts p_id
end
AccrualReport.first
ar = _
ar.report_status_id
PendingReportApprovals::NotifyPending.perform_async('cost', 'project_approval')
Sidekiq::RetrySet.new.size
Sidekiq::RetrySet.new.clear
Sidekiq::RetrySet.new.size
PendingReportApprovals::NotifyPending.perform_async('cost', 'project approval')
User.where('name like "%Wang"')
User.where('name like "%Wang"').first
User.where('name like "%Wang"').first.confirmed?
reload!
PendingReportApprovals::NotifyPending.perform_async('cost', 'project approval')
reload!
CloudHelpers::each_cloud
CloudHelpers.call(:each_cloud)
PendingReportApprovals::CronWorker.perform
NotifyPending.perform_async('cost', 'project approval')
NotifyPending.perform_async('cost', 'final approval')
NotifyPending.perform_async('accrual', 'project approval')
NotifyPending.perform_async('accrual', 'final approval')
PendingReportApprovals::NotifyPending.perform_async('cost', 'final approval')
reload!
PendingReportApprovals::NotifyPending.perform_async('cost', 'final approval')
reload!
PendingReportApprovals::NotifyPending.perform_async('cost', 'final approval')
p = Project.find_by(name: 'System Audit - 2015 TP Vision')
p.cost_management_
p.cost_manager
p.project_users.select {|pu| pu.cost_manager}
p.project_users.select {|pu| pu.cost_manager}.map(&:user)
reload!
p = Project.find_by(name: 'System Audit - 2015 TP Vision')
PendingReportApprovals::NotifyPending.perform_async('cost', 'final approval')
p = Project.find_by(name: 'System Audit - 2015 TP Vision')
p.project_users.select {|pu| pu.cost_manager}.map(&:user)
p.project_users.select {|pu| pu.cost_manager}.map(&:user).tap do |mgs|
  if approval_type == 'project approval'
    mgs << project.project_mgr
  end
end
reload!
PendingReportApprovals::NotifyPending.perform_async('cost', 'final approval')
exit
PendingReportApprovals::NotifyPending.perform_async('accrual', 'final approval')
PendingReportApprovals::NotifyPending.perform_async('accrual', 'project approval')
CostReport.find(43)
CostReport.find(43).report_status
PendingReportApprovals::NotifyPending.perform_async('cost', 'project approval')
[1,2,2,3] - [1,2,3]
con = Contract.find(2671)
exit
con = Contract.find(2671)
con.reporting_effective_date
con.effective_date
con.contract_effective_date
con.primary_entity
con
con.entity_id
Contract.includes(:contract_technologies).where('(contract_technologies.reporting_type_id IS NOT null and
con
con.primary_entity
con.contract_technologies
RoyaltyReportItem.where(technology_id: 20)
RoyaltyReportItem.where(technology_id: 20).contract
RoyaltyReportItem.joins(:contact).where(technology_id: 20)
RoyaltyReportItem.joins(:contact).where(technology_id: 20).count
RoyaltyReportItem.joins(:contract).where(technology_id: 20).count
RoyaltyReportItem.joins(:contract).where(technology_id: 20).select('contracts.entities')
.count
RoyaltyReportItem.joins(:contract).where(technology_id: 20).select('contracts.entities')
RoyaltyReportItem.joins(:contract).where(technology_id: 20).select('contracts.entities').to_sql
exit
PendingReportApprovals::NotifyPending.perform_async('cost', 'final_approval')
PendingReportApprovals::NotifyPending.perform_async('cost', 'final approval')
Sidekiq::Retries.new.clear
Sidekiq::RetrySet.new.clear
exit
reload!
PendingReportApprovals::NotifyPending.perform_async('cost', 'final approval')
Incident.first
i = _
i.incident_type
i.incident_type.name
i
IncidentType
Cloud.current
exit
Incident.sum(:amount)
Incident.sum(:amount).to_f
Incident.all.pluck(:amount)
Incident.all
exit
Incident.all.pluck(:amount)
Incident.where.not(amount: nil).pluck(:amount)
Incident.where.not(amount: nil).pluck(:id)
Incident.find(117)
exit
Entity.where(id: [992, 39, 208, 716])
Entity.where(id: [992, 39, 208, 716]).pluck(:name)
Date.parse('2014-10-01')
ActiveSupport::TimeWithZone.new
Time.zone.now
time = _
time.day
time.month
time.year
time
exit
reported_ids = Entities.businesses.to_sql
reported_ids = Entity.businesses.to_sql
reporter_ids = Entity.businesses.includes(:contracts, :contract_technologies).where('(contract_technologies.reporting_type_id IS NOT null and contract_technologies.reporting_type_id != ?)', REPORTING_TYPE_NOT).contract_reporting_range(Time.current.beginning_of_fincancial_quarter, Time.current.end_of_fincancial_quarter).to_sql
reporter_ids = Entity.businesses.includes(:contracts, :contract_technologies).where('(contract_technologies.reporting_type_id IS NOT null and contract_technologies.reporting_type_id != ?)', REPORTING_TYPE_NOT).contract_reporting_range(Time.current.beginning_of_financial_quarter, Time.current.end_of_financial_quarter).to_sql
reload!
boundaries = [Time.current.beginning_of_financial_quarter, Time.current.end_of_financial_quarter]
Entity.royalty_reporting(boundaries)
reload!
Entity.royalty_reporting(boundaries)
reload!
Entity.royalty_reporting(boundaries)
reload!
Entity.royalty_reporting(boundaries)
Entity.royalty_reporting(boundaries).count
Entity.find_by(name: 'Maxlinear')
max = _
max.contracts.includes(:contract_technologies)
max.contracts.includes(:contract_technologies).where      .where('(contract_technologies.reporting_type_id IS NOT null and
                contract_technologies.reporting_type_id != ?)', REPORTING_TYPE_NOT)
max.contracts.includes(:contract_technologies).where('(contract_technologies.reporting_type_id IS NOT null and contract_technologies.reporting_type_id != ?)', REPORTING_TYPE_NOT)
max.contracts.includes(:contract_technologies).where('(contract_technologies.reporting_type_id IS NOT null and contract_technologies.reporting_type_id != ?)', REPORTING_TYPE_NOT).first.contract
max.contracts.includes(:contract_technologies).where('(contract_technologies.reporting_type_id IS NOT null and contract_technologies.reporting_type_id != ?)', REPORTING_TYPE_NOT).first.map {|c| c.id, c.reporting_effective_date, c.reporting_end_date }
c = max.contracts.includes(:contract_technologies).where('(contract_technologies.reporting_type_id IS NOT null and contract_technologies.reporting_type_id != ?)', REPORTING_TYPE_NOT).first
con = max.contracts.includes(:contract_technologies).where('(contract_technologies.reporting_type_id IS NOT null and contract_technologies.reporting_type_id != ?)', REPORTING_TYPE_NOT).first
[con.id, con.reporting_effective_date, con.reporting_end_date]
con
reload!
Entity.royalty_reporting(boundaries).count
Entity.royaly_reporting
Entity.royalty_reporting
reload!
Entity.royalty_reporting
Entity.royalty_reporting.count
reload!
Entity.royalty_reporting.count
reload!
Entity.royalty_reporting.count
Entity.royalty_reporting.to_sql
Region.ids
Region.ids << nil
Region.create(id: -1, name: 'No Country')
none = Region.find(-1)
none.country_ids = [nil]
Entity.find_by(name: 'Amlogic')
Entity.find_by(name: 'Amlogic Inc')
Entity.find_by(name: 'Amlogic Inc').country
none.map_label_offset_y = 100
none.map_label_offset_x = 100
none.destroy
Country.find(-1)
Country.create(id: -1, name: 'No Country')
Entity.where(country_id: nil)
Entity.where(country_id: nil).count
Entity.where(country_id: nil).update_all(country_id: -1)
Entity.where(country_id: nil).count
none = Country.find(-1)
none.region
none.region_id
none.countries
none.regions
none.regions.first
RegionCountries.count
RegionCountry.count
none.regions
none.region_countries
Region.find(5)
Entity.where(country_id: -1).update_all(country_id: nil)
exit
PendingReportApprovals::NotifyPending.perform_async('accrual', 'final approval')
reload!
PendingReportApprovals::NotifyPending.perform_async('accrual', 'final approval')
exit
PendingReportApprovals::NotifyPending.perform_async('accrual', 'final approval')
AccrualReport.where(id: [12,7]
)
AccrualReport.all.to_sql
AccrualReport.to_sql
exit
PendingReportApprovals::NotifyPending.perform_async('accrual', 'final approval')
PendingReportApprovals::NotifyPending.perform_async('accrual', 'pending')
PendingReportApprovals::NotifyPending.perform_async('accrual', 'project approval')
PendingReportApprovals::NotifyPending.perform_async('cost', 'project approval')
PendingReportApprovals::NotifyPending.perform_async('cost', 'final approval')
exit
a = [1,2,3,4]
b = [3,4,5,6]
a | b
a |= b
a
a = [1,2,3]
b = [2, 3, 4, 5]
a & b
c = _
a - c
d = [2,3]
a - d
a
a -= d
a
b
b -= d
a
b
d
start_date = Date.today.beginning_of_year
start_date = Date.current.beginning_of_year
end_date = Date.current.beginning_of_year
end_date = Date.current.end_of_year
from = Date.current.beginning_of_year
to = Date.current.end_of_year
reported_ids = []
unreported_ids = []
reporting_periods = ReportingPeriodService.quarters_from_range(from, to)
reporting_periods.each do |period|
  period_reporter_ids = Entity.businesses.royalty_reporting
  .contract_reporting_range(period.start_date, period.end_date).uniq.pluck(:id)
  period_reported_ids = RoyaltyReport.includes(:entity)
  .where("(accepted_at >= :from AND accepted_at <= :to)", {
      from: period.start_date, to: period.end_date
  }).uniq.pluck(:entity_id)
  period_unreported_ids = period_reporter_ids - period_reported_ids
  reported_ids |= period_reported_ids
  unreporetd_ids |= period_unreported_ids
end
reporting_periods.each do |period|
  period_reporter_ids = Entity.businesses.royalty_reporting.contract_reporting_range(period.start_date, period.end_date).uniq.pluck(:id)
  period_reported_ids = RoyaltyReport.includes(:entity).where("(accepted_at >= :from AND accepted_at <= :to)", {
      from: period.start_date, to: period.end_date
  }).uniq.pluck(:entity_id)
  period_unreported_ids = period_reporter_ids - period_reported_ids
  reported_ids |= period_reported_ids
  unreporetd_ids |= period_unreported_ids
end
reported_ids
unreported_ids
reporting_periods.each do |period|
  period_reporter_ids = Entity.businesses.royalty_reporting.contract_reporting_range(period.start_date, period.end_date).uniq.pluck(:id)
  period_reported_ids = RoyaltyReport.includes(:entity).where("(accepted_at >= :from AND accepted_at <= :to)", {
      from: period.start_date, to: period.end_date
  }).uniq.pluck(:entity_id)
  period_unreported_ids = period_reported_ids - period_reporter_ids
  reported_ids |= period_reported_ids
  unreporetd_ids |= period_unreported_ids
end
reported_ids
unreported_ids
reporting_periods.each do |period|
  period_reporter_ids = Entity.businesses.royalty_reporting.contract_reporting_range(period.start_date, period.end_date).uniq.pluck(:id)
  period_reported_ids = RoyaltyReport.includes(:entity).where("(accepted_at >= :from AND accepted_at <= :to)", {
      from: period.start_date, to: period.end_date
  }).uniq.pluck(:entity_id)
  period_unreported_ids = period_reporter_ids - period_reported_ids
  reported_ids |= period_reported_ids
  unreporetd_ids |= period_unreported_ids
end
reported_ids
unreported_ids
reporting_periods.each do |period|
  period_reporter_ids = Entity.businesses.royalty_reporting.contract_reporting_range(period.start_date, period.end_date).uniq.pluck(:id)
  period_reported_ids = RoyaltyReport.includes(:entity).where("(accepted_at >= :from AND accepted_at <= :to)", {
      from: period.start_date, to: period.end_date
  }).uniq.pluck(:entity_id)
  period_unreported_ids = period_reporter_ids - period_reported_ids
  reported_ids |= period_reported_ids
  unreported_ids |= period_unreported_ids
end
reported_ids
unreported_ids
missing_ids = reported_ids & unreported_ids
reported_ids -= missing_ids
unreported_ids -= missing_ids
exit
Entity.find(716)
Entity.find(716).country
exit
RoyaltyReport
RoyaltyReport.minimum(:start_date)
RoyaltyReport.maximum(:start_date)
RoyaltyReport.maximum(:start_date).type
RoyaltyReport.maximum(:start_date)
start = _
start.type
start.class
exit
Contract.all.pluck('distinct technology_id')
Contract.joins(:technologies)
Contract.joins(:technologies).select('distinct technologies.id')
Contract.joins(:technologies).select('distinct technologies.id').count
Entity.royalty_reporting.joins(contracts: :technologies).select('distinct technologies.id').count
RoyaltyReport.includes(:entity)
RoyaltyReport.merge(Entity.businesses.royalty_reporting)
RoyaltyReport.all.merge(Entity.businesses.royalty_reporting)
RoyaltyReport.all.merge(Entity.businesses.royalty_reporting).count
RoyaltyReport.include(Entity.businesses.royalty_reporting).count
RoyaltyReport.includes(Entity.businesses.royalty_reporting).count
exit
ent = FactoryGirl.create(:entity, :with_contract)
ent
reload!
ent = FactoryGirl.create(:entity, :with_contract)
ent
ent = FactoryGirl.create(:entity, :with_contract)
Entity.last
ent = _
ent.contracts
exit
ent = FactoryGirl.create(:entity, :with_contract)
ent
ent.contracts
RoyaltyReport.first
RoyaltyReport.first.start_date
RoyaltyReport.first.end_date
RoyaltyReport.first.accepted_at
exit
AccrualReportItem.first
ari = AccrualReportItem.first
ari = AccrualReportItem
ari = AccrualReportItem.first
ari.project
ari.project.entity
ari.project.provider_entity
ari.project.project_mgr
ari.project.regions
ari.project
ari.project.regions
ari.project.regions.to_sql
Project.all.select {|p| p.regions.count == 0}
Project.find(44)
Project.find(44).regions.map(&:name).join(', ')
AccrualReport.count
AccrualReport.last
ar = _
ar.accrual_period
ar.accrual_period.name
ar.accrual_period.formatted_start_date
ar.start_date
ar.start_date.class
ar.start_date.to_date
ar.start_date.to_date.strftime('%b %-d, %Y')
AccrualPeriod.all.map { |p| Hash[p.id, p.long_name] }
exit
AccrualPeriod.all.map { |p| Hash[p.id, p.long_name] }
AccrualPeriod.all.map { |p| Hash[[p.id, p.long_name]] }
fields = [:id, :long_name
  []
fields = [:id, :long_name]
AccrualPeriod.pluck(*fields).map { |values| Hash[fields.zip(values)] }.unshift Hash[fields.zip([0, "All"])]
fields = [:id, :due_date]
AccrualPeriod.pluck(*fields).map { |values| Hash[fields.zip(values)] }.unshift Hash[fields.zip([0, "All"])]
fields = [:id, :deadline]
AccrualPeriod.pluck(*fields).map { |values| Hash[fields.zip(values)] }.unshift Hash[fields.zip([0, "All"])]
AccrualPeriod.all.map { |p| Hash[p.id, p.long_name] }
AccrualPeriod.all.map { |p| Hash[id: p.id, period: p.long_name] }
AccrualPeriod.order(:deadline).map { |p| Hash[id: p.id, period: p.long_name] }
Project.last(10)
Project.last(10).map(&:project_num)
[4,8,4,6,8].inject(:+)
30 / 2
exit
exi
exit
TargetList
Entity
TargetList.fields
TargetList
y TargetList
TargetList.first
tl = _
tl.list_type
tl.target_list_type
tl
tl.user
tl
tl.status
reload!
tl = TargetList.first
TargetList::Entity
TargetList::Entity.class
tl.targets
tl.targets_total
tl.startdate
tl.conversion_rate.class
tl.startdate.class
tl = TargetList.find 17
tl.target_list_items.where(target: true, converted: true).count.to_i
tl.target_list_items.where(target: true, converted: true).count
converted = tl.target_list_items.where(target: true, converted: true).count
conv_rate = (converted.to_f/tl.targets_total.to_f)
converted.to_f
targets_total.to_f
tl.targets_total.to_f
0 / 0
tl.targets_total
tl.targets_total.empty
tl.targets_total.empty?
tl.targets_total.zero?
rr = RoyaltyReport.find(71)
rr.invoice_num
rr.try(:invoice_num).present?
rr.invoice_num = nil
rr.save
nil.present?
rr.invoice_num = 12345
rr.save
exit
ar = AccrualReport.last
ar.projects
reload!
user = User.where('name like "Cole%"')
user = User.where('name like "Cole%"').first
Notifications::ProjectReportsHelper.user_projects(ar, user)
reload
reload!
user = User.where('name like "Cole%"').first
ar = AccrualReport.last
Notifications::ProjectReportsHelper.user_projects(ar, user)
reload!
ar = AccrualReport.last
user = User.where('name like "Cole%"').first
Notifications::ProjectReportsHelper.user_projects(ar, user)
reload!
ar = AccrualReport.last
user = User.where('name like "Cole%"').first
Notifications::ProjectReportsHelper.user_projects(ar, user)
user = User.where('name like "Drummond%"').first
Notifications::ProjectReportsHelper.user_projects(ar, user)
reload!
user = User.where('name like "Drummond%"').first
ar = AccrualReport.last
Notifications::ProjectReportsHelper.user_projects(ar, user)
reload!
user = User.where('name like "Drummond%"').first
ar = AccrualReport.last
Notifications::ProjectReportsHelper.user_projects(ar, user)
AccrualReport.last(5)
AccrualReport.last(6)
AccrualReport.last(6).destroy_all
AccrualReport.last(6).each(&:destroy)
exit
proj = Projection.find(6)
p = proj
p.region_projections
p.region_projections.map(&:amount)
p.region_projections.map(&:amount.to_f)
p.region_projections.pluck(:amount)
p.region_projections.pluck(:amount).map(&:to_f)
tl = TargetList.find(12)
tl.entities
tl.entity_ids
tl.target_list_items
tl.entities
reload!
tl = TargetList.find(12)
tl.entities
reload!
tl = TargetList.find(12)
tl.entities
tl
type = TargetListType.find(1)
TargetList.where(target_list_type: type)
tl
tl.approval_name
tl.approval
Approval
Approval.all
tl.status
Status.all
tl
tl.licensees_total
tl.entities_total
tl.licensees_total
tl.nonlicensees_total
tl.others_total
tl.nonlicensees_total
AccrualPeriod.where('end_date < ?', Date.current)
AccrualPeriod.where('end_date < ?', Date.current).count
AccrualPeriod.where('end_date < ?', Date.current).each {|p| p.lock_date = p.end_date }
AccrualPeriod.where('end_date < ?', Date.current).each {|p| p.save }
AccrualPeriod.where('end_date < ?', Date.current).each {|p| p.update_attributes(lock_date: p.end_date) }
reload!
LateProvidersNotificationWorker.new.perform(environment: Rails.env)
exit
LateProvidersNotificationWorker.new.perform(environment: Rails.env)
AccrualPeriod.first
AccrualPeriod.first.update_attributes(lock_date: nil)
LateProvidersNotificationWorker.new.perform(environment: Rails.env)
reload!
LateProvidersNotificationWorker.new.perform(environment: Rails.env)
exit
tl
tl.enddate
exit
TargetList
Approval
ApprovalType
Country
TargetList
TargetListType
TargetListType.all
Product
AccrualReport.last
exit
Product
exit
TargetList.first
_.url.class
TargetList.where('name like "Tradeshow%"')
User.find(3)
TargetList.last
TargetListType
Entity.where('name like "Global%')
Entity.where('name like "Global%"')
Entity.where('name like "Global%"').last
g = _
g.products
g.products.count
exit
Entity.find 387
e = _
e.products
e.products.count
p = e.products.first
p.tecnology
p.technologies
e.products.flat_map(&:technologies)
e.products.flat_map(&:technologies).uniq(&:id)
e.products.map(&:technologies).uniq(&:id)
e.products.flat_map(&:technologies).uniq(&:id)
exit
Product.where(entity_id: 387).where.not(royalty_paid_by_id: nil)
Product
Product.where(entity_id: 387).where.not(royalty_paid_by_entity_id: nil)
Product.where(entity_id: 387).where.not(royalty_paid_by_entity_id: nil).count
Product.where(entity_id: 387).where.not(royalty_paid_by_entity_id: nil).flat_map(&:technologies).uniq(&:id)
exit
LateProvidersNotificationWorker.new(Rails.env)
LateProvidersNotificationWorker.new.perform(environment: Rails.env)
late = LateProvidersFinder.late_accrual_reports_providers
reload!
late = LateProvidersFinder.late_accrual_reports_providers
Project.first
late
late.first
late.first['notifications'][0]['projects']
late.first['notifications']['projects']
late.first['notifications'][0]['projects']
late.first
late.first.class
late.first.[1]
late.first[1]
late.first[1]['notifications']
late.first[1]['notifications'][0]['projects']
late.first[1]['notifications'][0]['projects'].map(&:id)
late.first[1]['notifications'][0]['projects']
late.first[1]['notifications'][0]['projects'].flat_map(&:id)
late.first[1]['notifications'][0]['projects'].map(&:id)
late.first[1]['notifications'][0]['projects'].map {|h| h['id'}
late.first[1]['notifications'][0]['projects'].map {|h| h['id']}
project_ids = _
Project.find(project_ids)
Project.find(project_ids).pluck(:project_mgr_id, financial_mgr_id)
Project.find(project_ids).pluck(:project_mgr_id, :financial_mgr_id)
Project.where(id: project_ids).pluck(:project_mgr_id, :financial_mgr_id)
Project.where(id: project_ids).pluck(:project_mgr_id, :financial_mgr_id).flatten
reload!
LateProvidersNotificationWorker.new.perform(environment: Rails.env)
reload!
LateProvidersNotificationWorker.new.perform(environment: Rails.env)
reload!
LateProvidersNotificationWorker.new.perform(environment: Rails.env)
late = LateProvidersFinder.late_accrual_reports_providers
exit
late = LateProvidersFinder.late_accrual_reports_providers
reload!
late = LateProvidersFinder.late_accrual_reports_providers
User.merge(Project.where(id: [4,6,9]))
User.internal.merge(Project.where(id: [4,6,9]))
User.find(Project.where(id: [4,6,9]).pluck(:project_mgr, :financial_mgr).flatten)
User.find(Project.where(id: [4,6,9]).pluck(:project_mgr_id, :financial_mgr_id).flatten)
TargetList.last
exit
TargetList.last
TargetList.last.country
TargetList.last.target_list_type
Tradeshow.last
TargetList.last
TargetList.all
TargetList
TargetList.all
exit
TargetList
TargetList.count
TargetList.last
exit
Indient
Incident
i = Incident.last
i = Incident.first
exit
Incident.count
i = Incident.last
i.category_ids
tl = TargetList.last
i
TargetList.preload(:user).map(&:user)
TargetList.preload(:user).map(&:user).compact
TargetList.preload(:user).map(&:user).compact.count
TargetList.preload(:user).map(&:user).count
TargetList.create(name: 'TestList123', startdate: Date.current)
tl = TargetList.create(name: 'TestList123', startdate: Date.current)
tl.errors
tl = TargetList.create(target_list_type_id: 1, name: 'TestList123', startdate: Date.current)
TargetList.preload(:user).map(&:user).count
TargetList.preload(:user).map(&:user).uniq
TargetList.preload(:user).compact.map(&:user).uniq
TargetList.preload(:user).compact.uniq.count
TargetList.preload(:user).compact.uniq.map(&:user_id)
TargetList.preload(:user).compact.uniq.map(&:user_id).uniq
TargetList.where(user_id: nil)
TargetList.where.not(user_id: nil).uniq
TargetList.where.not(user_id: nil).map(&:user).uniq
TargetList.where.not(user_id: nil).map(&:user).uniq.count
TargetList.joins(:user).where.not(user_id: nil).map(&:user).uniq.count
TargetList.includes(:user).where.not(user_id: nil).map(&:user).uniq.count
users = Incident.preload(:assigned_to_user).map(&:assigned_to_user).compact.uniq.sort_by(&:name)
users = Incident.preload(:assigned_to_user).map(&:assigned_to_user).compact.uniq.sort_by(&:name).count
users = Incident.preload(:assigned_to_user).map(&:assigned_to_user).uniq.sort_by(&:name).count
TargetList.includes(:user).where.not(user_id: nil).uniq(:user)
TargetList.includes(:user).where.not(user_id: nil).uniq(:user).count
TargetList.includes(:user).where.not(user_id: nil).uniq(:user_id).count
TargetList.includes(:user).where.not(user_id: nil).uniq(:user_id)
TargetList.includes(:user).where.not(user_id: nil).map(&:user)
TargetList.includes(:user).where.not(user_id: nil).map(&:user).uniq
TargetList.includes(:user).map(&:user).compact.uniq
tl
tl.country_id
Region.find(1)
Region.where(id: 1).flat_map(&:country_ids)
[tl
tl
tl = TargetList.find(3)
tl.nonlicensees_total
reload!
TargetListTable.new
TargetListsTable.new
TargetListsTable.new.page(1).per(20).range('2015-01-01', '2015-12-31').list_type(1)
TargetListsTable.new.page(1).per(20).range('2015-01-01', '2015-12-31').list_type(1).order(nil)
reload!
current_user
user = User.find_by(email: 'manager@ruvixx.com')
TargetListsTable.new(user).page(1).per(20).range('2015-01-01', '2015-12-31').list_type([1]).order(nil).present_with(TargetList::Extended)
TargetListsTable.new(user).page(1).per(20).range('2015-01-01', '2015-12-31').list_type([1]).order(nil).present_with(TargetList::Extended).data
tl.country
tl
TargetList.select('target_list.*, count(target_list_items.target = true) AS targets').order('targets DESC')
TargetList.select('target_lists.*, count(target_list_items.target = true) AS targets').order('targets DESC')
TargetListItem
TargetList.select('target_lists.*, count(target_list_item.target = true) AS targets').order('targets DESC')
TargetList.select('target_lists.*, count(target_list_items.ta[Crget = true) AS targets').order('targets DESC')
TargetList.select('target_lists.*, count(target_list_items.target = true) AS targets').order('targets DESC')
TargetList.select('target_lists.*, count(target_list_items.target = true) AS targets').joins(:target_list_items).order('targets DESC')
TargetList.select('target_lists.*, count(target_list_items.target = true) AS targets').joins(:target_list_items).order('targets DESC').count
TargetList.select('target_lists.*, count(target_list_items.target = true) AS targets').joins(:target_list_items).order('targets DESC')
TargetListItem.count(conditions: ["target = true"])
TargetListItem.count(conditions: ["target = false"])
TargetListItem.count(conditions: {target: true})
TargetListItem.count(conditions: {target: true, target_list_id: 1})
exit
tl = TargetList.first
tl.targets
tl.targets_total
tl.target_list_items.count(target: true)
TargetListItem.count(target_list_id: tl.id, target: true)
TargetListItem.count(conditions: {target_list_id: tl.id, target: true})
exit
tl = TargetList.first
TargetList.all.each do |tl|
  tl.targets_count = TargetListItem.count(conditions: {target_list_id: tl.id, target: true})
  tl.save!
end
tl
tl.targets_count
tl.targets_count = TargetListItem.count(conditions: {target_list_id: tl.id, target: true})
tl.targets_count
tl.save!
tl.reload!
reload!
tl.reload
tl
reload!
TargetList.each
TargetList.all.each do |tl|
  tl.targets_count = TargetListItem.count(conditions: {target_list_id: tl.id, target: true})
  tl.save!
end
tl.targets_count
tl.target_list_items.where('target_list_items.target = true')
tl.target_list_items.where('target_list_items.target = true').first.update_attributes(target: false)
tl.targets_count
tl.reload
tl.targets_count
reload
reload!
tl = TargetList.first
TargetList.reset_column_information
tl.reload
tl
exit
gst
tl = TargetList.first
TargetList
reload!
TargetList
exit
tl = TargetList.first
TargetList.reset_column_information
tl.reload!
tl.reload
TargetList
exit
RvxSignalGenerator
rake db:seed
TargetListItems.count(target: true)
TargetListItem.count(target: true)
TargetListItem.count(conditions: {target: true})
TargetListItem.targeted.count
reload!
TargetListItem.targeted.count
TargetListItem.targeted.first.entity
e = _
e.target_list_items.targeted.count
e.target_list_items.targeted
e.target_list_items.targeted.count
TargetListItem.targeted.last.entity
e = _
e.target_list_items.targeted.count
e.target_list_items.converted.count
reload!
e.target_list_items.converted.count
exit
TargetList.each(&:name)
TargetList.each(&:country_name)
TargetList.all(&:country_name)
TargetList.all.each(&:country_name)
TargetList.all(&:update_counters!)
TargetList.all.each {|tl| tl.update_counters}
TargetList.all.each {|tl| tl.update_counters!}
reload!
TargetList.all.each {|tl| tl.update_counters!}
exit
tl = TargetList.first
tl.target_counts
tl.targets_count
tl.status
tl.status.class
tl
exit
TargetList.find(16)
docs = {is_array: true)
docs = {is_array: true}
docs.key?(:is_array)
docs = {is_array: false}
docs.key?(:is_array)
tl = TargetList.find(1)
tl.target_entities.count
tl.entities
tl.entities.count
query = tl.target_entities.joins(:country).select('countries.name').group('countries.name')
nonlic = query.where('entities.nonlicensee_cache > 0 AND entities.country_id IS NOT NULL')
targetcounts = query.count
query
query.count
tl.target_entities.where(country_id: nil)
tl.target_entities.where(country_id: nil).count
tl.entities.where(country_id: nil).count
reload!
tl = TargetListItem.last
tl = TargetListItem.first
tl.target_list_items
TargetListItem
tl
tl = TargetList.first
tl.target_list_items
tl.target_list_items.first
tli = _
tli.id
tli.user
tli.entity
tli
tli.entity
reload!
'FUZHOU ROCKCHIP ELECTRONICS CO'.length
'ATLAS SOUND (MITEK COMMUNICATI'.length
Entity.where('name like "ATLAS SOUND%"')
e = _.first
e.name
tli = TargetListItem.find(948)
tli.entity
tli.entity.name
TargetList
exit
Product.reportable(992)
Product.reportable(992).count
ReportingByProductTable.new().page(1).per(10).entity_id(992)
ReportingByProductTable.new().page(1).per(10).entity_id(992).query
ReportingByProductTable.new().page(1).per(10).entity_id(992)
Product.joins <<-SQL
ps = Product.joins <<-SQL
      left outer join royalty_report_items on products.id=royalty_report_items.product_id
      left outer join royalty_reports on royalty_reports.id=royalty_report_items.royalty_report_id
    SQL
ps.count
ps.reportable(992)
ps.reportable(992).count
ps.reportable(992).range('2014-02-24', '2015-11-23')
ReportingByProductTable.new().page(1).per(10).entity_id(992)
ReportingByProductTable.new().page(1).per(10).entity_id(992).@query
ReportingByProductTable.new().page(1).per(10).entity_id(992).query
reload!
ReportingByProductTable.new().page(1).per(10).entity_id(992).query
ReportingByProductTable.new().page(1).per(10).entity_id(992).query.count
ReportingByProductTable.new().entity_id(992).query.count
ReportingByProductTable.new().entity_id(992).range('2014-02-23', '2015-110=-23').query.count
ReportingByProductTable.new().entity_id(992).range('2014-02-23', '2015-110=-23').query
ReportingByProductTable.new().entity_id(992).range('2014-02-23', '2015-110=-23').query.count
ReportingByProductTable.new().entity_id(992).range('2014-02-23', '2015-110=-23').query.size
ReportingByProductTable.new().entity_id(992).range('2014-02-23', '2015-110=-23').query
ReportingByProductTable.new().entity_id(992).range('2014-02-23', '2015-110=-23')
ReportingByProductTable.new().entity_id(992).range('2014-02-23', '2015-110=-23').query
ReportingByProductTable.new().entity_id(992).range('2014-02-23', '2015-110=-23').query.length
ReportingByProductTable.new().entity_id(992).query.length
TargetListItem
SimpleType
Approval
Priority
TargetListItem
ProductCategory
String
Text
TargetListItem
reload!
exit
TargetListItem
reload
reload!
tl = TargetList.find(16)
tli = tl.target_list_items.first
tli.product_category
tli.product_categories
tli.product_category_id
Entity
tli
tli.products
tli.target_technologies
tli.targeted_technologies
TargetListItemTargetedTechnologies.first
TargetListItemTargetedTechnology.first
TargetListItem
TargetListItem.per(10)
TargetListItem.first
Entity.find(67983)
reload!
tl = TargetList.find(12)
tl.target_list_item.first
tl.target_list_items.first
tl.target_list_items.first.user
tl.target_list_items.first
tl.target_list_items.first.entity
tl.target_list_items.second.entity
tl.target_list_items.second.user
tl.target_list_items.take(5).last
tl.target_list_items.take(5).last.entity
tl.target_list_items.take(5).last.entity.user
tl.target_list_items.take(5).last.user
TargetList.where('name "CES 2014"')
TargetList.where('name "CES 2014"').first
TargetList.find(12)
tl = _
tl.contacts
tl.addresses
tl
tl.address
tl.addresses
tl.address
tl.contacts
tl.addresses
tl
tl = TargetList.find(12)
tl.addresses
tl.addresses.last.destroy
tl.addresses.first.update_attributes(street1: '123 Imaginary Rd.', city: 'San Diego', state: 'CA', postal_code: '92122', country_id: Country.where('name like "United States%"').first.id)
tl.adddresses
tl.addresses
tl.addresses.last
tl.addresses.last.destroy
tl.addresses
tl.reload
tl.addresses
tl
tl.contacts
tl.contacts.each {|ct| ct.update_attributes(name: Faker::Name.name, title: Faker::Name.title, email: Faker::Internet.email, phone: '555-555-5555')
tl.contacts.each {|ct| ct.update_attributes(name: Faker::Name.name, title: Faker::Name.title, email: Faker::Internet.email, phone: '555-555-5555') }
tl.reload
tl.contacts
tl.addresses
add = _.first
add.country
add.country.iso3_code
Country.all.map(&:iso3_code)
gst
exit
TargetList.all.select {|tl| tl.contacts.count > 0 }
tl = _.first
tl.contacts
tl.contacts.first
tl
tl = TargetList.find(16)
tli = TargetListItem.first
tli.nonlicensees
tli.nonlicensee_count
tli.licensee_count
TargetListItemsTable.new.target_list_id(16).page(1).per(10).present_with(TargetListItem::Extended)
TargetListItemsTable.new(User.find(3)).target_list_id(16).page(1).per(10).present_with(TargetListItem::Extended)
reload!
TargetListItemsTable.new(User.find(3)).target_list_id(16).page(1).per(10).present_with(TargetListItem::Extended)
exit
TargetListItem
TargetList
tli = TargetListItem.find(933)
tli.files
tli.attachments
tli
tli.attachment_ids
reload!
tli = TargetListItem.find(933)
tli.attachment_ids
tli.entity.attachments
TargetListItem.all.select {|tli| tli.attachments.count > 0 }
tli = TargetListItem.find(170)
tli.files
tli.attachments
reload!
tli.attachments.first
tli.attachments.first.entity_id
tli.attachments.first.attachable
gst
tli
tli.notices
tli.notice_status
tli.notice_statuses
TargetListItem.all.select {|tli| !tli.notice_status.nil? }
TargetListItem.find(796)
tli = _
tli.targeted_technologies
tli.entity.contract_technologies
tli.entity.contract_technologies.map &:name
tli.entity.technologies_of_contracts
tli.entity.technologies
Entity.first
e = Entity.first
e.technologies
e.technologies_of_contracts
tli.entity.technologies
e.technologies_of_contracts
tli.entity.technologies
tli
tli.approval
tli.products
TargetListItem.all.select {|tli| tli.id if tli.products.count > 0 }
prods = _
prods.count
prods.map &:id
tli
tli = TargetListItem.find(794_
tli = TargetListItem.find(794)
tli.about_us
tli.update_attributes(about_us: Faker::Hipster.sentences(10))
tli.update_attributes(about_us: Faker::Lorem.sentences(10))
tli.about_us
tli.about_us.class
tli.update_attributes(about_us tli.about_us.join(' '))
tli.update_attributes(about_us: tli.about_us.join(' '))
tli.about_us
tli
tli.booth
tli.targeted_technologies
TargetListItem.joins(:technologies)
TargetListItem.joins(:technologies).select('technologies.name')
TargetListItem.joins(:technologies).select('technologies.name').count
TargetListItem.joins(:technologies).to_sql
TargetListItem.includes(:technologies).to_sql
tli.technologies
tli
tli.targeted_technologies
tli.entities.technologies
tli.entity.technologies
TargetListItem.joins(:entities)
TargetListItem.joins(:entities).to_sql
TargetListItem.joins(:entity).to_sql
TargetListItem.joins(:entity).joins(:technologies)
TargetListItem.joins(:entity).joins(:technologies).to_sql
TargetListItem.joins(entity: :technologies)
TargetListItem.joins(entity: :technologies).where(id: 794)
TargetListItem.joins(entity: :technologies).where(id: 794).select('technologies.name')
TargetListItem.joins(entity: :technologies).where(id: 794).select('technologies.name').references(:technologies)
TargetListItem.joins(entity: {:technologies}).where(id: 794)
TargetListItem.joins(entity: :technologies).where(id: 794)
TargetListItem.joins(entity: :technologies).where(id: 794).count
TargetListItem.find(794).entity.technologies
TargetListItem.find(794)
TargetListItem.includes(entity: :technologies).where(id: 794)
tli
tli.products
tli.products.map &:technologies
tli.reminders
tli.notes
tli
tli = TargetListItem.find(796)
tli.target_technologies
tli.targeted_technologies
tli.targeted_technologies << Technology.all.first
tli.follow_ups
tli.reminders
Reminder.all
Reminder.all.first
tli.reminders
Reminder
tli
tli.reminders
Message
tli.notice_status
tli.messages
TargetListItem.all.select {|tli| tli.messages.size > 0 }
tli
tli.notice_status
TargetListItem.all.select {|tli| tli.notice_status }
TargetListItem.all.select {|tli| !tli.notice_status.nil? }
tli.messages.first
tli
tli.reload
tli.messages
Message
tli.reload
tli.messages
reload!
exit
tli = TargetListItem.find(796)
tli.messages
tli.messages.first.attachment
tli.messages.first.attachment.url
Approvals
Approval
Approval.all
exit
Priority.all
tli = TargetListItem.find(794)
tli.priority
tli.priority = nil
tli.save
tli.priority = Priority.first
tli.savwe
tli.save
tli
tli.products
tli.products.first
tli.products.first.technologies
tli.products.first.technologies << [Technology.find(1), Technology.find(2)
]
Technology.all
Technology.second
Technology.all.second
tli.products.first.technologies << [Technology.all.first, Technology.all.second]
tli.products.last.technologies << [Technology.all.second, Technology.all.third]
tli.reload
tli.products
gst
exit
TargetListItem.all.select {|tli| tli.products.size > 0 }
tli = _.first
tli.products
tli.targeted_technologies
tli
tli.targeted_technologies << Technology.all.first
ents = Entity.businesses.include(:addresses)
ents = Entity.businesses.includes(:addresses)
ents = Entity.businesses.includes(:addresses, :alternate_names)
ents = Entity.businesses.includes(:addresses, :alternate_names, :account_managers)
ents = Entity.businesses.includes(:addresses, :alternate_names, :account_managers, :contacts)
ents = Entity.businesses.includes(:addresses, :alternate_names, :account_managers, :contacts, :countries)
ents = Entity.businesses.includes(:addresses, :alternate_names, :account_managers, :contacts, :countries).first
ents = Entity.businesses.includes(:addresses, :alternate_names, :account_managers, :contacts, :country).first
Product
Product.technologies
Product.first.technologies
tli
tli.products
tli.products.first
tli.products.first.product_category
tli.targeted_technologies
tli = TargetListItem.find(1069)
tli.targeted_technologies
tli = TargetListItem.find 1071
tli.targeted_technologies
tli.converted
tli.targeted
tli.target
tli
tli.redload
tli.reload
tli
tli.reload
tli
nil.empty?
nil.blank?
''.blank
''.blank?
[].blank?
''.spit(','
''.spit(',')
'44,45'.spit(',')
'44,45'.split(',')
''.split(',')
Technology.where(id: [])
tli
tli = TargetListItem.find(1072)
tli
tli.targeted
tli.target
TargetListItem
tli.first
TargetList.first.target_list_items.first.entity
ent = _
ent.target_list_items
ent.target_list_items.count
target_list_item = TargetListItem.includes(entity: :technologies).find(params[:id])
tli)
tli
target_list_item = TargetListItem.includes(entity: :technologies).find(68081)
target_list_item = TargetListItem.includes(entity: :technologies).find(1072)
target_list_item = TargetListItem.includes(entity: :technologies).find(1072).entity_technologies
Product.last
p = _
p.technologies
p
techs = Technology.all.first(2)
techs.count
p
p << techs
p.technologies << techs
p
p.technologies
p
Technology.find(31)
Product.last(3)
exit
TargetListItem.find(798)
tli = _
p = tli.products.first
p.technologies
techs = _.drop(1)
techs
p.technologies = techs
p.technologies
p.technologies = []
p.technologies
phash = Hashie::Mash.new
phash.class
UnknownProduct.last
UnknownProduct.update_attributes({technologies: techs})
UnknownProduct.last
UnknownProduct.last.technologies
UnknownProduct.last.update_attributes({technologies: techs})
UnknownProduct.last.technologies
phash = {brand: 'unknown brand x', technologies: techs, entity: Entity.find(65545), product_category: ProductCategory.find(25)}
phash
phash = Hashie::Mash.new {brand: 'unknown brand x', technologies: techs, entity: Entity.find(65545), product_category: ProductCategory.find(25)}
phash = Hashie::Mash.new(brand: 'unknown brand x', technologies: techs, entity: Entity.find(65545), product_category: ProductCategory.find(25))
::UnknownProduct.new(phash)
up = _
p = ::UnknownProduct.new(phash)
p.valid?
p.save
p.technologies
reload!
UnknownProduct.last
UnknownProduct.last.target_list_item
UnknownProduct.last.target_list_items
tli = TargetListItem.find(798)
tli.unknown_products
tli
tli = TargetListItem.find 798
tli.targeted_technologies
tli.entity.technologies
TargetListItem.includes(:targeted_technologies)
TargetListItem.includes(:targeted_technologies).to_sql
TargetListItem.includes(:targeted_technologies).find(798).to_sql
TargetListItem.includes(:targeted_technologies).find(798)
tli
tli.entity_technologies
TargetList.tradeshow.count
TargetList.tradeshows.count
TargetList.count
e = TargetListItem.find(933).entity
e.technologies
exit
tli = TargetListItem.find(326)
tli.entity
exit
tli = TargetListItem.find 963
tli.entity
tli.entity.target_list_items
tli.entity.target_list_items.count
tli
'adsfas'.blank?
[].blank?
[1,2,3].blank?
2.blank?
0.blank?
nil.blank?
!nil
nil.empty?
nil.respond_to?(:empty?)
TargetListItem.where(target_list_id: nil)
TargetListItem.where(:entity_id => tli.entity_id)
TargetListItem.where(:entity_id => 67465)
tli = _.first
tli.user
tli.user_id
tli.assignee
tli.user
User.find(4)
tli.user_id
tli.user
tli.user_id
tli.target_list
tli
tli.reload
tli
tli.entity
Entity.find 65537
e = )
e = Entity.find 65537
e.target_list_items
Entity.businesses.select {|e| e.target_list_items.size >2 }
e = _.first
TargetList.first
TargetListItem.all.update({})
TargetListItem.all.each(&:touch)
e = Entity.find 67983
e.customs_cache
e = Entity.find 67403
IncidentEntity.count
IncidentEntity.all.each(&:touch)
reload
reload!
IncidentEntity.all.each(&:touch)
entities.businesses.where(id: target_lists.map {|elem| elem.entities.targeted }.flatten.uniq.map(&:id))
Entity.businesses.where(id: target_lists.map {|elem| elem.entities.targeted }.flatten.uniq.map(&:id))
Entity.businesses.includes(:target_lists).where(id: target_lists.map {|elem| elem.entities.targeted }.flatten.uniq.map(&:id))
TargetList.count
TargetList.where('target_list_type_id = 1').count
Entity.where(target_lists_targeted_cache > 0)
Entity.where('target_lists_targeted_cache > 0')
Entity.where('target_lists_targeted_cache > 0').where(target: false)
Entity.where('target_lists_targeted_cache > 0').target_list
Entity.where('target_lists_targeted_cache > 0').map(:target_list_id)
Entity.where('target_lists_targeted_cache > 0').map(&:target_list_id)
Entity.where('target_lists_targeted_cache > 0').map(&:target_list_item_id)
Entity.where('target_lists_targeted_cache > 0').map(&:target_list_items)
Entity.where('target_lists_targeted_cache > 0').map(&:target_list_items).flat_map(&:target_list_id)
Entity.where('target_lists_targeted_cache > 0').flat_map(&:target_list_items).flat_map(&:target_list)
TargetList.find(12)
TargetList.find(12).entities
TargetList.includes(:country, target_list_items: :entity)
tls = TargetList.includes(:country, target_list_items: :entity)
tls.map {|tl| tl.entities.targeted}.flatten.uniq.map(&:id)
tls.map {|tl| tl.entities.targeted}.flatten.uniq.map(&:id).count
tls.map {|tl| tl.entities.targeted}.flatten.uniq.map(&:id)
tls.map {|tl| tl.entities.targeted}.flatten.uniq.map(&:id).first
e = Entity.find(_)
e.target_list_ids
Entity.where(target_list_ids: [1])
Entity.targeted
Entity.where('target_lists_targeted_cache > 0')
Entity.where('target_lists_targeted_cache > 0').count
targeted = Entity.where('target_lists_targeted_cache > 0').count
lists = TargetLists.all
lists = TargetList.all
lists = TargetList.load
lists = TargetList
Entity.first
Entity.first.target_lists
TargetList.includes(:entities)
Entity.all.select {|e| e.incidents.size != e.customs_cache }
missed = _
missed.count
missed.first
missed.incidents
missed.first.incidents
missed.first.update_customs_counter!
missed.first.incidents
missed.first
missed.each(&:update_customs_counter!)
missed.select {|e| e.incidents.size != e.customs_cache }
Entity.businesses.count
Entity.count
Entity.businesses.each(&:update_customs_counter!)
Entity.businesses.all.each(&:update_customs_counter!)
Entity.businesses.count
Entity.businesses.each {|e| e.update_customs_counter! }
Entity.find(65537)
e = _
e.incidents
e.incidents.count
e.update_customs_counter!
Entity.all.each(&:update_customs_counter!)
EntityType
EntityType.all
Entity.where.not(entity_type_id: 9).each(&:update_customs_counter!)
Entity.first
Entity.businesses.each {|e| e.update_customs_counter! }
exit
TargetList.first
_.addresses
TargetList.select {|tl| tl.addresses.size > 0 }
tl = _
Entity.responds_to(:businesses)
Entity.respond_to?(:businesses)
NoMethodError
Entity.send('businesses')
Entity.send('businesses').count
Entity.fields
Entity.attribute_names
Entity.send('businesses')
Entity.send('businesses').select("#{Entity.tableize}.*")
Entity.table_name
Entity.send('businesses').select("#{Entity.tablename}.*")
Entity.send('businesses').select("#{Entity.table_name}.*")
Approval.table_name
table = _
Approvals.select(.select("#{table}.id")
Approvals.select("#{table}.id")
Approval.select("#{table}.id")
Approval.select("#{table.split('.').last}.id")
table
table.split('.').last
reload!
Approval.table_name
table = _
Approval.select("#{table}.id")
Approval.select("`#{table}`.id")
Approval.select("`#{table}.id`")
Approval.select("`#{table}`.`id`")
Approval.select("#{table}.id")
Approval.select("`#{table}.id`")
Approval.select("`#{table}`.`id`")
Approval.all.select("`#{table}`.`id`")
Approval.all.select(`id`)
Approval.select(`id`)
Approval.select(:id)
Region.select('regions.id')
Approval.select('approvals.id')
Approval.select('rvx-spa.approvals.id')
Approval.select('approvals.id')
table
Approval.select("#{table.split('.').last}.id")
Entity.businesses
Entity.businesses.select(:id, :name)
Entity.businesses.select(:id, :name).references('entities')
Entity.businesses.select(:id, :name).references(:entities)
table
table.sub('rvx-spa')
table.gsub('rvx-spa','' )
table.gsub('rvx-spa.','' )
Approval.select('rvx-spa.approvals.id')
Approval.select("#{table}.id")
Approval.select("`#{table}`.id")
Approval.select("#{table.gsub('rvx-spa.', ''}`.id")
Approval.select("#{table.gsub('rvx-spa.', '')}`.id")
Approval.select("#{table.gsub('rvx-spa.', '')}.id")
Apartment
Apartment.excluded_models
Apartment.excluded_models.include?(Approval.to_s)
Entity.table_name
Apartment.excluded_models.first.tableize
Apartment.excluded_models.include?(Approval)
Apartment.excluded_models.include?(Approval.to_s)
Approval.to_s.tableize
Approval.to_s.tableize == Approval.table_name
Apartment.excluded_models.include?(Approval.to_s)
Incident.preload(:assigned_to_user).map(&:assigned_to_user).uniq.sort_by(&:name).count
Entity.where('customs_cache > 0').first
e = >
e = Entity.where('customs_cache > 0').first
e.related_incidents.size
Entity.first {|e| e.technologies.size > 0}
Entity.find(1).technologies
Entity.select {|e| e.technologies.size > 0}
Entity.businesses.select {|e| e.technologies.size > 0}
e = _.first
e.technologies
Entity.find_by(name: '2DIRECT GMBH')
e = _
e.technologies
e
e.reload
e.entity_type_names
reload!
e.reload
e.entity_type_names
e = Entity.find(e.id)
e.entity_type_names
Entity.businesses.select {|e| e.total_value > 0}
e = Entity.find_by(name: 'APLE CO LTD')
e.num_incidents
e.update_customs_counter!
e
e.related_incidents
e.related_incidents.count
e
e.related_incidents.size!
e.related_incidents.size
e.related_incidents
e.related_incidents.uniq
e.related_incidents.uniq.count
e.related_incidents
e.related_incidents.map(&:id)
e.update_customs_counter!
e
e.reload
e
reload!
e.update_customs_counter!
e
e.reload
e
e = Entity.find(67983)
e.update_customs_counter!
e
reload!
e = Entity.find(67983)
e.update_customs_counter!
e
e.related_incidents.count
e
e.reload
e.related_incidents
e.related_incidents.uniq.count
e.related_incidents.count
e.related_incidents.map(&:id)
e.related_incidents.size
e.reload
e.related_incidents.size
e.related_incidents.uniq.size
e.related_incidents.to_a.sum(&:qty)
e.related_incidents.uniq.to_a.sum(&:qty)
e.related_incidents.to_a.sum(&:qty)
e.related_incidents.uniq.to_a.sum(&:qty)
reload!
e
e = Entity.find(67983)
e.related_entities.sum(&:qty)
e.related_incidents.sum(&:qty)
e.unit_qty
e.related_incidents.to_a.sum(&:amount)
e.related_incidents.to_a.sum(&:qty)
e.related_incidents.uniq.sum(&:qty)
e.related_incidents.uniq.size
e.related_entities
e.related_incidents
e.related_incidents.first.products
e.num_incidents
e.related_incidents
e.related_incidents.count
e.related_incidents.size
exit
e = Entity.find(67983)
e.num_incidents
e = Entity.find_by(name: 'APLE CO LTD')
e.num_incidents
e.related_entities.size
e.related_incidents.size
e.related_incidents
e = Entity.find_by(name: 'YIFANG DIGITAL TECHNOLOGY')
e.related_incidents
Entity.where('incidents.id is not null').where(id: 67983)
Entity.includes(:related_incidents).where('incidents.id is not null').where(id: 67983)
Entity.includes(:related_incidents).where('incidents.id is not null').where(id: 67983).num_incidents
Entity.includes(:related_incidents).where('incidents.id is not null').where(id: 67983).first.num_incidents
Entity.where(id: 67983).first.num_incidents
Entity.includes(:related_incidents).where('incidents.id is not null').where(id: 67983).first.num_incidents
Entity.includes(:related_incidents).where('incidents.id is not null').where(id: 67983).last.num_incidents
Entity.includes(:related_incidents).where('incidents.id is not null').where(id: 67983).count
Entity.includes(:related_incidents).where('incidents.id is not null').where(id: 67983).first.num_incidents
e
e.related_incidents
Entity.includes(:related_incidents).where('incidents.id is not null').where(id: 67983).first
e = _
e.related_incidents
Entity.includes(:related_incidents).where(id: 67983).first
e = Entity.includes(:related_incidents).where(id: 67983).first
e.num_incidents
e = Entity.find(67983)
e.related_incidents
e.related_incidents.last(2)
e.related_incidents
Entity.includes(:related_incidents).where('incidents.id is null').where(id: 67983).first
Entity.includes(:related_incidents).where('incidents.id is not null').where(id: 67983).first
Entity.includes(:related_incidents).where('incidents.id is not null').where(id: 67983).first.num_incidents
Entity.includes(:related_incidents).where(id: 67983).first.num_incidents
Entity.includes(:related_incidents).where('incidents.id is not null').where(id: 67983).first.num_incident
Entity.includes(:related_incidents).where('incidents.id is null').where(id: 67983).first.num_incident
Entity.includes(:related_incidents).where('incidents.id is not null').where(id: 67983).first
Entity.includes(:related_incidents).where('incidents.id is null').where(id: 67983).first.num_incidents
Entity.includes(:related_incidents).where('incidents.id is not null').where(id: 67983).first.num_incidents
IncidentEntities.where(incident_id: nil)
IncidentEntity.where(incident_id: nil)
Incident.where(id: nil)
IncidentEntities.where(entity_id: 67983)
IncidentEntity.where(entity_id: 67983)
IncidentEntity.where(entity_id: 67983).count
EntityType.find(10)
Entity.includes(:related_incidents).where('incidents.id is not null').where(id: 67983).first.num_incidents
IncidentEntity.where(entity_id: 67983).count
IncidentEntity.where(entity_id: 67983)
e.customs_cache
e.related_incidents.uniq
e.reload
reload!
e = Entity.find(e.id)
e.related_incidents.uniq
_.count
Apartment::Tenant.rollback(#{'demo'})
  Apartment::Tenant.migrate(#{'demo'})
Apartment::Tenant.migrate('demo')
Apartment::Tenant.drop('demo')
Entity.size
Entity.count
exit
Entity.count
p = Project.first
Project
Entity.where.not(entity_type_id: 9).each(&:update_related_incident_counter!)
Entity
ids = Entity.businesses.map(&:id)
Entity.where(id: ids).each(&:update_related_incident_counter!)
e
exit
Entity.includes(:related_incidents).where('incidents.id is null').where(id: 67983).first.num_incident
Entity.includes(:related_incidents).where('incidents.id is null').where(id: 67983).first
Entity.all.select{ |e| e.related_incidents > 0}
Entity.businesses.select{ |e| e.related_incidents > 0}
Entity.businesses.select{ |e| e.related_incidents.size > 0}
Entity.businesses.where('num_incidents_cache > 0')
exit
Entity.find_by name: 'APLE CO LTD'
e = _
e.related_incidents
e.related_incidents.count
e.related_incidents.map(&:id)
exit
Entity.where(id: Entity.businesses.map(&:id)).each(&:update_related_incident_counter!)
Entity.where(num_incidents_cache: nil)
Entity.where.not(id: Entity.businesses.map(&:id)).each(&:update_related_incident_counter!)
Entity.where.not(id: Entity.businesses.map(&:id), incident_num_cache: nil)
Entity.where.not(id: Entity.businesses.map(&:id), incident_num_cache: nil).count
Entity
Entity.where.not(id: Entity.businesses.map(&:id), num_incidents_cache: nil).count
Entity.where.not(id: Entity.businesses.map(&:id)).where('num_incidents_cache > 0).count
Entity.where.not(id: Entity.businesses.map(&:id)).where('num_incidents_cache > 0').count
Entity.where.not(id: Entity.businesses.map(&:id)).count
usinesses.map(&:id)).where('num_incidents_cache > 0').count
exit
Entity.businesses.includes(:country, :account_managers, :alternate_names, :technologies, :contacts, :entity_types, :addresses, related_incidents: [:incident_identified_products, :incident_unknown_products]).where('incidents.id is not null').references(:incidents).count
Entity.businesses.includes(:country, :account_managers, :alternate_names, :technologies, :contacts, :entity_types, :addresses, related_incidents: [:incident_identified_products, :incident_unknown_products]).where('incidents.id is null').references(:incidents).count
Entity.businesses.includes(:country, :account_managers, :alternate_names, :technologies, :contacts, :entity_types, :addresses, related_incidents: [:incident_identified_products, :incident_unknown_products]).references(:incidents).count
Entity.all.select {|e| e.incidents.count > 0 }.count
Entity.businesses.select {|e| e.incidents.count > 0 }.count
Entity.businesses.includes(:country, :account_managers, :alternate_names, :technologies, :contacts, :entity_types, :addresses, related_incidents: [:incident_identified_products, :incident_unknown_products]).where('incidents.id is not null').references(:incidents).count
Entity.businesses.select {|e| e.related_incidents.count > 0 }.count
Entity.businesses.includes(:country, :account_managers, :alternate_names, :technologies, :contacts, :entity_types, :addresses, related_incidents: [:incident_identified_products, :incident_unknown_products]).where('incidents.id is not null').references(:incidents).count
query = Incident.includes(:reporting_entity).where('reporting_entity_id is not null')
results = query.load.group_by(&:reporting_entity).map {|e, i| e.nil?}
Incident.where(reporting_entity_id: nil)
Incident.where(reporting_entity_id: nil).count
query = Incident.includes(:reporting_entity).where('reporting_entity_id is not null')
query.select {|inc| inc.reporting_entity.nil?}.map(&:id)
Incident.find(189)
Incident.find(189).reporting_entity_id
query.select {|inc| inc.reporting_entity.nil?}.map(&:reporting_entity_id)
Entity.find(0_
Entity.find(0)
Incident.find(189).reporting_entity_id
Incident.find(189)
query.select {|inc| inc.reporting_entity.nil?}.map(&:reporting_entity_id)
query.first.class
query.select {|inc| inc.reporting_entity.nil?}.destroy
query.select {|inc| inc.reporting_entity.nil?}.each(&:destroy)
exit
TargetListItem
TargetListItem.where(entity_id: nil).first
nil.try(:target_lists_attended_cache)
nil.target_lists_attended_cache
exit
nil.entity_types.map(&:name).join(', ')
nil.try(:entity_types).map(&:name).join(', ')
nil.try(:entity_types).try(map(&:name).join(', ')
  nil.try(:entity_types).try(map(&:name)).join(', ')
TargetListItem.first
TargetListItem.first.entity = nil
TargetListItem.first
tl = TargetListItem.first
tl.entity = nil
tl.save
tli = tl
tli.errors
tli
tli.entity
tli.entity.entity_types
def record_json(record)
  {
    id: record.id,
    notice_status: record.notice_status || "not sent",
    target: !!record.target,
    converted: !!record.converted,
    approval_id: record.approval_id,
    entity_name: (record.entity.try(:name) || "- Missing Entity Record -")[0..29],
    entity_id: record.entity.try(:id),
    entity_url: record.entity.try(:url).presence,
    entity_appearances_count:
    record.entity.try(:target_lists_attended_cache),
    entity_target_appearances_count:
    record.entity.try(:target_lists_targeted_cache),
    entity_types: (record.entity.entity_types.map(&:name).join(', ') if record.entity),
    priority_name: record.priority.try(:name),
    licensees_count: record.entity.try(:licensee_cache),
    nonlicensees_count: record.entity.try(:nonlicensee_cache),
    others_count: record.entity.try(:other_cache),
    customs_count: record.entity.try(:customs_cache),
    assignee_name: record.user.try(:short_name),
    flags: record.flags.map do |flag|
      {
        name: flag.name,
        color: flag.color
      }
    end,
    paths: {
      entity_show: entity_path(record.entity),
      entity_edit: edit_entity_path(record.entity),
      show: target_list_target_list_item_path(
        record.target_list.id, record.id
      ),
      edit: edit_target_list_item_path(record.id),
      destroy: target_list_item_path(record.id),
    }
  }
end
record_json(TargetListItem.last
)
record_json(TargetListItem.all.last)
tli2 = TargetListItem.last
record_json tli2
def record_json(record)
  {
    id: record.id,
    notice_status: record.notice_status || "not sent",
    target: !!record.target,
    converted: !!record.converted,
    approval_id: record.approval_id,
    entity_name: (record.entity.try(:name) || "- Missing Entity Record -")[0..29],
    entity_id: record.entity.try(:id),
    entity_url: record.entity.try(:url).presence,
    entity_appearances_count:
    record.entity.try(:target_lists_attended_cache),
    entity_target_appearances_count:
    record.entity.try(:target_lists_targeted_cache),
    entity_types: (record.entity.entity_types.map(&:name).join(', ') if record.entity),
    priority_name: record.priority.try(:name),
    licensees_count: record.entity.try(:licensee_cache),
    nonlicensees_count: record.entity.try(:nonlicensee_cache),
    others_count: record.entity.try(:other_cache),
    customs_count: record.entity.try(:customs_cache),
    assignee_name: record.user.try(:short_name),
    flags: record.flags.map do |flag|
      {
        name: flag.name,
        color: flag.color
      }
    end,
    # paths: {
    #   entity_show: entity_path(record.entity),
    #   entity_edit: edit_entity_path(record.entity),
    #   show: target_list_target_list_item_path(
    #     record.target_list.id, record.id
    #   ),
    #   edit: edit_target_list_item_path(record.id),
    #   destroy: target_list_item_path(record.id),
    # }
  }
end
record_json tli2
record_json tli
exit
4
TargetList.find 39
tl = _
tl.entities
tl.entities.count
tl.target_list_items.count
tl.target_list_items.where('entiies.name = "MOFI ELECTRONICS"')
tl.target_list_items.where('entities.name = "MOFI ELECTRONICS"')
tl.target_list_items.includes(:entities).where('entities.name = "MOFI ELECTRONICS"').references(:entities)
tl.target_list_items
reload!
TargetListItem.all.to_sql
reload!
TargetListItem.all.to_sql
reload!
TargetListItem.all.to_sql
reload!
TargetList.find 39
tl = _
tl.target_list_items.count
reload!
TargetList.find 39
tl = _
tl.target_list_items.count
User.find(2)
TargetListItem.where(id: [9187, 22099])
exit
Entity.find_by name: 'MYTESTEXPORTER'
e = _
e.incidents
e.related_incidents
exit
TargetListItem.all.select { |tli| tli.products.size > 0}
'index_tli_ip_on_target_list_item_id'.count
'index_tli_ip_on_target_list_item_id'.length
reload!
TargetListItem.find 550
exit
TargetListItem.find(550)
tli = _
tli.target_list_item_identified_products
tli
TargetListItemIdentifiedProducts.find(4)
TargetListItemIdentifiedProduct.find(4)
TargetListItemIdentifiedProduct.all
TargetListItemIdentifiedProduct.destroy_all
gst
TargetListItemIdentifiedProduct.all
TargetListItem.find 496
tli = _
tli.target_list_item_identified_products
Product.find(1110)
Product.find(1111)
tli = TargetListItem.find 550
tli.products
p = tli.products.first
p.product_category
tli = TargetListItem.find 550
tli.products
tli.products.first.product_category = ProductCategory.first
tli.products.first.save
tli.products.last.product_category = ProductCategory.first
tli.products.last.save
tli.products.first.product_category = ProductCategory.first
tli.products.first.save
tli.products.last.product_category = ProductCategory.first
tli.products.last.save
tli = TargetListItem.find 550
tli.products.first.product_category = ProductCategory.first
tli.products.last.save
tli.products.last.product_category = ProductCategory.first
tli.products.last.save
tli.products.first.product_category
tli.products.first
p = _
p.update_attributes(product_category: ProductCategory.first)
p.product_categories
p.product_category
tli.products
tli.products.last.update_attributes(product_category_id: 1)
tli.products.destroy_all
tli.unknown_products.destroy_all
Entity.find_by name: 'ULTMOST TECHNOLOGY GROUP'
e = _
e.technologies
e
e.technologies << Technology.first
true.class
Product.last
p = _
p.technologies
p
p.technologies
p.target_list_items
p.target_list_items.products
p.target_list_items.first.products
p.target_list_items.first.products.technology-ids
p.target_list_items.first.products.technologies
p.target_list_items.first.products.first.technologies
p
p.destroy
Product.last
Product.last.destroy
p
p = Product.last
p.technologies
p.target_list_items
TargetList.find(2)
tl = _
tl.technologies
tli
tli = TargetListItem.find 554
tli.products
tli.products.destroy_all
p = Product.find 1112
p = Product.find 1121
p.technologies
Product.last 5
prods = _
prods.map(&:technolgies)
prods.map(&:technologies)
p = prods.last
p.target_list_item_identified_product
p.target_list_item_identified_products
p.target_list_item_identified_products.first
p.target_list_item_identified_products.first.technologies
reload!
Product.last
Product.last.technologies
reload!
Product.last
Product.last.technologies
Product.last
Product.last.technologies
Address.last
ad = _
ad.addressable_ids
ad.addressable_id
Address.first.addressable_id
Address.where.not(addressable_id: nil)
Address.new {street1: "100 1st Street", city: "San Francisco", state: "CA", postal_code: "94109"}
Address
Address.new({street1: "100 1st Street", city: "San Francisco", state: "CA", postal_code: "94109"})
tl = TargetList.find 2
tl.addresses
add = tl.addresses.first
add.country = Country.find(840)
add.save
tl
tl.addresses
tl.reload
tl.addresses
Address
tl = TargetList.find 2
tl.addresses
Address
Contact
Contact.last
Contact.last.target_lists
Contact.last.target_lists << TargetList.find 2
Contact.last.target_lists << TargetList.find(2)
con = Contact.last
con.target_list_ids
Contact.where.not(target_list_ids: []).where.not(entity_ids: [])
Contact.all.count
Contact.include(:target_list_items, :entities, :trainings).select { |c| c.target_lists_items.size > 0 && c.entities.size > 0 }
Contact.include(:target_lists, :entities, :trainings).select { |c| c.target_lists_items.size > 0 && c.entities.size > 0 }
Contact.includes(:target_lists, :entities, :trainings).select { |c| c.target_lists_items.size > 0 && c.entities.size > 0 }
Contact.includes(:target_lists, :entities, :trainings).select { |c| c.target_lists.size > 0 && c.entities.size > 0 }
User.all
_.count
User.all
User.internal
EntitiesIndex.new
Entity.businesses.count
EntitiesIndex.new.size
EntitiesIndex.new.length
EntitiesIndex.new
EntitiesIndex.new.entities.length
EntitiesIndex.new.entities
entities = EntitiesIndex.new
entities.class
entities.
entities
entities['entities']
entities.json
entities.to_json
entities.to_json.length
puts 'hello', 'ana'
exit
TargetList
TargetListItem.first
tli = _
tli.entity.name
TargetList.find 3
tl = _
tl.target_list_items
tl.target_list_items.first
tl.target_list_items.first.entity.name
tl.target_list_items.first.identified_products
tl.target_list_items.first.target_list_item_identified_products
tl.target_list_items.first.target_list_item_identified_products.product
Entity.find_by(:name => 'D2C INC')
_.technologies << Technology.first
tli = TargetList.find(3).target_list_items.first
tli.product_categories
tli.target_list_item_identified_products
tli.target_list_item_identified_products.first.product
tli.target_list_item_identified_products.first.product.product_category
ProductCategory
TargetListItem.first
tl = TargetList.find(2)
tl.target_list_items.count
gst
Expense
ExpenseType
tli = TargetList.first
tli.entities_total
tli.targets_count
tli.messages_count
Notice
TargetList.notices
tli.notice_status
tli.target_list_items.first.notice_status
tli.class
tli.target_list_items.select {|c| c.notice_status == "delivered"}
tli.target_list_items.select {|c| c.notice_status == "delivered"}.count
reload!
tli = TargetList.first
tli.notices_sent
reload!
tli = TargetList.first
tli.notices_sent
tli.messages
Message
Product.group(:brand)
Product.group(:brand).count
Product.group(:brand).size
Product.group(:brand).select('brand')
Product.group(:brand).select('brand').count
Product.all.uniq(:brand).count
Product.all.uniq(:brand).pluck(:brand)
Product.target_list_items
Product.first.target_list_items
TargetListItem.first
tli = _
tli.products
p = tli.products.first
p.target_list_items
p.target_list_items.where(targeted: true)
p.target_list_items.where(targeted: true).count
p.target_list_items.where(targeted: true).references(:target_list_items).count
p.target_list_items.where('target_list_items.targeted =  true')
p.target_list_items.where('target_list_items.target =  true')
Product.includes(:target_list_items).where('target_list_items.target = true')
Product.includes(:target_list_items).where('target_list_items.target = true').count
Product.includes(:target_list_items).where('target_list_items.target = true').group(:brand)
Product.includes(:target_list_items).where('target_list_items.target = true').group(:brand).count
Product.includes(:target_list_items).where('target_list_items.target = true').group('products.brand')
Product.includes(:target_list_items).where('target_list_items.target = true').group('products.brand').count
Product.includes(:target_list_items).where('target_list_items.target = true').group('products.brand').first
Product.includes(:target_list_items).where('target_list_items.target = true').group('products.brand').last
Product.includes(:target_list_items).where('target_list_items.target = true').group('products.brand').ccount
Product.includes(:target_list_items).where('target_list_items.target = true').group('products.brand').count
Product.includes(:entity, :technologies).group(:brand).references(:products)
Product.includes(:entity, :technologies).group(:brand).references(:products).count
Incident.first
Incident.first.products
Incident.first.products.first
Incident.first.products.first.incidents
p
Product.includes(:target_list_items).where('target_list_items.target = true').first
p = _
p.target_lists
p.target_lists_items
p.target_list_items
p.target_list_items.count
p.target_list_items.pluck('target_list_items.target_list_id').uniq.count
Product.includes(target_list_items: [:target_list]).where('target_list_items.target = true')
Product.includes(target_list_items: [:target_list]).where('target_list_items.target = true').first.target_list_items.count
Product.includes(target_list_items: [:target_list]).where('target_list_items.target = true').first.target_list_items.pluck(:target_list_id).uniq
Product.includes(target_list_items: [:target_list]).where('target_list_items.target = true').first.target_list_items.pluck(:target_list_id).uniq.count
Product.includes(target_list_items: [:target_list]).where('target_list_items.target = true').first
query = Product.includes(target_list_items: [:target_list]).where('target_list_items.target = true')
query.load.map | record |
id: record.id
query.load.map do | record |
  id: record.id
  brand: record.brand
Product.includes(target_list_items: [:target_list]).where('target_list_items.target = true').uniq(:brand).pluck(:brand)
query
query.where('products.brand = Dell').where('target_list_item.target = true')
query.where('products.brand = Dell').where('target_list_item.target = true').count
query.where('products.brand = Dell').where('target_list_items.target = true').count
query.where('products.brand = Dell').where('target_list_items.target = true').references(:target_list_items).count
query.where('brand = Dell').where('target_list_items.target = true').references(:target_list_items).count
query.where('brand = Dell').references(:products).where('target_list_items.target = true').references(:target_list_items).count
Product.includes(target_list_items: [:target_list]).where('target_list_items.target = true').where('brand = Dell').references(:products).where('target_list_items.target = true').references(:target_list_items).count
query.where('brand = Dell').where('target_list_items.target = true').target_list_items
query.where('brand = Dell').where('target_list_items.target = true')
query.where('brand = Dell').where('target_list_items.target = true').count
query
query.count
query.where(brand: 'Dell')
query.where('brand = "Dell"').where('target_list_items.target = true').references(:target_list_items).count
query.where('products.brand = "Dell"').where('target_list_items.target = true').references(:target_list_items).count
query.where('products.brand = "Dell"').where('target_list_items.target = true').references(:target_list_items).pluck('target_list_item.id')
query.where('products.brand = "Dell"').where('target_list_items.target = true').references(:target_list_items).pluck('target_list_items.id')
query.where('products.brand = "Dell"').where('target_list_items.target = true').references(:target_list_items).pluck('target_list_items.id').uniq.count
query.where('products.brand = ?', brand)
.where('target_list_items.target = true')
.flat_map('target_list_items.id').count
query.where('products.brand = ?', brand).
where('target_list_items.target = true').
flat_map('target_list_items.id').count
query.where('products.brand = ?', 'Dell').
where('target_list_items.target = true').
flat_map('target_list_items.id').count
map('target_list_items.id').count
query.last
query.last.target_list_items.count
query
q = Product.includes(target_list_items: [:target_list]).where('target_list_items.target = true')
q.where(brand: 'Dell').pluck('target_list.id')
q.where(brand: 'Dell').pluck('target_lists.id')
q.where(brand: 'Dell').pluck('target_lists.id').uniq.count
q.where(brand: 'Dell').references(:products).pluck('target_lists.id').uniq.count
q = Product.includes(target_list_items: [:target_list]).where('target_list_items.target = true').references(:target_list_items)
q.where(brand: 'Dell').references(:products).pluck('target_lists.id').uniq.count
Product.where(brand: 'Dell')
Product.where(brand: 'Dell').map {|p| p.target_list_items.map(&:target_list_id) }
q.count('target_list_items.notice_status = "delivered"')
q = Product.includes(target_list_items: [:target_list])
q.count('target_list_items.notice_status = "delivered"')
q
q.current_page
q = Project.includes(target_list_items: :target_list)
q.group_by(:brand)
q = Product.includes(target_list_items: :target_list)
q.group_by(:brand)
q.group_by
q.group_by(&:brand)
q.group_by(&:brand).class
q.group_by(&:brand).count
q.where('target_list_items.target = true').group_by(&:brand).count
q.where('target_list_items.target = true').group_by(&:brand)
q.where('target_list_items.target = true').group_by(&:brand)['Dell']
q.where('target_list_items.target = true').group_by(&:brand)['Dell'].first
q.where('target_list_items.target = true').group_by(&:brand)['Dell'].first.target_list_items
q.where('target_list_items.target = true').group_by(&:brand)['Dell'].last.target_list_items
q
q.where('target_list_items.target = true').group_by(&:brand)
q.count
q = Product.includes(target_list_items: :target_list).group_by(&:brand)
q.count
q.total_count
q.count
q.current_page
q.total_pages
q
b1 = q.first
b1
b1.[0]
b1[0]
b1[1]
b1[2]
b1[1].count
b1[1].count { |p| p.target_list_items.where('target = true')
}
b1[1].count
b1[1].sum { |p| p.target_list_items.where('target = true').count }
p = b1[1].first
p.target_lists
p.reload
reload
reload!
p = Product.find 1
p.target_lists
p.target_list_items
TargetListItem.first.products
p = _.first
p.target_list_items
p.target_lists
p.target_list_items
p.target_lists
Product.where(brand = 'ARD')
Product.where(brand: 'ARD')
Product.where(brand: 'ARD').map(&:target_list_id).uniq.count
Product
Product.where(brand: 'ARD').map(&:target_list_ids).flatten.uniq.count
Product.first(100)
Product.first(100).each do |p|
Product.first(100).each_with_index do |p|
Product.first(100).each_with_index do |p, idx|
  puts idx
end
TargetListItem.count
Product.first(100).each_with_index do |p, idx|
  p.target_list_items << TargetListItem.find(idx)
Product.first(100).each_with_index do |p, idx|
  tli = TargetListItem.find idx
  tli.target_list_item_identified_products << p
  tli.update_attributes(target: true)
end
Product.first(100).each_with_index do |p, idx|
TargetListItem.random
tlis = TargetListItem.all.shuffle[0..100]
tli[0]
tlis[0]
Product.first(100).each_with_index do |p, idx|
  tli = tlis[idx]
  tli.target_list_item_identified_products << p
  tli.update_attributes(target: true)
end
Product.first(100).each_with_index do |p, idx|
  tli = tlis[idx]
  p.target_list_items << tli
  tli.update_attributes(target: true)
end
tlis = TargetListItem.all.shuffle[0..100]
Product.first(100).each_with_index do |p, idx|
  tli = tlis[idx]
  p.target_list_items << tli
end
Product.last(100).each_with_index do |p, idx|
  tlis = TargetListItem.all.shuffle[0..100]
tlis = TargetListItem.all.shuffle[0..100]
Product.last(100).each_with_index do |p, idx|
  tli = tlis[idx]
  p.target_list_items << tli
  tli.update_attributes(target: true)
end
tlis = TargetListItem.all.shuffle[0..400]
Product.last(400).each_with_index do |p, idx|
  tli = tlis[idx]
  p.target_list_items << tli
  tli.update_attributes(target: true)
end
Product.all.shuffle[0..400].each_with_index do |p, idx|
  tli = tlis[idx]
  p.target_list_items << tli
  tli.update_attributes(target: true)
end
tlis = TargetListItem.all.shuffle[0..400]
Product.all.shuffle[0..400].each_with_index do |p, idx|
  tli = tlis[idx]
  p.target_list_items << tli
  tli.update_attributes(target: true)
end
Product.includes(:target_list_items).where('target_list_items.target = true').pluck(:brand).uniq.count
Entity.businesses
Entity.businesses.join(:target_list_items)
Entity.businesses.joins(:target_list_items)
Entity.businesses.joins(:target_list_items).count
TargetListItem.count
Entity.businesses.includes(:target_list_items).count
Entity.businesses.includes(:target_list_items).where('target_list_item.target = true')
Entity.businesses.includes(:target_list_items).where('target_list_items.target = true')
Entity.businesses.includes(:target_list_items).where('target_list_items.target = true').count
Entity.businesses.select('entities.*, count(target_list_items.id) as tli_count').includes(:target_list_items).where('target_list_items.target = true').count
Entity.businesses.select('entities.*, count(target_list_items.id) as tli_count').includes(:target_list_items).where('target_list_items.target = true').first
Entity.businesses.select('entities.*, count(target_list_items.id) as tli_count').includes(:target_list_items).where('target_list_items.target = true').first.tli_count
Entity.businesses.select('entities.*, count(target_list_items.id) as tli_count').includes(:target_list_items).where('target_list_items.target = true')
Entity.find(18)
e = _
e.target_list_items
Product.where(brand: '--')
Entity.find(2)
TargetList.all.map {|tl| tl.id, tl.target_list_items.count }
TargetList.all.map {|tl| [tl.id, tl.target_list_items.count] }
exit
TargetListItem
TargetListItem.joins(target_list: [:user]).where('target_list.target_list_type_id = 1').references(:target_lists)
TargetListItem.joins(target_list: [:user]).where('target_lists.target_list_type_id = 1').references(:target_lists)
TargetListItem.joins(target_list: [:user]).where('target_lists.target_list_type_id = 1').references(:target_lists).uniq(&:user_id)
TargetListItem.joins(target_list: [:user]).where('target_lists.target_list_type_id = 1').references(:target_lists).pluck(:user_id)
TargetListItem.joins(target_list: [:user]).where('target_lists.target_list_type_id = 1').references(:target_lists).pluck(:user_id).uniq
TargetListItem.joins(target_list: [:user]).where('target_lists.target_list_type_id = 1').references(:target_lists).uniq(:user_id)
TargetListItem.joins(target_list: [:user]).select(:user_id).where('target_lists.target_list_type_id = 1').references(:target_lists).uniq
TargetListItem.joins(target_list: [:user]).where('target_lists.target_list_type_id = 1').references(:target_lists)
TargetListItem.joins(target_list: [:user]).where('target_lists.target_list_type_id = 1').references(:target_lists).count
TargetListItem.joins(target_list: [:user]).where('target_lists.id = 2 AND target_lists.target_list_type_id = 1').references(:target_lists)
TargetListItem.joins(target_list: [:user]).where('target_lists.id = 2 AND target_lists.target_list_type_id = 1').references(:target_lists).count
TargetList.find(2).target_list_items.map(&:user_id)
TargetList.find(2).target_list_items.uniq(&:user_id)
TargetList.find(2).target_list_items.pluck(&:user_id)
TargetList.find(2).target_list_items.pluck(:user_id)
TargetList.find(2).target_list_items.pluck(:user_id).uniq
TargetList.find(2).target_list_items.pluck('distint target_list_items.user_id')
TargetList.find(2).target_list_items.pluck('distint user_id')
TargetList.find(2).target_list_items.pluck('distinct user_id')
TargetList.where(id: 2).target_list_items.pluck('distinct user_id')
TargetList.find(id: 2).target_list_items.pluck('distinct user_id')
TargetList.find(2).target_list_items.pluck('distinct user_id')
TargetListItem.where(target_list_id: 2).pluck('distinct user_id')
TargetListItem.where(target_list_id: 2).pluck('DISTINCT user')
TargetListItem.where(target_list_id: 2).pluck('DISTINCT user_id')
User.where(id: TargetListItem.where(target_list_id: 2).pluck('DISTINCT user_id'))
User.where(id: TargetListItem.where(target_list_id: 2).pluck('distinct user_id'))
['$state', '$stateParams'].sort
tl = TargetList.find(2).target_list_items.count
TargetListItem.first
TargetListItem.first.user
TargetListItem.first.user_id
TargetListItem.first.approval
TargetListItem.first.approval_id
[].presence
[].empty?
[1].empty?
[1].presence
{}.presence
{}.is_present?
{}.present?
[].present?
Entity.find 564
e = _
e.products
gst
Entity
EntityType.all
EntityStatus
Entity.last
Entity.first
Entity.first.status
Entity.first.entity_status
Entity.first
Entity.first.entity_status_id
Entity.where.not(entity_status_id: nil)
EntityStatus.all
Technologies.all
Technology.all
fuzzy_data = {}
fuzzy_data[:index]
model = 'target_list'
model = 'entity'
options = {}
#{model + 'id'}.to_sym
"#{model}_id".to_sym
model_id = 1
options["#{model}_id".to_sym] = model_id
options
options = {file: 'file.xls', ids: {target_list_id: 1, user_id: 2, approval_id: 3}}
file = options[:file]
additonal_options = options[:ids]
options = {file: 'file.xls', model: 'target_list', ids: {target_list_id: 1, user_id: 2, approval_id: 3}}
options[:model]
options[:model].constantize
options[:model].camelize
options[:model].camelize+'::VerifyImport'
options[:model].camelize+'::VerifyImport'.constantize
options[:model].camelize
options[:model].constantize
options[:model]camelize.constantize
options[:model].camelize.constantize
Object.const_get(options[:model].camelize+'::MatchEntities')
Object.const_get(options[:model].camelize+'Importer::MatchEntities')
model
model.tableize
'target_list'.tableize
'target_list'.camelize
'entities'.camelize
'entity'.camelize
'entity'.tabelize.camelize
'entity'.tableize.camelize
'target_list'.tableize.camelize
'product'.tableize.camelize
import_options
import_options = {model: 'entity'}
importer = Object.const_get(params[:import_options][:model].tableize.camelize+'Importer')
importer = Object.const_get(import_options[:model].tableize.camelize+'Importer')
importer::MatchEntities
importer
import.to_s
importer.to_s
Entity.find_by(name: 'Apple')
Entity.find_by(name: 'Aple co ltd')
e = _
e.alternate_names
tl = TargetListItem.find 2
tl.entities
tl.target_list_items.map(&:entity)
tl.target_list_items
tl = TargetListItem.find 2
tl.target_list_items
tl = TargetList.find 2
tl.target_list_items
tl.target_list_items(&:entity_name)
tl.target_list_items.map(&:entity_name)
tl.target_list_items.map { |tli| tli.entity.name }
Entity.find_by(name: 'Samsung')
e = _
e.target_list_items
Entity.last
Entity.last.target_list_items
Apartment::Tenant.create('demo')
exit
exit
Apartment::Tenant.create('demo')
exit
Apartment::Tenant.create("demo")
exit
attachments = select_all <<-SQL
      select * from attachments where attachable_type = 'Product'
    SQL
attachments.each do |attachment|
  actionable_id = attachment['attachable_id']
  actionable_type = attachment['attachable_type']
  created_at = attachment['created_at'].strftime("%F %T")
  action_id = insert <<-SQL
        insert into actions(actionable_id, actionable_type, created_at)
        values ('#{actionable_id}', '#{actionable_type}', '#{created_at}')
      SQL
  update <<-SQL
        update attachments
        set attachable_type = 'Action', attachable_id = #{action_id}
        where id = #{attachment['id']}
      SQL
    end
attachments = select_all <<-SQL
select * from attachments where attachable_type = 'Product'
SQL
Entity.where(id: Entity.businesses.map(&:id).each(&:update_related_incident_counter!)
Entity.where(id: Entity.businesses.map(&:id)).each(&:update_related_incident_counter!)
rake apartment:migrate
Apartment::Tenant.migrate("demo")
Apartment::Migrator.migrate("demo")
Apartment.tenant_names
Apartment::Migrator.migrate "demo"
require 'apartment/migrator'
Apartment::Migrator.migrate "demo"
Apartment::Migrator.rollback "demo"
Apartment::Migrator.migrate "demo"
require 'db/migrate/20151203001905_add_name_to_revenue_opportunity.rb'
require './db/migrate/20151203001905_add_name_to_revenue_opportunity.rb'
AddNameToRevenueOpportunity.up
exit
Apartment::Migrator.run :up, "demo", 20151203001905
require 'apartment/migrator'
Apartment::Migrator.run :up, "demo", 20151203001905
Apartment::Migrator.run :up, "demo", 20151129212956
Apartment::Migrator.run :up, "demo", 20151006223949
Apartment::Migrator.run :up, "demo"
Apartment::Migrator.migrate "demo"
reload!
Apartment::Migrator.migrate "demo"
exit
'contracts'.camelcase
'target_lists'.camelcase
'target_lists'.camelize
git log
exit
Sidekiq::Queue.new('default').clear
Sidekiq::Queue.new('default').clear; Sidekiq::RetrySet.new.clear
Product.last
Product.find_by(model: 'HD2000')
Product.last
Incident.last
Incident.last.infringement_type
Incident.last
Incident.last.description
ProductCategory.all
Product.last
Product.count
ProductCategory.first
ProductCategory.all
Product.count
Sidekiq::Queue.new('default').clear; Sidekiq::RetrySet.new.clear
Product.last
Product.last.product_category
Product.find(1140).product_category
Contact.count
Contact.where('name like "a%"').count
Contact.where('email like "a%"').count
exit
TargetListItem
TargetListItem.find([548,550,554])
TargetList.find(2)
tl = _
tl.target_list_items
tl.target_list_items.where(converted: true)
tl.target_list_items.where(converted: true).count
tl.target_list_items.where(converted: true).update_all(coverted: false)
tl.target_list_items.where(converted: true).update_all(converted: false)
tl.target_list_items.where(converted: true).count
tl.target_list_items.where(converted: true).update_all(converted: false)
TargetListItem.find 550
tli = _
tli.target
tli.converted
tli.entity_id
*Technology.all.map(&:name)
var = 'Alpha'
case var
when *Technology.all.map(&:name)
  puts 'It worked', "#{var}
else
puts 'Fail'
end
case var
when *Technology.all.map(&:name)
  puts 'It worked', "#{var}"
else
  puts 'Fail'
end
var = 'ALPHA'
case var
when *Technology.all.map(&:name)
  puts 'It worked', "#{var}"
else
  puts 'Fail'
end
EmailTemplate
names = ['Approval', 'Resolution', 'ContractType', 'TargetListType', 'TrainingType', 'CostCenter', 'GeneralLedgerAccount',
  'ProjectType', 'ProjectComplexity', 'ProjectPhase', 'ApplicationType', 'Component', 'MarketSegment',
  'ProductCategory', 'ProductSource', 'CostType', 'UserType', 'Country', 'InfringementType', 'EntityType',
  'IncidentType', 'Entity', 'Status', 'Priority', 'Stage', 'Region', 'RvxSignalGenerator', 'SignalCategory',
  'SignalGroup', 'Technology', 'RvxSignalStatus', 'Projection', 'DeveloperKey'
]
names.sort
AccrualReport.last
EmailTemplate.all
EmailTemplate.all.each {|t| t.update(name: "Template#{t.id}")}
TargetListItem.find([550, 554, 548])
TargetListItem.find([550, 554, 548]).map(&:entity_id
)
ids = _
Contact
Contact.first
Contact.where(entity_ids: ids)
Contact.includes(:entities).where('entities.id in ?', ids).references(:entities)
Contact.includes(:entities).where('entities.id in (?)', ids).references(:entities)
Contact.includes(:entities).where('entities.id in (?)', ids).references(:entities).to_sql
Contact.joins(:entities).where('entities.id in (?)', ids).references(:entities).to_sql
Contact.joins(:entities).where('entities.id in (?)', ids).references(:entities)
Cloud.current
Cloud.current.email_defaults
reload!
data = {
  env Rails.env,
  current_cloud: {
Cloud.current
Cloud.current.logo_attachment
Cloud.current.as_json
Cloud.current.email_defaults.as_json
Cloud.current.as_json(include: logo_attachment)
Cloud.current.as_json(include: :logo_attachment)
Cloud.current.as_json(only: [:database_name, email_defaults], include: :logo_attachment)
Cloud.current.as_json(only: [:database_name, :email_defaults], include: :logo_attachment)
Cloud.current.as_json(only: [:database_name, :email_defaults.as_json], include: :logo_attachment)
Cloud.current.as_json(only: [:database_name, :email_defaults, :logo_attachment], include: :logo_attachment)
Cloud.current.as_json(only: [:database_name, :email_defaults], include: :logo_attachment)
Cloud.current.as_json(include: :logo_attachment)
exit
TargetList.find(2)
tl = _
tli = tl.target_list_items.first
tli.entity_id
tli = tl.target_list_items.last
Cloud.current.as_json(only: [:database_name, :email_defaults, :logo_attachment], include: :logo_attachment)
Cloud.current.as_json(only: [:database_name, :email_defaults, :logo_attachment], include: :logo_attachment).to_s
Cloud.current
Cloud.current.logo_attachment
reload!
Cloud.current.as_json
reload!
Cloud.current.as_json
Cloud.current.database_name
Cloud.current
reload!
Cloud.current.as_json
reload!
Cloud.current.as_json
reload!
Cloud.current.as_json
exit
Cloud.current
reload!
Cloud.current
exit
Cloud.current
Cloud.currentreload!
reload!
Cloud.current
reload!
Cloud.current
reload!
Cloud.current
reload!
Cloud.current
Cloud.current.as_json
reload!
Cloud.current.as_json
exit
Cloud.current.as_json
Cloud.current_entity
Apartment::Tenant.current
Cloud.all
Cloud.current
Cloud.current.logo_url
Cloud.current
Cloud.current.as_json
reload!
Cloud.current.as_json
Cloud.current
Cload.all
Cloud.all
reload!
Cloud.current.as_json
SharedCache
SharedCache.fetch 'cloud'
SharedCache.clear_cache!
Cloud.current.as_json
Cload.all
Cloud.all
Cloud.current
Cloud.all.first
Cloud.all.first.logo_url
Cloud.current
reload!
Cloud.current
Apartment::Tenant.current
Cloud.current
Cloud.all.first.logo_url
Cloud.current
SharedCache.clear_cache!('cloud')
Cloud.current
reload!
Cloud.current
Apartment::Tenant.current
Cloud.all
reload!
Cloud.all
Cloud.current
exit
Cloud
Cloud.current
reload!
Cloud.current
ud id: nil, name: nil, domain_name: nil, database_name: "rvx-spa", active: nil, created_at: nil, updated_at: nil, smtp_settings: {"authentication"=>false, "user_name"=>nil, "password"=>nil, "ssl"=>false, "enable_starttls_auto"=>false}, email_defaults: {}, enable_protect: nil, enable_comply: nil, enable_illuminate: nil, header_attachment_id: nil, footer_attachment_id: nil, hellofax_email: nil, hellofax_password: nil, hellofax_apikey: nil, skip_invoiced_email: false, royalty_contracts: false, logo_attachment_id: nil, application_type_validation: nil, days_before_accruals_reminder: 5, secret_key: nil, enable_late_providers_notifications: true, quarters_start_month_as_index: 1>
Apartment::Tenant.current
Apartment::Tenant.switch 'demo'
Apartment::Tenant.current
Cloud.current
exit
Cloud.current
reload!
Cloud.current
Cloud.current.as_json
Cloud.current
reload!
Cloud.current.as_json
gst
Cloud.current.as_json.email_defaults
Cloud.current.as_json
Apartment::Tenant.current
SharedCache.all
SharedCache.display
SharedCache.fetch 'cloud'
SharedCache.clear_cache 'cloud'
SharedCache.clear_cache! 'cloud'
reload!
SharedCache.clear_cache!
Cloud.current
Apartment::Tenant.switch! 'demo'
Cloud.current
SharedCache.display
SharedCache.fetch 'cloud'
SharedCache.clear_cache! 'cloud'
SharedCache.fetch 'cloud'
Cloud.current
SharedCache.fetch 'cloud'
Cloud.current
SharedCache.fetch 'cloud'
cloud = _
Cloud.new(cloud)
newc = Cloud.new(cloud)
newc.errors
SharedCache.clear_cache! 'cloud'
reload!
Cloud.current
Apartment::Tenant.switch! 'demo'
Cloud.current
SharedCache.clear_cache! 'cloud'
Cloud.current
Cloud.all
ActiveRecord::Base.find_by(database_name: 'demo')
Cloud.find_by(database_name: 'demo')
Cloud.new(database_name: 'demo')
Cloud.new(Cloud.new(database_name: 'demo'))
cloud_hash = SharedCache.fetch "cloud" do
  find_by(database_name: Apartment::Tenant.current) || Cloud.new(
    :database_name => Apartment::Tenant.current
  )
end
cloud_hash
Cloud.new(cloud_hash)
Cloud.attributes
Cloud.attribute_names
cloud_hash
cloud_hash[:logo_url] = "www.com"
cloud_hash
cloud_hash.select {|k,v| Cloud.attributes_names.include? k.to_s }
cloud_hash.select {|k,v| Cloud.attribute_names.include? k.to_s }
reload!
Cloud.current
Apartment::Tenant.current
Cloud.all
SharedCache
SharedCache.clear_cache! 'cloud'
Cloud.all
Cloud.current
exit
Cloud.current
SharedCache.clear_cache!
Cloud.current
Apartment::Tenant.switch! 'demo'
Cloud.current
reload!
Cloud.current
exit
Redis.current
conn = Redis.current
conn.keys
SharedCache.cache
conn.keys
conn.del("shared_cache:demo:cloud")
SharedCache.display
SharedCache.clear_cache!
conn.keys
Cloud.current
Cloud.find_by database_name: 'demo'
found = _
Cloud.new(found)
Cloud.current
Redis.current.keys
Redis.del("shared_cache:demo:cloud")
Redis.current.del("shared_cache:demo:cloud")
Redis.current.keys
exit
Cloud.current.as_json
reload!
Cloud.current.as_json
Cloud.current
Cloud.current.as_json
Cloud.current
Apartment::Tenant.current
SharedCache.all
reload!
Cloud.current
SharedCache.fetch "cloud" do
  Cloud.find_by(database_name: Apartment::Tenant.current)
end
hash = _
Cloud.find_or_initialize_by(hash)
exi
exit
Cloud.find_by(database_name: Apartment::Tenant.current)
Cloud.current
Cloud.current.as_json
Cloud.current
Cloud.current.as_json
Cloud.current
reload!
Cloud.current
Cloud.find_by(database_name: Apartment::Tenant.current)
val = _
val.to_json
reload!
Cloud.current.as_json
Cloud.find_by(database_name: Apartment::Tenant.current).as_json
reload!
Cloud.find_by(database_name: Apartment::Tenant.current).as_json
reload!
Cloud.find_by(database_name: Apartment::Tenant.current).as_json
reload!
Cloud.find_by(database_name: Apartment::Tenant.current).as_json
relaod!
reload!
Cloud.find_by(database_name: Apartment::Tenant.current).as_json
reload!
Cloud.find_by(database_name: Apartment::Tenant.current).as_json
Attachment.first
Attachment.first.as_json
JSON.parse(_)
Attachment.first.as_json.to_s
reload!
Cloud.find_by(database_name: Apartment::Tenant.current).as_json
reload!
Cloud.find_by(database_name: Apartment::Tenant.current).as_json
Cloud.find_by(database_name: Apartment::Tenant.current)
Cloud.find_by(database_name: Apartment::Tenant.current).as_json
Cloud.find_by(database_name: Apartment::Tenant.current).to_json
reload!
Cloud.find_by(database_name: Apartment::Tenant.current).as_json
Cloud.find_by(database_name: Apartment::Tenant.current).to_json
Cloud.find_by(database_name: Apartment::Tenant.current).as_json
Cloud.current
Entity.where(id: [560, 512])
Entity.where(id: [560, 512]).map {|e| e.contacts.map(&:email)}
Entity.where(id: [560, 512]).map {|e| e.contacts.map(&:name)}
Entity.where(id: [560, 512]).map {|e| e.contacts.update(email: "contact@#{e.name}.com".downcase})
Entity.where(id: [560, 512]).map {|e| e.contacts.update(email: "contact@#{e.name}.com".downcase)}
Entity.where(id: [560, 512]).map {|e| e.contacts.update_all(email: "contact@#{e.name}.com".downcase)}
Entity.where(id: [560, 512]).map {|e| e.contacts.map(&:email)}
Entity.where(id: [560, 512]).map {|e| e.contacts.update_all(email: "contact@#{e.name.underscore}.com".downcase)}
Entity.where(id: [560, 512]).map {|e| e.contacts.map(&:email)}
Entity.where(id: [560, 512]).map {|e| e.contacts.update_all(email: "contact@#{e.name.gsub(' ', '')}.com".downcase)}
Entity.where(id: [560, 512]).map {|e| e.contacts.map(&:email)}
Entity.where(id: [560]).map {|e| e.contacts.map(&:email)}
tl
tl = TargetList.find(2)
tl.target_list_items.select{|tli| tli.contacts.size > 0}
tl.target_list_items.select{|tli| tli.contacts.size > 0}.map(&:entity_id)
Entity.where(id: [560, 266, 545]).map {|e| e.contacts.map(&:email)}
Entity.where(id: [560, 266, 545]).map {|e| e.contacts.map(&:name)}
Entity.where(id: [560, 266, 545]).map {|e| e.contacts.update_all(email: "contact@#{e.name.gsub(' ', '')}.com".downcase)}
Entity.where(id: [560, 266, 545]).map {|e| e.contacts.update_all(email: "#{FactoryGirl::Name.name@#{e.name.gsub(' ', '')}.com".downcase)}
Entity.where(id: [560, 266, 545]).map {|e| e.contacts.update_all(email: "#{FactoryGirl::Name.name}@#{e.name.gsub(' ', '')}.com".downcase)}
FactoryGirl
FactoryGirl.name
FactoryGirl::Name
Faker
Faker::Name
Faker::Name.name
Faker::Name.first_name
Entity.where(id: [560, 266, 545]).map {|e| e.contacts.update_all(email: "#{Faker::Name.name}@#{e.name.gsub(' ', '')}.com".downcase)}
Entity.where(id: [560, 266, 545]).map {|e| e.contacts.each {|c| c.update(email: "#{Faker::Name.name}@#{e.name.gsub(' ', '')}.com".downcase)}}
Contact.joins(:entities).where('entities.id in (?)', 512)
Contact.joins(:entities).where('entities.id in (?)', 560)
Contact.joins(:entities).where('entities.id in (?)', 266)
tl
tl.target_list_items
tl.target_list_items.select{|tli| tli.contacts.count < 1}
tl.target_list_items.select{|tli| tli.contacts.count < 1}.destroy
tl.target_list_items.select{|tli| tli.contacts.count < 1}.destroy_all
tl.target_list_items.select{|tli| tli.contacts.count < 1}.each(&:destroy)
TargetListItem.joins(entity: [:contacts])
TargetListItem.joins(entity: [:contacts]).where(id: [550, 554])
Contacts.joins(entity: [:target_list_items]).where('target_list_items.id in (?)', [550, 554]).references(:target_list_items)
Contact.joins(entity: [:target_list_items]).where('target_list_items.id in (?)', [550, 554]).references(:target_list_items)
Contact.joins(entity: [:target_list_items]).where('target_list_items.id in (?)', [550, 554]).references(:target_list_items).count
Contact.joins(entities: [:target_list_items]).where('target_list_items.id in (?)', [550, 554]).references(:target_list_items).count
Contact.joins(entities: [:target_list_items]).where('target_list_items.id in (?)', [550, 554]).references(:target_list_items)
Notice
EntityNoticeEmail
EntityNoticeEmail.all
Contact.joins(entities: [:target_list_items]).where('target_list_items.id in (?)', [550]).references(:target_list_items).select('contacts.*, entities.id')
Contact.joins(entities: [:target_list_items]).where('target_list_items.id in (?)', [550]).references(:target_list_items).first
Contact.joins(entities: [:target_list_items]).where('target_list_items.id in (?)', [550]).references(:target_list_items).first.entity
Contact.joins(entities: [:target_list_items]).where('target_list_items.id in (?)', [550]).references(:target_list_items).first.entity_ids
Contact.find(id: [557])
Contact.find([508, 485])
Contact.find([508, 485]).index_by(&:id)
Contact.find(id: [508, 485])
Contact.find(id: [508, 485]).index_by(&:id)
Contact.find(id: [508, 485]).index_by(&:entity_ids)
Contact.find([508, 485]).index_by(&:entity_ids)
contacts = Contact.joins(entities: [:target_list_items])
contacts = Contact.joins(entities: [:target_list_items]).where('target_list_items.id in (?)', 550, 554).references(:target_list_items).index_by('target_list_items.entity_id')
contacts = Contact.joins(entities: [:target_list_items]).where('target_list_items.id in (?)', [550, 554]).references(:target_list_items).index_by('target_list_items.entity_id')
contacts = Contact.joins(entities: [:target_list_items]).where('target_list_items.id in (?)', [550, 554]).references(:target_list_items).index_by(&:entity_id)
contacts = Contact.joins(entities: [:target_list_items]).where('target_list_items.id in (?)', [550, 554]).references(:target_list_items)
contacts = Contact.joins(entities: [:target_list_items]).where('target_list_items.id in (?)', [550, 554]).references(:target_list_items).index_by { |c| c.entity_ids }
contacts = Contact.joins(entities: [:target_list_items]).where('target_list_items.id in (?)', [550, 554]).references(:target_list_items).pluck('contact.*, target_list_items.id')
contacts = Contact.joins(entities: [:target_list_items]).where('target_list_items.id in (?)', [550, 554]).references(:target_list_items).pluck('contacts.*, target_list_items.id')
tlis = TargetListItem.where(id: [550, 554])
tlis = TargetListItem.where(id: [550, 554]).index_by(&:entity_id)
tlis
Contact.select{|c| c.entities.count > 1}
Contact.select{|c| c.entities.count > 0}
Contact.select{|c| c.entities.count > 0}.count
Contact.joins(entities: [:target_list_items]).where('target_list_items.id in (?)', [550]).references(:target_list_items).first.entity_ids
Contact.joins(entities: [:target_list_items]).where('target_list_items.id in (?)', [550]).references(:target_list_items)
con = +
Contact.joins(entities: [:target_list_items]).where('target_list_items.id in (?)', [550]).references(:target_list_items)
con = _
con.entities
con.entity_ids
con = con.first
con.enitities
con.entities
e2 = Entity.find_by name: 'SOUTHERN TELECOM'
con.entities << e2
Contact.joins(entities: [:target_list_items]).where('target_list_items.id in (?)', [550]).references(:target_list_items).entities
Contact.joins(entities: [:target_list_items]).where('target_list_items.id in (?)', [550]).references(:target_list_items).first.entities
Entity.joins(:target_list_items).where('target_list_item.id in (?)', [550]')
Entity.joins(:target_list_items).where('target_list_item.id in (?)', [550])
Entity.joins(:target_list_items).where('target_list_items.id in (?)', [550])
Entity.joins(:target_list_items).includes(:contacts).where('target_list_items.id in (?)', [550])
Entity.joins(:target_list_items).includes(:contacts).where('target_list_items.id in (?)', [550]).first.contacts
Entity.joins(:target_list_items).includes(:contacts).where('target_list_items.id in (?)', [559]).first.contacts
tlids = TargetList.find(2).target_list_items.map(&:id)
Entity.joins(:target_list_items).includes(:contacts).where('target_list_items.id in (?)', tlids).first.contacts
Entity.joins(:target_list_items).includes(:contacts).where('target_list_items.id in (?)', tlids).pluck('entities.id, contacts.name, contacts.email, contacts.fax')
Entity.joins(:target_list_items).includes(:contacts).where('target_list_items.id in (?)', tlids).pluck('entities.id, contacts.name, contacts.email, contacts.fax').index_by {|i| i.first}
Entity.joins(:target_list_items).includes(:contacts).where('target_list_items.id in (?)', tlids).pluck('entities.id, contacts.name, contacts.email, contacts.fax')
first = Entity.joins(:target_list_items).includes(:contacts).where('target_list_items.id in (?)', tlids).pluck('entities.id, contacts.name, contacts.email, contacts.fax').first
first.drop(1)
contacts_by_id = Hash.new {|h,k| h[k] = [] }
Entity.joins(:target_list_items).includes(:contacts).where('target_list_items.id in (?)', tlids).pluck('entities.id, contacts.name, contacts.email, contacts.fax').each {|c| contacts_by_id[c.take(1)] = c.drop(1) }
contacts_by_id
O
q
qq
q
Entity.joins(:target_list_items).includes(:contacts).where('target_list_items.id in (?)', tlids).pluck('entities.id, contacts.name, contacts.email, contacts.fax').each {|c| contacts_by_id[c.take(1)] = c.drop(1) }
xi
Entity.joins(:target_list_items).includes(:contacts).where('target_list_items.id in (?)', tlids).pluck('entities.id, contacts.name, contacts.email, contacts.fax').each {|c| contacts_by_id[c.take(1)] << c.drop(1) }
tlids = TargetList.find(2).target_list_items.map(&:id)
Entity.joins(:target_list_items).includes(:contacts).where('target_list_items.id in (?)', tlids).pluck('entities.id, contacts.name, contacts.email, contacts.fax').each {|c| contacts_by_id[c.take(1)] << c.drop(1) }
contacts_by_id = Hash.new {|h,k| h[k] = [] }
Entity.joins(:target_list_items).includes(:contacts).where('target_list_items.id in (?)', tlids).pluck('entities.id, contacts.name, contacts.email, contacts.fax').each {|c| contacts_by_id[c.take(1)] << c.drop(1) }
contacts_by_id
Entity.joins(:target_list_items).includes(:contacts).where('target_list_items.id in (?)', tlids).pluck('entities.id, contacts.name, contacts.email, contacts.fax')
Entity.joins(:target_list_items).includes(:contacts).where('target_list_items.id in (?)', tlids)
contacts_by_id = Hash.new {|h,k| h[k] = [] }
Entity.joins(:target_list_items).includes(:contacts).where('target_list_items.id in (?)', tlids).each {|e| contacts_by_id[e.id] = e.contacts.reject{|c| c.email.blank? && c.fax.blank? } }
contacts_by_id
contacts_by_id = Hash.new {|h,k| h[k] = [] }
Messages.all
Message.all
Message.all.first
Message.all.first.status
exit
contacts_by_entity = Hash.new {|h,k| h[k] = []}
[2] pry(main)> 
Entity.joins(:target_list_items).includes(:contacts).where('target_list_items.id' in (?)', [552, 553]).each {|e| contacts_by_entity[e.id] = e.contacts.reject { |c| c.email.blank? && c.fax.blank? } }
Entity.joins(:target_list_items).includes(:contacts).where('target_list_items.id in (?)', [552, 553]).each {|e| contacts_by_entity[e.id] = e.contacts.reject { |c| c.email.blank? && c.fax.blank? } }
contacts_by_entity
TargetListITem.find(550
)
TargetListItem.find(550)
tli = _
tli.notices
tli.notice_status
TargetListItem.find(550)
tli.notice_status
tli.reload
e = Entity.find 557
e.contacts
Message.last
message = Message.find(46)
message.target_list_item
message
message = Message.find(46)
message.delivered_at
Entity.find(550).contacts.count
Entity.find(550).contacts.empty?
Entity.includes(:contacts).where(id: [559, 560])
es = _
entities = _
contacts_by_entity = Hash.new {|h,k| h[k] = []}
entities.each do |e|
  contacts_by_entity[e.id] = e.contacts.reject { |c| c.email.blank? && c.fax.blank? } unless e.contacts.empty?
end
contacts_by_entity
TargetListItem.find(549)
Entity.includes(:contacts).where(id: [559, 560])
ids = [559, 560]
Entity.includes(:contacts).where(id: ids)
entities = _
entities.inject({}){|h, e| h.merge e.id: e.contacts.reject { |c| c.email.blank? && c.fax.blank? } unless e.contacts.empty? }
entities.inject({}){|h, e| h.merge e.id=> e.contacts.reject { |c| c.email.blank? && c.fax.blank? } unless e.contacts.empty? }
ids
cons = Entity.includes(:contacts).where(id: ids).inject({}) {|h, e| h[e.id] = e.contacts.reject {|c| c.email.blank? && c.fax.blank? } unless c.contacts.empty? }
cons = Entity.includes(:contacts).where(id: ids).inject({}) {|h, e| h[e.id] = e.contacts.reject {|c| c.email.blank? && c.fax.blank? } unless e.contacts.empty? }
cons
cons = Entity.includes(:contacts).where(id: ids).inject({}) {|h, e| e.id => e.contacts.reject {|c| c.email.blank? && c.fax.blank? } unless e.contacts.empty? }
cons = Entity.includes(:contacts).where(id: ids).inject({}) {|h, e| e.id=> e.contacts.reject {|c| c.email.blank? && c.fax.blank? } unless e.contacts.empty? }
entities = Entity.includes(:contacts).find(ids)
entities.flat_map do |entity|
  entity.contacts.map do |contact|
    {entity: entity, contact: contact}
  end
end
recs = _
recs
recs.first
recs.first.entity
recs.first[entity]
recs.first["entity]
recs.first[:entity]
e = _
e.last_delivered_message
e.messages
e.entity
e.as_json
e.as_json(extended: true)
e.last_delivered_message
e.messages
present
Grape::
messages
e
ids = [559, 560]
e = Entity.find(
e = Entity.find(ids)
e = Entity.find(ids).first
e.messages
e.messages.find(&:delivered_at)
e.messages.last
TargetListItem
TargetListItem.find(&:notice_stauts)
TargetListItem.find(&:notice_status)
Message.last(2)
Message.last(2).each { |m| m.update(delivered_at: Time.current)}
e = Entity.find 560
e.last_delivered_message
Message.delivered
reload!
Message.delivered
e.messages.delivered
e = Entity.find e.id
reload!
e = Entity.find 560
e.messages
e.messages.last.update(delivered_at: nil)
e.messages
e.messages.delivered
e.last_delivered_message.try(:delivered_at).tap do |delivered_at|
  puts delivered_at
end
e.last_delivered_message
e.reload!
e.reload
e.last_delivered_message
reload!
e = Entity.find e.id
e.last_delivered_message
e.last_delivered_message.try(:delivered_at).tap do |delivered_at|
  puts delivered_at
end
e.last_delivered_message.try(:email_template).tap do |delivered_at|
  puts delivered_at
end
e.last_delivered_message.try(:email_template).tap do |delivered_at|
  puts delivered_at.name
end
e.last_delivered_message
e.last_delivered_message.email_template
e.last_delivered_message.email_template.name
entities = Entity.includes(:contacts, messages: [:email_template])
ids
entities = Entity.includes(:contacts, messages: [:email_template]).where(id: ids)
ids
Entity.joins(:contacts).joins(messages: [:email_templates])
Entity.joins(:contacts).joins(messages: [:email_templates]).all
Entity.joins(:contacts).joins(messages: [:email_template]).all
Entity.joins(:contacts).joins(messages: [:email_template]).load
Entity.joins(:contacts, messages: [:email_template]).load
EntityNoticeEmail
ids = [537,262,266]
Entities.find(ids)
Entity.find(ids)
ents = Entity.includes(:contacts).where(id: ids)
ents.map{|e| e.contacts}
goldyip@goldyip.com.cn,,,,,,
ents.map{|e| e.contacts}
ents.flat_map{|e| e.contacts}
cons = _
cons.length
rs = cons.each do |c|
rs = []
cons.each do |c|
  if !c.name.empty? || rs.include? c
cons.each do |c|
  if (!c.name.empty? || rs.include? c)
cons.each do |con|
  if !con.name.empty? || rs.include? con
cons.each do |con|
  if !con.name.empty? || rs.include?(con)
    rs << con.name
  elsif
cons.each do |con|
  if !con.name.empty? || rs.include?(con)
    rs << con.name
cons.each do |con|
  if !con.name.empty? || rs.include?(con.name)
    rs << con.name
    if !con.email.empty? || rs.include?(con.email)
cons.each do |con|
  if !con.name.empty? || rs.include?(con)
cons.each do |con|
  if !con.name.empty? || rs.include?(con.name)
    rs << con.name
  elsif !con.email.empty? || rs.include?(con.email)
    rs << con.email
    elseif !con.fax.empty? || rs.include?(con.fax)
cons.each do |con|
  if !con.name.empty? || rs.include?(con.name)
    rs << con.name
  elsif !con.email.empty? || rs.include?(con.email)
    rs << con.email
  elsif !con.fax.empty? || rs.include?(con.fax)
    rs << con.fax
  end
end
cons.each do |con|
  if !con.name.blank? || rs.include?(con.name)
    rs << con.name
  elsif !con.email.blank? || rs.include?(con.email)
    rs << con.email
  elsif !con.fax.blank? || rs.include?(con.fax)
    rs << con.fax
  end
end
rs
rs.uniq
entities = ::Entity.joins(:contacts, messages: [:email_template]).where('entities.id in (?)', params[:entities])
entities = ::Entity.joins(:contacts, messages: [:email_template]).where('entities.id in (?)', [559])
entities = ::Entity.joins(:contacts, messages: [:email_template]).where('entities.id in (?)', [559]).count
reload!
entities = ::Entity.joins(:contacts, messages: [:email_template]).where('entities.id in (?)', [559]).count
entities = ::Entity.joins(:contacts, messages: [:email_template]).where('entities.id in (?)', [559]).uniq
entities.joins(:target_list_items).where('target_list_items.target_list_id in (?)', [2])
entities.first
entities.first.target_list_items
Entity.find(559).target_list_items.count
Entity.businesses.select {|e| e.target_list_items.count > 1}
multis = _
multies.map(&:id)
multies.first
multis.first
multis.first.target_list_items
multis.first.target_list_items.map(&:target_list_id)
entities = ::Entity.joins(:contacts, messages: [:email_template]).where('entities.id in (?)', [262]).uniq.joins(:target_list_items).where('target_list_items.id = 2')
entities = ::Entity.joins(:contacts, messages: [:email_template]).where('entities.id in (?)', [262]).joins(:target_list_items).where('target_list_items.id = 2')
entities = ::Entity.joins(:contacts, messages: [:email_template]).where('entities.id in (?)', [262]).joins(:target_list_items).where('target_list_items.target_list_id = 2')
entities = ::Entity.joins(:contacts, messages: [:email_template]).where('entities.id in (?)', [262]).includes(:target_list_items).where('target_list_items.target_list_id = 2')
entities = ::Entity.includes(:contacts, messages: [:email_template]).where('entities.id in (?)', [262]).includes(:target_list_items).where('target_list_items.target_list_id = 2')
entities.first.target_list_items
entities.first.contacts
entities.first.messages
entities.first.products
reload!
e.target_list_items
reload!
"IncidentDocument"
"IncidentDocument".constantize
reload!
e.id
entities = ::Entity.joins(:contacts, messages: [:email_template]).where('entities.id in (?)', [262]).includes(:target_list_items).where('target_list_items.target_list_id = 2')
entities = ::Entity.includes(:contacts, messages: [:email_template]).where('entities.id in (?)', [262]).includes(:target_list_items).where('target_list_items.target_list_id = 2')
e = Entity.find 262
reload!
Entity.businesses.select {|e| e.target_list_items.count > 1}
Apartment::Tenant.switch 'demo'
e = Entity.find 262
e.target_list_items
entities = ::Entity.includes(:contacts, messages: [:email_template]).where('entities.id in (?)', [262]).includes(:target_list_items).where('target_list_items.target_list_id = 2')
e = entities.first
e.target_list_items
Entity.find(e.id).target_list_items
reload!
Apartment::Tenant.switch 'demo'
entities = ::Entity.includes(:contacts, messages: [:email_template]).where('entities.id in (?)', [262]).includes(:target_list_items).where('target_list_items.target_list_id = 2')
e = entities.first
e.target_list_items
entities = ::Entity.includes(:contacts, messages: [:email_template]).where('entities.id in (?)', [262]).includes(:target_list_items).where('target_list_items.target_list_id = 2')
e.target_list_items
entities = ::Entity.includes(:contacts, messages: [:email_template]).where('entities.id in (?)', [262]).includes(:target_list_items).where('target_list_items.target_list_id = 2')
entities.load.map do |e|
  e.target_list_items
end
e = Entity.find_by(name: "TELESTAR-DIGITAL GMBH")
e.target_list_items_for(2)
e.target_list_items
Entity.find([559, 560, 520])
Entity.find([559, 560, 520]).map do |e|
  e.contacts
end
e = Entity.find(560)
e.messages.last
e.messages.delivered.last
Message.find(&:delivered_at)
Incident.find(3)
i = _
i.related_entity_ids
IncidentEntity.where(incident
IncidentEntity.where(incident_id: 3)
EntityType.all
i.related_entity_ids
Contact.find [470, 474, 477, 510, 507]
Incident.includes(incident_entities: [:entity, :entity_type]
Incident.includes(incident_entities: [:entity, :entity_type])
incidents = _
incidents.first.entity_ids
incidents.first.related_entity_ids
Incident.includes(incident_entities: [:entity, :entity_type]).where(incident_id: 3)
Incident.includes(incident_entities: [:entity, :entity_type]).where(id: 3)
Incident.includes(incident_entities: [:entity, :entity_type]).where(id: 3).first
inc = Incident.includes(incident_entities: [:entity, :entity_type]).where(id: 3).first
inc.related_entity_ids
inc = Incident.includes(incident_entities: [:entity, :entity_type]).where(id: 3).first
inc.incident_entities
inc.incident_entities.first.entity
inc.incident_entities.first.entity.id
inc.incident_entities.entities
inc.incident_entities
inc.incident_entities.pluck(:entity_id)
inc.incident_entities
inc.incident_entities.map(&:enitity_id)
inc.incident_entities.map(&:entity_id)
inc.incident_entities.pluck(:entity_id)
inc.incident_entities.map(&:entity_id)
inc = Incident.includes(:related_entities, incident_entities: [:entity, :entity_type]).where(id: 3).first
inc.related_entity_ids
inc = Incident.includes(incident_entities: [:entity, :entity_type]).where(id: 3).first
inc.related_entity_ids
inc = Incident.includes(:related_entities, incident_entities: [:entity, :entity_type]).where(id: 3).first
Contacts.includes(:entities)
Contact.includes(:entities)
Entity.includes(:contacts).where(id: [514,513,515,602,512])
entities = Entity.includes(:contacts).where(id: [514,513,515,602,512])
entities.first
entities.first.contacts
entities.map |e|
e.contacts
entities.map do |e|
  e.contacts
end
ids = [514,513,515,602,512]
entities = Enitity.includes(:contacts, messages: [:email_template]).where(id: ids).where('contacts.email IS NOT NULL or contacts.fax IS NOT NULL').references(:contacts)
entities = Entity.includes(:contacts, messages: [:email_template]).where(id: ids).where('contacts.email IS NOT NULL or contacts.fax IS NOT NULL').references(:contacts)
entities = Entity.preload(:contacts, messages: [:email_template]).where(id: ids).where('contacts.email IS NOT NULL or contacts.fax IS NOT NULL').references(:contacts)
entities = Entity.includes(:contacts, messages: [:email_template]).where(id: ids).references(:contacts)
e = entities.last
e.contacts
e.contacts << Contact.create(name: 'Ali Baba')
cont = Contact.find(560)
cont.update(email: 'ali@alibaba.com')
Entity.find([1,2,3])
Entity.select(:id, :name)
Entity.find([559,560,520]).select(:id, :name)
Entity.where([559,560,520]).select(:id, :name)
Entity.where(id: [559,560,520]).select(:id, :name)
Entity.find(id: [559,560,520])
Entity.find([559,560,520])
Entity.where(id: [559,560,520]).select(:id, :name)
Entity.find(id: [559,560,520])
Entity.find([559,560,520])
Entity.find([559,560,520]).select(:id, :name)
Entity.find([559,560,520]).pluck(:id, :name)
Entity.where([559,560,520]).pluck(:id, :name)
Entity.where(id: [559,560,520]).pluck(:id, :name)
Project.template
Project.template.where(id: [1,2,3,4,5]).destroy_all
Entity.includes(country: [:regions]).where(id: [559,560,520]).pluck(:id, :name)
Entity.includes(:country, :regions)
Entity.includes(:country, :regions).count
Entity.includes(:country, :regions)
Entity.includes(:country, :regions).unscoped
Entity.includes(:country, :regions).unscoped.count
Entity.includes(:country, :regions).unscoped
entities = Entity.includes(:country, :regions).unscoped
enities.first
entities.first
entities.first.country
entities = Entity.includes(:country, :regions)
entities.first.country
entities = Entity.includes(:country)
entities.first.country
exit
User.internal.includes(:profile_picture_attachment)
Project.last
Project.last(3)
Project.last
Project.all
Project.last
User.all
User.find 5
u = _
u.password
u.email
p = Project.find 7
p.country = nil
p.save
Project.last
Project.last(2)
exit
TargetList.first
TargetList.first.country_id
[:id, :name] + ['country_id'].map(&:to_sym)
[:id, :name] + ['country_id', 'startdate'].map(&:to_sym)
opts = [{'country': 'id'}, 'startdate']
opts.map(&:to_sym)
opts.each {|opt| puts opt.class}
[:id, :name] + ['country', 'startdate'].map(&:to_sym)
TargetList.includes(:country)
TargetList.count
TargetList.last.update(country: nil)
TargetList.includes(:country)
opts = {country: true, startdate: false}
opts.reject {|k,v| !v}.keys
TargetList.include([:country])
TargetList.include(:country)
TargetList.includes(:country)
TargetList.includes([:country])
TargetList.includes([:country, [:status])
TargetList.includes([:country, :status])
TargetList.includes([])
opts.keys
User.all
Country.find(nil)
country = asdf || 1
country = try(:asdf) || 1
try(asd)
try(:asd)
Reminder.all
User.find(5)
u = _
u.reminders
u.reminders.count
r = u.reminders.first
r.actionable
r
r.name
r.aciion
r.action
r.action.name
r.action.actionable.name
Incident.where.not(name: nil)
Entity.businesses.first
e = _
e.entity_type
e.type
exit
Date.tody
Date.today
Date.today.day
"TargetList".upcase
user.email
c
TargetList.all.map(&:id)
TargetList.where(target_lists[:id].eq(1).or(target_lists[:id].eq(2)))
User.find(5)
u = _
u.roles
Roles
Role.all
u.roles << Role.find(3)
u = User.find 5
u.role_ids
AccrualReport.all
ar = AccrualReport.first
ar.entity
query = Project.includes(provider_entity: [:accrual_reports]).where.not(provider_entity_id: [nil, 0]).where("projects.subject_entity_id is NULL OR (projects.provider_entity_id != projects.subject_entity_id)").group(:provider_entity_id)
query = Project.includes(provider_entity: [:accrual_reports]).where.not(provider_entity_id: [nil, 0]).where("projects.subject_entity_id is NULL OR (projects.provider_entity_id != projects.subject_entity_id)").group(:provider_entity_id).count
query = Project.includes(provider_entity: [:accrual_reports]).where.not(provider_entity_id: [nil, 0]).where("projects.subject_entity_id is NULL OR (projects.provider_entity_id != projects.subject_entity_id)").group(:provider_entity_id)
base = query.load.select(&:provider_entity)
not_submitted = true
base.select! do |project|
  reports = project.provider_entity.accrual_reports_in_period(1)
  !reports.present? || reports.select {|report| !report.invalid?}.count == 0
end
res = {
  data: base.map do |record|
    {
      provider_entity: record.provider_entity.as_json
    }
  end
}
Entity.find(11)
Entity.find(11).update(name: 'Provider Entity')
Entity.find(11)
reload
reload!
Entity.find(11)
Entity.find(11).clean_names
Entity.find(11)
Entity.find(11).clean_names
Entity.find(11)
e = _
e.clean_names
reload!
Entity.find(11).clean_names
e = Entity.find(11)
name = e.name.strip
e.clean_name
e.clean_name = name
e.save!
AccrualPeriod.all
period = AccrualPeriod.find 2
Entity.all
e = Entity.find 12
vadmin = FactoryGirl.create(:entity_user)
e_assoc = EntityAssoc.create(entity_id: 12, user_id: 7)
e_assoc.roles = Role.find(1)
e_assoc.roles << Role.find(1)
user = FactoryGirl.create(:entity_admin)
vadmin
vadmin.entities.first
e
project_mgr = FactoryGirl.create(:user)
finance_mgr = FactoryGirl.create(:user)
cost_mgr = FactoryGirl.create(:user)
user = FactoryGirl.create(:entity_admin)
user = FactoryGirl.create(:entity_user)
e_assoc = EntityAssoc.create(entity_id: e.id, user_id: 11)
e_assoc.roles << Role.find(1)
project1 = FactoryGirl.create(:project,
  name: 'TEST PROJECT 1',
  provider_entity: entity,
  project_mgr: project_mgr,
financial_mgr: finance_mgr)
entity = e
project1 = FactoryGirl.create(:project,
  name: 'TEST PROJECT 1',
  provider_entity: entity,
  project_mgr: project_mgr,
financial_mgr: finance_mgr)
User.all
User.all.pluck(:id, :email)
User.last(3).destroy_all
User.last(3).each(&:destroy)
User.all.pluck(:id, :email)
User.last.destroy
User.all.pluck(:id, :email)
e = 
e
e.accrual_reports
a = e.accrual_reports.first
a
a.archived_at = nil
a.save
a
a.accrual_period
a
a.archive_by_user_id = nil
a.save
a.accrual_report_items
a.accrual_report_items.count
a.accrual_report_items.each do |ari|
  ari.approved_at = nil
  ari.approved_by_user = nil
  ari.finally_approved_at = nil
  ari.finally_approved_by_user_id = nil
end
a.accrual_report_items.each(&:save!)
a
a.accrual_period
a.report_status
a.report_status = ReportStatus.find 2
a.save!
a.accrual_report_items
a.accrual_report_items.map(&:approved_at)
a.accrual_report_items.map(&:approved_by_user_id)
a.report_status
a.accrual_report_items.status
a.projects
p = a.projects.first
project_accruals = a.accrual_report_items.where(project_id: p.id)
status = project_accruals.first.status
a.accrual_report_items.map(&:status)
a.accrual_report_items.update_all(status: 'pending')
a.accrual_report_items.each {|ari| ari.update_all(status: 'pending') }
a.accrual_report_items.map(&:archived_at)
a.accrual_report_items.update_all(archived_at: nil)
a.accrual_report_items.map(&:status)
a
reload!
a = AccrualReport.find a.id
a.accrual_report_items
a.accrual_report_items.map(&:status)
a = AccrualReport.find a.id
a.accrual_report_items.map(&:status)
a.accrual_report_item.approved
a.accrual_report_items.approved
Role.find(1)
User.find(5)
u = _
u.entity_user
u
u.entity_user?
u
u.roles
u
y u
{
  id: 5
  name: 'INTERNAL USER #1'
title: Boss
department: Compliance
email: internal_user@ruvixx.com
phone: 555-555-5555
mobile: 415-111-2221
address_id: 
created_at: 2015-11-16 21:56:22.000000000 Z
updated_at: 2015-12-23 19:45:11.000000000 Z
password_digest: 
activated: 
notes: 
smtp_password: 
encrypted_password: "$2a$10$/YHVc4bg9sq0D2b1BnZ4nO7eDlQ3vQu5T7MB51ZXQ.JSSFx5lJwva"
reset_password_token: "-FZ3NrnB36H3zxpsy5Nd"
reset_password_sent_at: 
remember_created_at: 
sign_in_count: 10
current_sign_in_at: 2015-12-23 19:45:11.000000000 Z
last_sign_in_at: 2015-12-23 02:46:17.000000000 Z
current_sign_in_ip: 127.0.0.1
last_sign_in_ip: 127.0.0.1
failed_attempts: 0
unlock_token: 
locked_at: 
user_type_id: 3
time_zone: Pacific Time (US & Canada)
profile_picture_attachment_id: 
receive_emails: true
confirmation_token: 
confirmed_at: 2015-11-16 21:57:23.000000000 Z
confirmation_sent_at: 2015-11-16 21:56:23.000000000 Z
preferred_name: ''
specific_format_name: ''
authy_id: 
last_sign_in_with_authy: 
authy_enabled: false
u.activated
u
u.user_type
u
Date.parse('2015-12-25')
Date.parse('2015-12-25').day
u
u.roles << Role.find 4
Role.all
save_and_open_page
exit
Country.find_by(name: 'United States')
entity.last
Entity.businesses.last
e = _
e.update(country: Country.find(840))
e.countries
e.countru
e.country
e.save
e.reload
e.country
e.update(country_id: 840)
e = Entity.find(e.id)
e.update(country_id: 840)
Role.all
Priority.all
Technology.all
TargetListItem.find(1)
TargetListItem.first
[1,2,3].count
[1,2,3].size
[1,2,3].length
[1,2,3].count
{}.keys.length
Project.last
Project.last.country
Project.last.country.regions
Project.last.entity
Project.last
Project.last.subject_entity
Project.last.provider_entity
TargetListItem.find 604
tli = +
tli = TargetListItem.find 604
tli.targeted_technologies
tli.targeted_technologies = []
tli.save
tli.reload
User.all
User.all.pluck(:id, :email)
tli.targeted_technologies = []
tli.save
Date.parse('2015-12-25')
Date.parse('2015-12-25').to_s
DateTime.parse('2015-12-25').to_s
s = Scrape.last
s
a = Asset.last
a.url
a
p = Product.find(312)
p.asset
p.images
p = Product.find(312)
p.images
p.image_ids
FactoryGirl
exit
FakerGirl.create :product
FactoryGirl.create :product
FactoryGirl.create_list :product, 31
Product.last
p = _
p.scrapes
scrape = FactoryGirl.create :scrape
scrape.source
Scrape.last
Scrape.first
Scrape.first.products
Scrape.first.products.count
Scrape.internet.products
Scrape.internet.includes(:products)
Source.internet
p
p.scrapes
p.scrapes << FactoryGirl.create_list(:scrape, 3)
p.scrapes
p.scrapes.count
Product.includes(:sources).where.not('sources.source_type_id = ?', 2).references(:sources).count
Source.all
source = Source.find(5)
source
source.verified?
Source.last
Source.last.destroy
Source.last
Source.last.destroy
Source.last
Source.last.destroy
Source.last
Source.last.destroy
Source.last
Source.last.verified?
SourceType
Source.last
SourceType.last
SourceType.last.verified?
Faker::Time.backward
Scrape.last
s
s = Scrape.last
s
s.num_products
s.num_sellers
s.num_products
ScrapeProductSeller.last
Product.find(442)
exit
Product.find(442)
s = Scrape.last
s.num_products
s = Scrape.last
exit
s = FactoryGirl.create(:source)
Product.includes(sources: :source_type).where('source_types.verified is 1').references(:source_types)
Product.includes(sources: :source_type).where('source_types.verified is 1').references(:source_types).count
Product.includes(sources: :source_type).where('source_types.verified = true').references(:source_types).count
Product.includes(:source_types).where('source_types.verified = true').references(:source_types).count
Product.includes(sources: :source_type).where('source_types.verified = true').references(:source_types).count
exit
scrapes = FactoryGirl.create_list :source, 10
Source.count
exit
Source.count
scrapes = FactoryGirl.create_list :source, 10
Source.internet
exit
Source.first
exit
Source.first
Source.count
Source.internet
scrapes = FactoryGirl.create_list :source, 10
Source.count
Source.internet
Source.internet.count
Product.merge(Source.unverified)
reload!
Product.merge(Source.unverified)
Product.all.merge(Source.unverified)
Product.all.merge(Source.unverified).count
Product.includes(:sources).merge(Source.unverified).count
exit
Product.count
exit
s = Scrape.create
s.source
Source.create name: 'test'
s = _
s.verified
s.verified?
s.source_type
SourceType.find 2
Source.last
Source.last.destroy
Source.last
Source.last.scrapes
exit
@source = SourceType.find(SourceType::INTERNET).sources
@source
@source.count
@source
SourceType::IMPORT
@source.last
@source.last.update(source_type_id: 2)
@source = SourceType.find(SourceType::INTERNET).sources
@source.count
@source = SourceType.find(SourceType::INTERNET).sources.map(&:verified?)
Source.last
exit
OtherProduct
exit
OtherProduct
OtherProduct.connection
Digest
Digest::MD5
exit
OtherProduct.where('name like "%Andoer%"')
source = Source.find 3
azus = _
azus.scrapes
azus.scrapes.last.products
azus.scrapes.last.other_products
unknown_lights = azus.scrapes.last.other_products.first
azus.scrapes.last.products.first
p = _
p.url
p
Product
p.original_url
unknown_lights
other_product.where(name: 'Triangle Bulbs® Blue LED Strip light, Waterproof LED Flexible Light Strip 12V with 300 SMD 3258 LED, 16.4 Ft / 5 Meter')
OtherProduct.where(name: 'Triangle Bulbs® Blue LED Strip light, Waterproof LED Flexible Light Strip 12V with 300 SMD 3258 LED, 16.4 Ft / 5 Meter')
unknown_lights.categories
Category.count
180 - 79
Category.all
source
source.products
Source.first
Source.find 1
gsus = _
gsus
gsus.products
gsus.scrapes
Product.where(scrape_id: [1, 3])
Product.merge(Scrape.where(id: [1, 3]))
gsus_scrapes = Scrape.where(id: [1, 3])
gsus_scrapes.map {|s| s.products.size, s.other_products.size }
gsus_scrapes.map {|s| [s.products.size, s.other_products.size] }
gsus_scrapes.map do |s|
prods = 0
other = 0
gsus_scrapes.map do |s|
  prods += s.products
gsus_scrapes.map do |s|
  prods += s.products.size
  other += s.other_products.size
end
prods
other
exit
gsus_scrapes = Scrape.where(id: [1, 3])
Scrape.where source_id: 1
gsus_scrapes = Scrape.where(id: [1, 3])
prods = 0
other = 0
gsus_scrapes.map do |s|
  prods += s.products.size
  other += s.other_products.size
end
Scape.find 1
Scrape.find 1
Scrape.find 3
gsus_scrapes
gsus_scrapes.map do |s|
prods = 0
other = 0
gsus_scrapes.map do |s|
  prods += s.products.size
  other += s.other_products.size
end
prods
other
gsus_prods = Products.includes(:scrapes).where('scrapes.source_id in (?)', 1)
gsus_prods = Product.includes(:scrapes).where('scrapes.source_id in (?)', 1)
gsus_prods = Product.includes(:scrapes).where('scrapes.source_id in (?)', 1).references(:scrapes)
gsus_prods.size
gsus_other_prods = OtherProduct.includes(:scrapes).where('scrapes.source_id in (?)', 1).references(:scrapes)
gsus_other_prods.size
gsus_prods.size
gsus.scrapes
gsus = Source.find 1
gsus.scrapes
products = 0
other_products = 0
has_image = 0
has_description = 0
has_sellers = 0
has_alt_models = 0
has_snapshot = 0
has_price = 0
gsus.scrapes.each do |s|
  products = s.products
gsus.scrapes.each do |s|
  prods = s.products
  others = s.other_products
  products += s.products.size
  other_products = s.other_products.size
gsus.scrapes.each do |s|
  prods = s.products
  others = s.other_products
  products += s.products.size
  other_products += s.other_products.size
  has_image += ProductImage.where(assetable_id: prods.ids).size
  has_sellers += prods.select { |p| p.sellers.size > 0 }.size
gsus.scrape.first.products.first
gsus.scrapes.first.products.first
p = _
p.price
p.product_description
p.detail
p.detail.data_hash
p.sellers
prods = gsus.scapes.inject([]) {|s, arr| arr << s.products }
prods = gsus.scrapes.inject([]) {|s, arr| arr << s.products }
prods = gsus.scrapes.inject([]) {|arr, s| arr << s.products }
arr.size
arr.count
prods.count
prods = gsus.scrapes.inject([]) {|arr, s| arr << s.products }
prods
prod
prods.count
prods.countprods.fist.count
exit
prods = gsus.scrapes.inject([]) {|arr, s| arr << s.products }
gsus = Source.find 1
prods = gsus.scrapes.inject([]) {|arr, s| arr << s.products }
prods.count
prods.flatten
prods.flatten!
prods.count
p = prods.first
p.scrape
p.scrapes
p.source
p.sources
Product.where(source_ids: 1)
p
p.scrape_product_sellers
p.scrape_product_sellers.count
sps = ScrapeProductSellers.includes(
  scrape: [:source_id],
  :product,
:seller).
where(source_id: 1)
sps = ScrapeProductSellers.includes(scrape: [:source_id], :product, :seller).where(source_id: 1)
sps = ScrapeProductSellers.includes(:product, :seller, scrape: [:source_id]).where(source_id: 1)
sps = ScrapeProductSeller.includes(:product, :seller, scrape: [:source_id]).where(source_id: 1)
sps = ScrapeProductSeller.includes(:product, :seller, scrape: [:source_id]).where(source_id: 1).references(:scrapes)
sps.size
sps = ScrapeProductSeller.includes(:product, :seller, scrape: [:source_id]).where(id: 1).references(:sources)
sps.size
sps = ScrapeProductSeller.includes(:product, :seller, scrape: [:source]).where(id: 1).references(:sources)
sps.size
sps = ScrapeProductSeller.includes(:product, :seller, scrape: [:source]).where('sources.id = 1').references(:sources)
sps.size
sps.uniq(:product_id)
uniq_prods = _
uniq_prods.size
uniq_prods = Product.where(id: sps.pluck(:product_id).uniq)
uniq_prods.size
gsus.scrapes.sum(&:num_products)
gsus.scrapes.inject(&:num_products)
num_prods = 0
num_other = 0
gsus.scrapes.each do |s|
  num_prods += s.products.size
  num_other += s.other_products.size
end
num_prods
num_other
uniq_prods
num_prods
num_other
uniq_prods.size
p = gsus.scrapes.first.products.first
p.scrape_product_sellers
uniq_prods.uniq.size
num_prods
gsus.scrapes.each do |s|
g_prods = 0
gsus.scrapes.each do |s|
  g_prods = s.products.uniq.size
end
g_prods
g_prods = 0
gsus.scrapes.each do |s|
  g_prods += s.products.uniq.size
end
g_prods
exit
sps = ScrapeProductSeller.includes(:product, :seller, scrape: [:source]).where('sources.id = 1').references(:sources)
ops = ScrapeOtherProductSeller.includes(:other_product, :seller, scrape: [:source]).where('sources.id = 1').references(:sources)
uniq_prods = Product.where(id: sps.pluck(:product_id).uniq)
uniq_other_prods = OtherProduct.where(id: sps.pluck(:other_product_id).uniq)
uniq_other_prods = OtherProduct.where(id: ops.pluck(:other_product_id).uniq)
uniq_prods.size
uniq_other_prods
uniq_other_prods.size
gsus
gsus = Source.find(1)
gsus.scrape_ds
gsus.scrape_ids
ProductImage.where(scrape_id: gsus.scrape_ids).size
ProductImage.where(scrape_id: gsus.scrape_ids, assetable_id: uniq_prods.ids).size
ProductImage.where(scrape_id: gsus.scrape_ids, assetable_id: uniq_prods.ids).distinct(:assetable_id).size
ProductImage.where(scrape_id: gsus.scrape_ids, assetable_id: uniq_prods.ids).map(&:assetable_id)
assetable_ids = _
assetable_ids.uniq.count
Product.find 16
Product.find(16).image
ProductImage.where(scrape_id: gsus.scrape_ids, assetable_id: uniq_prods.ids).select(:assetable_id).uniq.size
uniq_prods
uniq_prods.where.not(product_description: nil).size
sps.where.not(seller_id: nil).size
sps.where.not(seller_id: nil).select(:product_id).size
sps.where.not(seller_id: nil).select(:product_id).distinct.size
sps.where.not(seller_id: nil).select(:product_id)
sps.where.not(seller_id: nil).pluck(:product_id).uniq.size
sps.where(seller_id: nil).pluck(:product_id).uniq.size
sps.where(seller_id: nil).pluck(:product_id).uniq
Product.find(1)
p = _
p.seller_ids
uniq_prods.select {|p| p.sellers.empty?}.to_sql
no_sellers = uniq_prods.select {|p| p.sellers.empty?}
no_sellers.count
Product.find(1120)
Product.find(1120).sellers
Product.find(1120).min_price
no_sellers.size
sellers = uniq_prods.reject {|p| p.sellers.empty?}
sellers.size
sellers.size + no_sellers.size
ProductsAlternateModel.where(id: uniq_prods.ids).size
ProductsAlternateModel.where(id: uniq_prods.ids).pluck(:product_id).uniq.size
5961 - 1371
p = uniq_prods.first
Product.has_models
uniq_models.first.models
uniq_prods.first.models
no_mods = uniq_prods.select {|p| p.models.empty?}
no_mods.size
models = uniq_prods.reject {|p| p.models.empty?}
models.size
no_mods.size
no_mods.size + models.size
ProductsAlternateModel.where(id: uniq_prods.ids).pluck(:product_id).uniq.size
5961 - 1371
no_mods
no_mods.pluck(:id).uniq.size
no_mods.map(&:id)
no_mods.map(&:id).uniq.size
p.model
p.models
uniq_prods.select(&:screenshots).size
p.screenshots
p.screenshot_ids
uniq_prods.reject{|p| p.screenshot_ids.empty?}.size
ProductScreenshot.where(assetable_id: uniq_prods.ids).pluck(:assetable_id).uniq.size
ProductScreenshot.where(assetable_id: uniq_prods.ids).pluck(:assetable_id)
ProductScreenshot.where(assetable_id: uniq_prods.ids)
ProductScreenshot.where(assetable_id: uniq_prods.ids).pluck(:assetable_id)
reload!
ProductScreenshot.where(assetable_id: uniq_prods.ids).pluck(:assetable_id)
ProductScreenshot.where(assetable_id: uniq_prods.ids).pluck(:assetable_id).uniq.count
p
p.price
p.min_price
sps.where(min_price: nil)
sps.where.not(min_price: nil).pluck(:product_id).uniq.size
5961 - 5902
sps.where.(min_price: nil).pluck(:product_id).pluck(:product_id).uniq.size
sps.where.(min_price: nil).pluck(:product_id).uniq.size
alius = Source.find(2)
sps = ScrapeProductSeller.includes(:product, :seller, scrape: [:source]).where('sources.id = 2').references(:sources)
ops = ScrapeOtherProductSeller.includes(:other_product, :seller, scrape: [:source]).where('sources.id = 2').references(:sources)
uniq_prods = Product.where(id: sps.pluck(:product_id).uniq)
uniq__other_prods = OtherProduct.where(id: ops.pluck(:other_product_id).uniq)
uniq_other_prods.size
uniq_prods.size
uniq_prods.uniq.size
uniq_prods.size
uniq_other_prods.size
uniq_other_prods = OtherProduct.where(id: ops.pluck(:other_product_id).uniq)
uniq_other_prods.size
ProductScreenshot.where(assetable_id: uniq_prods.ids).pluck(:assetable_id).uniq.count
uniq_prods.reject{|p| p.screenshot_ids.empty?}.size
sps.where.not(min_price: nil).pluck(:product_id).uniq.size
uniq_other_prods.size
uniq_prods.where.not(product_description: nil).size
reload!
uniq_prods.where.not(product_description: nil).size
uniq_prods.where(product_description: nil).size
uniq_prods.first
uniq_prods.where(product_description: nil).first
uniq_prods.reject {|p| p.sellers.empty?}
uniq_prods.include(:seller).reject {|p| p.sellers.empty?}
uniq_prods.includes(:seller).reject {|p| p.sellers.empty?}
uniq_prods.includes(scrape_product_seller: :seller).reject {|p| p.sellers.empty?}
uniq_prods.includes(scrape_product_sellers: :seller).reject {|p| p.sellers.empty?}
uniq_prods.includes(scrape_product_sellers: [:seller]).where.not('scrape_product_sellers.seller_id is NULL').pluck(:id).size
sps.where(seller_id: nil)
sps.where(seller_id: nil).sieze
sps.where(seller_id: nil).size
reload!
sps.where(seller_id: nil).size
uniq_prods.includes(scrape_product_sellers: [:seller]).where.not('scrape_product_sellers.seller_id is NULL').pluck(:id).uniq.size
sps.where.not(seller_id: nil).pluck_product_id.uniq.size
sps.where.not(seller_id: nil).pluck(:product_id).uniq.size
uniq_prods.includes(scrape_product_sellers: [:seller]).where.not('scrape_product_sellers.seller_id is NULL').pluck(:id).uniq.size
old_sps = sps
sps = OtherProduct.where(id: ops.pluck(:other_product_id).uniq)
sps.size
old_sps.size
sps = ScrapeProductSeller.includes(:product, :seller, scrape: [:source]).where('sources.id = ?', id).references(:sources)
sps = ScrapeProductSeller.includes(:product, :seller, scrape: [:source]).where('sources.id = ?', 3).references(:sources)
sps.size
sps = ScrapeProductSeller.includes(:product, :seller, scrape: [:source]).where('sources.id = ?', 2).references(:sources)
sps.size
old_sps.size
Source.find(1).name
no_mods = uniq_prods.select {|p| p.models.empty?}
no_mods.size
ProductsAlternateModel.where(id: uniq_prods.ids).pluck(:product_id).uniq.size
no_mods.size
num = _
uniq_prods.size - num
ProductsAlternateModel.where(id: uniq_prods.ids).pluck(:product_id).uniq.size
num = _
uniq_prods.size - num
no_mods.size
no_mods = uniq_prods.select {|p| p.models.empty?}
no_mods.count
num
num + no_mods.count
num = ProductsAlternateModel.where(product_id: uniq_prods.ids).pluck(:product_id).uniq.size
no_mods.count
no_mods.count + num
uniq_prods - num
uniq_prods.size - num
source_data = {}
sources = Source.where(id: [1,2,3,4])
)
uniq_other_prods = OtherProduct.where(id: ops.pluck(:other_product_id).uniq)
OOAOO
source = Source.find(1)
sps = ScrapeProductSeller.includes(:product, :seller, scrape: [:source]).where('sources.id = ?', source.id).references(:sources)
sps.where.not(min_price: nil).pluck(:product_id).uniq.size
ProductImage.where(scrape_id: Source.find(1).scrape_ids, assetable_id: uniq_prods.ids).pluck(:assetable_id).uniq.size
exit
Source.find(4).scrapes
amazon = Source.find(2)
amazon = Source.find(3)
amazon.scrapes
Category.count
Category.first
Category.first.parent
Category.last
cats = 'Consumer\ Electronics'.split('>')
cats
cats.first.strip
Category.where.not(parent_id: nil)
Category.find_by(name: 'Gaming')
exit
load '.tmp.rb'
load 'tmp.rb'
Category.first
Category.last
Category.size
Category.count
sps = ScrapeProductSeller.includes(:seller, product: [:category], scrape: [:source]).where('categories.id = ?', 1).references(:categories).where('sources.id = ?', 1).references(:sources)
sps.size
sps = ScrapeProductSeller.includes(:seller, product: [:categories], scrape: [:source]).where('categories.id = ?', 1).references(:categories).where('sources.id = ?', 1).references(:sources)
sps.size
uniq_prods = Product.where(id: sps.pluck(:product_id).uniq)
uniq_prods.size
load 'tmp.rb'
exit
Category.sample 3
Category.where(id: Category.ids.sample(5))
exit
Source.first
s = _
s.categories
Source.find(1).categories
Source.find(1).categories.size
exit
s = Source.find 1
s.categories.first
data = {
  "sources": {
    "gsus": {
      "product_count": 5961,
      "other_product_count": 6614,
      "has_image": 5239,
      "description": 5460,
      "sellers": 5903,
      "no_alt_models": 3649,
      "screenshot": 5961,
      "w_price": 5902,
      "categories": {
        "Consumer Electronics": {
          "product_count": 5538,
          "other_product_count": 4996,
          "has_image": 4853,
          "description": 5049,
          "sellers": 5486,
          "no_alt_models": 3447,
          "screenshot": 5538,
          "w_price": 5485
        },
        "TV & Video": {
          "product_count": 1023,
          "other_product_count": 1149,
          "has_image": 903,
          "description": 953,
          "sellers": 990,
          "no_alt_models": 666,
          "screenshot": 1023,
          "w_price": 989
        },
        "High Definition": {
          "product_count": 222,
          "other_product_count": 12,
          "has_image": 206,
          "description": 214,
          "sellers": 220,
          "no_alt_models": 127,
          "screenshot": 222,
          "w_price": 219
        },
        "1080p": {
          "product_count": 179,
          "other_product_count": 13,
          "has_image": 164,
          "description": 173,
          "sellers": 177,
          "no_alt_models": 99,
          "screenshot": 179,
          "w_price": 176
        },
        "4k": {
          "product_count": 81,
          "other_product_count": 132,
          "has_image": 80,
          "description": 71,
          "sellers": 81,
          "no_alt_models": 61,
          "screenshot": 81,
          "w_price": 81
        },
        "720p": {
          "product_count": 94,
          "other_product_count": 46,
          "has_image": 82,
          "description": 86,
          "sellers": 84,
          "no_alt_models": 64,
          "screenshot": 94,
          "w_price": 84
        },
        "Under 32 inch": {
          "product_count": 104,
          "other_product_count": 21,
          "has_image": 99,
          "description": 98,
          "sellers": 103,
          "no_alt_models": 66,
          "screenshot": 104,
          "w_price": 102
        },
        "32-40 inch": {
          "product_count": 115,
          "other_product_count": 22,
          "has_image": 103,
          "description": 107,
          "sellers": 113,
          "no_alt_models": 80,
          "screenshot": 115,
          "w_price": 113
        },
        "40-48 inch": {
          "product_count": 232,
          "other_product_count": 63,
          "has_image": 196,
          "description": 211,
          "sellers": 214,
          "no_alt_models": 166,
          "screenshot": 232,
          "w_price": 213
        },
        "48-55 inch": {
          "product_count": 120,
          "other_product_count": 16,
          "has_image": 110,
          "description": 111,
          "sellers": 119,
          "no_alt_models": 79,
          "screenshot": 120,
          "w_price": 119
        },
        "55-65 inch": {
          "product_count": 99,
          "other_product_count": 45,
          "has_image": 96,
          "description": 93,
          "sellers": 97,
          "no_alt_models": 59,
          "screenshot": 99,
          "w_price": 97
        },
        "Over 65 inch": {
          "product_count": 74,
          "other_product_count": 9,
          "has_image": 72,
          "description": 69,
          "sellers": 73,
          "no_alt_models": 45,
          "screenshot": 74,
          "w_price": 73
        },
        "DVD & Blu-ray Players (VCD)": {
          "product_count": 163,
          "other_product_count": 303,
          "has_image": 130,
          "description": 160,
          "sellers": 163,
          "no_alt_models": 109,
          "screenshot": 163,
          "w_price": 163
        },
        "Digital Video Recorders": {
          "product_count": 58,
          "other_product_count": 12,
          "has_image": 53,
          "description": 58,
          "sellers": 58,
          "no_alt_models": 29,
          ot": 58,
          "w_price": 58
},
"Media Streaming Devices": {
"product_count": 86,
"other_product_count": 206,
"has_image": 79,
"description": 86,
"sellers": 85,
"no_alt_models": 48,
"screenshot": 86,
"w_price": 85
},
"Multimedia Projectors": {
"product_count": 49,
"other_product_count": 28,
"has_image": 47,
"description": 49,
"sellers": 49,
"no_alt_models": 35,
"screenshot": 49,
"w_price": 49
},
"Computer & Devices": {
"product_count": 2616,
"other_product_count": 1854,
"has_image": 2250,
"description": 2322,
"sellers": 2604,
"no_alt_models": 1705,
"screenshot": 2616,
"w_price": 2604
},
"Computer Components": {
"product_count": 1129,
"other_product_count": 708,
"has_image": 953,
"description": 960,
"sellers": 1126,
"no_alt_models": 817,
"screenshot": 1129,
"w_price": 1126
},
"Network Storage Systems": {
"product_count": 103,
"other_product_count": 24,
"has_image": 83,
"description": 89,
"sellers": 102,
"no_alt_models": 77,
"screenshot": 103,
"w_price": 102
},
"Audio Cards & Adapters": {
"product_count": 48,
"other_product_count": 4,
"has_image": 46,
"description": 45,
"sellers": 48,
"no_alt_models": 27,
"screenshot": 48,
"w_price": 48
},
"Computer Processors": {
"product_count": 105,
"other_product_count": 119,
"has_image": 90,
"description": 87,
"sellers": 105,
"no_alt_models": 81,
"screenshot": 105,
"w_price": 105
},
"KVM Switches": {
"product_count": 97,
"other_product_count": 22,
"has_image": 71,
"description": 89,
"sellers": 97,
"no_alt_models": 70,
"screenshot": 97,
"w_price": 97
},
"Hard Drives": {
"product_count": 291,
"other_product_count": 57,
"has_image": 238,
"description": 239,
"sellers": 290,
"no_alt_models": 222,
"screenshot": 291,
"w_price": 290
},
"Motherboards": {
"product_count": 120,
"other_product_count": 4,
"has_image": 101,
"description": 109,
"sellers": 120,
"no_alt_models": 93,
"screenshot": 120,
"w_price": 120
},
"Modems": {
"product_count": 55,
"other_product_count": 197,
"has_image": 53,
"description": 51,
"sellers": 54,
"no_alt_models": 30,
"screenshot": 55,
"w_price": 54
},
"USB Flash Drives": {
"product_count": 73,
"other_product_count": 3,
"has_image": 70,
"description": 45,
"sellers": 73,
"no_alt_models": 37,
"screenshot": 73,
"w_price": 73
},
"Graphic Cards & Video Adapters": {
"product_count": 60,
"other_product_count": 0,
"has_image": 41,
"description": 57,
"sellers": 60,
"no_alt_models": 39,
"screenshot": 60,
"w_price": 60
},
"Network Cards & Adapters": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"RAM": {
"product_count": 80,
"other_product_count": 220,
"has_image": 73,
"description": 50,
"sellers": 80,
"no_alt_models": 50,
"screenshot": 80,
"w_price": 80
},
"Power Supplies": {
"product_count": 89,
"other_product_count": 43,
"has_image": 79,
"description": 72,
"sellers": 89,
"no_alt_models": 68,
"screenshot": 89,
"w_price": 89
},
"Computer Peripherals": {
"product_count": 513,
"other_product_count": 680,
"has_image": 452,
"description":          "descriptilers": 513,
"no_alt_models": 274,
"screenshot": 513,
"w_price": 513
},
"Keyboard": {
"product_count": 195,
"other_product_count": 604,
"has_image": 177,
"description": 189,
"sellers": 195,
"no_alt_models": 107,
"screenshot": 195,
"w_price": 195
},
"Graphics Tablets": {
"product_count": 27,
"other_product_count": 10,
"has_image": 20,
"description": 27,
"sellers": 27,
"no_alt_models": 12,
"screenshot": 27,
"w_price": 27
},
"Webcams": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Computer Monitors": {
"product_count": 101,
"other_product_count": 40,
"has_image": 86,
"description": 98,
"sellers": 101,
"no_alt_models": 60,
"screenshot": 101,
"w_price": 101
} 
"Scanners": {
"product_count": 67,
"other_product_count": 6,
"has_image": 63,
"description": 67,
"sellers": 67,
"no_alt_models": 31,
"screenshot": 67,
"w_price": 67
},
"Computer Accessories": {
"product_count": 534,
"other_product_count": 199,
"has_image": 469,
"description": 472,
"sellers": 531,
"no_alt_models": 305,
"screenshot": 534,
"w_price": 531
},
"Memory Card Readers": {
"product_count": 100,
"other_product_count": 91,
"has_image": 86,
"description": 84,
"sellers": 99,
"no_alt_models": 76,
"screenshot": 100,
"w_price": 99
},
"Routers": {
"product_count": 149,
"other_product_count": 18,
"has_image": 129,
"description": 148,
"sellers": 149,
"no_alt_models": 74,
"screenshot": 149,
"w_price": 149
},
"Flash Memory Cards": {
"product_count": 50,
"other_product_count": 14,
"has_image": 47,
"description": 48,
"sellers": 49,
"no_alt_models": 19,
"screenshot": 50,
"w_price": 49
},
"Computer Servers": {
"product_count": 156,
"other_product_count": 61,
"has_image": 136,
"description": 138,
5,
"no_alt_models": 113,
"screenshot": 156,
"w_price": 155
},
"Desktop Computers": {
"product_count": 53,
"other_product_count": 16,
"has_image": 47,
"description": 50,
"sellers": 52,
"no_alt_models": 29,
"screenshot": 53,
"w_price": 52
},
"Laptops": {
"product_count": 105,
"other_product_count": 60,
"has_image": 98,
"description": 89,
"sellers": 101,
"no_alt_models": 90,
"screenshot": 105,
"w_price": 101
},
"Tablet Computers": {
"product_count": 37,
"other_product_count": 13,
"has_image": 27,
"description": 36,
"sellers": 37,
"no_alt_models": 21,
"screenshot": 37,
"w_price": 37
},
"E-Readers": {
"product_count": 5,
"other_product_count": 15,
"has_image": 4,
"description": 5,
"sellers": 5,
"no_alt_models": 4,
"screenshot": 5,
"w_price": 5
},
"GPS Navigation Systems": {
"product_count": 46,
"other_product_count": 0,
"has_image": 42,
"description": 46,
"sellers": 46,
"no_alt_models": 9,
"screenshot": 46,
"w_price": 46
},
"Activity Monitors": {
"product_count": 95,
"other_product_count": 64,
"has_image": 80,
"description": 90,
"sellers": 95,
"no_alt_models": 62,
"screenshot": 95,
"w_price": 95
},
"Telephone": {
"product_count": 152,
"other_product_count": 119,
"has_image": 135,
"description": 143,
"sellers": 152,
"no_alt_models": 87,
"screenshot": 152,
"w_price": 152
},
"Mobile Phones": {
"product_count": 101,
"other_product_count": 35,
"has_image": 85,
"description": 96,
"sellers": 101,
"no_alt_models": 64,
"screenshot": 101,
"w_price": 101
},
"Radios": {
"product_count": 41,
"other_product_count": 7,
"has_image": 37,
"description": 39,
"sellers": 41,
"no_alt_models": 22,
"screenshot": 41,
"w_price": 41
},
"Smartwatch": {
"product_count": 37,
"other_product_count": 172,
"has_image": 32,
"description": 37,
"sellers": 35,
"no_alt_models": 28,
"screenshot": 37,
"w_price": 35
},
"printers, copiers and fax machines": {
"product_count": 90,
"other_product_count": 0,
"has_image": 81,
"description": 90,
"sellers": 90,
"no_alt_models": 38,
"screenshot": 90,
"w_price": 90
},
"PDAs": {
"product_count": 18,
"other_product_count": 27,
"has_image": 18,
"description": 16,
"sellers": 18,
"no_alt_models": 12,
"screenshot": 18,
"w_price": 18
},
"Home Security": {
"product_count": 99,
"other_product_count": 89,
"has_image": 83,
"description": 93,
"sellers": 98,
"no_alt_models": 80,
"screenshot": 99,
"w_price": 98
},
"Digital Cameras": {
"product_count": 44,
"other_product_count": 0,
"has_image": 40,
"description": 44,
"sellers": 44,
"no_alt_models": 17,
"screenshot": 44,
"w_price": 44
},
"Surveillance Cameras": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Video Cameras": {
"product_count": 61,
"other_product_count": 38,
"has_image": 58,
"description": 59,
"sellers": 61,
"no_alt_models": 38,
"screenshot": 61,
"w_price": 61
},
"Digital": {
"product_count": 72,
"other_product_count": 52,
"has_image": 64,
"description": 48,
"sellers": 72,
"no_alt_models": 52,
"screenshot": 72,
"w_price": 72
},
"Game Consoles": {
"product_count": 42,
"other_product_count": 201,
"has_image": 37,
"description": 41,
"sellers": 42,
"no_alt_models": 32,
"screenshot": 42,
"w_price": 42
},
"Game Controller": {
"product_count": 84,
"other_product_count": 165,
"has_image": 73,
"description": 81,
"sellers": 84,
"no_alt_models": 39,
"screenshot": 84,
"w_price": 84
},
"Audio": {
"product_count": 977,
"other_product_count": 634,
"has_image": 876,
"description": 920,
"sellers": 975,
"no_alt_models": 516,
"screenshot": 977,
"w_price": 975
},
"Audio Components": {
"product_count": 412,
"other_product_count": 105,
"has_image": 355,
"description": 406,
"sellers": 411,
"no_alt_models": 220,
"screenshot": 412,
"w_price": 411
},
"Home Theater Systems": {
"product_count": 3,
"other_product_count": 0,
"has_image": 3,
"description": 3,
"sellers": 3,
"no_alt_models": 1,
"screenshot": 3,
"w_price": 3
},
"Speakers": {
"product_count": 46,
"other_product_count": 5,
"has_image": 40,
"description": 46,
"sellers": 46,
"no_alt_models": 22,
"screenshot": 46,
"w_price": 46
},
"Public Address Systems": {
"product_count": 51,
"other_product_count": 9,
"has_image": 40,
"description": 51,
"sellers": 51,
"no_alt_models": 28,
"screenshot": 51,
"w_price": 51
},
"Power & Headphone Amplifiers": {
"product_count": 34,
"other_product_count": 34,
"has_image": 32,
"description": 33,
"sellers": 34,
"no_alt_models": 22,
"screenshot": 34,
"w_price": 34
},
"Equilizers": {
"product_count": 29,
"other_product_count": 25,
"has_image": 29,
"description": 28,
"sellers": 29,
"no_alt_models": 17,
"screenshot": 29,
"w_price": 29
},
"Audio Accessories": {
"product_count": 299,
"other_product_count": 147,
"has_image": 266,
"description": 278,
"sellers": 298,
"no_alt_models": 143,
"screenshot": 299,
"w_price": 298
},
"Headphones & Headsets": {
"product_count": 18,
"other_product_count": 2,
"has_image": 17,
"description": 17,
"sellers": 18,
"no_alt_models": 1,
"screenshot": 18,
"w_price": 18
},
"Microphones": {
"product_count": 172,
"other_product_count": 81,
"has_image": 153,
"description": 161,
"sellers": 171,
"no_alt_models": 77,
"screenshot": 172,
"w_price": 171
},
"MP3 Players": {
"product_count": 19,
"other_product_count": 8,
"has_image": 18,
"description": 19,
"sellers": 19,
"no_alt_models": 10,
"screenshot": 19,
"w_price": 19          "w_price": 19    Recorders": {
"product_count": 33,
"other_product_count": 122,
"has_image": 32,
"description": 31,
"sellers": 33,
"no_alt_models": 14,
"screenshot": 33,
"w_price": 33
},
"Boomboxes & Stereo Systems": {
"product_count": 103,
"other_product_count": 75,
"has_image": 101,
"description": 81,
"sellers": 103,
"no_alt_models": 80,
"screenshot": 103,
"w_price": 103
},
"Karaoke Systems": {
"product_count": 42,
"other_product_count": 110,
"has_image": 40,
"description": 39,
"sellers": 42,
"no_alt_models": 19,
"screenshot": 42,
"w_price": 42
},
"Turntables": {
"product_count": 36,
"other_product_count": 12,
"has_image": 36,
"description": 36,
"sellers": 36,
"no_alt_models": 15,
"screenshot": 36,
"w_price": 36
},
"MP4 Players": {
"product_count": 11,
"other_product_count": 48,
"has_image": 10,
"description": 11,
"sellers": 11,
"no_alt_models": 7,
"screenshot": 11,
"w_price": 11
},
"Lighting (LED)": {
"product_count": 151,
"other_product_count": 698,
"has_image": 140,
"description": 145,
"sellers": 149,
"no_alt_models": 76,
"screenshot": 151,
"w_price": 149
},
"LED Light Bulbs": {
"product_count": 111,
"other_product_count": 161,
"has_image": 94,
"description": 105,
"sellers": 109,
"no_alt_models": 69,
"screenshot": 111,
"w_price": 109
},
"Ceiling Lighting Fixtures (LED)": {
"product_count": 109,
"other_product_count": 198,
"has_image": 103,
"description": 109,
"sellers": 107,
"no_alt_models": 51,
"screenshot": 109,
"w_price": 107
},
"LED": {
"product_count": 47,
"other_product_count": 166,
"has_image": 44,
"description": 44,
"sellers": 47,
"no_alt_models": 33,
"screenshot": 47,
"w_price": 47
},
"Desk (LED)": {
"product_count": 65,
"other_product_count": 147,
"has_image": 64,
"description": 63,
"sellers": 65,
"no_alt_models": 17,
"screenshot": 65,
"w_price": 65
},
"Video Camera Lights (LED)": {
"product_count": 14,
"other_product_count": 38,
"has_image": 14,
"description": 14,
"sellers": 14,
"no_alt_models": 10,
"screenshot": 14,
"w_price": 14
},
"Solar Panels": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Miffy": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
}
},
"alius": {
"product_count": 3487,
"other_product_count": 5129,
"has_image": 3363,
"description": 61,
"sellers": 3444,
"no_alt_models": 3435,
"screenshot": 3460,
"w_price": 1248,
"categories": {
"Consumer Electronics": {
"product_count": 3487,
"other_product_count": 5129,
"has_image": 3363,
"description": 61,
"sellers": 3444,
"no_alt_models": 3435,
"screenshot": 3460,
"w_price": 1248
},
"TV & Video": {
"product_count": 138,
"other_product_count": 285,
"has_image": 130,
"description": 1,
"sellers": 138,
"no_alt_models": 138,
"screenshot": 138,
"w_price": 54
},
"Televisions": {
"product_count": 4,
"other_product_count": 11,
"has_image": 4,
"description": 0,
"sellers": 4,
"no_alt_models": 4,
"screenshot": 4,
"w_price": 1
},
"High Definition": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"1080p": {
"product_count": 2,
"other_product_count": 7,
"has_image": 2,
"description": 0,
"sellers": 2,
"no_alt_models": 2,
"screenshot": 2,
"w_price": 1
},
"4k": {
"product_count": 0,
"other_product_count": 6,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"720p": {
"product_count": 1,
"other_product_count": 1,
"has_image": 1,
"description": 0,
"sellers": 1,
"no_alt_models": 1,
"screenshot": 1,
"w_price": 0
},
"Under 32 inch": {
"product_count": 2,
"other_product_count": 3,
"has_image": 2,
"description": 0,
"sellers": 2,
"no_alt_models": 2,
"screenshot": 2,
"w_price": 0
},
"48-55 inch": {
"product_count": 1,
"other_product_count": 0,
"has_image": 1,
"description": 0,
"sellers": 1,
"no_alt_models": 1,
"screenshot": 1,
"w_price": 1
},
"Over 65 inch": {
"product_count": 0,
"other_product_count": 2,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"DVD & Blu-ray Players (VCD)": {
"product_count": 36,
"other_product_count": 126,
"has_image": 33,
"description": 0,
"sellers": 36,
"no_alt_models": 36,
"screenshot": 36,
"w_price": 13
},
"Digital Video Recorders": {
"product_count": 52,
"other_product_count": 32,
"has_image": 48,
"description": 0,
"sellers": 52,
"no_alt_models": 52,
"screenshot": 52,
"w_price": 17
},
"Media Streaming Devices": {
"product_count": 10,
"other_product_count": 5,
"has_image": 10,
"description": 0,
"sellers": 10,
"no_alt_models": 10,
"screenshot": 10,
"w_price": 7
},
"Multimedia Projectors": {
"product_count": 4,
"other_product_count": 7,
"has_image": 4,
"description": 0,
"sellers": 4,
"no_alt_models": 4,
"screenshot": 4,
"w_price": 2
},
"Computer & Devices": {
"product_count": 2292,
"other_product_count": 2661,
"has_image": 2199,
"description": 46,
"sellers": 2256,
"no_alt_models": 2257,
"screenshot": 2270,
"w_price": 809
},
"Routers": {
"product_count": 131,
"other_product_count": 115,
"has_image": 127,
"description": 7,
"sellers": 131,
"no_alt_models": 127,
"screenshot": 131,
"w_price": 17
},
"Network Storage Systems": {
"product_count": 83,
"other_product_count": 39,
"has_image": 83,
"description": 0,
"sellers": 83,
"no_alt_models": 83,
"screenshot": 83,
"w_price": 1
},
"Audio Cards & Adapters": {
"product_count": 2,
"other_product_count": 11,
"has_image": 2,
"description": 0,
"sellers": 2,
"no_alt_models": 2,
"screenshot": 2,
"w_price": 0
},
"Computer Processors": {
"product_count": 27,
"other_product_count": 82,
"has_image": 22,
"description": 7,
"sellers": 27,
"no_alt_models": 26,
"screenshot": 27,
"w_price": 5
},
"KVM Switches": {
"product_count": 30,
"other_product_count": 27,
"has_image": 30,
"description": 4,
"sellers": 30,
"no_alt_models": 29,
"screenshot": 30,
"w_price": 14
},
"Hard Drives": {
"product_count": 129,
"other_product_count": 199,
"has_image": 123,
"description": 6,
"sellers": 129,
"no_alt_models": 127,
"screenshot": 129,
"w_price": 20
},
"Motherboards": {
"product_count": 10,
"other_product_count": 17,
"has_image": 10,
"description": 1,
"sellers": 10,
"no_alt_models": 9,
"screenshot": 10,
"w_price": 4
},
"Modems": {
"product_count": 70,
"other_product_count": 123,
"has_image": 67,
"description": 8,
"sellers": 69,
"no_alt_models": 67,
"screenshot": 70,
"w_price": 16
},
"Optical Drives": {
"product_count": 91,
"other_product_count": 59,
"has_image": 91,
"description": 0,
"sellers": 91,
"no_alt_models": 90,
"screenshot": 91,
"w_price": 4
},
"Graphic Cards & Video Adapters": {
"product_count": 18,
"other_product_count": 10,
"has_image": 18,
"description": 2,
"sellers": 18,
"no_alt_models": 18,
"screenshot": 18,
"w_price": 0
},
"Network Cards & Adapters": {
"product_count": 199,
"other_product_count": 47,
"has_image": 195,
"description": 5,
"sellers": 199,
"no_alt_models": 196,
"screenshot": 199,
"w_price": 100
},
"RAM": {
"product_count": 117,
"other_product_count": 314,
"has_image": 105,
"description": 1,
"sellers": 115,
"no_alt_models": 114,
"screenshot": 116,
"w_price": 21
},
"Power Supplies": {
"product_count": 21,
"other_product_count": 14,
"has_image": 17,
"description": 0,
"sellers": 17,
"no_alt_models": 20,
"screenshot": 19,
"w_price": 8
},
"Keyboard": {
"product_count": 40,
"other_product_count": 33,
"has_image": 40,
"description": 3,
"sellers": 40,
"no_alt_models": 36,
"screenshot": 40,
"w_price": 17
},
"Graphics Tablets": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Webcams": {
"product_count": 24,
"other_product_count": 47,
"has_image": 23,
"description": 0,
"sellers": 24,
"no_alt_models": 24,
"screenshot": 24,
"w_price": 17
},
"Computer Monitors": {
"product_count": 37,
"other_product_count": 95,
"has_image": 37,
"description": 0,
"sellers": 37,
"no_alt_models": 35,
"screenshot": 37,
"w_price": 12
},
"Scanners": {
"product_count": 28,
"other_product_count": 75,
"has_image": 27,
"description": 0,
"sellers": 28,
"no_alt_models": 28,
"screenshot": 28,
"w_price": 13
},
"Memory Card Readers": {
"product_count": 23,
"other_product_count": 54,
"has_image": 23,
"description": 0,
"sellers": 23,
"no_alt_models": 23,
"screenshot": 23,
"w_price": 12
},
"USB Flash Drives": {
"product_count": 10,
"other_product_count": 18,
"has_image": 10,
"description": 0,
"sellers": 10,
"no_alt_models": 9,
"screenshot": 10,
"w_price": 6
},
"Flash Memory Cards": {
"product_count": 51,
"other_product_count": 194,
"has_image": 47,
"description": 0,
"sellers": 51,
"no_alt_models": 50,
"screenshot": 51,
"w_price": 23
},
"Computer Servers": {
"product_count": 43,
"other_product_count": 91,
"has_image": 40,
"description": 2,
"sellers": 43,
"no_alt_models": 40,
"screenshot": 43,
"w_price": 14
},
"Desktop Computers": {
"product_count": 162,
"other_product_count": 98,
"has_image": 162,
"description": 3,
"sellers": 162,
"no_alt_models": 159,
"screenshot": 162,
"w_price": 17
},
"Laptops": {
"product_count": 42,
"other_product_count": 72,
"has_image": 40,
"description": 0,
"sellers": 42,
"no_alt_models": 42,
"screenshot": 42,
"w_price": 7
},
"Tablet Computers": {
"product_count": 87,
"other_product_count": 60,
"has_image": 85,
"description": 3,
"sellers": 82,
"no_alt_models": 84,
"screenshot": 85,
"w_price": 29
},
"E-Readers": {
"product_count": 25,
"other_product_count": 13,
"has_image": 25,
"description": 0,
"sellers": 25,
"no_alt_models": 25,
"screenshot": 25,
"w_price": 14
},
"GPS Navigation Systems": {
"product_count": 98,
"other_product_count": 166,
"has_image": 94,
"description": 0,
"sellers": 98,
"no_alt_models": 98,
"screenshot": 98,
"w_price": 39
},
"Activity Monitors": {
"product_count": 45,
"other_product_count": 14,
"has_image": 45,
"description": 0,
"sellers": 45,
"no_alt_models": 45,
"screenshot": 45,
"w_price": 5
},
"Telephone": {
"product_count": 171,
"other_product_count": 401,
"has_image": 165,
"description": 5,
"sellers": 171,
"no_alt_models": 168,
"screenshot": 171,
"w_price": 74
},
"Mobile Phones": {
"product_count": 31,
"other_product_count": 79,
"has_image": 30,
"description": 1,
"sellers": 31,
"no_alt_models": 30,
"screenshot": 31,
"w_price": 9
},
"Radios": {
"product_count": 34,
"other_product_count": 59,
"has_image": 34,
"description": 1,
"sellers": 34,
"no_alt_models": 33,
"screenshot": 34,
"w_price": 5
},
"Smartwatch": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"printers, copiers and fax machines": {
"product_count": 57,
"other_product_count": 95,
"has_image": 55,
"description": 4,
"sellers": 57,
"no_alt_models": 54,
"screenshot": 57,
"w_price": 20
},
"PDAs": {
"product_count": 9,
"other_product_count": 47,
"has_image": 9,
"description": 0,
"sellers": 9,
"no_alt_models": 9,
"screenshot": 9,
"w_price": 4
},
"Home Security": {
"product_count": 7,
"other_product_count": 17,
"has_image": 7,
"description": 0,
"sellers": 7,
"no_alt_models": 7,
"screenshot": 7,
"w_price": 6
},
"Digital Cameras": {
"product_count": 40,
"other_product_count": 131,
"has_image": 34,
"description": 0,
"sellers": 40,
"no_alt_models": 40,
"screenshot": 40,
"w_price": 7
},
"Surveillance Cameras": {
"product_count": 57,
"other_product_count": 19,
"has_image": 57,
"description": 0,
"sellers": 57,
"no_alt_models": 57,
"screenshot": 57,
"w_price": 16
},
"Video Cameras": {
"product_count": 80,
"other_product_count": 83,
"has_image": 80,
"description": 0,
"sellers": 80,
"no_alt_models": 78,
"screenshot": 80,
"w_price": 24
},
"Digital": {
"product_count": 40,
"other_product_count": 105,
"has_image": 40,
"description": 0,
"sellers": 40,
"no_alt_models": 40,
"screenshot": 40,
"w_price": 20
},
"Game Consoles": {
"product_count": 5,
"other_product_count": 9,
"has_image": 5,
"description": 0,
"sellers": 5,
"no_alt_models": 5,
"screenshot": 5,
"w_price": 0
},
"Game Controller": {
"product_count": 15,
"other_product_count": 30,
"has_image": 15,
"description": 1,
"sellers": 15,
"no_alt_models": 13,
"screenshot": 15,
"w_price": 11
},
"Audio": {
"product_count": 467,
"other_product_count": 935,
"has_image": 458,
"description": 10,
"sellers": 460,
"no_alt_models": 456,
"screenshot": 462,
"w_price": 160
},
"Audio Components": {
"product_count": 106,
"other_product_count": 165,
"has_image": 106,
"description": 2,
"sellers": 106,
"no_alt_models": 105,
"screenshot": 106,
"w_price": 51
},
"Power & Headphone Amplifiers": {
"product_count": 22,
"other_product_count": 15,
"has_image": 22,
"description": 0,
"sellers": 22,
"no_alt_models": 21,
"screenshot": 22,
"w_price": 11
},
"Home Theater Systems": {
"product_count": 23,
"other_product_count": 14,
"has_image": 23,
"description": 0,
"sellers": 23,
"no_alt_models": 23,
"screenshot": 23,
"w_price": 10
},
"Speakers": {
"product_count": 37,
"other_product_count": 39,
"has_image": 37,
"description": 2,
"sellers": 37,
"no_alt_models": 37,
"screenshot": 37,
"w_price": 20
},
"Public Address Systems": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Equilizers": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Headphones & Headsets": {
"product_count": 165,
"other_product_count": 233,
"has_image": 157,
"description": 4,
"sellers": 158,
"no_alt_models": 159,
"screenshot": 160,
"w_price": 53
},
"Microphones": {
"product_count": 23,
"other_product_count": 51,
"has_image": 22,
"description": 1,
"sellers": 23,
"no_alt_models": 22,
"screenshot": 23,
"w_price": 10
},
"MP3 Players": {
"product_count": 54,
"other_product_count": 244,
"has_image": 54,
"description": 1,
"sellers": 54,
"no_alt_models": 54,
"screenshot": 54,
"w_price": 28
},
"Boomboxes & Stereo Systems": {
"product_count": 24,
"other_product_count": 72,
"has_image": 24,
"description": 0,
"sellers": 24,
"no_alt_models": 24,
"screenshot": 24,
"w_price": 14
},
"Karaoke Systems": {
"product_count": 78,
"other_product_count": 32,
"has_image": 78,
"description": 1,
"sellers": 78,
"no_alt_models": 75,
"screenshot": 78,
"w_price": 1
},
"MP4 Players": {
"product_count": 10,
"other_product_count": 65,
"has_image": 10,
"description": 0,
"sellers": 10,
"no_alt_models": 10,
"screenshot": 10,
"w_price": 6
}
},
"amazonus": {
"product_count": 68283,
"other_product_count": 377776,
"has_image": 68282,
"description": 67424,
"sellers": 60059,
"no_alt_models": 67688,
"screenshot": 68283,
"w_price": 59946,
"categories": {
"TV & Video": {
"product_count": 11974,
"other_product_count": 22980,
"has_image": 11974,
"description": 11748,
"sellers": 10176,
"no_alt_models": 11782,
"screenshot": 11974,
"w_price": 10102
},
"Televisions": {
"product_count": 2030,
"other_product_count": 2757,
"has_image": 2030,
"description": 1962,
"sellers": 1425,
"no_alt_models": 1920,
"screenshot": 2030,
"w_price": 1372
},
"High Definition": {
"product_count": 1381,
"other_product_count": 460,
"has_image": 1381,
"description": 1363,
"sellers": 970,
"no_alt_models": 1286,
"screenshot": 1381,
"w_price": 929
},
"1080p": {
"product_count": 1035,
"other_product_count": 341,
"has_image": 1035,
"description": 1022,
"sellers": 727,
"no_alt_models": 960,
"screenshot": 1035,
"w_price": 692
},
"4k": {
"product_count": 174,
"other_product_count": 94,
"has_image": 174,
"description": 143,
"sellers": 165,
"no_alt_models": 159,
"screenshot": 174,
"w_price": 160
},
"720p": {
"product_count": 42,
"other_product_count": 1,
"has_image": 42,
"description": 37,
"sellers": 41,
"no_alt_models": 32,
"screenshot": 42,
"w_price": 34
},
"Under 32 inch": {
"product_count": 643,
"other_product_count": 288,
"has_image": 643,
"description": 630,
"sellers": 458,
"no_alt_models": 607,
"screenshot": 643,
"w_price": 442
},
"32-40 inch": {
"product_count": 363,
"other_product_count": 160,
"has_image": 363,
"description": 353,
"sellers": 253,
"no_alt_models": 339,
"screenshot": 363,
"w_price": 233
},
"40-48 inch": {
"product_count": 287,
"other_product_count": 111,
"has_image": 287,
"description": 275,
"sellers": 216,
"no_alt_models": 249,
"screenshot": 287,
"w_price": 193
},
"48-55 inch": {
"product_count": 72,
"other_product_count": 4,
"has_image": 72,
"description": 66,
"sellers": 72,
"no_alt_models": 44,
"screenshot": 72,
"w_price": 62
},
"55-65 inch": {
"product_count": 260,
"other_product_count": 94,
"has_image": 260,
"description": 242,
"sellers": 204,
"no_alt_models": 232,
"screenshot": 260,
"w_price": 195
},
"Over 65 inch": {
"product_count": 132,
"other_product_count": 58,
"has_image": 132,
"description": 119,
"sellers": 111,
"no_alt_models": 115,
"screenshot": 132,
"w_price": 106
},
"DVD & Blu-ray Players (VCD)": {
"product_count": 1468,
"other_product_count": 619,
"has_image": 1468,
"description": 1438,
"sellers": 937,
"no_alt_models": 1432,
"screenshot": 1468,
"w_price": 919
},
"Digital Video Recorders": {
"product_count": 787,
"other_product_count": 147,
"has_image": 787,
"description": 767,
"sellers": 331,
"no_alt_models": 765,
"screenshot": 787,
"w_price": 318
},
"Media Streaming Devices": {
"product_count": 917,
"other_product_count": 8224,
"has_image": 917,
"description": 909,
"sellers": 802,
"no_alt_models": 887,
"screenshot": 917,
"w_price": 799
},
"Video Accessories": {
"product_count": 4417,
"other_product_count": 7872,
"has_image": 4417,
"description": 4370,
"sellers": 4205,
"no_alt_models": 4406,
"screenshot": 4417,
"w_price": 4204
},
"Multimedia Projectors": {
"product_count": 661,
"other_product_count": 4129,
"has_image": 661,
"description": 658,
"sellers": 584,
"no_alt_models": 654,
"screenshot": 661,
"w_price": 583
},
"Computer & Devices": {
"product_count": 33028,
"other_product_count": 174381,
"has_image": 33027,
"description": 32747,
"sellers": 28559,
"no_alt_models": 32713,
"screenshot": 33028,
"w_price": 28524
},
"Computer Components": {
"product_count": 13217,
"other_product_count": 94972,
"has_image": 13216,
"description": 13132,
"sellers": 11514,
"no_alt_models": 13130,
"screenshot": 13217,
"w_price": 11497
},
"Network Storage Systems": {
"product_count": 612,
"other_product_count": 5615,
"has_image": 612,
"description": 609,
"sellers": 501,
"no_alt_models": 607,
"screenshot": 612,
"w_price": 499
},
"Audio Cards & Adapters": {
"product_count": 115,
"other_product_count": 1501,
"has_image": 115,
"description": 113,
"sellers": 102,
"no_alt_models": 114,
"screenshot": 115,
"w_price": 101
},
"Computer Processors": {
"product_count": 744,
"other_product_count": 6524,
"has_image": 744,
"description": 739,
"sellers": 572,
"no_alt_models": 736,
"screenshot": 744,
"w_price": 569
},
"KVM Switches": {
"product_count": 1115,
"other_product_count": 7638,
"has_image": 1115,
"description": 1110,
"sellers": 1022,
"no_alt_models": 1103,
"screenshot": 1115,
"w_price": 1019
},
"Hard Drives": {
"product_count": 1665,
"other_product_count": 15083,
"has_image": 1664,
"description": 1655,
"sellers": 1422,
"no_alt_models": 1649,
"screenshot": 1665,
"w_price": 1417
},
"Motherboards": {
"product_count": 175,
"other_product_count": 8091,
"has_image": 175,
"description": 170,
"sellers": 142,
"no_alt_models": 170,
"screenshot": 175,
"w_price": 141
},
"Modems": {
"product_count": 631,
"other_product_count": 2765,
"has_image": 631,
"description": 613,
"sellers": 511,
"no_alt_models": 625,
"screenshot": 631,
"w_price": 508
},
"Optical Drives": {
"product_count": 522,
"other_product_count": 13296,
"has_image": 522,
"description": 507,
"sellers": 371,
"no_alt_models": 522,
"screenshot": 522,
"w_price": 371
},
"Graphic Cards & Video Adapters": {
"product_count": 933,
"other_product_count": 6415,
"has_image": 933,
"description": 924,
"sellers": 529,
"no_alt_models": 926,
"screenshot": 933,
"w_price": 528
},
"Network Cards & Adapters": {
"product_count": 1117,
"other_product_count": 7629,
"has_image": 1117,
"description": 1113,
"sellers": 1000,
"no_alt_models": 1115,
"screenshot": 1117,
"w_price": 1000
},
"RAM": {
"product_count": 563,
"other_product_count": 8710,
"has_image": 563,
"description": 563,
"sellers": 529,
"no_alt_models": 558,
"screenshot": 563,
"w_price": 529
},
"Power Supplies": {
"product_count": 3600,
"other_product_count": 4533,
"has_image": 3600,
"description": 3592,
"sellers": 3467,
"no_alt_models": 3595,
"screenshot": 3600,
"w_price": 3467
},
"Computer Peripherals": {
"product_count": 5376,
"other_product_count": 30445,
"has_image": 5376,
"description": 5306,
"sellers": 4890,
"no_alt_models": 5297,
"screenshot": 5376,
"w_price": 4887
},
"Keyboard": {
"product_count": 1009,
"other_product_count": 7826,
"has_image": 1009,
"description": 1005,
"sellers": 887,
"no_alt_models": 991,
"screenshot": 1009,
"w_price": 886
},
"Graphics Tablets": {
"product_count": 4,
"other_product_count": 67,
"has_image": 4,
"description": 4,
"sellers": 4,
"no_alt_models": 3,
"screenshot": 4,
"w_price": 4
},
"Webcams": {
"product_count": 294,
"other_product_count": 4480,
"has_image": 294,
"description": 283,
"sellers": 238,
"no_alt_models": 293,
"screenshot": 294,
"w_price": 238
},
"Computer Monitors": {
"product_count": 539,
"other_product_count": 7354,
"has_image": 539,
"description": 535,
"sellers": 372,
"no_alt_models": 534,
"screenshot": 539,
"w_price": 370
},
"Scanners"   
"product_count"          "product"other_product_count": 4211,
"has_image": 1378,
"description": 1369,
"sellers": 1281,
"no_alt_models": 1359,
"screenshot": 1378,
"w_price": 1281
},
"Computer Accessories": {
"product_count": 4697,
"other_product_count": 34474,
"has_image": 4697,
"description": 4628,
"sellers": 4313,
"no_alt_models": 4621,
"screenshot": 4697,
"w_price": 4308
},
"Memory Card Readers": {
"product_count": 456,
"other_product_count": 8703,
"has_image": 456,
"description": 447,
"sellers": 414,
"no_alt_models": 451,
"screenshot": 456,
"w_price": 413
},
"Routers": {
"product_count": 990,
"other_product_count": 7480,
"has_image": 990,
"description": 984,
"sellers": 782,
"no_alt_models": 967,
"screenshot": 990,
"w_price": 778
},
"USB Flash Drives": {
"product_count": 172,
"other_product_count": 3327,
"has_image": 172,
"description": 172,
"sellers": 162,
"no_alt_models": 171,
"screenshot": 172,
"w_price": 162
},
"Flash Memory Cards": {
"product_count": 982,
"other_product_count": 8130,
"has_image": 982,
"description": 972,
"sellers": 906,
"no_alt_models": 967,
"screenshot": 982,
"w_price": 906
},
"Computer Servers": {
"product_count": 322,
"other_product_count": 422,
"has_image": 322,
"description": 320,
"sellers": 257,
"no_alt_models": 310,
"screenshot": 322,
"w_price": 255
},
"Desktop Computers": {
"product_count": 3541,
"other_product_count": 1200,
"has_image": 3541,
"description": 3510,
"sellers": 2581,
"no_alt_models": 3473,
"screenshot": 3541,
"w_price": 2579
},
"Laptops": {
"product_count": 529,
"other_product_count": 14,
"has_image": 529,
"description": 528,
"sellers": 382,
"no_alt_models": 521,
"screenshot": 529,
"w_price": 382
},
"Tablet Computers": {
"product_count": 4770,
"other_product_count": 3246,
"has_image": 4770,
"description": 4723,
"sellers": 4186,
"no_alt_models": 4745,
"screenshot": 4770,
"w_price": 4183
},
"E-Readers": {
"product_count": 5,
"other_product_count": 18,
"has_image": 5,
"description": 5,
"sellers": 3,
"no_alt_models": 4,
"screenshot": 5,
"w_price": 3
},
"GPS Navigation Systems": {
"product_count": 757,
"other_product_count": 2684,
"has_image": 757,
"description": 753,
"sellers": 636,
"no_alt_models": 740,
"screenshot": 757,
"w_price": 632
},
"Activity Monitors": {
"product_count": 16,
"other_product_count": 134,
"has_image": 16,
"description": 16,
"sellers": 15,
"no_alt_models": 16,
"screenshot": 16,
"w_price": 15
},
"Telephone": {
"product_count": 2972,
"other_product_count": 12315,
"has_image": 2972,
"description": 2857,
"sellers": 2591,
"no_alt_models": 2963,
"screenshot": 2972,
"w_price": 2590
},
"Mobile Phones": {
"product_count": 152,
"other_product_count": 7341,
"has_image": 152,
"description": 149,
"sellers": 132,
"no_alt_models": 152,
"screenshot": 152,
"w_price": 132
},
"Radios": {
"product_count": 1937,
"other_product_count": 3659,
"has_image": 1937,
"description": 1923,
"sellers": 1770,
"no_alt_models": 1926,
"screenshot": 1937,
"w_price": 1769
},
"Smartwatch": {
"product_count": 135,
"other_product_count": 8749,
"has_image": 135,
"description": 134,
"sellers": 132,
"no_alt_models": 135,
"screenshot": 135,
"w_price": 132
},
"printers, copiers and fax machines": {
"product_count": 2268,
"other_product_count": 6487,
"has_image": 2268,
"description": 2251,
"sellers": 2122,
"no_alt_models": 2239,
"screenshot": 2268,
"w_price": 2122
},
"PDAs": {
"product_count": 75,
"other_product_count": 2520,
"has_image": 75,
"description": 73,
"sellers": 64,
"no_alt_models": 74,
"screenshot": 75,
"w_price": 64
},
"Home Security": {
"product_count": 209,
"other_product_count": 8707,
"has_image": 209,
"description": 208,
"sellers": 192,
"no_alt_models": 209,
"screenshot": 209,
"w_price": 192
},
"Digital Cameras": {
"product_count": 148,
"other_product_count": 14461,
"has_image": 148,
"description": 147,
"sellers": 147,
"no_alt_models": 148,
"screenshot": 148,
"w_price": 147
},
"Surveillance Cameras": {
"product_count": 88,
"other_product_count": 8904,
"has_image": 88,
"description": 87,
"sellers": 78,
"no_alt_models": 88,
"screenshot": 88,
"w_price": 78
},
"Video Cameras": {
"product_count": 698,
"other_product_count": 8198,
"has_image": 698,
"description": 694,
"sellers": 674,
"no_alt_models": 696,
"screenshot": 698,
"w_price": 674
},
"Digital": {
"product_count": 26,
"other_product_count": 1243,
"has_image": 26,
"description": 26,
"sellers": 25,
"no_alt_models": 26,
"screenshot": 26,
"w_price": 25
},
"Game Consoles": {
"product_count": 2,
"other_product_count": 246,
"has_image": 2,
"description": 2,
"sellers": 0,
"no_alt_models": 2,
"screenshot": 2,
"w_price": 0
},
"Game Controller": {
"product_count": 155,
"other_product_count": 25057,
"has_image": 155,
"description": 152,
"sellers": 124,
"no_alt_models": 151          "noscreenshot": 155,
"w_price": 124
},
"Audio Components": {
"product_count": 8175,
"other_product_count": 9507,
"has_image": 8175,
"description": 8112,
"sellers": 7647,
"no_alt_models": 8152,
"screenshot": 8175,
"w_price": 7646
},
"Home Theater Systems": {
"product_count": 582,
"other_product_count": 528,
"has_image": 582,
"description": 578,
"sellers": 502,
"no_alt_models": 575,
"screenshot": 582,
"w_price": 502
},
"Speakers": {
"product_count": 2783,
"other_product_count": 2157,
"has_image": 2783,
"description": 2755,
"sellers": 2553,
"no_alt_models": 2769,
"screenshot": 2783,
"w_price": 2553
},
"Public Address Systems": {
"product_count": 42,
"other_product_count": 1795,
"has_image": 42,
"description": 42,
"sellers": 42,
"no_alt_models": 41,
"screenshot": 42,
"w_price": 42
},
"Power & Headphone Amplifiers": {
"product_count": 155,
"other_product_count": 598,
"has_image": 155,
"description": 153,
"sellers": 148,
"no_alt_models": 153,
"screenshot": 155,
"w_price": 147
},
"Equilizers": {
"product_count": 15,
"other_product_count": 85,
"has_image": 15,
"description": 15,
"sellers": 15,
"no_alt_models": 15,
"screenshot": 15,
"w_price": 15
},
"Audio Accessories": {
"product_count": 4667,
"other_product_count": 42129,
"has_image": 4667,
"description": 4594,
"sellers": 4538,
"no_alt_models": 4646,
"screenshot": 4667,
"w_price": 4537
},
"Headphones & Headsets": {
"product_count": 414,
"other_product_count": 25629,
"has_image": 414,
"description": 408,
"sellers": 399,
"no_alt_models": 410,
"screenshot": 414,
"w_price": 399
},
"Microphones": {
"product_count": 247,
"other_product_count": 11943,
"has_image": 247,
"description": 239,
"sellers": 235,
"no_alt_models": 247,
"screenshot": 247,
"w_price": 235
},
"MP3 Players": {
"product_count": 2783,
"other_product_count": 2346,
"has_image": 2783,
"description": 2725,
"sellers": 2398,
"no_alt_models": 2769,
"screenshot": 2783,
"w_price": 2398
},
"Voice Recorders": {
"product_count": 503,
"other_product_count": 3339,
"has_image": 503,
"description": 488,
"sellers": 465,
"no_alt_models": 494,
"screenshot": 503,
"w_price": 465
},
"Boomboxes & Stereo Systems": {
"product_count": 655,
"other_product_count": 473,
"has_image": 655,
"description": 652,
"sellers": 577,
"no_alt_models": 645,
"screenshot": 655,
"w_price": 574
},
"Karaoke Systems": {
"product_count": 21,
"other_product_count": 1729,
"has_image": 21,
"description": 21,
"sellers": 19,
"no_alt_models": 21,
"screenshot": 21,
"w_price": 19
},
"Turntables": {
"product_count": 139,
"other_product_count": 2639,
"has_image": 139,
"description": 139,
"sellers": 126,
"no_alt_models": 138,
"screenshot": 139,
"w_price": 126
},
"MP4 Players": {
"product_count": 850,
"other_product_count": 1122,
"has_image": 850,
"description": 820,
"sellers": 779,
"no_alt_models": 844,
"screenshot": 850,
"w_price": 779
},
"Solar Panels": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Lighting (LED)": {
"product_count": 5,
"other_product_count": 265,
"has_image": 5,
"description": 0,
"sellers": 4,
"no_alt_models": 5,
"screenshot": 5,
"w_price": 4
},
"LED Light Bulbs": {
"product_count": 1,
"other_product_count": 989,
"has_image": 1,
"description": 0,
"sellers": 1,
"no_alt_models": 1,
"screenshot": 1,
"w_price": 1
},
"Ceiling Lighting Fixtures (LED)": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"LED": {
"product_count": 3,
"other_product_count": 3921,
"has_image": 3,
"description": 0,
"sellers": 3,
"no_alt_models": 3,
"screenshot": 3,
"w_price": 3
},
"Desk (LED)": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Video Camera Lights (LED)": {
"product_count": 17,
"other_product_count": 4671,
"has_image": 17,
"description": 0,
"sellers": 17,
"no_alt_models": 17,
"screenshot": 17,
"w_price": 17
}
},
"gsuk": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0,
"categories": {
"Televisions": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"High Definition": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"1080p": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"4k": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"720p": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Under 32 inch": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"32-40 inch": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"40-48 inch": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"48-55 inch": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Over 65 inch": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"DVD & Blu-ray Players (VCD)": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Digital Video Recorders": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Media Streaming Devices": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Multimedia Projectors": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Network Storage Systems": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Audio Cards & Adapters": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Computer Processors": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"KVM Switches": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Hard Drives": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Motherboards": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Modems": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"USB Flash Drives": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Graphic Cards & Video Adapters": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Network Cards & Adapters": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"RAM": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Power Supplies": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Keyboard": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Graphics Tablets": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Webcams": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Computer Monitors": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Scanners": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Computer Accessories": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Memory Card Readers": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Routers": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Flash Memory Cards": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Computer Servers": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Desktop Computers": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Laptops": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Tablet Computers": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"E-Readers": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"GPS Navigation Systems": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Activity Monitors": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Telephone": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Mobile Phones": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Radios": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Smartwatch": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"printers, copiers and fax machines": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"PDAs": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Home Security": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Digital Cameras": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Surveillance Cameras": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Video Cameras": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Digital": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Game Consoles": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Game Controller": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Audio": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Audio Components": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Home Theater Systems": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Speakers": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Public Address Systems": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Power & Headphone Amplifiers": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Equilizers": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Audio Accessories": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Headphones & Headsets": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Microphones": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"MP3 Players": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Voice Recorders": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Boomboxes & Stereo Systems": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Karaoke Systems": {
"product_count":           "  "other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Turntables": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"MP4 Players": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Lighting (LED)": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"LED Light Bulbs": {
"product_count": 0,
"other_product_count": 0,
"has_image": 0,
"description": 0,
"sellers": 0,
"no_alt_models": 0,
"screenshot": 0,
"w_price": 0
},
"Ceiling Lighting Fixtures (LED)": {
hash = { a: 1, b: 2, c: 3}
hash.values
hash.values.take(1)
hash.values.drop(1)
hash.values.drop(-1)
hash.values[0..-1)
hash.values[0..-1]
hash.values[0..-2]
f = File.read('scrape_data.json')
data_hash = JSON.parse(f)
data_hash['gsus'.to_sym
data_hash['gsus'.to_sym]
data_hash['gsus']
data_hash[:sources]['gsus']
data_hash['sources']['gsus']
data_hash['sources']['gsus'].values
data_hash['sources']['gsus']['categories'][Category.find(1).name]
data_hash['sources']['gsus']['categories'][Category.find(1).name].each do |cat|
  cat.name
end
data_hash['sources']['gsus']['categories'][Category.find(1).name].each do |cat|
  puts cat
end
data_hash['sources']['gsus']['categories'][Category.find(1).name].each do |cat|
  puts [Category.first.name] + cat.values
end
data_hash['sources']['gsus']['categories'][Category.find(1).name].values do |cat|
data_hash['sources']['gsus']['categories'].each do |cat|
  puts cat
end
data_hash['sources']['gsus']['categories'].each do |cat|
  puts [cat.key] + cat.values
end
OtherProduct.count
Product.count
p = _
products = _
other = OtherProduct.count
products / (products + other)
products.to_f / (products + other)
other_products = OtherProduct.where.not(model: nil)
other_products.count
other
Product
amazon = Source.find 3
amazon_others = OtherProduct.includes(:sources).where.not(model: nil).where('sources.id = 3').references(:sources)
amazon_others.size
amazon_others.first
227500 / 60
227500 / 60 / 60
227500 / 60 / 60 / 24
227500 / 5
227500 / 60 / 60 / 24
45500 / 60 / 60 / 24
45500 / 60 / 60
other
OtherProduct.count
amazon_others.first
amazon_others.second
amazon_others.first 2
p1, p2 = _
p1.url
p2.url
amazon_others.last
OtherProduct
2.39 / 3
_ * 4
mem - (2.39 / 3)
mem = 2.39 / 3
mem * 5
gsus = Source.find(1)
gsus_other_products = OtherProduct.includes(:sources).where('sources.id = 1')
gsus_other_products = OtherProduct.includes(:sources).where('sources.id = 1').references(:sources)
gsus_other_products
gsus_other_products.first
gsus_other_products.limit(20).pluck(:name, :brand, :model, :url)
other
OtherProduct.count
gsus_other_products.limit(20).pluck(:name, :brand, :model, :url)
gsus_products = Product.includes(:sources).where('sources.id = 1').references(:sources)
OOqeit
xt
ei
xt
gsus_other_products = OtherProduct.includes(:sources).where('sources.id = 1')
gsus_products = Product.includes(:sources).where('sources.id = 1').references(:sources)
gsus_products.first
gsus_products.first.url
gsus_products.first.original_url
gsus_other_products = OtherProduct.includes(:sources).where('sources.id = 1').references(:sources)
gsus_other_products.first.url
gsus_other_products.first
gsus_other_products.first.scrapes
scrape = Scrape.find 1
scrape.urls
scrape.urls.where.not(url_type: 'Product')
Source.with_running_scrape
Source.unverified.with_running_scrape
Source.find 1
SourceType.find 1
exit
SourceType.find 1
Source.unverified.with_running_scrape
SourceCategory.where(source_id: 1, test_process: true).pluck(:id)
SourceCategory.where(source_id: 1, test_process: true)
Sidekiq::Queue.new("dedup_other_product")
Sidekiq::Queue.new("dedup_other_product").clear
gsus_other_products.first
gsus_other_products = OtherProduct.includes(:sources).where(id: 1).references(:sources)
gsus_other_products = OtherProduct.includes(:sources).where('sources.id: 1').references(:sources)
gsus_other_products = OtherProduct.includes(:sources).where('sources.id = 1').references(:sources)
gsus_other_products.where.not(model: nil)
gsus_other_products.where.not(model: nil).size
gsus_other_products.size
gsus_other_products.where(model: nil)
gsus_other_products.where.not(model: nil).size
gsus_other_products.where.not(model: nil)
Product.where(model: ["HD-CarDVR", "HDCarDVR"])
Product.where(brand: "Zuma")
Product.where('brand like "%Zuma%"')
OtherProduct.where('brand like "%Zuma%"')
OtherProduct.where('name like "%Zuma%"')
OtherProduct.where('name like "%Zuma%"').size
OtherProduct.where('name like "% Zuma %"').size
OtherProduct.where('name like "%Zuma %"').size
OtherProduct.where('name like "%Zuma %"')
Product.find_by model: "FD6300WMHKBOX"
gsus_other_products.sample 10
OtherProduct.find(403).name
OtherProduct.find(403)
aops = OtherProduct.includes(:sources).where(brand: nil).where('sources.id': 1) 
aops = OtherProduct.includes(:sources).where(brand: nil).where('sources.id': 1).find_in_batches(batch_size: 100) do |products| 
  products.each do |product|
aops = OtherProduct.includes(:sources).where(brand: nil).where('sources.id': 1).size
aops = OtherProduct.includes([scrapes: :sources]).where(brand: nil).where('sources.name': 'amazonus').size
aops = OtherProduct.includes([scrapes: :source]).where(brand: nil).where('sources.name': 'amazonus').size
aops = OtherProduct.includes(:sources).where(brand: nil).where('sources.id': 3).size
aops = OtherProduct.includes(:sources).where(brand: nil).where('sources.id': 3).first 10
aops = OtherProduct.includes(:sources).where(brand: nil).where('sources.id': 3).limit 10
aops = OtherProduct.includes(:sources).where(brand: nil).where('sources.id': 3).limit(10).each do |product|
  next if product.url.nil?
  redis = Redis::Namespace.new('amazonus', redis: Redis.new(url: ENV['REDIS_URL']))
  pool = ConnectionPool.new { redis }
  item = { 'class' => AmazonDedupOtherProductJob, 'args' => [product.id] }
  Sidekiq::Client.new(pool).push(item.stringify_keys)
end
exit
other_count = OtherProduct.count
first_other = OtherProduct.includes(:sources).where("sources.id": 3).first
first_other
OtherProduct.includes(:sources).where(brand: nil).where('sources.id': 3).limit(10)
OtherProduct.includes(:sources).where(brand: nil).where('sources.id': 3).limit(10).first
OtherProduct.includes(:sources).where(brand: nil).where('sources.id': 3).find_in_batches(100)
OtherProduct.includes(:sources).where(brand: nil).where('sources.id': 3).find_in_batches(batch_size: 100)
ops = _
ops
ops do |products|
  p product
end
ops
ops.each do |p|
  puts p
end
AmazonDedupOtherProductJob.new.perform_async(
OtherProduct.includes(:sources).where(brand: nil).where('sources.id': 3).limit(10).first
OtherProduct.connection
OtherProduct.includes(:sources).where(brand: nil).where('sources.id': 3).limit(10).first
reload!
OtherProduct.includes(:sources).where(brand: nil).where('sources.id': 3).limit(10).first
AmazonDedupOtherProductJob.new.perform_async(10864)
AmazonDedupOtherProductJob.new.perform(10864)
aops
aops = OtherProduct.includes(:sources).where(brand: nil).where('sources.id': 3)
aops.size
aops.limit(100)
aops.limit(100).each do |p|
  next if p.url.nil?
  redis = Redis::Namespace.new('amazonus', redis: Redis.new(url: ENV['REDIS_URL']))
  pool = ConnectionPool.new { redis }
  item = { 'class' => AmazonDedupOtherProductJob, 'args' => [p.id] }
  Sidekiq::Client.new(pool).push(item.stringify_keys)
end
Sidekiq::Client.registered_queues
gsus_other_products = Sidekiq::Client.registered_queues
gsus_other_products = OtherProduct.includes(:sources).where("sources.id": 1)
gsus.size
gsus_other_products.size
gsus_other_products.first
gsus_other_products.sample(10)
aops
aops.count
aops.size
aops.length
aops.first 10
AmazonDedupOtherProductJob.new.perform_async(10864)
AmazonDedupOtherProductJob.new.perform(10864)
relaod!
reload!
AmazonDedupOtherProductJob.new.perform(10864)
op = OtherProduct.find 10864
op.url
reload!
AmazonDedupOtherProductJob.new.perform(10864)
reload!
AmazonDedupOtherProductJob.new.perform(10864)
op
op.reload
Product.last
OtherProduct.first
OtherProduct.first.sources
aops = OtherProduct.includes(:sources).where(brand: nil).where('sources.id': 3)
AmazonDedupOtherProductJob.new.perform(10865)
OtherProduct.find(10865)
aops.ids
aops.ids[0..10]
aops.ids.first(10)
aops.ids.limit(10)
aops.limit(10).ids
reload!
aops.limit(10).ids
ids = _
ids.each do |id|
  AmazonDedupOtherProductJob.new.perform(id)
end
OtherProduct.find(ids.first)
reload!
AmazonDedupOtherProductJob.new.perform(ids.first)
ids.first
OtherProduct.find(ids.first)
aops.limit(10).ids
OtherProduct.find(ids.first)
OtherProduct.find(ids.[1])
OtherProduct.find(ids[1])
AmazonDedupOtherProductJob.new.perform(ids[1])
OtherProduct.find(ids[1])
AmazonDedupOtherProductJob.new.perform(ids[1])
OtherProduct.find(ids[1])
Product.last
aops.where(name: nil)
Source.find 3
Product.last
Product.last.original_url
OriginalUrl
Product.last.scrape_product_sellers
Product.last 3
Product.last 4
Product.last(5).pluck(:name, :id)
Product.sort(id: :desc).limit(5).pluck(:name, :id)
Product.includes(:sources).where("sources.id": 3).sort(id: :desc).limit(5).pluck(:name, :id)
reload!
Product.includes(:sources).where("sources.id": 3).order(updated_at: :desc).limit(5).pluck(:name, :id)
Product.includes(:sources).where("sources.id": 3).order(updated_at: :desc).limit(4).pluck(:name, :id)
Product.includes(:sources).where("sources.id": 3).order(updated_at: :desc).limit(4).uniq.pluck(:name, :id)
ids = Product.includes(:sources).where("sources.id": 3).order(updated_at: :desc).limit(4).uniq.pluck(:id)
Product.where(id: ids).map &:original_url
no_url = Product.includes(:scrape_product_sellers).where("scrape_product_sellers.original_url": nil)
ids = no_url.ids.uniq
ids.length
Product.where(id: ids).map &:original_url
no_url - Product.where(id: ids)
no_url = Product.where(id: ids)
no_url = Product.where(id: ids).map {|p| p.try(:original_url)}
sps = ScrapeProductSeller.find 3756
no_url = Product.where(id: ids).map {|p| p.scrape_product_sellers.first.try(:original_url)}
no_url = Product.includes(:sources).where("sources.id": 3).where(id: ids).map {|p| [p.id, p.scrape_product_sellers.first.try(:original_url)]}
no_url.select {|p| p[1].nil?}
ids = _.map {|el| el.first}
no_url = Product.where(id: ids)
no_url.first.original_url
no_url.[1].original_url
no_url[1].original_url
no_url[2].original_url
no_url[3].original_url
no_url[4].original_url
no_url[5].original_url
no_url[6].original_url
no_url[7].original_url
no_url[8].original_url
no_url[9].original_url
no_url[10].original_url
no_url[11].original_url
no_url[12].original_url
no_url[13].original_url
no_url[14].original_url
ids.size
no_url[13].original_url
no_url
no_url.first.scrape_product_sellers.map {|sps| sps.original_url}
no_url[1].scrape_product_sellers.map {|sps| sps.original_url}
no_url[2].scrape_product_sellers.map {|sps| sps.original_url}
ids
no_url.map do |no_url|
  no_url.scrape_product_sellers.map {|sps| sps.original_url}
end
no_url.scrape_product_sellers.map {|sps| [sps.original_url, sps.seller_url]}
no_url.map do |no_url|
  no_url.scrape_product_sellers.map {|sps| [sps.original_url, sps.seller_url]}
end
exit
aops = OtherProduct.includes(:sources).where("sources.id": 3)
aops.first
op = _
op.url
op.scrape_other_product_sellers
OtherProduct.where(url: nil)
op.scrape
op.scrape_id
op.scrapes
p = Product.last
p.scrape_product_sellers
op
Product.has_models(op.model).first
op
aops.last
op = aops.last
Product.has_models(op.model).first
op.model
op.brand
op.url
aops.where.not(model: nil)
op = OtherProduct.find 10872,
op = OtherProduct.find 10872
op.url
Product.has_models(op.model).first
sops = ScrapeProductSeller.where(other_product: op)
sops = ScrapeOtherProductSeller.where(other_product: op)
sops.first
sops.first.scrape
sops.first.scrape.urls
reload!
sops.first
s = _
s.as_json
s.as_json.reject! { |k| %w(other_product_id id other_product_source_path) }
params = s.as_json.reject! { |k| %w(other_product_id id other_product_source_path).include? k }
Seller.find(11791)
seller = _
params
s.id
sops = ScrapeOtherProductSeller.find 22676
exit
sops = ScrapeOtherProductSeller.find 22676
params = sops.as_json.reject! { |k| %w(other_product_id id other_product_source_path).include? k }
p = Product.last
params[:product_id] = p.id
params[:product_source_path] = sops.other_product_source_path
params[:product_id] = p.id + 1
ScrapeProductSeller.create params
params[:product_id] = p.id
ScrapeProductSeller.create params
ScrapeOtherProduct.includes(:other_product).each do |sops|
  p sops.url
end
ScrapeOtherProductSeller.includes(:other_product).all
reload!
ScrapeOtherProductSeller.where(other_product_id: nil)
ScrapeOtherProductSeller.count
reload!
ScrapeOtherProductSeller.count
OtherProduct.count
OtherProduct.first
op = _
op.scrape_other_product_sellers
OtherProduct.all.select {|op| op.scrape_other_product_sellers.size > 1 }
OtherProduct.select {|op| op.scrape_other_product_sellers.size > 1 }
Product.search("Dolby")
Product.search("Dolby").results
ElasticSeach::Model.search("Dolby", [Product], size: 100000).page(1).results
ElasticSearch::Model.search("Dolby", [Product], size: 100000).page(1).results
Elasticsearch::Model.search("Dolby", [Product], size: 100000).page(1).results
Elasticsearch::Model.search("Dolby", [Product], size: 100000).page(1).response
exit
op
op = OtherProduct.first
op.scrape_other_product_sellers.map(&:original_url).compact.first
Elasticsearch::Model.search("Dolby", [Product], size: 100000).page(1).results
Elasticsearch::Model.search("Dolby", [Product], size: 100000).page(1).response
ids = results['hits']['hits'].map do |hit|
  hit['id']
end
results = Elasticsearch::Model.search("Dolby", [Product], size: 100000).page(1).response
ids = results['hits']['hits'].map do |hit|
  hit['id']
end
results.first
results
results["hits"]
results[:hits]
results[:hits][:hits]
results[:hits][:hits].first
ids = results['hits']['hits'].map do |hit|
  hit['_id'].to_i
end
ids
ids.size
Product.where(id: ids)
prods = _
results = Elasticsearch::Model.search("Dolby", [Product], size: 100000).page(1).response
results = Elasticsearch::Model.search("Dolby", [Product, OtherProduct], size: 100000).response
results[:hits]
results[:hits][:hits]
results[:hits][:hits].size
results
results[:hits][:hits].first
products, other_products = results[:hits][:hits].partition {|r| r['_index'] == "products_development"}
products
products.count
other_products.count
results = Elasticsearch::Model.search("Dolby", [Product, OtherProduct]).response
products, other_products = results[:hits][:hits].partition {|r| r['_index'] == "products_development"}
products.size
products.count
results = Elasticsearch::Model.search("Dolby", [Product, OtherProduct], size: 1000000).response
products, other_products = results[:hits][:hits].partition {|r| r['_index'] == "products_development"}
products.count
products
products.map(&:_id)
products.map(&:_id).map(&:to_i)
products.map(&:_id).map(&:to_i).size
products.map(&:_id).map(&:to_i).uniq.size
load 'scrape_stats.rb'
exit
Sidekiq::Queue.new("elasticsearch_indexing_queue").clear
exit
Product.import
exit
OtherProduct.create_index!
OtherProduct.import
results = Elasticsearch::Model.search("Dolby", [Product, OtherProduct], size: 1000000).response
results = Elasticsearch::Model.search("Dolby", [Product], size: 1000000).response
results = Elasticsearch::Model.search("Dolby Digital Plus", [Product], size: 1000000).response
results = Elasticsearch::Model.search("Dolby Digital Plus", [Product], size: 1000000, exact: true).response
results = Elasticsearch::Model.search("Dolby Digital Plus", [Product]).response
products, other_products = results[:hits][:hits].partition {|r| r['_index'] == "products_development"}
products.count
results = Elasticsearch::Model.search("Dolby Digital Plus", [Product], size: 1000000, exact: true).response
results = Elasticsearch::Model.search("Dolby Digital Plus", [Product], size: 1000000, match: true).response
results = Elasticsearch::Model.search("Dolby Digital Plus", [Product], size: 1000000, exact: true).response
reload!
results = Elasticsearch::Model.search("Dolby Digital Plus", [Product], size: 1000000).response
Product.generate_query_by_keywords('Dolby Digital Plus')
terms = { keywords: "Dolby Digital Plus" }
Product.generate_query_by_keywords(terms)
Product.generate_query_by_keywords(terms, 'AND')
models [Product, OtherProduct]
models = [Product, OtherProduct]
@products = []
@other_products = [[
@other_products = []
query Product.generate_query_by_keywords(terms, 'AND')
query = Product.generate_query_by_keywords(terms, 'AND')
keywords
terms
query = Product.generate_query_by_keywords(terms, 'AND')
results = Elasticsearch::Model.search(query, models, size: 10000000).response
terms
reload!
load 'scrape_stats.rb'
terms = { keywords: "DVB-VOB" }
results = Elasticsearch::Model.search(query, models, size: 10000000).response
products, other_products = results[:hits][:hits].partition {|r| r['_index'] == "products_development"}
products.count
terms = { keywords: "DVB-VOB" }
query = Product.generate_query_by_keywords(terms, 'AND')
results = Elasticsearch::Model.search(query, models, size: 10000000).response
terms = { keywords: "android" }
query = Product.generate_query_by_keywords(terms, 'AND')
results = Elasticsearch::Model.search(query, models, size: 10000000).response
products, other_products = results[:hits][:hits].partition {|r| r['_index'] == "products_development"}
products.count
0/0
NaN
0.to_f/0
0.to_f/0 || 0
(0.to_f/0).class
(0.to_f/0).nan?
0.nan?
356.try(:nan?) ? 0 : 356
OtherProduct.last
OtherProduct.last.seller
OtherProduct.last.sellers
OtherProduct.last.scrape_other_product_sellers
OtherProduct.last.scrape_other_product_sellers.first
sops = _
exit
Product.import refresh: true
{index: 'something', other: 'other'}.merge({refresh: true})
exit
Product.__elasticsearch__.
Product.__elasticsearch__.import
Product.__elasticsearch__.import refresh: true
reload!
Product.__elasticsearch__.import refresh: true
reload!
Product.dump_all_product_info
Product.dump_all_product_info [1]
Scrape.find 1
s = 
_
brands = %w(Boss Audio, Curtis International, Ematic, Hauppauge, Kobo, Leapfrog, Viewsonic, Visteon, Visual-Land, Westinghouse)
brands
exit
brands = %w(Boss Audio, Curtis International, Ematic, Hauppauge, Kobo, Leapfrog, Viewsonic, Visteon, Visual-Land, Westinghouse)
boss_prods = Product.where(brand: brands.first)
boss_prods.size
OtherProduct.where(brand: 'Boss')
boss_others = _
boss_others.size
brand_breakdown = {}
brands.each do |brand|
data = {}
brands.each do |brand|
  brand_prods = Product.where(brand: brand)
boss_prods.first
bp = _
bp.categories
bp.sources
brands = %w(Boss\ Audio, Curtis\ International, Ematic, Hauppauge, Kobo, Leapfrog, Viewsonic, Visteon, Visual-Land, Westinghouse)
boss_prods = Product.where(brand: brands.first)
boss_prods = OtherProduct.where(brand: brands.first)
boss_prods = OtherProduct.where(brand: [brands.first, 'Boss'])
boss_prods = Product.where(brand: brands[1])
boss_prods = OtherProduct.where(brand: [brands[1], 'Curtis'])
Product.where("name like '%Curtis%'")
brands[1]
brands = %w(Boss\ Audio Curtis\ International Ematic Hauppauge Kobo Leapfrog Viewsonic Visteon Visual-Land Westinghouse)
boss_prods = OtherProduct.where(brand: [brands[1], 'Curtis'])
_.size
brand_products = Product.where("brand = :brand OR name LIKE '%:brand%'", brand: brand)
brand_products = Product.where("brand = :brand OR name LIKE '%:brand%'", brand: brands[0])
brand_products = Product.where("brand = :brand OR name LIKE '%#{brands[0]}%'", brand: brands[0])
prods = _
prods.size
prods
brand_products = Product.where("brand = ? OR name LIKE '%?%'", brands[0])
brand = brands.first
brand.substrings
brand.split.first
query = "brand = #{brand} OR brand = #{alt_brand} OR name LIKE '%#{brand}%' OR name LIKE '%#{alt_brand}%'"
alt_brand = brand.split.first
query = "brand = #{brand} OR brand = #{alt_brand} OR name LIKE '%#{brand}%' OR name LIKE '%#{alt_brand}%'"
prods = Product.where(query)
query = "brand = '#{brand}' OR brand = #{alt_brand} OR name LIKE '%#{brand}%' OR name LIKE '%#{alt_brand}%'"
prods = Product.where(query)
query = "brand = '#{brand}' OR brand = '#{alt_brand}' OR name LIKE '%#{brand}%' OR name LIKE '%#{alt_brand}%'"
prods = Product.where(query)
prods.size
other_prods = OtherProduct.where(query)
other_prods.size
prods
prods.first.categories
cat = Category.first
category = Category.first
prods.includes(:categories)
prods.includes(:categories).size
prods.includes(:categories).where("categories.id": category.id).size
prods = Product.where(query).references(:products)
prods.includes(:categories).where("categories.id": category.id).size
query = "products.brand = '#{brand}' OR products.brand = '#{alt_brand}' OR products.name LIKE '%#{brand}%' OR products.name LIKE '%#{alt_brand}%'"
prods = Product.where(query)
prods.size
prods.includes(:categories).where("categories.id": category.id).size
prod.first.categories.map(&:id)
prods.first.categories.map(&:id)
prods.includes(:categories).where("categories.id": 83).size
prods.includes(:categories).where("categories.id": 83)
prod = prods.first
prod.scrapes
prods.includes(:scrape_product_sellers).where.not("scrape_product_sellers.min_price": nil)
_.size
prods.includes(:categories).where("categories.id": 83)
Category.all
Source.first.categories.first
Source.last.categories.first
reload!
exit
load 'scrape_stats.rb'
prods
categories = Category.all
brand = "Boss Audio"
alt_brand = "Boss"
query = "products.brand = '#{brand}' OR products.brand = '#{alt_brand}' OR products.name LIKE '%#{brand}%' OR products.name LIKE '%#{alt_brand}%'"
alt_brand = brand.split.first
prod_query = "products.brand = '#{brand}' OR products.brand = '#{alt_brand}' OR products.name LIKE '%#{brand}%' OR products.name LIKE '%#{alt_brand}%'"
brand_products = Product.includes(:categories, :scrape_product_sellers).where(prod_query)
cat_id = categories[3]
brand_products.includes(:categories).where("categories.id": 4)
load 'scrape_stats.rb'
brands
brands = %w(Boss\ Audio Curtis\ International Ematic Hauppauge Kobo Leapfrog Viewsonic Visteon Visual-Land Westinghouse)
brands.drop 1
load 'scrape_stats.rb'
brands = %w(Boss\ Audio Curtis\ International Ematic Hauppauge Kobo Leapfrog Viewsonic Visteon Visual-Land Westinghouse)
brand = brands.first
alt_brand = brands.first.split.first
all_prod_query = "products.brand = '#{brand}' OR products.brand = '#{alt_brand}' OR products.name LIKE '%#{brand}%' OR products.name LIKE '%#{alt_brand}%'"
all_other_query = "other_products.brand = '#{brand}' OR other_products.brand = '#{alt_brand}' OR other_products.name LIKE '%#{brand}%' OR other_products.name LIKE '%#{alt_brand}%'"
brands.each do |brand|
  alt_brand = brand.split.first
  all_prod_query += " OR products.brand = '#{brand}' OR products.brand = '#{alt_brand}' OR products.name LIKE '%#{brand}%' OR products.name LIKE '%#{alt_brand}%'"
  all_other_query += " OR other_products.brand = '#{brand}' OR other_products.brand = '#{alt_brand}' OR other_products.name LIKE '%#{brand}%' OR other_products.name LIKE '%#{alt_brand}%'"
end
all_prods = Product.where(all_prod_query)
all_other_prods = OtherProduct.where(all_other_query)
reload!
brands = %w(Boss\ Audio Curtis\ International Ematic Hauppauge Kobo Leapfrog Viewsonic Visteon Visual-Land Westinghouse)
brand = brands.first
alt_brand = brands.first.split.first
all_prod_query = "products.brand = '#{brand}' OR products.brand = '#{alt_brand}' OR products.name LIKE '%#{brand}%' OR products.name LIKE '%#{alt_brand}%'"
all_other_query = "other_products.brand = '#{brand}' OR other_products.brand = '#{alt_brand}' OR other_products.name LIKE '%#{brand}%' OR other_products.name LIKE '%#{alt_brand}%'"
brands.each do |brand|
  alt_brand = brand.split.first
  all_prod_query += " OR products.brand = '#{brand}' OR products.brand = '#{alt_brand}' OR products.name LIKE '%#{brand}%' OR products.name LIKE '%#{alt_brand}%'"
  all_other_query += " OR other_products.brand = '#{brand}' OR other_products.brand = '#{alt_brand}' OR other_products.name LIKE '%#{brand}%' OR other_products.name LIKE '%#{alt_brand}%'"
end
all_prods = Product.where(all_prod_query)
all_prods.size
all_other_prods.size
reload!
all_other_prods = OtherProduct.where(all_other_query)
all_other_prods.size
prod_query = "products.brand = '#{brand}' OR products.brand = '#{alt_brand}' OR products.name LIKE '%#{brand}%' OR products.name LIKE '%#{alt_brand}%'"
brand_prods = all_prods.includes(:categories, :scrape_product_sellers).where(prod_query)
brands = %w(Boss\ Audio Curtis\ International Ematic Hauppauge Kobo Leapfrog Viewsonic Visteon Visual-Land Westinghouse)
all_prod_query = "products.brand = '#{brand}' OR products.brand = '#{alt_brand}' OR products.name LIKE '%#{brand}%' OR products.name LIKE '%#{alt_brand}%'"
all_other_query = "other_products.brand = '#{brand}' OR other_products.brand = '#{alt_brand}' OR other_products.name LIKE '%#{brand}%' OR other_products.name LIKE '%#{alt_brand}%'"
brands.each do |brand|
  alt_brand = brand.split.first
  all_prod_query += " OR products.brand = '#{brand}' OR products.brand = '#{alt_brand}' OR products.name LIKE '%#{brand}%' OR products.name LIKE '%#{alt_brand}%'"
  all_other_query += " OR other_products.brand = '#{brand}' OR other_products.brand = '#{alt_brand}' OR other_products.name LIKE '%#{brand}%' OR other_products.name LIKE '%#{alt_brand}%'"
end
all_prod_query
all_prods = Products.where(all_prod_query)
all_prods = Product.where(all_prod_query)
all_prods.size
prod_query = "products.brand = '#{brand}' OR products.brand = '#{alt_brand}' OR products.name LIKE '%#{brand}%' OR products.name LIKE '%#{alt_brand}%'"
brand = brands.first
alt_brand
brand
alt_brand = brand.split.first
prod_query = "products.brand = '#{brand}' OR products.brand = '#{alt_brand}' OR products.name LIKE '%#{brand}%' OR products.name LIKE '%#{alt_brand}%'"
all_prods.load
all_prods.load.where(prod_query).size
all_prods.where(prod_query).size
all_prods = Product.where(all_prod_query)
all_prods.where(prod_query).size
Product.where('name like "%Curtis%"')
Product.where('name like "%Curtis%"').size
Product.where('name like "%Curtis%"')
Product.where('name like "%Curtis%" or brand like "%Curtis%"')
prods = Product.where('name like "%Curtis%" or brand like "%Curtis%"')
prods.size
prods.where.not(brand: 'Curtis International')
Product.where(brand: 'Curtis')
_.size
prods = Product.where(brand: 'Curtis')
prods.pluck(:name)
prods.pluck(:name, :model)
header_row = %w(Product\ Name Brand Model Product\ Description)
header_row.each_with_index.each_with_object({}) do |e, a|
  puts e
  a[e.first.to_sym] = e.last
end
header_row.each_with_index.each_with_object({}) do |e, a|
header_rows.first.downcase
header_row.first.downcase
header_row.first.downcase.snakecase
header_row.first.downcase.snakeize
header_row.first.downcase.underscoreize
header_row.first.downcase.underscore
header_row.first.downcase.camelize
header_row.first.downcase.gsub(' ', '_')
header_row.first.parameterize
header_row.first.parameterize.underscore
"ProductDescription".parameterize.underscore
:product_description.parameterize.underscore
:product_description.underscore
:product_description.to_s.underscore
"Product Description".parameterize.underscore
"Product Description".parameterize.underscore.to_sym
":product_description".to_sym
"product_description".to_sym
Sidekiq::Queue.new("dedup_other_product").clear
Sidekiq::Queue.new("update_original_url_queue").clear
Source.last
Source.last.destroy
Source.last
Source.last.scrapes
Source.last.scrapes.products
Source.last.scrapes.first.products
Source.last
Source.last.destroy
Scrape.last
Scrape.last.destroy
Scrape.last
Scrape.last.destroy
Scrape.last
Source.last
Source.last 2
Source.last
Source.last.destroy
exit
uuid = SecureRandom.uuid
folder_path = File.join('/tmp/import', uuid)
Zip::File.open('AmazonManualScrape.zip') do |zipfile|
  zipfile.each do |e|
    fpath = File.join(folder_path, e.to_s)
    puts e.to_s
    FileUtils.mkdir_p(File.dirname(fpath))
    zipfile.extract(e, fpath) { true }
  end
end
folder_path
tmp_folder_path = folder_path
data_file_paths = Dir["#{tmp_folder_path}/*.csv", "#{tmp_folder_path}/*.xlsx", "#{tmp_folder_path}/*.xls"]
tmp_folder_path
tmp_folder_path += '/AmazonManualScrape'
data_file_paths = Dir["#{tmp_folder_path}/*.csv", "#{tmp_folder_path}/*.xlsx", "#{tmp_folder_path}/*.xls"]
logger.info 'Importing scrape data'
scrape = Scrape.create source: source,
name: scrape_name,
running: true,
created_at: start_time
f = open_spreadsheet data_file_paths.first
first = f.first_row
key_mapping = generate_key_mapping f.row(first)
data_hash_keys = key_mapping.keys - @keys
(first + 1..f.last_row).each do |i|
  row = f.row(i)
  data = {}
  @keys.each do |key|
    value = key_mapping[key]
    data[key] = row[value] unless value.nil?
  end
  data_hash_keys.each do |key|
    h = row[key_mapping[key]]
    next if h.blank?
    data_hash_category = :General
    data[:data_hash] ||= {}
    data[:data_hash][data_hash_category] ||= {}
    data[:data_hash][data_hash_category][key] = row[key_mapping[key]]
  end
  p = import_product scrape, data
  # Ignore capture image and manual if can't create product
  # Ignore duplicate record of product in data file
  next if p.nil? || p.images.where(scrape_id: scrape.id).any?
  save_assets(ProductImage, scrape, p, "#{tmp_folder_path}/images/#{data[:model].strip}")
  save_assets(ProductManual, scrape, p, "#{tmp_folder_path}/manuals/#{data[:model].strip}")
end
folder_path
tmp_folder_path = folder_path
data_file_paths = Dir["#{tmp_folder_path}/*.csv", "#{tmp_folder_path}/*.xlsx", "#{tmp_folder_path}/*.xls"]
data_file_paths = Dir["#{tmp_folder_path}/**/*.csv", "#{tmp_folder_path}/**/*.xlsx", "#{tmp_folder_path}/**/*.xls"]
rails c
reload!
folder_path
data_file_paths = Dir["#{tmp_folder_path}/*.csv", "#{tmp_folder_path}/*.xlsx", "#{tmp_folder_path}/*.xls"]
data_file_paths += Dir["#{tmp_folder_path}/**/*.csv", "#{tmp_folder_path}/**/*.xlsx", "#{tmp_folder_path}/**/*.xls"]
tmp_folder_path = folder_path
data_file_paths += Dir["#{tmp_folder_path}/**/*.csv", "#{tmp_folder_path}/**/*.xlsx", "#{tmp_folder_path}/**/*.xls"]
data_file_paths = Dir["#{tmp_folder_path}/*.csv", "#{tmp_folder_path}/*.xlsx", "#{tmp_folder_path}/*.xls"]
Source.last
Source.last.destroy
Source.last
open(Source.last.upload_url)
file = _
tmp_file = _
def extract_data_file(file)
  uuid = SecureRandom.uuid
  folder_path = File.join('/tmp/import', uuid)
  Zip::File.open(file.path) do |zipfile|
    zipfile.each do |e|
      fpath = File.join(folder_path, e.to_s)
      FileUtils.mkdir_p(File.dirname(fpath))
      zipfile.extract(e, fpath) { true }
    end
  end
  folder_path
ensure
  # Delete tmp file after extract
FileUtils.rm(file.path) rescue nil
end
file.class
url = 'https://rvx-rds-dev.s3.amazonaws.com/import/1452450827319-ibgUof2AaxgXzA-8b153037a3ce76fd74c51b4484916d95/doorstep_small.zip'
open(url)
other_url = Source.last.upload_url
url
open(other_url)
Source.last 2
Source.last(2).first
another_url = Source.last(2).first.upload_url
open another_url
open(url)
open(open(url).base_uri)
def write_stream_to_a_temp_file(stream)
  ext = begin
    "."+MIME::Types[stream.meta["content-type"]].first.extensions.first
  rescue #In case meta data is not available
    #It seems sometimes the content-type is binary/octet-stream
    #In this case we should grab the original ext name.
    File.extname(stream.base_uri.path)
  end
  file = Tempfile.new ["temp", ext]
  begin
    file.binmode
    file.write stream.read
  ensure
  file.flush rescue nil
  file.close rescue nil
  end
  file
end
file = open(url)
data = write_stream_to_a_temp_file file unless file.is_a? Tempfile
data
Source.last 3
Source.last 2
Source.last(2).first
Source.last(2).first.destroy
Source.last
require 'tempfile'
TempFile
data = write_stream_to_a_temp_file file unless file.is_a? Tempfile
Source.last
Source.last.destroy
Source.last
Source.last 2
Product.last
p = _
p
p.scrapes
p.alternate_names
p.name
p.models
p.model
Source.last
head = "Product Name"
head.gsub(\s+\, '_')
head.gsub(/s+/, '_')
head.gsub(/\s+/, '_')
head.downcase.gsub(/\s+/, '_')
head.downcase.gsub(/\s+/, '_').to_sym
@keys = [
  :product_name, :product_description, :product_category, :product_url,
  :brand, :model, :alternate_models, :product_features, :contact_name,
  :seller_name, :min_price, :max_price
]
sym = head.downcase.gsub(/\s+/, '_').to_sym
@keys.include(sym)
@keys.include?(sym)
Source.last 3
Source.find(8).destroy
Source.last
Source.last.destroy
Source.last
Source.last.destroy
Source.last
Source.last.destroy
uuid = SecureRandom.uuid
folder_path = FIle.join('tmp/import', uuid)
folder_path = File.join('tmp/import', uuid)
zipfile = '/Users/Jonathan/Downloads/Amazon-example.zip'
def extract_data_file(file)
  uuid = SecureRandom.uuid
  folder_path = File.join('/tmp/import', uuid)
  Zip::File.open(file.path) do |zipfile|
    zipfile.each do |e|
      fpath = File.join(folder_path, e.to_s)
      FileUtils.mkdir_p(File.dirname(fpath))
      zipfile.extract(e, fpath) { true }
    end
  end
  folder_path
end
tmp_folder_path = extract_data_file zipfile
def extract_data_file(file)
  uuid = SecureRandom.uuid
  folder_path = File.join('/tmp/import', uuid)
  Zip::File.open(file) do |zipfile|
    zipfile.each do |e|
      fpath = File.join(folder_path, e.to_s)
      FileUtils.mkdir_p(File.dirname(fpath))
      zipfile.extract(e, fpath) { true }
    end
  end
  folder_path
end
tmp_folder_path = extract_data_file zipfile
data_file_paths = Dir["#{tmp_folder_path}/*.csv", "#{tmp_folder_path}/*.xlsx", "#{tmp_folder_path}/*.xls"]
Zip::File.open(zipfile)
Zip::File.open(zipfile) do |zipfile|
  zipfile.each do |e|
    puts e.to_s
    fpath = File.join(folder_path, e.to_s)
    puts fpath
  end
end
zipfile = '/Users/Jonathan/Downloads/Hisense-example.zip'
Zip::File.open(zipfile) do |zipfile|
  zipfile.each do |e|
    puts e.to_s
    fpath = File.join(folder_path, e.to_s)
    puts fpath
  end
end
tmp_folder_path = extract_data_file tmp_file
tmp_folder_path = extract_data_file zipfile
data_file_paths = Dir["#{tmp_folder_path}/*.csv", "#{tmp_folder_path}/*.xlsx", "#{tmp_folder_path}/*.xls"]
data_file_paths += Dir["#{tmp_folder_path}/**/*.csv", "#{tmp_folder_path}/**/*.xlsx", "#{tmp_folder_path}/**/*.xls"]
data_file_paths = Dir["#{tmp_folder_path}/**/*.csv", "#{tmp_folder_path}/**/*.xlsx", "#{tmp_folder_path}/**/*.xls"]
Zip::File.open(zipfile) do |zipfile|
Zip::File.open(zipfile) do |file|
  p file
end
Zip::File.open(zipfile).first
Zip::File.open(zipfile)[1]
file = Zip::File.open(zipfile)
file.class
file.each do |zipfile|
  puts zipfile.path
end
file.each do |zipfile|
  puts zipfile.to_s
end
file.each do |zipfile|
file.class
file.first.class
file.each do |entry|
  fpath = File.join(folder_path, entry.to_s)
  puts File.dirname(fpath)
end
file
zipfile
tmp_folder_path = "/Users/Jonathan/Downloads/Hisense-example"
data_file_paths = Dir["#{tmp_folder_path}/*.csv", "#{tmp_folder_path}/*.xlsx", "#{tmp_folder_path}/*.xls"]
Source.last
Source.last.destroy
Source.last
Source.last.destroy
Source.last
Source.last.destroy
Source.last
zipfile
amz-path = "/Users/Jonathan/Downloads/Amazon-example/Amz.zip"
amzpath = "/Users/Jonathan/Downloads/Amazon-example/Amz.zip"
extract_data_file
tmp_folder_path = extract_data_file amzpath
data_file_paths = Dir["#{tmp_folder_path}/**/*.csv", "#{tmp_folder_path}/**/*.xlsx", "#{tmp_folder_path}/**/*.xls"]
images_path = Dir["#{tmp_folder_path}/**/images"]
Product.where(model: 'LED42EC510N')
Product.where(model: 'E200t')
Product.first
Source.last
Scrape.last
Source.last
Source.last 3
OAOq
Source.last
Source.last.destroy
Source.last
Source.last.destroy
Source.last
Source.last.destroy
Source.last
Product.count
start = 77425
Scrape.last
start = 77425
Product.count - start
p = Product.last
p = Product.where(model: 'LED32E330N')
p = Product.where(model: 'LED32H35C')
p.minimum
p.scrape_product_sellers
p.reload!
reload!
p = Product.where(model: 'LED32H35C')
p = Product.where(model: 'LED32H35C').first
p.scrape_product_sellers
p.scrape_product_sellers.first
p.scrape_product_sellers.first.min_price
p.scrape_product_sellers.first.max_price
p.product_description
p.details
p.details.first
p.original_url
reload!
Scrape.last
s = Scrape.last
scrape_prods = s.products
with_manuals = scrape_prods.where(has_manual: true)
s.id
Product.includes(:scrapes).where("scrapes.id": 17)
Product.includes(:scrapes).where("scrapes.id": 17).size
products = Product
.includes(scrapes: [:categories, scrape_product_sellers: [:seller]])
products = Product.includes(scrapes: [:categories, scrape_product_sellers: [:seller]])
products = Product.includes(scrapes: [:categories, scrape_product_sellers: [:seller]]).where("scrapes.id": 17)
products.size
products = Product.includes(:scrapes, :categories, scrape_product_sellers: [:seller])
products = Product.includes(:scrapes, :categories, scrape_product_sellers: [:seller]).where("scrapes.id": 17)
products.size
products.first
products.first.min_price
products.first.min_price?
products.where.not("scrape_product_sellers.min_price": nil)
w_price = products.where.not("scrape_product_sellers.min_price": nil).pluck(:product_id).uniq.size
w_price = products.where.not("scrape_product_sellers.min_price": nil).uniq.size
products.last.categories
products.map(&:categories()
products.map(&:categories)
_.reject(&:empty?)
load 'scrape_stats.rb'
reload!
load 'scrape_stats.rb'
Sellers
Seller.last
Seller.last 5
p = products.last
p.details
Category
Category.all
Category.find_by name: 'TV'
Category.find_by name: 'TABLETS'
Category.find_by name: 'MOBILE PHONE'
Category.pluck(:name)
scrape_cats = %w[Television\ (ATSC)]
scrape_cats = %w[Television\ (ATSC) TV Television\ Combo\ (ATSC)\ DVD\ Player Smartbox Soundbar Projector Mobile\ Phone]
data = {:min_price: 2000}
data = {min_price: 2000}
price = data.extract!(:min_price, :max_price)
price
price = '¥ 5499'
Currency
Currency.find_by symbol: price.split(' ').first
Currency.all
price
price_val = price.map {|x| x[/\d+/]}
price_val = price.scan(/\d+/).first
price += '.50'
price_val = price.scan(/\d+/).first
price_val = price.scan(/\d+/./\d+/).first
price_val = price.scan(/\d+.\d+/).first
price
Money.parse(price)
Monetize.parse price
p = _
p.amount
p.amount.to_f
Monetize.assume_from_symbol = true
Monetize.parse price
price
price.gsub!(' ', '')
Monetize.parse price
p.amount.to_f
p.to_money
p
Monetize.assume_from_symbol
Monetize.parse(price)
price = '¥ 5499.55'
Monetize.parse(price)
price = '¥5499.55'
Monetize.parse(price)
Monetize.parse("YEN 5499")
money = _
money.to_dollars
money.to_money
Money.default_bank
exit
require 'money/bank/google_currency'
exit
Money.default_bank
Money.default_bank = Money::Bank::GoogleCurrency.new
require 'money/bank/google_currency'
Money.default_bank = Money::Bank::GoogleCurrency.new
price = '¥5499.55'
Monetize.parse(price)
money = 5499.55.to_money(:YEN)
money.exchange_to(:USD)
money.exchange_to(:USD).to_f
money = 5499.55.to_money(:CNY)
money.exchange_to(:USD).to_f
money
money.symbol
Money.new(5500, money.symbol)
Money.default_bank
Monetize.parse(price)
Money::Currency
Money::Currency.al
Money::Currency.all
Money::Currency.all.select {|s| s.symbol == money.symbol }
price = 'CNY 1930'
p = Monetize.parse price
p.convert_to(:USD)
money.exchange_to(:USD)
price
price = {min_price: price}
min_price = Monetize.parse(price[:min_price])
min_price.symbol
min_price.to_f
min_price.amount
Monetize.parse(nil)
money
money.currency
money Monetize.parse("1000 CNY")
money = Monetize.parse("1000 CNY")
money = Monetize.parse("1000.00 CNY")
money.exchange_to(:USD)
money.exchange_to(:USD).to_f
OtherProduct.includes(:scrape_other_product_sellers).where("scrape_other_product_sellers.original_url": nil)
OtherProduct.ids.first
op = OtherProduct.includes(:scrape_products_sellers).find(1)
op = OtherProduct.includes(:scrape_product_sellers).find(1)
op = OtherProduct.includes(:scrape_other_product_sellers).find(1)
op.scrape_other_product_sellers.where(original_url: nil)
op = OtherProduct.last
op.scrape_other_product_sellers.where(original_url: nil)
op
op = OtherProduct.includes(:scrape_other_product_sellers).where("scrape_other_product_sellers.original_url": nil)
op = OtherProduct.includes(:scrape_other_product_sellers).where("scrape_other_product_sellers.original_url": nil).find(op.id)
op
op = OtherProduct.includes(:scrape_other_product_sellers).where("scrape_other_product_sellers.original_url": nil).find(OtherProduct.last.id)
reload!
op = OtherProduct.includes(:scrape_other_product_sellers).where("scrape_other_product_sellers.original_url": nil).find(OtherProduct.last.id)
op.scrape_other_product_sellers
op = OtherProduct.last
op.scrape_other_product_sellers
exit
Sidekiq::Queue.new("dedup_other_product").clear
Sidekiq::Queue.new("update_original_url").clear
Sidekiq::Queue.new("update_original_url_queue").clear
exit
price = "CNY 1000"
Monetize.parse price
money - Monetize.parse price
money = Monetize.parse price
money.exchange_to(:USD)
exit
price = "CNY 1000"
money = Monetize.parse price
money.exchange_to(:USD)
bank
require 'money/bank/google_currency'
bank = Money::Bank::GoogleCurrency.new
Money::Bank::GoogleCurrency.ttl_in_seconds = 604800
bank = Money::Bank::GoogleCurrency.new
Money.default_bank = bank
price = "CNY 1000"
money = Monetize.parse price
money.exchange_to(:USD)
exit
price = "CNY 1000"
money = Monetize.parse price
money.exchange_to(:USD)
money.exchange_to(:USD).to_f
Currency
Currency.connection
exit
price = "464.70:
price = "464.70"
money = Monetize.parse price
money.exchange_to(:CNY).to_f
price = "464.70"
money = Monetize.parse price
money.exchange_to(:CNY).to_f
ScrapeOtherProductSeller.first
Product.find_by model: 'LCD TV 26"http://data.manualslib.com/pdf3/77/7612/761175-konka_group/dlc3211ut.pdf?ac213cc073e0a79d04180218a0bd7500&take=binaryKONKADLC-2611UTTelevision (ATSC)"ATSC
DVD"http://data.manualslib.com/pdf3/77/7612/761175-konka_group/dlc3211ut.pdf?ac213cc073e0a79d04180218a0bd7500&take=binary"Color System : NTSC_M, ATSC/Free QAM
Sound System : BTSC/SAP
Frequency range: Antenna(2~69), Cable(2~135)
Audio output power 10%THD 10W X 2
Antenna Impedance 75Ω(Unbalance)
Power Consumption 160W(26”) , 180W(32”)，200W(37”)
Power Supply : AC~110-240V, 60Hz
item Port list
1 RF ANTENNA & Cable
3 Composite
4 S_VIDEO
5 Y、Cb/Pb、Cr/Pr
6 VGA
7 HDMI
Product.find_by model: 'DLC-2611UT'
ProductManual.last
p = Product.find 77414
p.scrapes
p
model = ProductManual
ProductManual.is_a? ProductManual
asset = ProductManual.last
scrape.last.products
Scrape.last.products
Scrape.last.products.size
Scrape.last.products.destroy_all
Source.last
Source.last.destroy
Scrape.last
Scrape.last.destroy
Scrape.last
Scrape.last.destroy!
ProductAlternateNames
ProductAlternateName
ProductAlternateName.where(scrape_id: 17)
ProductAlternateName.where(scrape_id: 17).destroy_all
Scrape.last.destroy
ProductDetail.where(scrape_id: 17).destroy_all
Scrape.last.destroy
Asset
Asset.where(scrape_id: 17).destroy_all
Scrape.last
Scrape.last.destroy
Source.last
Scrape.last
Source.last
Scrape.last
Scrape.last 2
Scrape.last(2).destroy_all
Scrape.last(2).each &:destroy
Source.last
Source.last 2
Scrape.last
scrape = Scrape.last
scrape.products
scrape.products.where(has_manual: true)
scrape.products.where(model: 'TDM1211')
Product.where(model: 'TDM1211')
Product.where(model: 'TDM1311')
p = Product.where(model: 'TDM1311')
ProductManual.where(assetable_id: p.id)
ProductManual.where(assetable_id: p.first.id)
ProductManaul.last
ProductManual.last
Scrape.last
Asset.where(scrape_id: 20)
p = Product.last
p.product_description
p.details
Product.where model: 'LED42H35D'
p = _
p = p.last
p.scrapes
p
Scrape.last.products.destroy_all
Scrape.last.destroy
ProductAlternateName.where(scrape_id: 20).destroy_all
Scrape.last.destroy
ProductDetails.where(scrape_id: 20).destroy_all
ProductDetail.where(scrape_id: 20).destroy_all
Scrape.last.destroy
Source.last
Source.last.destroy
Source.last
Scrape.last
Source.last
Source.last.destroy
Source.last
Product.last
Product.last.scrapes
Scrape.last
Scrape.last.products
Scrape.last.products.last
Scrape.last.products.first
Scrape.last.products
prods = _
prods.where(has_manual: true)
prods.where(model: 'DLC3211UT')
Product.where(model: 'DLC3211UT')
Product.where(model: 'DLC3211UT').first.manuals
p = Product.where(model: 'DLC3211UT').first
p.images
p.image
scrape
scrape = Scrape.last
scrape.products
scrape.products.size
scrape.products
p = Product.find_by(model: 'DLC-2611UT')
model = p.model
model = p.model.gsub(/\//, '')
model = p.model.gsub(/\/\-/, '')
model = p.model.gsub(/-/, '')
p.model.gusb(/[^0-9A-Za-z]/, '')
p.model.gsub(/[^0-9A-Za-z]/, '')
p.model
model = Product
model == Product
paths = ["path/to/model-A", "path/to/model-A"]
paths.uniq
paths = ["path/to/model-A", "path/to/modelA"]
paths.uniq
p
p.models
Product.any? {|p| p.models.size > 0 }
p = Product.find 2
p.models
p.model
scrape.last
s = Scrape.last
s.products
s.products.destroy_all
s.destroy
ProductAlternateName.where(scrape: s).destroy_all
s.destroy
ProductDetail.where(scrape: s).destroy_all
s.destroy
Asset.where(scrape: s).destroy_all
s.destroy
Asset.where(scrape_id: s.id).destroy_all
s.destroy
source.last
Source.last
Source.last.destroy
Source.last
s = Scrape.last
prods = s.products
prods
prods.where(has_manual: true)
prods.where(has_manual: true).size
mans = _
manuals = prods.where(has_manual: true)
mans.last
manuals.last
manuals.last.images
prods.where(model: 'DT138U').first
_.images
ProductImage.to_s.downcase.gsub('Product', '')
ProductImage.last
s = Scrape.last
p = Product.last
p.scrapes
s = Scrape.last
prods = s.products
prods.where(model: 'DT138U')
prods.where(model: 'DT138U').first
p = _
p.images
p = prods.where(model: 'LED50H35A').first
p.scrape_product_sellers
p.details
p
p = prods.where model: 'NS27HTV'
prods.where(has_manual: true)
p = prods.where(has_manual: true).first
mans = p.manuals
mans.size
mans.last
mans
ScrapeProductSeller.where(seller_id: nil)
ScrapeProductSeller.where.not(min_price: nil)
p
p.scrape_product_sellers
p
p.scrape_product_sellers
sps = ScrapeProductSeller.find_or_create_by(scrape_id: 23, product_id: p.id, seller_id: nil, original_url: 'http://data.manualslib.com/pdf3/77/7612/761175-konka_group/dlc3211ut.pdf?ac213cc073e0a79d04180218a0bd7500&take=binary')
Seller.find_by name: nil
ProductManual.last
s = Scrape.last
prods = s.products
prods
p = prods.where(model: 'LED32H35C').first
p.scrape_product_sellers.last
p.scrape_product_sellers.last.min_price
p.scrape_product_sellers.last.min_price.to_f
p = prods.where(model: 'LED50H35A').first
p.scrape_product_sellers.last.min_price.to_f
ProductManual.where(id: 984537)
history
hist --grep queue
Sidekiq::Queue.new("elasticsearch_indexing_queue").clear
edit product.rb
edit 'app/models/product.rb'
edit -t
save_images
edit -t
.pwd
.ls
.mv AmazonManualScrape.zip ~/
.ls
.mv scrape_stats.rb ~/
show-doc Array#sort
.gem install pry-doc
show-doc Array#sort
reload!
show-doc Array#sort
require 'pry-doc'
require 'prydoc'
exit
s = Scape.last
s = Scrape.last
s.products
ProductsAlternateModel
s.products.destroy_all
s.destroy
ProductAlternateName.where(scrape_id: s.id).each(&:destroy)
s.destroy
ProductDetail.where(scrape_id: s.id).each(&:destroy)
s.destroy
Asset.where(scrape_id: s.id).each(&:destroy)
s.destroy
Source.last
Source.last.destroy
Source.last
Scrape.last
Scrape.last.products.each(&:destroy)
Scrape.last
Scrape.last.destroy
exit
@keys = [
  :product_name, :product_description, :product_category, :product_url,
  :brand, :model, :alternate_models, :product_features, :contact_name,
  :seller_name, :min_price, :max_price
]
keys = _
keys.map! {|k| k.to_s.gsub('_', ' ').capitalize}
keys
keys.map! {|k| k.split(' ').map(&:capitalize).join(' ') }
p = Product.last
p.detail
p
p.original_url
def generate_key_mapping(header_row)
  header_row.each.with_index.each_with_object({}) do |e, a|
    a[e.first.to_sym] = e.last
  end
end
def open_spreadsheet(file_path)
  case File.extname(file_path)
  when '.csv' then Roo::CSV.new(file_path)
  when '.xls' then Roo::Excel.new(file_path)
  when '.xlsx' then Roo::Excelx.new(file_path)
  else fail "Unknown file type: #{file_path}"
  end
end
f = open_spreadsheet '/Users/jonathan/Desktop/small-konka/small_konka.xlsx'
first = f.first_row
key_mapping = generate_key_mapping f.row(first)
data_hash_keys = key_mapping.keys - @keys
@keys
@keys = [
  :product_name, :product_description, :product_category, :product_url,
  :brand, :model, :alternate_models, :product_features, :contact_name,
  :seller_name, :min_price, :max_price
]
data_hash_keys = key_mapping.keys - @keys
row = f.row(2)
data = {}
value = key_mapping[@keys.first]
data[@keys.first] = row[0]
data
key = data_hash_keys.first
h = row[key_mapping[key]]
h
key
h
data_hash_category = :General
data[:data_hash] ||= {}
data[:data_hash][:data_hash_category] ||= {}
data[:data_hash][:data_hash_category][key] = {}
data[:data_hash][:data_hash_category]
data[:data_hash][:data_hash_category][key] = h
f.row(1)
header_row = _
header_row.each.with_index.each_with_object({}) do |e, a|
  a[e.first.to_sym] = e.last
end
header_row.first
[header_row.first, header_row.first.map{|v| v.gsub(' ')}
]
[header_row.first, header_row.first.map{|v| v.gsub(' ')} ]
[header_row.first, header_row.first.gsub(' ') ]
[header_row.first, header_row.first.gsub('_', ' ') ]
map_key =     header_row.each.with_index.each_with_object({}) do |e, a|
  a[e.first.to_sym] = e.last
end
map_key = [header_row.first, header_row.first.gsub('_', ' ') ]
hash = { map_key => :product_name }
hash["product name"]
key = "product name"
key = "Product Name"
key.downcase
key.downcase.gsub(' ', '_')
@keys
key
key_sym = key.downcase.gsub(' ', '_').to_sym
@keys.include? key_sym
def generate_key_mapping(header_row)
  header_row.each.with_index.each_with_object({}) do |e, a|
    key_sym = e.first.downcase.gsub(' ', '_').to_sym
    key = @keys.include? key_sym ? key_sym : e.first.to_sym
    a[key] = e.last
  end
end
@keys
hist --grep key_mapping
hist --grep open_spreadsheet
f = open_spreadsheet '/Users/jonathan/Desktop/small-konka/small_konka.xlsx'
first = f.first_row
key_mapping = generate_key_mapping f.row(first)
header_row = f.row(first)
key_sym = header_row.first.downcase.gsub(' ', '_').to_sym
key = @keys.include? key_sym ? key_sym : header_row.first.to_sym
key = @keys.include?(key_sym) ? key_sym : e.first.to_sym
def generate_key_mapping(header_row)
  header_row.each.with_index.each_with_object({}) do |e, a|
    key_sym = e.first.downcase.gsub(' ', '_').to_sym
    key = @keys.include?(key_sym) ? key_sym : e.first.to_sym
    a[key] = e.last
  end
end
key_mapping = generate_key_mapping f.row(first)
f = open_spreadsheet '/Users/jonathan/Desktop/small-konka/small_konka.xlsx'
first = f.first_row
key_mapping = generate_key_mapping f.row(first)
header_row = f.row(first)
data_hash_keys = key_mapping.keys - @keys
row = f.row(2)
data = {}
@keys.each do |key|
  value = key_mapping[key]
  data[key] = row[value] unless value.nil?
end
data
Scrape.last
reload!
s = Scrape.last
s.products.first
s = Scrape.last
s.products
s.products.last
p = _
p.detail
p2 = Product.find(p.id - 1)
p.original_url
p.scrape_product_sellers
"min_price".downcase.gsub(' ', '_').to_sym
":min_price".downcase.gsub(' ', '_').to_sym
s = Scrape.last
s.products.last
p = _
p.scrape_product_sellers
s.products
s.products.includes(:scrape_product_sellers).where.not("scrape_product_sellers.min_price": nil)
str = 'Min Price'
str.downcase.gsub(' ', '_').to_sym
s = Scrape.last
prods = s.products
prods.last
p = _
p.scrape_product_sellers
s = Scrape.last
s.products
s.products.where(has_manual: true).size
ProductImage
ProductImage.where(scrape_id: s.id).size
Category.last
Category.last 10
Category.last 11
Category.last 12
Category.last 13
Category.last 12
s
s.source
Category.last 12
s.source
s.products
s.products.select {|p| p.images.empty? }
p
Product
OtherProduct
price = price: 'USD 1000'
price = {price: 'USD 1000'}
Monetize.parse(price[:min_price]).exchange_to(:USD)
min_price = _
min_price.nil?
price
min_price = price[:min_price] && Monetize.parse(price[:min_price]).exchange_to(:USD)
price = price[:price] && Monetize.parse(price[:price]).exchange_to(:USD)
p = {min: 1234.00, max: nil, currency: '$'}
p.compact
Monetized.parse("￥8999""
Monetized.parse("￥8999"")
￥8999
no_imgs = s.products.select {|p| p.images.empty? }
no_imgs.pluck(:model)
no_imgs.map(&:model)
no_imgs.map(&:model).sort
Product.first
p = _
p.details
p.details.size
p = Product.find_by model: 'LA1027KND6'
s.products
s.products.first
s.products.first.details
p = Product.find_by model: '
p = Product.find_by model: 'NS27HTV'
p.details
p = Product.find_by model: 'DLC-2611UT'
p.details
data = {web_specs: _.data_hash[:General][:Specifications], manual_specs:  _.data_hash[:General][:"User Manual Specs"]}
detail = ProductDetail.find(157538)
detail.data_hash
data = {web_specs: detail.data_hash[:General][:Specifications], manual_specs: detail.data_hash[:General][:"User Manual Specs"]}
data[:manual_specs]
data[:manual_specs].gsub(' :', :)
data[:manual_specs].gsub(' :', ':')
JSON.parse _
data[:manual_specs].gsub(' :', ':')
edit -t
pm = ProductManual.last
pm.original_url
pm.url
pm.name
pm.product
p = _
p.detail
pm
pm.attachment
pm.attachment.url
Source.last
Scrape.last
s = _
s.source
sps = ScrapeProductSeller.where.not(seller_id: nil).first
sps_map = {}
edit -t
Seller.new name: "UNKNOWN"
seller = _
seller.destroy
Seller.last
seller = Seller.new name: "UNKNOWN"
Seller.last
sps_map
sps
sps.seller
sps_map[sps.seller] = sps_map.key?(sps.seller) ?
[sps_map[sps.seller], sps].max_by(&:updated_at)
:
sps
sps_map
sps2 = ScrapeProductSeller.last
sps2 = ScrapeProductSeller.where(seller_id: nil).last
sps3 = ScrapeProductSeller.last
sps3.seller
Seller.where name: nil
_.size
Scrape.last
sps2
sps3
sps = sps3
sps_map[sps.seller] = sps_map.key?(sps.seller) ?
[sps_map[sps.seller], sps].max_by(&:updated_at)
:
sps
sps_map
Seller.first
Seller.where.not(image_url: nil).first
Seller.where.not(image_url: nil, name: ["", nil]).first
map =     sps_map.values.map do |sps|
  {
    id: sps.seller.id,
    name: sps.seller.name,
    image_url: sps.seller.image_url,
    min_price_cents: sps.min_price_cents
  }
end
map
product
p
p.scrape_product_sellers.map do |sps|
scrape_product_sellers.inject({}) do |h, sps|
p.scrape_product_sellers.inject({}) do |hash, sps|
  next unless h[sps.seller].nil?
p.scrape_product_sellers.inject({}) do |hash, sps|
  next unless hash[sps.seller].nil?
p.scrape_product_sellers.inject({}) do |hash, sps|
  hash[sps.seller] = hash.key?(sps.seller) ?  [sps_map[sps.seller], sps].max_by(&:updated_at) : sps
end.values.map do |sps|
  { id: sps.seller.id, name: sps.seller.name, image_url: sps.seller.image_url, min_price_cents: sps.min_price_cents }
end
p.scrape_product_sellers.inject({}) do |hash, sps|
  hash[sps.seller] = hash.key?(sps.seller) ?  [sps_map[sps.seller], sps].max_by(&:updated_at) : sps
end
_
object = p
sps_map = {}
hash =     sps_map = {}
object.scrape_product_sellers.each do |sps|
  next if sps.seller.nil?
  sps_map[sps.seller] = sps_map.key?(sps.seller) ?
  [sps_map[sps.seller], sps].max_by(&:updated_at)
  :
  sps
end
sps_map.values.map do |sps|
  {
    id: sps.seller.id,
    name: sps.seller.name,
    image_url: sps.seller.image_url,
    min_price_cents: sps.min_price_cents
  }
end
hash
p
sps_map = {}
p.scrape_product_sellers.each do |sps|
  next if sps.seller.nil?
  sps_map[sps.seller] = sps_map.key?(sps.seller) ?
  [sps_map[sps.seller], sps].max_by(&:updated_at)
  :
  sps
end
sps_map.values.map do |sps|
  {
    id: sps.seller.id,
    name: sps.seller.name,
    image_url: sps.seller.image_url,
    min_price_cents: sps.min_price_cents
  }
end
sps_map
sps_map = {}
sps_map
p
p.serializable_hash
ProductSerializer.new(p)
ps = _
ps.sellers
p.product_scrape_sellers
p.scrape_product_sellers
p = Product.last
ProductSerializer.new(p)
_.sellers
p
sps = p.scrape_product_sellers.first
sps = ScrapeProductSeller.where.not(seller_id: nil).first
sps = ScrapeProductSeller.where.not(seller_id: nil).last
sps
sps.seller
p = sps.product
ps = ProductSerializer.new(p)
ps.sellers
nil.present?
Currency.where symbol: nil
{}.present
{}.present?
{}[:price]
_
h = {}
h[:price]
h
price = nil
def parse_price(price)
  return {} if price[:min_price].nil? && price[:min_price].nil? && price[:price].nil?
  # ScrapeProductSeller expects price in USD -- Stores price as cents
  min_price = price[:min_price] && Monetize.parse(price[:min_price]).exchange_to(:USD)
  max_price = price[:max_price] && Monetize.parse(price[:max_price]).exchange_to(:USD)
  price = Monetize.parse(price[:price]).exchange_to(:USD)
  symbol = '$'
  {
    min: min_price.try(:to_f) || max_price.try(:to_f) || price.try(:to_f),
    max: max_price.try(:to_f),
    currency: symbol
  }
end
parse_price price
parse_price {}[:price]
parse_price {}.extract!(:price)
{}.extract!(:price)
price = {}.extract!(:price)
parse_price price
p
p.scrape_product_sellers
p
reload!
ps = ProductSerializer.new(p)
ps.sellers
ScrapeProductSeller.where(seller: nil)
nil_sps = _
nil_sps
nilwprice = ScrapeProductSeller.where(seller_id: nil).where.not(min_price: nil)
Seller.where(name: nil)
seller = )
seller = Seller.where(name: nil)
ScrapeProductSeller.where(seller: seller)
sps = _
sps.size
sps.each {|sps| sps.update(seller: nil) }
seller.destroy
seller
seller.first.destroy
seller = seller.first
ScrapeProductSeller.where(seller: seller)
sps = _
seller
sps
sps = ScrapeProductSeller.where(seller: seller)
sps.each {|s| s.update_attributes(seller: nil) }
sps.each {|s| s.update_attributes(seller_id: nil) }
sps
sps.to_a.each {|s| s.update_attributes(seller: nil) }
sps.first
sps
ScrapeProductSeller.where(seller_id: seller.id)
sps = ScrapeProductSeller.where(seller_id: seller.id)
sps
sps.first.update_attributes(seller: nil)
sps.each(&:destroy)
_.size
p
p.scrape_product_sellers
p.reload
p.scrape_product_sellers
ps = ProductSerializer.new(p)
ps.sellers
reload!
ps = ProductSerializer.new(p)
ps.sellers
reload!
ps = ProductSerializer.new(p)
ps.sellers
map = _
map.compact
reload!
ps = ProductSerializer.new(p)
ps.sellers
map = _
map.map(:min_price)
map.map(&:min_price)
map.map(|h| h[:min_price_cents].to_f / 100)
map.map {|h| h[:min_price_cents].to_f / 100 }
ps.sellers
reload!
ps = ProductSerializer.new(p)
reload!
ps = ProductSerializer.new(p)
ps.sellers
["NO SELLER INFO"] * 3
3 * ["NO SELLER INFO"]
["NO SELLER INFO"] * 3 << nil
empty = _
ps.sellers.first
ps.sellers.first.values
ps.sellers.first.values == empty
reload!
ps = ProductSerializer.new(p)
ps.sellers
5502 / 100
p = Product.find 76940
p.scrape_product_sellers
h = {}
h[{name: "no"} = 1
h[{name: "no"}] = 1
h[name: "no"}] = 2
h[{name: "no"}] = 2
h
h[Seller.new(name: "no")] = 3
h
h[Seller.new(name: "no")] = 4
h
f = File.open("#{Rails.root}/spec/fixtures/sample_import_data.csv")
cd Tempfile.new
p Tempfile.new
cd Tempfile
ls
cd open
cd
path = "#{Rails.root}"
path = "#{Rails.root}/spec/fixtures/sample-import-data.zip"
s = Source.last
Scape.last 3
Scrape.last 3
s = Scrape.find 10
s.upload url
url = s.upload_url
s
Scrape
Scrape.last
s
s.source
url = s.source.upload_url
file = open(url)
file.class
Rails.env
Rails.env == 'test'
path = "#{Rails.root}/spec/fixtures/sample-import-data.zip"
File.new     allow(subject).to receive(:write_stream_to_a_temp_file).and_return(File.open(tmp_data_file))
File.new path
_.class
Tempfile.new path
f = File.new path
f.open?
f.closed?
f.close
f.closed?
f
s = Scrape.last
prods = s.products
prods.first
prods.first.image
p = Product.last
p.categories
ProductCategory.last
CategoryProduct.last
cp = _
cp.product
cp.product.scrapes
nil.blank?
nil.present?
''.present?
'  '.present?
Tempfile.class
File.class
String.class
sio = StringIO.new
sio.is_a?(File)
sio.closed?
sio.closed
sio.close
sio.class
OtherProduct.size
OtherProduct.count
hist --grep queue
j
exit
op = OtherProduct.where(brand: nil, model: nil)
op.size
op = OtherProduct.where(brand: nil)
op.size
op = OtherProduct.where(brand: nil).where.not(model: nil)
op.size
exit
edit -t
amazon_other_products
amazon_other_products.size
edit -t
amazon_other_products = OtherProducts.includes(:sources).where("sources.id": 3).where.not(url: nil)
amazon_other_products = OtherProduct.includes(:sources).where("sources.id": 3).where.not(url: nil)
amazon_other_products.size
amazon_products.size
amazon_products.first
p = _
p.detail
detail = p.detail
detail
detail.data_hash
p.original_url
p
p.original_url
p.scrape_product_sellers
p.details
p.details.map(&:data_hash)
gtins = ["00615798400767, 00615798401252, 00615798900052"]
p.model
p.original_url
op = OtherProduct.sample
op = OtherProduct.all.sample
op.detail
op.details
ops = OtherProduct.includes(:other_product_details).select do |op|
  op.detail.data_hash[:General][:ASIN]
asin_to_id = OtherProduct.includes(:other_product_details).map do |op|
hash = Hash.new {|h, k| h[k] = [] }
hash
hash[a]
hash[:a]
hash
hash[:a] << 'a'
hash[:b] << 'b'
hash
hash = Hash.new {|h, k| h[k] = [] }
OtherProduct.includes(:other_product_details).find_in_batches(batch_size: 100) do |products|
  products.each do |p|
op
op.detail
op.detail[:General][:ASIN]
op.detail.data_hash[:General][:ASIN]
OtherProduct.includes(:other_product_details).find_in_batches(batch_size: 100) do |products|
  products.each do |p|
    asin = p.detail.data_hash[:General][:ASIN]
    asin ||= p.detail.data_hash[:General][:asin]
    gtin = p.detail.data_hash[:General][:GTIN]
    gtin ||= p.detail.data_hash[:General][:gtin]
    hash[id] = {
OtherProduct.includes(:other_product_details).find_in_batches(batch_size: 100) do |products|
  products.each do |p|
    asin = p.detail.data_hash[:General][:ASIN]
    asin ||= p.detail.data_hash[:General][:asin]
    gtin = p.detail.data_hash[:General][:GTIN]
    gtin ||= p.detail.data_hash[:General][:gtin]
    hash[:by_asin] ||= {}
    hash[:by_gtin] ||= {}
    hash[:by_asin][asin] << p.id unless asin.nil?
    hash[:by_gtin][gtin] << p.id unless gtin.nil?
  end
end
OtherProduct.includes(:details).find_in_batches(batch_size: 100) do |products|
  products.each do |p|
    asin = p.detail.data_hash[:General][:ASIN]
    asin ||= p.detail.data_hash[:General][:asin]
    gtin = p.detail.data_hash[:General][:GTIN]
    gtin ||= p.detail.data_hash[:General][:gtin]
    hash[:by_asin] ||= {}
    hash[:by_gtin] ||= {}
    hash[:by_asin][asin] << p.id unless asin.nil?
    hash[:by_gtin][gtin] << p.id unless gtin.nil?
  end
end
edit -t
hist --grep hash
hash = Hash.new {|h, k| h[k] = [] }
edit -t
op = OtherProduct.find 10861
op.detail
op.details
ops = OtherProduct.includes(:details).all.select {|op| op.details.size > 1 }
reload!
edit -t
reload!
edit -t
reload!
edit -t
hash
p = OtherProduct.first
p.detail
p.details
OtherProduct.includes(:details).where.not("details.id": nil).first
OtherProduct.includes(:details).where.not("other_product_details.id": nil).first
p = _
p.detail
OtherProduct.includes(:details).where.not("other_product_details.id": nil).size
OtherProduct.size
OtherProduct.count
_ - 452896
edit -t
hash
edit -t
hash
hash[:by_asin]
hash.to_json
File.open('../asin_data.json', 'w') { |file| file.write(hash.to_json) }
edit -t
File.open('../asin_data.json', 'w') { |file| file.write(hash.to_json) }
p = Product.find_by model: 'FL5500A'
p = Product.find_by model: 'FL-4000'
p.details
p.details.first
detail = _
detail.attribute_values
detail
detail.attribute
detail.attributes
p.original_url
p.sources
p.detail
p.details
'00615798400767'.length
p.detail
op = OtherProduct.last
op.detail
op.detail.attributes
op.detail
ops = OtherProduct.select {|op| op.details.size > 1 }
p = Product.find_by model: 'FL-4000'
p.details
relaod!
reload!
Product
Product.includes(:details)
reload!
product = Product.includes(:details).where.not("product_details.id": nil).where('products.asin IS NULL OR products.gsin IS NULL').first
product = Product.includes(:details).where.not("product_details.id": nil).where('products.asin IS NULL OR products.gtin IS NULL').first
asin = nil
gtin = nil
product.details.each do |detail|
  next if detail.data_hash.nil?
  unless asin.present? || p.detail.data_hash[:General].nil?
    asin = p.detail.data_hash[:General][:ASIN]
  end
  unless gtin.present? || p.detail.data_hash[:"Universal Product Identifiers"].nil?
    gtin = p.detail.data_hash[:"Universal Product Identifiers"][:GTIN]
  end
  break if asin && gtin
end
asin
gtin
product
product.details
asin
product
product = Product.includes(:details).where.not("product_details.id": nil).where('products.asin IS NULL OR products.gtin IS NULL').first
asin = nil
gtin = nil
asin
gtin
product
product.details.each do |detail|
  next if detail.data_hash.nil?
  unless asin.present? || p.detail.data_hash[:General].nil?
    asin = p.detail.data_hash[:General][:ASIN]
  end
  unless gtin.present? || p.detail.data_hash[:"Universal Product Identifiers"].nil?
    gtin = p.detail.data_hash[:"Universal Product Identifiers"][:GTIN]
  end
  if asin && gtin
    break
  end
end
asin
product.details
product.detail
product = Product.includes(:details).where.not("product_details.id": nil).where('products.asin IS NULL OR products.gtin IS NULL').first
asin = nil
gtin = nil
detail_id = nil
product.details.each do |detail|
  next if detail.data_hash.nil?
  unless asin.present? || detail.data_hash[:General].nil?
    asin = detail.data_hash[:General][:ASIN]
    detail_id = p.details
  end
  unless gtin.present? || detail.data_hash[:"Universal Product Identifiers"].nil?
    gtin = detail.data_hash[:"Universal Product Identifiers"][:GTIN]
  end
  if asin && gtin
    break
  end
end
asin
gtin
product.update_attributes(asin: asin, gtin: gtin)
Product.find_by gtin: product.update_attributes(asin: asin, gtin: gtin)
Product.find_by gtin: gtin
exit
OtherProductDetail
reload!
OtherProductDetail
OtherProductDetail.connection
op = OtherProduct.first
op.details
exit
Product.where.not(gsin: nil)
Product.where.not(gtin: nil)
prods = _
prods.map(&:gtin)
prods.map(&:asin)
Product.where.not(asin: nil)
prods = _
prods.select {|p| p.asin.split(', ').length > 1}
prods = Product.where.not(gtin: nil)
prods.select {|p| p.asin.split(', ').length > 1}
prods.select {|p| p.gtin.split(', ').length > 1}
multi_gtin = _
multi_gtin.size
multi_gtin.map(&:gtin)
p = Product.find_by gtin: "00692114111002, 00852161003174"
p.asin
exit
prods = Product.where('products.gtin IS NOT NULL OR products.asin IS NOT NULL')
prods.size
exit
Sidekiq::Queue.new("elasticsearch_indexing_queue").clear
exit
Product.where.not(asin: nil).size
Product.where.not(gtin: nil).size
Product.size
Product.count
Product.size
Product.where.not(asin: nil).size
OtherProduct
OtherProduct.first
p = Product.all.sample
p
p.attribute_keys
p.detail.product_attribute_keys
p = Product.all.sample
p.detail
p.detail.attribute_keys
p.detail.attribute_values
p.detail.attribute_values.map &:value
p.detail.attribute_values
AttributeKey
ProductAttributeKey.all.map &:name
prods = Product.includes(details: [:attribute_keys]).where('product_attribute_keys.name': ['ASIN', 'GTIN'])
op = OtherProduct.first
Product.has_models(op.model).first
op.asin
op.detail
op = OtherProduct.sample
op = OtherProduct.all.sample
op.detail
Product.find_by(asin: 'B00YGIV38A')
op
op.detail
OtherProductDetail.where('data_hash like "%:ASIN%')
OtherProductDetail.where('data_hash like "%ASIN%')
OtherProductDetail.where("other_product_details.data_hash like %#{'ASIN'}%")
OtherProductDetail.where("other_product_details.data_hash like '%#{'ASIN'}%'")
reload!
OtherProduct.includes(:details).where.not("other_product_details.id": nil).where("data_hash is like '%ASIN%' OR data_hash is like '%GTIN%'").references(:other_product_details)
OtherProduct.includes(:details).where.not("other_product_details.id": nil).where("data_hash is like '%ASIN%' OR data_hash is like '%GTIN%'").references(:other_product_details).first
OtherProduct.includes(:details).where.not("other_product_details.id": nil).where("data_hash is like '%#{'ASIN'}%' OR data_hash is like '%#{'GTIN'}%'").references(:other_product_details)
OtherProduct.includes(:details).where.not("other_product_details.id": nil).where("data_hash is like '%#{'ASIN'}%' OR data_hash is like '%#{'GTIN'}%'").references(:other_product_details).first
OtherProduct.includes(:details).where.not("other_product_details.id": nil).where("other_product_details.data_hash is like '%#{'ASIN'}%' OR other_product_details.data_hash is like '%#{'GTIN'}%'").references(:other_product_details).first
OtherProduct.includes(:details).where.not("other_product_details.id": nil).where("other_product_details.data_hash is like '%#{'ASIN'}%' OR other_product_details.data_hash is like '%#{'GTIN'}%'")
OtherProductDetail.where("other_product_details.data_hash like '%#{'ASIN'}%'")
OtherProductDetail.where("other_product_details.data_hash like '%#{'ASIN'}%'").first
reload!
OtherProductDetail.where("other_product_details.data_hash like '%#{'ASIN'}%'").first
OtherProductDetail.where("other_product_details.data_hash like '%#{'ASIN'}%'").last
OtherProductDetail.where("other_product_details.data_hash like '%#{':ASIN:'}%'").last
OtherProduct.includes(:details).where.not("other_product_details.id": nil).where("data_hash like '%#{':ASIN:'}%' OR data_hash like '%#{':GTIN:'}%'")
OtherProduct.includes(:details).where.not("other_product_details.id": nil).where("data_hash like '%#{':ASIN:'}%' OR data_hash like '%#{':GTIN:'}%'").first 10
OtherProduct.includes(:details).where.not("other_product_details.id": nil).where("data_hash like '%#{':ASIN:'}%' OR data_hash like '%#{':GTIN:'}%'").first
op = _
op.detail
op.model
op.brand
op.details
op.name
OtherProduct.includes(:details).where.not("other_product_details.id": nil).where("data_hash like '%#{':ASIN:'}%' OR data_hash like '%#{':GTIN:'}%'").last
op.details
OtherProduct.includes(:details).where.not("other_product_details.id": nil).where("data_hash like '%#{':ASIN:'}%' OR data_hash like '%#{':GTIN:'}%'").size
OtherProduct.includes(:details).where.not("other_product_details.id": nil).where("data_hash like '%#{':ASIN:'}%' OR data_hash like '%#{':GTIN:'}%'").last
op = _
op.details
OtherProduct.includes(:details).where.not("other_product_details.id": nil).where("data_hash like '%#{':GTIN:'}%'").last
op = _
op.details
op.name
Product.includes(details: [:attribute_keys]).where('product_attribute_keys.name': ['ASIN', 'GTIN'])
edit -t
prods
prods.size
relaod!
reload
reload!
edit -t
prods.size
Product.count
Product.where(asin: nil)
_.size
edit -t
others.size
edit -t
reload!
others.size
edit -t
relaod!
reload!
edit -t
without_asin.size
OtherProduct.count - _
edit -t
reload!
edit -t
asins = asins.compact
asins.size
asins.uniq.size
Product.where(asin: asins.uniq)
Product.where(asin: asins.uniq).size
19.0 / asins.size
19.0 /1000
.019 * 400000
0.019 * 400000
edit -t
reload!
edit -t
prods = Product.includes(details: [:attribute_keys]).where('product_attribute_keys.name': ['ASIN', 'GTIN']).where.not("product_details.id": nil).where("data_hash like '%#{':ASIN:'}%' OR data_hash like '%#{':GTIN:'}%'").references(:product_details)
prods.size
edit -t
others.frist
others.first.details
edit -t
reload!
edit i-t
edit -t
others.size
others.first
others.first.details
Product.find_by_batch(batch_size: 100).each do |p|
  puts p.id
end
Product.all.find_by_batch(batch_size: 100).each do |p|
  puts p.id
end
reload!
Product.all.find_in_batches(batch_size: 100).each do |p|
  puts p.id
end
Product.all.find_in_batches(batch_size: 100).each do |p|
  p.each do |prod|
    puts p.id
  end
end
Product.all.find_in_batches(batch_size: 100).each do |prods|
  prods.each {|p| puts p.id }
end
Product.find_in_batches(batch_size: 100).each do |prods|
  prods.each {|p| puts p.id }
end
reload!
Product.find_in_batches(batch_size: 100).each do |prods|
  prods.each {|p| puts p.id }
end
reload!
['a', 'b', 'c'] | ['b']
['a', 'b', 'c'] & ['b']
Product.frist
Product.first
Product.all.first
reload!
Product.all.first
end
reload!
Product.all.first
Product.all.first 10
gtin = "00615798400767, 00615798401252, 00615798900052"
gtin.split(','
)
gtin.split(',').map!(&:trim)
gtin.split(',').map!(&:strip)
['a', 'b', 'c'] | ['d']
['a', 'b', 'c'] & ['d']
(['a', 'b', 'c'] & ['d']).empty?
(['a', 'b', 'c'] & ['d']).any?
gtin = "00615798400767, 00615798401252, 00615798900052"
reload!
gtin
gint.find { |g| g == '00615798900052' }
gtin.find { |g| g == '00615798900052' }
gtin
gtin.split(',').map(&:strip).find { |g| g == '00615798900052' }
gtin
reload!
Product.find_by_asin_or_gtin 'alaskdfj', gtin
gtin = Product.where.not(gtin: nil).last.gtin
Product.find_by_asin_or_gtin 'alaskdfj', gtin
reload!
Product.find_by_asin_or_gtin 'alaskdfj', gtin
_
gtin
exit
Product.first
Product.first.gtin?
Product.first.gtin
OtherProduct.first.gtin?
OtherProduct.first.gtin
p
p = Product.first
p.asin?
a = "blah" if p.asin.nil? && false
a
p
p.asin
p.asin = nil
p.save
p
'a' + 'b'
['a', ' b'].join(',')
['a', 'b'].join(',')
p
p.gtin
op.gtin
edit -t
product
op = product
p
p.gtin
op.gtin
gtin = (p.gtin.split(',').map(&:trim) + op.gtin.split(',').map(&:trim)).uniq.join(',')
gtin = (p.gtin.split(',').map(&:strip) + op.gtin.split(',').map(&:strip)).uniq.join(',')
p.gtin = _
gtin = (p.gtin.split(',').map(&:strip) + op.gtin.split(',').map(&:strip)).uniq.join(',')
p.reload!
p.reload
p.gtin
prods = Product.where.not(asin: nil, gtin: nil)
prods
prods.first
OtherProduct.count
Product.count
Product.where.not(asin: nil).size
OtherProduct.where.not(asin: nil).size
op_asins = OtherProduct.pluck(:asin).uniq
op_asins = OtherProduct.where.not(asin: nil).pluck(:asin).uniq
p_asins = Product.where.not(asin: nil).pluck(:asin).uniq
(p_asins & op_asins).size
(p_asins & op_asins)
join = _
join.size
join.uniq.size
join
OtherProduct.where(asin: join)
dedupable = _
dedupable.size
reload!
p_asins
ops = OtherProduct.where.not(asin: p_asins + [nil])
ops.size
ops.first.asin
op = ops.first
OtherProduct.where.not(gtin: nil).first
OtherProduct.where.not(gtin: nil).last
OtherProduct.where.not(asin: nil).first
OtherProduct.where.not(gtin: nil).last
other_products = OtherProduct.where.not('model is NULL OR asin IS NULL OR gtin IS NULL')
other_products = OtherProduct.where.not('other_products.asin IS NULL OR other_products.gtin IS NULL')
other_products = OtherProduct.where.not(asin: nil)
other_products.size
other_products = OtherProduct.where.not(asin: nil).where(gtin: nil)
other_products.last.asin
other_products.sample.asin
other_products.sample
op = OtherProduct.where.not(gtin: nil).sample
op.gtin
Entity.sample
exit
Entity.sample
Entity.all.sample
e = _
e.brands
edit entity.rb
p Product.all.sample
p = Product.all.sample
p
p.detail
p.details
products = Product.where.not(asin: nil)
p = products.sample
p.details
p
Product.search('HDMI')
Product.search('HDMI').results
Product.search('HDMI').response
exit
other_products.first
p Product.where.not(asin: nil).first
p.asin
p.details
p
p = Product.where.not(asin: nil).first
p.asin
p.brand
Product.search('HDMI').response
Sidekiq::Queue.new("elasticsearch_indexing_queue").clear
Sidekiq::Queue.new("dedup_other_product").clear
p_asin
p_asins
p_asin.size
p_asins.size
OtherProduct.where(asin: p_asins).size
p_gtins = Product.where.not(gtin: nil).pluck(:gtin).uniq
gtins_all = p_gtins.join(',')
gtins_all = gtins_all.split(',').map(&:strip)
gtins_all.size
gtins_all.uniq.size
gtins
p_gtins
gtins_all
gtins_all.join(',')
reload!
other_products_w_gtin = OtherProduct.find_by_asin_or_gtin(nil, gtins_all)
other_products_w_gtin = OtherProduct.find_by_asin_or_gtin(nil, gtins_all.join(',')
other_products_w_gtin = OtherProduct.find_by_asin_or_gtin(nil, gtins_all.join(','))
other_products_w_gtin.size
exit
Sidekiq::Queue.new("dedup_other_product").clear
reload!
gtins_all
p_gtins = Product.where.not(gtin: nil).pluck(:gtin).uniq
gtins_all = p_gtins.join(',')
gtins_all = gtins_all.split(',').map(&:strip)
gtins_all = _.uniq.join(',')
gtins_all
OtherProduct.find_by_gtin(gtins_all)
op = _
op
reload!
OtherProduct.find_by_asin_or_gtin(nil, gtins_all)
reload!
OtherProduct.find_by_asin_or_gtin(nil, gtins_all)
Product.find_by_gtin(_.gtin)
op
p = Product.find_by_gtin(_.gtin)
op
p
p = Product.find_by_asin_or_gtin('asdflaksjdfl')
p = Product.find_by_asin_or_gtin(nil, 'asdflaksjdfl')
reload!
p = Product.find_by_asin_or_gtin(nil, nil)
p = Product.find_by_asin_or_gtin(nil, op.gtin)
reload!
p = Product.find_by_asin_or_gtin(nil, op.gtin)
s = Scrape.last
s = Scrape.where(source_id: [1,2,3,4]).last
s.products
prods = _
prods.count
s.other_products
other_prods = _
other_prods.size
exit
p.nil?
p
a
a.nil?
exit
Product.find_by(asin: nil)
Product.where.not(asin: nil).find_by(asin: nil)(
Product.where.not(asin: nil).find_by(asin: nil)
Product.find_by(asin: nil)
p = _
p2 = Product.create(name: 'Jonathan', model: 'newman', gtin: p.gtin)
_.destroy
p = Product.where.not(asin: nil).last
p.asin
p2 = Product.create(name: 'Jonathan', model: 'newman', gtin: p.asin)
p2.destroy
reload!
p.asin
p2 = Product.create(name: 'Jonathan', model: 'newman', asin: p.asin)
p2.errors
p2.valid?
ops = OtherProduct.where('asin IS NOT NULL OR gtin is NOT NULL OR model IS NOT NULL')
ops.size
OtherProduct.count
p
Product.create(brand: p.brand, model: p.model)
Product.all(group: :asin)
Product.group(:asin).count
asins = Product.group(:asin).count
asins.class
asins.select {|k,v| v > 1}
asins = _
asins
asins.first
asins.delete {|k,v| k.nil?}
asins.delete_if {|k,v| k.nil?}
asins
Product.find_by(asin: asins.keys.first)a
Product.find_by(asin: asins.keys.first)
Product.where(asin: asins.keys.first)
asins
asins.size
keywords = ['local dimming', 'HDR', 'high dynamic range', 'AAC', 'multizone backlight',
'UHD dimming', 'Pixel Dimming', 'backlight scanning']
results = {}
query = Product.generate_query_by_keywords({ keywords: keyword }, 'AND')
search_results = Elasticsearch::Model.search(query, [Product, OtherProduct], size: 10000000).response
query = Product.generate_query_by_keywords({ keywords: keywords.first }, 'AND')
search_results = Elasticsearch::Model.search(query, [Product, OtherProduct], size: 10000000).response
products, other_products = search_results[:hits][:hits].partition { |r| r['_index'] == "products_development" }
products
ids = products.map(&:_id).map(&:to_i)
_prods = Product.where(id: ids)
uniq_prods
ids
uniq_prods = Product.where(id: ids)
uniq_prods.where(brand: 'Samsung')
uniq_prods.where(brand: 'Samsung').size
uniq_prods.where(brand: 'Samsung').first.name
Product
OtherProduct.count
load 'scrape_stats.rb'
keywords = ['Local Dimming', 'HDR', 'High Dynamic Range', 'Multizone Backlight', 'Full Array Backlight',
'UHD Dimming', 'Pixel Dimming', 'Backlight Scanning', 'AAC']
keywords = keywords.map do |keyword|
  [keyword, keyword.upcase, keyword.downcase]
end.flatten
keywords
load 'scrape_stats.rb'
results
edit -t
results
results.sort
reload!
Product.find_by_asin(nil)
Product.has_models(nil).first
reload!
Product.has_models(nil).first
reload!
Product.has_models(nil).first
reload!
Product.has_models(nil).first
reload!
Product.has_models(nil).first
reload!
Product.has_models(nil).first
reload!
Product.has_models(nil).first
Product.where(asin: nil)
Product.find_by_asin(nil)
Product.first(1000)
Product.first(1000).pluck(:asin, :gtin)
Product.pluck(:asin, :gtin).first(1000)
Product.first(1000).pluck(:asin, :gtin).each_with_object({}) do |p, obj| 
Product.first(1000).pluck(:asin, :gtin).each_with_object(Hash.new {|h, k| h[k] = []) do |p, obj| 
Product.first(1000).pluck(:asin, :gtin).each_with_object((Hash.new {|h, k| h[k] = [])) do |p, obj| 
Product.first(1000).pluck(:asin, :gtin).each_with_object((Hash.new {|h, k| h[k] = []})) do |p, obj| 
  obj[:asin] << p.asin
  obj[:gtin] << p.gtin
end
Product.first(1000).pluck(:asin, :gtin).each_with_object((Hash.new {|h, k| h[k] = []})) do |p, obj| 
Product.pluck(:asin, :gtin).first(1000).each_with_object((Hash.new {|h, k| h[k] = []})) do |p, obj| 
  obj[:asin] << p.asin
  obj[:gtin] << p.gtin
end
Product.pluck(:asin, :gtin).first(1000).each_with_object((Hash.new {|h, k| h[k] = []})) do |p, obj| 
  obj[:asin] << p[0]
  obj[:gtin] << p[1]
end
h[:asin]
Product.pluck(:asin, :gtin).first(1000).each_with_object((Hash.new {|h, k| h[k] = []})) do |p, obj| 
  obj[:asin] << p[0]
  obj[:gtin] << p[1]
end
h = _
h[:asin].delete_if {|e| e.nil? }
h[:gtin].delete_if {|e| e.nil? }
h
identifiers = Product.pluck(:asin, :gtin).each_with_object((Hash.new {|h, k| h[k] = []})) do |p, obj|
  obj[:asins] << p[0] unless p[0].nil?
  obj[:gtins] << p[1] unless p[1].nil?
end
identifiers[:asins
identifiers[:asins]
identifiers[:asins].uniq
identifiers[:asins].uniq.size
identifiers[:asins].size
uuids = Product.pluck(:asin, :gtin).each_with_object((Hash.new {|h, k| h[k] = []})) do |p, obj|
  obj[:asins] << p[0] unless p[0].nil?
  obj[:gtins] << p[1] unless p[1].nil?
end
asins = Product.where.not(asin: nil).pluck(:asin).uniq
OtherProduct.where('asin in :asins OR gtin IS NOT NULL OR model IS NOT NULL', asins: asins).first(1000)
reload!
OtherProduct.where('asin in :asins OR gtin IS NOT NULL OR model IS NOT NULL', asins: asins).first(1000)
OtherProduct.where('asin in (:asins) OR gtin IS NOT NULL OR model IS NOT NULL', asins: asins).first(1000)
ops = _
op.size
ops.size
OtherProduct.where('asin in (:asins) OR gtin IS NOT NULL OR model IS NOT NULL', asins: asins)
ops = _
ops.size
ops.where(asin: nil)
reload!
ops.where(asin: nil).size
op = OtherProduct.where.not(gtin: nil).first
op
op.gtin
op = OtherProduct.where.not(gtin: nil).first(50)
op = OtherProduct.find(12)
op
op.gtin
gtin = 
op.gtin
gtin = "5555555555555"
op.gtin.concat(gtin)
op.gtin.partition(',')
op.gtin.split(',')
op.reload
op.gtin
gtin = nil
g = op.gtin + ", #{gtin}"
g
g.split(',')
g.split(',').compact
g.split(',').map(&:strip).compact
g.split(',').map(&:strip).delete_if(&:blank?)
g.split(',').map(&:strip).delete_if(&:blank?).uniq.join(',')
nil.to_s
nil + ""
exit
def test(*keys)
  key = keys.map(&:to_sym)
end
test('ASIN')
test('Part number')
exit
*['ASIN']
def test(*keys)
  test('Part number')
def test(*keys)
  key = keys.map(&:to_sym)
end
test('ASIN')
test(['ASIN'])
test(*['ASIN'])
exit
:ASIN.to_sym
p
p = Product.where.not(gtin: nil).first
p.data_hash
p.detail.data_hash
exit
OtherProduct.count
exit
OtherProduct.count
exit
OtherProduct.count
458392 - _
OtherProduct.count
last = _
start = 458392
start - last
last
OtherProduct.count
last - _
last = last - _
last - OtherProduct.count
last = last - _
OtherProduct.count
start - _
start - OtherProduct.count
_ / 23000
_.to_f / 23000
_.to_f / 23000.to_f
2589.0 / 23000
_ * 100
_ * start
.11256 * start
0.11256 * start
OtherProduct.count
last
last = 455226
(start - last).to_f / start
_ * start
op = OtherProduct.all.sample
reload!
asins = Product.where.not(asin: nil).pluck(:asin).uniq
OtherProduct.where('asin in (:asins) OR gtin IS NOT NULL OR model IS NOT NULL', asins: asins).size
total = _
211600.0 / total
total - 211635
total / _
4 * 17
_ / 24
Sidekiq::Queue.new('dedup_other_product).clear
Sidekiq::Queue.new('dedup_other_product').clear
Product.where.not(gtin: nil)
_
p = Product.find 3
p.gtin
gtin = _
gtins = gtin.split(',').map(&:strip)
query = 'LIKE "%' + gtins.join('%" OR LIKE "%')
query
OtherProduct.where("gtin " + query + '%"')
query = "LIKE '%" + gtins.join("% OR LIKE "%')
query = "LIKE '%" + gtins.join("%' OR LIKE %')
query = "LIKE '%" + gtins.join("%' OR LIKE '%")
OtherProduct.where("gtin " + query + "%')
OtherProduct.where("gtin " + query + "%'")
OtherProduct.where("gtin " + query + "%')
OtherProduct.where("gtin " + query + "%'")
OtherProduct.where("other_products.gtin " + query + "%'")
OtherProduct.where(("other_products.gtin " + query + "%'"))
OtherProduct.where(gtin: gtin)
OtherProduct.where("other_products.gtin " + query + "%'")
OtherProduct.where("`other_products`.`gtin` " + query + "%'")
query
query = "LIKE '%" + gtins.join("%' OR gtin LIKE '%")
OtherProduct.where("`other_products`.`gtin` " + query + "%'")
reload!
Product.last 50
Oei
reload!
Product.where.not(gtin: nil).last 100
Product.where.not(gtin: nil).last(100).map(&:gtin)
gtins = _.first(20)
gtins = _.first(20).uniq.join(','
)
gtin = gtins.split(',').map(&:strip).join(',')
OtherProduct.find_all_by_gtin(gtin)
reload!
OtherProduct.find_all_by_gtin(gtin)
gtin
relaod!
reload!
OtherProduct.find_all_by_gtin(gtin)
gtin = gtins.split(',').map(&:strip).join(',')
Product.where.not(gtin: nil).last(10000).map(&:gtin)
gtin = gtins.split(',').map(&:strip).uniq.join(',')
gtins = Product.where.not(gtin: nil).last(10000).map(&:gtin)
gtin = gtins.split(',').map(&:strip).uniq.join(',')
gtins
gtin = gtins.uniq.join(',').split(',').map(&:strip).uniq.join(',')
OtherProduct.find_all_by_gtin(gtin)
gtin = gtins.first(500).uniq.join(',').split(',').map(&:strip).uniq.join(',')
OtherProduct.find_all_by_gtin(gtin)
_.size
relaod!
reload!
OtherProduct.find_all_by_gtin(gtin)
OtherProduct.find_all_by_gtin(Product.where.not(gtin.nil).sample)
OtherProduct.find_all_by_gtin(Product.where.not(gtin: nil).sample)
OtherProduct.find_all_by_gtin(Product.where.not(gtin: nil).sample.gtin)
reload!
OtherProduct.find_all_by_gtin(Product.where.not(gtin: nil).sample.gtin)
Product.where.not(gtin: nil).find_in_batches do |products|
  products.each do |product|
    others = OtherProduct.find_all_by_gtin(product.gtin)
    break unless others.empty?
  end
end
Product.where.not(asin: nil).find_in_batches do |products|
  products.each do |product|
    others = OtherProduct.find_all_by_asin(product.asin)
    break unless others.empty?
  end
end
reload!
Product.where.not(asin: nil).find_in_batches do |products|
  products.each do |product|
    others = OtherProduct.find_all_by_asin(product.asin)
    break unless others.empty?
  end
end
reload!
['a', 'b', 'd'] | [
['a', 'b', 'd'] | ['b', 'c']
['a', 'b', 'd'] | ['b', 'c', 'b']
exit
Product.where('asin IS NOT NULL OR gtin IS NOT NULL').first
Product.where('asin IS NOT NULL AND gtin IS NOT NULL').first
p = _
Product.where('asin IS NOT NULL AND gtin IS NOT NULL').last
p = _
OtherProduct.where(asin: p.asin).ids
OtherProduct.find_all_by_gtin(p.gtin).ids
OtherProduct.empty
OtherProduct.none
OtherProduct.none.ids
Sidekiq::Queue.new('dedup_other_product).clear
Sidekiq::Queue.new('dedup_other_product').clear
p = Product.find(8)
p.gtin
p.asin
OtherProduct.count
exit
exit
OtherProduct.count
exit
OtherProduct.where.not(gtin: nil).find_all_by_gtin(Product.where.not(gtin: nil).first.gtin)
Product.where('asin IS NOT NULL OR gtin IS NOT NULL').first
ids = OtherProduct.where.not(asin: nil).where(asin: product.asin).ids |
OtherProduct.where.not(gtin: nil).find_all_by_gtin(product.gtin).ids
product = Product.where('asin IS NOT NULL OR gtin IS NOT NULL').first
ids = OtherProduct.where.not(asin: nil).where(asin: product.asin).ids |
OtherProduct.where.not(gtin: nil).find_all_by_gtin(product.gtin).ids
OtherProduct.count
exit
[] | []
nil | []
[] | nil
Product.sort(update_at: :desc)
Product.all.sort(update_at: :desc)
Product.order(update_at: :desc)
Product.order(update_at: :desc).first
reload!
Product.order(update_at: :desc).first
Product.order(updated_at: :desc).first
p = _
p.models
p.details
p
p.details
OtherProduct.count
Product.find_by_asin('B0018O5STU')
OtherProduct.count
455037 - _
Product.where(asin: 'B00EEJZGGG')
Product.last.id
Sidekiq::Queue.new('dedup_other_product').clear
OtherProduct.count
exit
p = Product.first
p.models
p = Product.all.sample
p.models
ProductsAlternateModel.first
p = Product.find(2)
p.models
p.model
p.models.map(&:model) + [p.model]
p.model + p.models.map(&:model)
[p.mode]l + p.models.map(&:model)
[p.model] + p.models.map(&:model)
p = Product.where.not(gtin: nil, asin: nil)
Sidekiq::Queue.new('dedup_other_product').clear
Sidekiq::RetrySet.new.clear
Sidekiq::ScheduledSet.new.clear
Product.count
relaod!
reload!
Product.import
reload!
exit
p = Product.where(has_manual: true).first
Product.group(&:id)
Product.group(:id)
Product.group(:id).first(1000)
Product.group(:id).first(1000).first
p
p.to_hash
p.serializable_hash
ProductManual.first
pm = _
pm.text
exit
ProductImage.first
ProductImage.first.attachment.url
url = _
image = MiniMagick::Image.open(url)
image.width
image.height
image.size
image.size / 1000
path = '/Users/jonathan/Downloads/KONKA/images/A48F.png'
image = MiniMagick::Image.open(path)
image.size
image.size.to_f / 1000
image.resize "#{image.width * 0.5}x#{image.heigth * 0.5}"
image.hieght
image.height
image.resize "#{image.width * 0.5}x#{image.height * 0.5}"
image.path
image.resize "#{image.width * 0.6}x#{image.height * 0.6}"
image = MiniMagick::Image.open(path)
image.size
image.resize "#{image.width * 0.6}x#{image.height * 0.6}"
image.resize "#{image.width * 0.65}x#{image.height * 0.65}"
image = MiniMagick::Image.open(path)
image.resize "#{image.width * 0.65}x#{image.height * 0.65}"
path
s = Scrape.first
s = Scrape.includes(:source).where("sources.id": 3).first
prods = s.products
prods.first
prods.first.screenshots
prods.first.screenshots.first
prods.first.screenshots
ProductScreenshot.find 154532
screenshot = _
screenshot = _.size
screenshot = ProductScreenshot.find 154532
screenshot.attachment.url
url = screenshot.attachment.url
image = MiniMagick::Image.open(url)
screenshot
screenshot.attachment_file_size
screenshot.attachment_file_size.to_f / 1000
screenshot.attachment_file_size.to_f / 1000 / 1000
screenshot.attachment.url
path
encoded_image = Base64.encode64(File.open(path, 'rb').read)
asdf asdf new_file = File.open('../img', 'w') { |file| file.write() }
new_file = File.open('../img', 'w') { |file| file.write(encoded_image) }
new_file.open?
new_file.closed?
new_file
new_file.class
file = File.open('../img', 'rb').read
file
file.class
file = File.open('../img.png', 'wb') do |file|
file
encoded_file
encoded_file = file
file = File.open('../img.png', 'wb') do |file|
  file.write(Base64.decode64(encoded_file)
file = File.open('../img.png', 'wb') do |file|
  file.write(Base64.decode64(encoded_file))
end
path
image
image.size
image = MiniMagick::Image.open(url)
image = MiniMagick::Image.open path
image.szie
image.size
image.type
image.instance_methods
image.dimensiosn
image.dimensions
cmd = "echo 'hi'"
value = `#{cmd}`
wasGood = system( "echo 'hi'" )
wasGood = system( cmd )
directorylist = %x[find . -name '*test.rb' | sort]
exit
p = Product.first
p.get_document
Elasticsearch.client
p.client
Product.client
Elasticsearch::Client
Elasticsearch::Client.__id__
@client
Product.client
Product.__elasticsearch__.client
client = Product.__elasticsearch__.client
client.
edit -t
other_products.first.details
other_products.last.details
OtherProduct.count
OtherProduct.last
OtherProduct.where.not(asin: nil)
_.last
other_products = OtherProduct.where.not(gtin: nil)
OtherProduct.find 12
OtherProduct.find(12).gtin
OtherProduct.find(12).gtin.squish
OtherProduct.find(12).gtin.gsub(/\s+/, '')
Product.count
products = Product.where.not(gtin: nil).first
p = _
p.details
p
p.gtin.split(',').uniq
p.gtin.split(',').uniq.join(',')
edit -t
Sidekiq::Queue.new('elasticsearch_indexing_queue').clear
edit -t
Product.where.not(gtin: nil).first
Product.where.not(gtin: nil).third
p = _
p.gtin
p.gtin.split(',').uniq.join(',')
p = Product.where.not(gtin: nil).last
p = Product.where.not(gtin: nil).last(4)
Product.where.not(gtin: nil).each do |product|
p.gtin
p
p = Product.where.not(gtin: nil).third
p.gtin
p.gtin == p.gtin.split(',').sort
p.update_attributes gtin: p.gtin.split(',').sort.join(',')
reload!
p.update_attributes gtin: p.gtin.split(',').shuffle.join(',')
p = Product.find(3)
p.gtin
p.update_attributes gtin: p.gtin.split(',').shuffle.join(',')
p.update_attributes gtin: p.gtin.split(',').sort.join(',')
Product.where.not(gtin: nil).each do |p|
  p.update_attributes gtin: p.gtin.split(',').uniq.sort.join(',')
end
ops = OtherProduct.where.not(gtin: nil).where('model is NULL OR brand is NULL')
op.first
ops.first
op = _
Product.find_by_gtin(p.gtin)
op
Product.find_by_gtin(op.gtin)
Product.find_by_gtin(p.gtin)
Product.find_all_by_gtin(p.gtin)
reload!
Product.find_all_by_gtin(p.gtin)
Product.find_all_by_gtin(p.gtin).where.not(asin: nil)
Product.find_all_by_gtin(p.gtin).where.not(asin: nil).to_sql
reload!
Product.find_all_by_gtin(p.gtin).where.not(asin: nil).to_sql
Product.find_all_by_gtin(p.gtin).where.not(asin: nil)
Product.find_all_by_gtin(op.gtin)
Product.find_all_by_gtin(ops.sample.gtin)
relaod!
reload!
prods = Product.where.not(gtin: nil)
OtherProduct.find_all_by_gtin(prods.sample.gtin)
reload!
OtherProduct.find_all_by_gtin(prods.sample.gtin)
p
p.models
OtherProduct.find_by(model: p.models.map(&:model))
op
op.brand
Product.where(brand: nil)
Product.where(model: nil)
reload!
OtherProduct.find_all_by_gtin(p.gtin).merge( OtherProduct.find_all_by_asin(p.asin) ).to_sql
.where(Category.arel_table[:id].in(category_ids).
OtherProduct.find_all_by_asin(nil).ids
OtherProduct.find_all_by_asin(nil)
OtherProduct.find_all_by_asin(nil).class
OtherProduct.find_by(name: nil).class
OtherProduct.find_by(name: nil)
_.url
p
op
p.gtin.split(',') | op.gtin.split(',')
p.gtin.split(',') & op.gtin.split(',')
op.gtin = op.gtin + ",#{op.gtin}"
p.gtin.split(',') | op.gtin.split(',')
p.gtin.split(',') + op.gtin.split(',')
p.gtin.split(',') | op.gtin.split(',')
arr = [' ', 'asdf']
arr.compact
arr.reject(&:blank?)
asins = Product.where('asin IS NOT NULL OR gtin IS NOT NULL').pluck(:asin, :gtin)
asins.flatten
asins
asins.map_with_object(Hash.new {|h, k| h[k] = []} do |p|
asins.map_with_object(Hash.new {|h, k| h[k] = []}) do |p|
asins.map_with_object(Hash.new {|h, k| h[k] = []}) do |p, result|
  result[:asins] << p[0] unless p[0].nil?
  result[:gtins] << p[1] unless p[1].nil?
end
asins.each_with_object(Hash.new {|h, k| h[k] = []}) do |p, result|
  result[:asins] << p[0] unless p[0].nil?
  result[:gtins] << p[1] unless p[1].nil?
end
asins, gtins = _[:asins], _[:gtins]
asins
exit
hash = {a: "first", b: "second", c: 'third'}
a,b = hash
a
b
a,b = hash.values
a
b
c
p
Product.where(id: [123, 31, 3, 1]) + Product.where(id: [2, 4, 59, 7])
results = _
results.class
Product.first
p = _
Product.where.not(id: p)
by_asin = OtherProduct.find_all_by_asin(p.asin)
by_model = OtherProduct.where(model: [p.model] + p.models.map(&:model))
by_asin = OtherProduct.find_all_by_asin(p.asin)
by_gtin = OtherProduct.where.not(id: (by_asin | by_model)).find_all_by_gtin(p.gtin)
by_gtin = OtherProduct.where.not(id: (Product.where("name like 'Samsung%'" | by_model)).find_all_by_gtin(p.gtin)
by_gtin = OtherProduct.where.not(id: (Product.where("name like 'Samsung%'")| by_model)).find_all_by_gtin(p.gtin)
by_gtin = OtherProduct.where.not(id: (Product.where("name like 'Samsung%'")| by_model))
s = Scrape.last
s.destroy
Source.last
s = Scrape.last
s.products.first
OtherProducts.where(asin: s.asin)
OtherProduct.where(asin: s.asin)
p = s.products.first
OtherProduct.where(asin: p.asin)
p
p.seller
p.scrape_product_sellers
p.scrape_product_sellers.map(&:seller)
price = "CNY1200"
price = Monetize.parse(price).exchange_to(:USD)
price.
price
price.to_f
price = "CNY 1200"
price = Monetize.parse(price).exchange_to(:USD)
price = "CNY 12,000.99"
price = Monetize.parse(price).exchange_to(:USD)
price.to_f
s = Scrape.last
s.products
Scrape.last 3
scrapes = Scrape.last 5
scrapes = Scrape.last 2
scrapse
scrapes
scrapes.first.sellers
scrapes.last.sellers
scrape_product_sellers
scrapes.last.scrape_product_sellers
Product.last
Product.find_by(brand: 'hisense')
Product.where(brand: 'hisense')
2.gigabytes
exit
Source.last
Source.last 2
Source.last.scrapes
Scrape.last.destroy
Scrape.last
Scrape.last.destroy
Scrape.last
Scrape.last.destroy
Source.last
Source.last.destroy
Source.last
s =  Scrape.last
s
s =  Scrape.last
Scrape.last 2
s = Scrape.find 19
s.relaod
s.reload
Scrape.last
s.reload
Scrape.last 3
s = Scrape.find 2-
s = Scrape.find 20
s = _
s.products.first
s.products.first.scrape_product_sellers
s = Scrape.last
Source.last
s = Scrape.last.destroy
Source.last.destroy
s = Scrape.last
s = Scrape.running.last
s = Scrape.where(running: true).last
s.reload
s.products.first
OtherProduct.where(model: s.products.pluck(:model).uniq)
hp_other_products = _
hp_other_products.size
hp_other_products
s.id
existing_hp_products = Product.includes(:scrapes).where.not("scrapes.id": s.id).where(model: s.products.pluck(:model))
existing_hp_products.size
s = Scrape.last
s.products.first
id = 1057300
Product.includes(:images).where("assets.id": id)
exit
s = Scrape.last
s.products.where(has_manual: true)
s.products.where(has_manual: true).size
s.reload
exit
OtherProduct.size
OtherProduct.count
Product.count
exit
Sidekiq::Queue.new('elasticsearch_import_queue').clear
Sidekiq::Queue.new('dedup_other_product').clear
OtherProduct.count
460160 - _
start = 460160
start - OtherProduct.count
Product.count
start - OtherProduct.count
_.to_f / start
_ * start
start - OtherProduct.count
Product.find(10512)
p = )
p = Product.find(10512)
p.asin
p.gtin
OtherProduct.where(asin: p.asin)
OtherProduct.where(model: p.model)
p
OtherProduct.where(model: p.model)
op = _
op.image
op
op.images
op = op.first
op.image
op.images
op.images.class
start - OtherProduct.count
(start - OtherProduct.count) / 12000
(start - OtherProduct.count).to_f / 12000
start - OtherProduct.count
Product.order(updated_at: :desc).limit(10)
p = _.first
p
p.models
p.model
p
p.scrape_product_sellers
p.details
start - OtherProduct.count
Product.where.not(gtin: nil).to_a.select {|p| p.gtin.split(',').length == 1 }
prods = _
prods.size
others = OtherProduct.where.not(gtin: nil).to_a.select {|p| p.gtin.split(',').length == 1 }
others.size
start - OtherProduct.count
Product.where.not(brand: nil, model: nil)
''.present?
edit -t
asins
gtins
gtins.join(',')
gtins.uniq.join(',')
edit -t
actual
expected
expected - actual
_ / 1000
exit
edit -t
other_products.size
p.asin
p.name
other_products.map(&:name)
p.model
p.model + p.models
p.model + p.models.map(:model)
p.model + p.models.map(&:model)
p.models
p.model + p.models.map(&:model)
[p.model] + p.models.map(&:model)
p.gtin
gtin_query = "OR `other_products`.`gtin` LIKE '%" +
p.gtin.split(',').map(&:strip).join("%' OR `other_products`.`gtin` LIKE '%") + "%'"
gtin_query
gtin_query = "`other_products`.`gtin` LIKE '%" +
p.gtin.split(',').map(&:strip).join("%' OR `other_products`.`gtin` LIKE '%") + "%'"
gtin_query
OtherProduct.where(gtin_query)
query
p.asin
OtherProduct.where(asin: p.asin)
models = [p.model] + p.models.map(&:model)
edit -t
query
OtherProduct.where(model: models)
other_products
query
OtherProduct.where(query, model: models, asin: p.asin, gtin: p.gtin)
OtherProduct.where(query, models: models, asin: p.asin, gtin: p.gtin)
hash = {models: models}
OtherProduct.where(query, hash)
hash = {models: models, asin: p.asin, gtin: p.gtin}
OtherProduct.where(query, hash)
Entity.count
Entity.businesses.count
Entity.businesses.last.id
Entity.businesses
exit
Entity.businesses.size
Entity.businesses.last.id
exit
Entity.businesses.first
e = )
e = Entity.businesses.first
e.related_incidents
Entity.businesses.count
Product.where(asin: nil)
exit
Product.where(asin: nil)
_.size
Product.where(asin: nil, gtin: nil)
_.size
Product.count
edit -t
other_products
OtherProduct.last
OtherProduct.where.not(asin: nil).last
p = Product.where.not(asin: _.asin)
op = OtherProduct.where.not(asin: nil).last
p = Product.where.(asin: _.asin)
p = Product.where(asin: _.asin)
p = Product.where(asin: op.asin)
edit -t
other_products
other_products.size
edit -t
other_products.size
other_products.first
edit -t
Product.find 1
Product.find(2)
p = _
OtherProduct.where(asin: p.asin)
p
OtherProduct.find_all_by_gtin(p.gtin)
p.models
OtherProduct.where(model: [p.model] + p.models.map(&:model))
edit -t
query
edit -t
query
query_values
edit -t
other_products.size
reload!
other_products.size
OtherProduct.count
Sidekiq::Queues.new('dedup_other_product').clear
Sidekiq::Queue.new('dedup_other_product').clear
OtherProduct.count
'Product'.constantize
exit
uuid = SecureRandom.uuid
folder_path = File.join('tmp/import', uuid)
exit
Product.find(109)
Product.find 42
Product.find 444
Product.find 44
roduct.includes(:scrape_product_sellers, :categories, :detail, :models, :manuals)
Product.includes(:scrape_product_sellers, :categories, :detail, :models, :manuals).where('manuals.status': 1).first(100)
ProductManual
Asset
Product.includes(:scrape_product_sellers, :categories, :detail, :models, :manuals).where('assets.status': 1).first(50)
reload!
exit
p = Product.includes(:scrape_product_sellers, :categories, :detail, :models, :manuals).first
p.manuals.size
p.manuals.count
Product.where(has_manual: true).first
reload!
exit
require 'pdf-reader'
exit
results = {}
keywords.each do |keyword|
  query = Product.generate_query_by_keywords({ keywords: keyword }, 'AND')
  search_results = Elasticsearch::Model.search(query, [Product, OtherProduct], size: 10000000).response
  products, other_products = search_results[:hits][:hits].partition { |r| r['_index'] == 'products_development' }
edit -t
p
p = Product.where(has_manual: true).first
edit -t
products
Product.where(has_manual: true).first
p.manuals
exit
SecureRandom.uuid
uuid = SecureRandom.uuid
folder_path = File.join('/tmp/import', uuid)
file = open '/Users/jonathan/Desktop/scrapes_for_import/Uploaded'
file.path
Zip::File.open(file.path) do |zipfile|
  zipfile.each do |e|
    fpath = File.join(folder_path, e.to_s)
    puts fpath
    FileUtils.mkdir_p(File.dirname(fpath))
    zipfile.extract(e, fpath) { true }
  end
end
file = open '/Users/jonathan/Desktop/scrapes_for_import/Uploaded/'
file = open '/Users/jonathan/Desktop/scrapes_for_import/Uploaded/konka_manual_scrape.zip'
folder_path = File.join('/tmp/import', uuid)
Zip::File.open(file.path) do |zipfile|
  zipfile.each do |e|
    fpath = File.join(folder_path, e.to_s)
    FileUtils.mkdir_p(File.dirname(fpath))
    zipfile.extract(e, fpath) { true }
Zip::File.open(file.path) do |zipfile|
  zipfile.each do |e|
Zip::File.open(file.path) do |zipfile|
  zipfile.each do |e|
    fpath = File.join(folder_path, e.to_s)
    puts fpath
    FileUtils.mkdir_p(File.dirname(fpath))
    zipfile.extract(e, fpath) { true }
  end
end
file.closed?
file.close
file.closed?
exit
reload!
ManualText
ManualText.connection
ManualText
reload!
ManualText
a2
s = Scrape.last
Seller.last
ScrapeProductSeller.last
ScrapeProductSeller.last.seller
ScrapeProductSeller.last.scrape
ScrapeProductSeller.last.scrape.source
ScrapeProductSeller.last
Seller.last
seller = _
ScrapeProductSeller.where(seller_id: seller.id)
ScrapeProductSeller.where(seller_id: seller.id).size
ScrapeProductSeller.where(seller_id: seller.id).pluck(:id)
_.min
ScrapeProductSeller.where(seller_id: seller.id).pluck(:id).max
ScrapeProductSeller.last.id
last = ScrapeProductSeller.last
elektra = ScrapeProductSeller.where(seller_id: seller.id).pluck(:id).first
elektra = ScrapeProductSeller.where(seller_id: seller.id).first
elektra.scrape
last.scrape
elektra.scrape
elektra
last
Product.where('asin is not null OR gtin is not null').size
Product.count
Product.includes(:sources).where('products.asin is not null OR products.gtin is not null OR sources.type = ?', SourceType::IMPORT).size
Product.includes(:sources).where('products.asin is not null OR products.gtin is not null OR sources.source_type_id = ?', SourceType::IMPORT).size
Source
Product.includes(:sources).where('products.asin is not null OR products.gtin is not null OR sources.source_type_id = ?', SourceType::IMPORT).size
Product.includes(:sources).where('`products`.`asin` is not null OR `products`.`gtin` is not null OR `sources`.`source_type_id` = ?', SourceType::IMPORT).size
Product.includes(:sources).where('`products`.`asin` is not null OR `products`.`gtin` is not null OR `sources`.`source_type_id` = ?', SourceType::IMPORT).to_sql
Product.includes(:sources).where("sources.source_type_id": 2)
prods = _
qxtei
Product.includes(:sources).where("sources.source_type_id": 2).size
Product.where('asin is not null OR gtin is not null').size
asin_prods = Product.where('asin is not null OR gtin is not null')
import_prods = Product.includes(:sources).where("sources.source_type_id": 2)
combo = (asin_prods | import_prods)
combo.size
Product.size
Product.count
Product.where('asin IS NOT NULL OR gtin IS NOT NULL OR model IS NOT NULL').size
Product.where('asin IS NOT NULL OR gtin IS NOT NULL OR brand IS NOT NULL').size
Product.where('asin IS NOT NULL OR gtin IS NOT NULL').size
Product.where.not(asin: nil)
_.size
Product.limit(10000).find_in_batches do |products|
  products.each do |p|
    puts p.id
  end
end
count = 0
Product.limit(10000).find_in_batches do |products|
  products.each do |p|
    puts p.id
    count += 1
  end
end
count
Product.limit(10000).each do |products|
  count += 1
  puts products.id
end
count
count = 0
Product.limit(10000).each do |products|
  count += 1
end
count
reload!
ProductIdentifier
p = Product.where.not(asin: nil).first
p.asin
p.class.to_s
ProductIdentifier.create(uniq_id: p.asin, type: :ASIN, identifiable_type: p.class.to_s)
ProductIdentifier
ProductIdentifier.where(type: :ASIN)
ProductIdentifier.where(type: :ASIN).to_s
ProductIdentifier.where(type: :ASIN).as_sql
ProductIdentifier.where(type: :ASIN).to_sql
reload!
exit
p = Product.where.not(asin: nil).first
ProductIdentifier.create(uniq_id: p.asin, type: :ASIN, identifiable_type: p.class.to_s)
reload!
p = Product.where.not(asin: nil).first
ProductIdentifier.create(uniq_id: p.asin, id_type: :ASIN, identifiable_type: p.class.to_s)
reload!
ProductIdentifier.create(uniq_id: p.asin, id_type: :ASIN, identifiable_type: p.class.to_s)
ident = _
ident.errors
reload!
ProductIdentifier.count
p = Product.where.not(asin: nil).first
p.asin
ProductIdentifier.create(uniq_id: p.asin, id_type: :asin, identifiable: p, identifiable_type: p.class.to_s)
ider = _
ider.errors
relaod!
reload!
ProductIdentifier.create(uniq_id: p.asin, id_type: :asin, identifiable: p, identifiable_type: p.class.to_s)
p.reload!
p.reload
p.product_identifiers
p.product_identifiers.where(id_type: :asin)
reload!
p.product_identifiers.where(id_type: 0)
p.product_identifiers.where(id_type: :asin)
exit
p = Product.find 3
p.product_identifiers.where(id_type: :asin)
reload!
ProductIdentifier.where(id_type: :asin)
reload!
ProductIdentifier.where(id_type: :asin)
ProductIdentifier.destroy_all
ProductIdentifier.where(id_type: :asin)
exit
p = Product.find 3
ProductIdentifier.create(uniq_id: p.asin, id_type: :asin, identifiable: p, identifiable_type: p.class.to_s)
ProductIdentifier.where(id_type: :asin)
p.product_identifiers
p.product_identifiers.last.destroy
reload!
ProductIdentifier::TYPES[:ASIN]
reload!
exit
ProductIdentifier.where(id_type: :ASIN)
ProductIdentifier.where(id_type: :GTIN)
ProductManual.where(status: :deleted)
reload!
ProductIdentifier.where(id_type: id_types[:asin])
ProductIdentifier.where(id_type: id_types['asin'])
reload!
ProductManual.confirmed
ProductManual.confirmed.to_sql
ProductIdentifier.asin
exit
p = Product.where.not(gtin: nil)
Identifier
Product.identifiers
p.identifiers
p.reload!
p.reload
exit
p = Product.where.not(asin: nil).first
p.identifiers
p.identifiers.create(uniq_id: p.asin, id_type: 0)
p.identifiers << Identfier.find_or_create_by(uniq_id: p.asin, id_type: 0)
p.identifiers << Identifier.find_or_create_by(uniq_id: p.asin, id_type: 0)
Identifer.count
Identifier.count
Identifier.all
p.identifiers.find_or_create_by(uniq_id: p.asin, id_type: 0)
exit
p = Product.where.not(asin: nil).first
p.identifiers
p.other_products
p.asin
OtherProduct.where(asin: p.asin)
exit
p = Product.where.not(asin: nil).first
p.asin
op = OtherProduct.where(asin: nil).last
op.asin
op.asin = p.asin
op.save
p.identifiers << Identifier.find_or_create_by(id_type: 0, uniq_id: p.asin)
p.identifiers
p.identifiers.asins
p.identifiers.asin
p.identifiers.gtin
op.identifier << Identifier.find_or_create(id_type: 0, uniq_id: op.asin)
op.identifiers << Identifier.find_or_create(id_type: 0, uniq_id: op.asin)
op.identifiers << Identifier.find_or_create_by(id_type: 0, uniq_id: op.asin)
op.identifiers
p.other_products
reload!
op.relaod
op.reload
op.products
exit
p = Product.where.not(asin: nil).first
op = OtherProduct.where(asin: nil).last
op.asin = p.asin
op.save
p.identifiers << Identifier.find_or_create_by(id_type: 0, uniq_id: p.asin)
op.identifiers << Identifier.find_or_create_by(id_type: 0, uniq_id: op.asin)
op.products
op.products.size
reload!
op.reload
exit
p = Product.where.not(asin: nil).first
op = OtherProduct.where(asin: nil).last
op.asin = p.asin
op.save
p.identifiers << Identifier.find_or_create_by(id_type: 0, uniq_id: p.asin)
op.identifiers << Identifier.find_or_create_by(id_type: 0, uniq_id: op.asin)
op.other_products_matching_identifiers
op.id
exit
p1, p2 = Product.where.not(asin: nil).first 2
p1
p2
p2.asin = p1.asin
p2.save
p1.identifiers << Identifier.find_or_create_by(id_type: 0, uniq_id: p1.asin)
p2.identifiers << Identifier.find_or_create_by(id_type: 0, uniq_id: p2.asin)
Identifier.count
p1.products_matching_identifiers
p.reload
relaod!
reload!
p.reload
p1.reload
p1.products_matching_identifiers
exit
p1, p2 = Product.where.not(asin: nil).first 2
p2.asin = p1.asin
p2.save
p1.identifiers << Identifier.find_or_create_by(id_type: 0, uniq_id: p1.asin)
p2.identifiers << Identifier.find_or_create_by(id_type: 0, uniq_id: p2.asin)
p1.products_matching_identifiers
op = OtherProduct.where(asin: nil).first
op.asin = p1.asin
op.save
p1.other_products_matching_identifiers
p1.identifiers
op.identifiers
op.identifiers << Identifier.find_or_create_by(uniq_id: op.asin, id_type: 0)
Identifier.count
p1.other_products_matching_identifiers
p1.identifier_ids
op.identifier_ids
p1.reload!
p1.reload
p1.other_products_matching_identifiers
p1.other_products_matching_identifiers.size
p2
p2.identifiers
p2
p2.gtin = "12414611251"
p2.save
identifier = Identifier.find_or_create_by(uniq_id: p2.gtin, id_type: 1)
Identifer.gtins
Identifer.gtin
Identifier.gtin
identifier
ProductIdentifier.find_or_create_by(product: p2, identifier: identifier)
p2.identifiers
p2.reload
p2.identifiers
exit
p = Product.where.not(gtin: nil, asin: nil)
AssignUniqueIdentifierJob.perform p.id, 'Product'
p = Product.where.not(gtin: nil, asin: nil).first
AssignUniqueIdentifierJob.perform p.id, 'Product'
AssignUniqueIdentifierJob.perform_async p.id, 'Product'
p.identifiers
p.reload
p.identifiers
p
p.gtin
AssignUniqueIdentifierJob.perform_async p.id, 'Product'
exit
p = Product.where.not(gtin: nil, asin: nil).first
product = p
model = 'Product'
edit -t
product
product.identifiers
p.identifiers.asin
p.models
''.present?
'  '.present?
nil.present?
hash = []
hash = {}
hash[:ASIN].present?
gtin = p.gtin
detail = p.details.first
gtins = detail.data_hash[:"Universal Product Identifiers"][:GTIN].gsub(/\s+/, '').split(',').uniq
detail.data_hash[:"Universal Product Identifiers"][:GTIN] = ''
gtins = detail.data_hash[:"Universal Product Identifiers"][:GTIN].gsub(/\s+/, '').split(',').uniq
OtherProduct.includes(:details).where.not("other_product_details.id": nil).where("data_hash like '%:ASIN:%' OR data_hash like '%:GTIN:%'").first
op = _
op.details
p
p.details.first
detail = _
details.attribute_keys
detail.attribute_keys
op
op.details.first.attributes
Product.includes(:details).where("data_hash like '%:ASIN:' OR data_hash like '%:GTIN:'").first 10
product_details
p.details
ProductDetail 3
ProductDetail.find 3
Product.includes(:details).where("data_hash like '%:ASIN:' OR data_hash like '%:GTIN:'").references(:details).first 10
Product.includes(:details).where("data_hash like '%:ASIN:%' OR data_hash like '%:GTIN:%'").references(:details).first 10
p = _.first
p.details
Product.includes(details: [:attribute_keys]).where('product_attribute_keys.name': ['ASIN', 'GTIN']).size
Product.includes(:details).where("data_hash like '%:ASIN:%' OR data_hash like '%:GTIN:%'").references(:details).size
OtherProduct.includes(:details).where("data_hash like '%:ASIN:%' OR data_hash like '%:GTIN:%'").references(:other_product_details).size
exit
'OtherProduct'.to_sym
'OtherProduct'.underscre
'OtherProduct'.underscore
params = {}
params['OtherProduct'.underscore.to_sym] = 1
params
'Product'.underscore
sym = 'Product'.underscore.to_sym
sym
hash
params
params[sym] = 2
params
exit
Identifier.count
Identifier.first
Identifier.first.other_products.size
Identifier.last
Identifier.second
Identifier.second.other_products
Identifier.third.other_products
OtherProduct.where.not(gtin: nil)
OtherProduct.where.not(gtin: nil).first
op.identifiers
op = OtherProduct.where.not(gtin: nil).first
op.identifiers
op.other_products_with_matching_identifiers
identifier = op.identifiers.first
identifier.other_products
identifier.other_products.size
Identifier.all.first {|i| i.other_products.size > 1 }
id = _
id.other_products
Identifier.all.any? {|i| i.other_products.size > 1 }
Identifier.find(123)
i = _
i.other_products
i.other_products.map(&:name)
p = Product.find 3
p.identifiers
p.identifiers.asin
p.products_with_matching_identifiers
p.other_products_with_matching_identifiers
reload!
p = Product.find 3
p.products_matching_identifiers
p.other_products_matching_identifiers
Product.all.any? {|p| p.products_matching_identifiers.size > 0 }
p = Product.find 469
p.products_matching_identifiers
reload!
p.reload
p.products_matching_identifiers
exit
p = Product.find 469
p.products_matching_identifiers
p.name
[p.id, p.brand, p.model]
p
p.models
p.model
p.products_matching_identifiers.first.models
git log
exit
p = Product.find 469
p.identifiers
AssignUniqueIdentifierJob.perform_async p.id, 'Product'
p.reload!
p.reload
p.identifiers
AssignUniqueIdentifierJob.perform_async 470, 'Product'
p.reload
p.products_matching_identifiers
p.identifiers.asins
p.identifiers.asin
p
p.identifiers
exit
p = Identifier.first.products.first
ProductIdentifier.where(product_id: nil)
OtherProductIdentifier.where(other_product_id: nil)
p = ProductIdentifier.first.product
p
p.identifiers << Identifier.find_or_create(uniq_id: '156424154161', id_type: 2)
p.identifiers << Identifier.find_or_create_by(uniq_id: '156424154161', id_type: 2)
p.identifiers
ProductIdentifier.last
Identifier.find _.identifier_id
p.name.changed?
p.changed?
p.name = "new"
p.changed?
p.reload
p.changed/
p.changed?
p.description
p.product_description
p.product_description.class
OtherProduct.first
Identifiers.asin.first
Identifier.asin.first
Identifiers.includes(:other_products).where.not("other_products.id": nil).first
Identifier.includes(:other_products).where.not("other_products.id": nil).first
OtherProductIdentifier.all
Identifier.includes(:products).where.not("products.id": nil).first
Identifier.products
ident = Identifier.includes(:products).where.not("products.id": nil).first
ident.products.first
ident.products.size
ident.products.first
ident = Identifier.asin.includes(:products).where.not("products.id": nil).first
ident = Identifier.asin.includes(:products).where.not("products.id": nil)
[].present?
Product.includes(:identifiers).where("identifiers.uniq_id": 'jonathan')
Product.includes(:identifiers).where("identifiers.uniq_id": 'jonathan').first
p = Product.find 369
p = Product.find 469
exit
Identifier.asin.first
id = _
id.products
id.products.size
Identifier.find_by(uniq_id: nil)
Identifier.count
Product.count
identifiers = Identifiers.includes(:products).where.not("products.id": nil)
identifiers = Identifier.includes(:products).where.not("products.id": nil)
identifiers.size
Identifier.size
Identifier.count
identifiers = Identifier.includes(:products).where("products.id": nil)
id = Identifier.find(13)
id.products
id.other_products
id
OtherProduct.find_by_gtin(id.uniq_id)
identifiers.first(100)
ids = _
ids.flat_map(&:products)
identifiers = Identifier.includes(:products).where("products.id": nil).limit(100)
ids.flat_map(&:products)
identifiers.flat_map(&:products)
identifiers = Identifier.includes(:products).where.not("products.id": nil).limit(100)
identifiers.flat_map(&:products)
identifiers.flat_map(&:product_ids)
Identifier.asin.select { |id| 
products = Product.includes(:identifiers).where("identifiers.type_id": 0)
products.size
products = Product.includes(:identifiers).where("identifiers.id_type": 0)
products.size
dups = products.to_a.select { |p| p.identifiers.asin.size > 1 }
dups.size
dups.map(&:asin)
dups.map(&:asin).count
dups.map(&:asin).uniq.count
products
dups.size
Identifier.asin
Identifier.asin.size
Identifier.asin.includes(:products).where.not("product.id": nil).size
Identifier.asin.includes(:products).where.not("products.id": nil).size
Product.where.not(asin: nil).size
Identifier.asin.pluck(:uniq_id)
ids = _
ids.size
ids.uniq.size
Identifier.create(uniq_id: 'B00T63YW38', id_type: 0)
id = _
id.errors
id = Identifier.create(uniq_id: 'B00T63YW38', id_type: 1)
id
id.errors
id.destroy
Identifer.asin.last.products
Identifier.asin.last.products
multi_asin = Product.includes(:identifiers).where("identifiers.id_type": 0).all.select { |p| p.identifiers.asin.size > 1 }
reload!
multi_asin = Product.includes(:identifiers).where("identifiers.id_type": 0).all.select { |p| p.identifiers.asin.size > 1 }
reload!
OtherProduct.where.not(asin: nil).sample
op = _
Identifier.where(uniq_id: op.asin)
reload!
Identifier.includes(:products, :other_products).where.not("products.id": nil).first(200)
ids = _
:W
OtherProducts.first(500).each do |op|
  DedupOtherProductJob.perform_async op.id
end
OtherProduct.first(500).each do |op|
  DedupOtherProductJob.perform_async op.id
end
OtherProduct.first(500).each do |op|
  DedupOtherProductJob.perform_async op.id
end
OtherProduct.first(500).each do |op|
  DedupOtherProductJob.perform_async op.id
end
OtherProduct.where.not(asin: nil).first(500).each do |op|
  DedupOtherProductJob.perform_async op.id
end
nil.first
nil.try(:first)
reload!
Product
p.first
Product.first
p = _
p.identifiers
id = Identifier.asin.last
id.products
id.products.size
id.products.first
p = _
id.products.class
op = OtherProduct.first
op.asin?
op.gtin?
op.details
op
op.url
op.asin?
"Hello".include?("Hell")
"Hello".include?("hell")
Faker::Commerce.department
require 'faker'
Faker::Commerce.department
p
p.product_details
p.details
p.detail
exit
s = Scrape.last
require 'factory_girl'
require 'factory_girl_rails'
require 'faker'
require 'factory_girl_rails'
scrape = FactoryGirl.create(:scrape)
scrape.products
scrape.products = FactoryGirl.create :product, name: 'keyword1'
Product.last
scrape.products
p = Product.last
scrape.products << p
scrape.products
scrape.products += p
Identifiers.asin
Identifier.asin
asins = _
asins.size
p = Identifier.asin.first.products.first
p
Product.includes(:identifiers).where("identifiers.uniq_id": p.asin).first
Product.includes(:identifiers).where("identifiers.uniq_id": p.identifiers.asin.first.uniq_id).first
p = Identifier.asin.first.products.second
p = Identifier.asin.first.products.first
p
Product.includes(:identifiers).where("identifiers.id": p.identifiers.map(&:id),"identifiers.uniq_id": p.identifiers.asin.first.uniq_id).first
Product.includes(:identifiers).where("identifiers.id": p.identifiers.map(&:id),"identifiers.uniq_id": p.identifiers.asin.first.uniq_id).to_sql
Product.includes(:identifiers).where("identifiers.id": p.identifiers.map(&:id),"identifiers.uniq_id": p.identifiers.asin.first.uniq_id).first
450000 / 2000
225 / 60
15 * 60
15 * 60 * 60
15 * 60 * 60 * 2
15 * 60 * 60 * 3
Identifier.last.other_products
exit
Product.count
Entity.count
Entity.businesses.count
Signal.count
Project.count
p = Product.last
p.revenue_opportunity
p.try(:revenue_opportunity).present?
i = Incident.last
proj = Projection.last
proj.quarter
proj = Projection.first
proj.quarter
proj.quarter.quarter_symbol
CRUD::Models::Projection.quarters
CRUD::Models::Projection::quarters
"q1 2005".match(/\d{4}/)[0]
inc = Incident.find 90
inc = Incident.find 3
inc.entities
inc.products
inc.unknown_products
p = Products.last
p = Product.last
IncidentIdentifiedProducts.last
IncidentIdentifiedProduct.last
IncidentIdentifiedProduct.last.incident
IncidentUnknownProduct.last.incident
exit
User.count
User.all
i = Incident.find 2
i.incident_entities
i.related_entities
i.related_entitites.pluck(:name)
i.related_entities.pluck(:name)
exit
Rails.configuration.action_controller.asset_host
Rails.env
Rails.env.development?
exit
Rails.application.assets
ActionController::Base.helpers.asset_url("default_template_accrual_report.xlsx")
exit
AccrualReport.get_template_name
Incident.get_template_name
exit
AccrualReport.get_template_name
Incident.get_template_name
AccrualReport.get_template_url
AccrualReport.get_template_url('complex')
AccrualReport.get_template_url(type: 'complex')
Incident.get_template_url
Rails.root.join('app', 'assets', 'files', 'templates') 
source_path = Rails.root.join('app', 'assets', 'files', 'templates')
Dir.glob("#{source_path}/*.xlsx").each do |f|
  puts f.path
end
source_path = Rails.root.join('app', 'assets', 'files')
Dir.glob("#{source_path}/*.xlsx").each do |f|
  puts f.path
end
Dir.glob("#{source_path}/*.xlsx").each do |f|
  puts f
end
source_path = 'assets/files'
Dir.glob("#{source_path}/*.xlsx").each do |f|
  puts f
end
source_path = '/assets/files'
Dir.glob("#{source_path}/*.xlsx").each do |f|
  puts f
end
source_path = './assets/files'
Dir.glob("#{source_path}/*.xlsx").each do |f|
  puts f
end
Rails.application.assets
Rails.application.assets.files
Rails.root
Rails.root.path
Rails.root.to_s
source_path = Rails.root.join('app', 'assets', 'files')
Dir.glob("#{source_path}/*.xlsx").each do |f|
  puts f.gsub(Rails.root.to_s, '')
end
puts f
Dir.glob("#{source_path}/*.xlsx").each do |f|
end
Dir.glob("#{source_path}/*.xlsx").each do |f|
  puts f
end
Rails.application.assets
Dir.glob("#{source_path}/*.xls").each do |f|
  puts f
end
source_path
Dir.glob("#{source_path}/*.xls*").each do |f|
  puts f
end
source_path = Rails.root.join('app', 'assets', 'files', 'import_templates')
Dir.glob("#{source_path}/*.xls*").each do |f|
  puts f
end
def templates
  asset_path = Rails.configuration.action_controller.asset_host
  @templates ||= build_templates_hash(asset_path)
end
# angular templates loaded via rails-templates no longer with this method
def build_templates_hash(asset_path)
  source_path = Rails.root.join('app', 'assets', 'files', 'import_templates')
  Hash[
    Dir.glob("#{source_path}/*.xls")
    .map { |file| match_files_to_asset_paths(file) }
    .map { |file, path| complete_assets_paths_uri(file, path, asset_path) }
  ]
end
def match_files_to_asset_paths(file)
  [file, ActionController::Base.helpers.asset_path(file)]
end
def complete_assets_paths_uri(file, path, asset_path)
  path = URI.join(asset_path, path) if asset_path.present?
  [file, path.to_s]
end
Hash[
  Dir.glob("#{source_path}/*.xls*")
  .map { |file| match_files_to_asset_paths(file) }
  .map { |file, path| complete_assets_paths_uri(file, path, asset_path) }
]
edit -t
def match_files_to_asset_paths(file)
  [file, ActionController::Base.helpers.asset_path(file)]
end
def match_files_to_asset_paths(file)
  [file, ActionController::Base.helpers.asset_path(file)]
end
def templates
  asset_path = Rails.configuration.action_controller.asset_host
  @templates ||= build_templates_hash(asset_path)
end
# angular templates loaded via rails-templates no longer with this method
def build_templates_hash(asset_path)
  source_path = Rails.root.join('app', 'assets', 'files', 'import_templates')
  Hash[
    Dir.glob("#{source_path}/*.xls*")
    .map { |file| match_files_to_asset_paths(file) }
    .map { |file, path| complete_assets_paths_uri(file, path, asset_path) }
  ]
end
def match_files_to_asset_paths(file)
  [file, ActionController::Base.helpers.asset_path(file)]
end
def complete_assets_paths_uri(file, path, asset_path)
  path = URI.join(asset_path, path) if asset_path.present?
  [file, path.to_s]
end
templates
Dir.glob("#{source_path}/*.xls*")
match_files_to_asset_paths(Dir.glob("#{source_path}/*.xls*")[0])
source_path
def build_templates_hash(asset_path)
  source_path = Rails.root.join('app', 'assets', 'files', 'import_templates')
  Hash[
    Dir.glob("#{source_path}/*.xls*")
    .map { |file| match_files_to_asset_paths(file) }
    .map { |file, path| complete_assets_paths_uri(file, path, asset_path) }
  ]
end
edit -t
asset_path = nil
build_templates_hash(asset_path)
AccrualReport.name.underscore
Incident.name
RoyaltyReport.name
chain
RoyaltyReport.chain
RoyaltyReport.name
Product.name
templates = templates
templates
def templates
  asset_path = Rails.configuration.action_controller.asset_host
  @templates ||= build_templates_hash(asset_path)
end
templates = templates
templates = build_templates_hash(nil)
match_files_to_asset_paths('blah_blah')
edit -t
templates
edit -t
templates
templates.first.value
templates.values.first
ActionController::Base.helpers.asset_url(templates.values.first)
edit -t
Incident.get_template_url
Rails.env?
Rails.env
Rails.env.production?
Incident.get_template_name
template_name = _
cloud_template = Cloud.current.cloud_templates.find_by(template_type: template_name)
file_name = Incident.name.underscore + '_import'
template_url = ActionController::Base.helpers.asset_url("default_#{file_name}_template.xlsx")
if Rails.env.production?
  template_url = 'https://assets.ruvixx.com' + template_url
end
template_url
exit
Incident.get_template_url
[Incident, AccuralReport, CostReport, Product, RoyaltyReport,
Entity, Contract, TargetList].sort
[Incident, AccrualReport, CostReport, Product, RoyaltyReport,
Entity, Contract, TargetList].sort
w%[Incident, AccrualReport, CostReport, Product, RoyaltyReport,
Entity, Contract, TargetList].sort
%w[Incident, AccuralReport, CostReport, Product, RoyaltyReport,
     Entity, Contract, TargetList]
%w[Incident AccuralReport CostReport Product RoyaltyReport Entity Contract TargetList]
%w[Incident AccuralReport CostReport Product RoyaltyReport Entity Contract TargetList].sort.map(&:constantize)
%w[Incident AccrualReport CostReport Product RoyaltyReport Entity Contract TargetList].sort.map(&:constantize)
%w[Incident AccrualReport CostReport Product RoyaltyReport Entity Contract TargetList].sort
Incident == Incident
Incident == AccrualReport
Incident.get_template_name
Incident.get_template_name.underscoure
Incident.get_template_name.underscor
Incident.get_template_name.underscore
edit -t
template_urls
edit -t
template_urls
exit
edit -t
template_urls
ulrs
urls
template_urls
exit
edit -t
urls = template_urls
edit -t
exit
def template_urls
  import_classes = [AccrualReport, Contract, CostReport, Entity, Incident, Product, RoyaltyReport, TargetList]
  import_classes.each_with_object({}) do |klass, urls|
    if klass == AccrualReport
      urls["complex_accrual_report_import"] = klass.get_template_url(type: :complex)
    end
    urls["#{klass.name.underscore}_import"] = klass.get_template_url
  end
end
template_urls
edit -t
templates
edit -t
templates
exit
edit -t
template_urls
exit
Incident.get_template_url
Cloud.current.template_urls
exit
Cloud.current.template_urls
reload!
Cloud.current.template_urls
exit
Cloud.current.template_urls
Incident.get_template_url
exit
Cloud.current.template_urls
Incident.get_template_url
CloudTemplate.last
CloudTemplate
reload!
Cloud.current
reload!
Cloud.current
Cloud.current.template_urls
reload!
Cloud.current.template_urls
reload!
Incident.get_template_url
reload!
Incident.get_template_url
Cloud.current.template_urls
reload!
Cloud.current.template_urls
urls = _
urls.reduce({}, :merge)
reload!
Cloud.current.template_urls
urls = _
urls['complex_accrual_report']
urls
reload!
urls
Cloud.current.template_urls
reload!
Cloud.current.template_urls
exit
Cloud.current.template_urls
exit
edit -t
Cloud.current.as_json
Cloud.current.as_json.to_json
Cloud.current.as_json
Cloud.current.as_json.merge(template_urls)
Cloud.current.as_json.merge({'template_urls' => template_urls})
Cloud.current.as_json.merge({template_urls: template_urls})
edit -t
template_urls
exit
Cloud.current.as_json
Cloud.current.as_json(extended: true)
exit
Cloud.current.as_json
Cloud.current.as_json(extended: true)
Cloud.current.template_urls
Incident.get_template_url
Rails.root
Rails.root.join('app')
Rails.root.join('app').to_so
Rails.root.join('assets', 'files', 'import_templates')
def build_templates_hash(asset_path)
  Hash[
    Rails.application
    .assets
    .each_logical_path
    .select { |file| file.ends_with?("html") && (!file.include? " ") }
    .map { |file| match_files_to_asset_paths(file) }
    .map { |file, path| complete_assets_paths_uri(file, path, asset_path) }
  ]
end
build_templates_hash(nil)
edit -t
build_templates_hash nil
edit -t
build_templates_hash nil
edit -t
build_templates_hash(nil)
build_templates_hash("https://assets.ruvixx.com")
edit -t
Rails.application.assets.find_asset('default_template_incident.xlsx')
Rails.application.assets.find_asset('default_template_incident.xlsx').logical_path
Rails.application.assets.find_asset('default_template_incident.xlsx')
asset = Rails.application.assets.find_asset('default_template_incident.xlsx')
asset.relative_pathname
asset.logical_path
file = _
ActionController::Base.helpers.asset_path(file)
asset_host = 'www.assets.ruvixx.com'
URI.join(asset_host, '/assets/default_template_incident.xlsx')
asset_host = "https://assets.ruvixx.com"
URI.join(asset_host, '/assets/default_template_incident.xlsx')
reload!
Cloud.current.template_urls
Cloud Templates
CloudTemplate.all
exit
Cloud.current.template_urls
CloudTemplate
Incident.get_template_url
Incident.get_template_url.first.value
Incident.get_template_url.values
Incident.get_template_url.values.first
exit
Incident.get_template_url.html_safe?
Incident.get_template_url.html_safe
Incident.get_template_url.values.first.html_safe?
Incident.get_template_url.values.first.html_safe
{"incident":"/assets/default_template_incident.xlsx"}.to_json
{"incident":"/assets/default_template_incident.xlsx"}['incident']
{"incident":"/assets/default_template_incident.xlsx"}
{"incident":"/assets/default_template_incident.xlsx"}[:incident[
{"incident":"/assets/default_template_incident.xlsx"}[:incident]
TargetList.get_template_url
exit
h = {a: 1, b: 2}
h
h.delete(:a)
h
:a = :a
:a == :a
:a == :b
exit
CloudTemplate.count
CloudTemplate
exit
CloudTemplate.count
CloudTemplate.all
TargetList.get_template_url
CloudTemplate.find(5).attachment.url
Cloud.current.cloud_templates
TargetList.get_template_name
reload!
TargetList.get_template_name
exit
TargetList.get_template_name
TargetList.get_template_url
reload!
TargetList.get_template_url
CloudTemplate.all
'cost_report'.include?('report')
RoyaltyReport.get_template_name
Entity::Entity
Entity
Incident.find 3
i = _
i.addresses
i.address
ad = _
ad.attributes
Address.last
ad
Country.find_by(name: China)
Country.find_by(name: 'China')
c = _
ad
ad.country = c
ad.save
Address
ad
Address.last
EmailTemplate
EmailTemplate.count
Address.boogar
Address.class_eval do
  def hello
    puts 'HI'
  end
end
Address.hello
Address.last.hello
exit
p = Project.find 6
p.actions
p.actions.destroy_all
edit -t
edit it
edit -t
log
log[:actions].length
Status.all
l = Lead.find 6
l = Project.find 6
l.actions
l.actions.destroy_all
i = Incident.find 3
i.related_entities
i = Incident.find 3
i.related_entities
IncidentEntity
IncidentEntity.last
IncidentEntity.where(incident_id: nil)
IncidentEntity.where(entity_id: 564)
reload!
IncidentEntity.where(entity_id: 564)
IncidentEntity.count
IncidentEntity.last
i.attributes
IncidentEntity.find 129
i.errors
IncidentEntity.find_by(entity_id: 564)
IncidentEntity.last
i = Incident.find 3
i.attachments
exit
Incident.where("name like '%incident%'")
Incident.where("name like '%incident%'").size
inc56 = Incident.find 56
inc56.name
inc1 = Incident.find(1)
inc1.name
inc56.name
inc1.attributes['name']
Incident.find 84
Incident.find 21
Incident.find(21).name
Incident.where('incidents.id = "56"')
query = 56
query = '56'
Incident.where('incidents.id = :query', query)
Incident.where("incidents.id = :query", query)
inc3
inc3 = Incident.find 3
inc3.description
inc3.entities_incident_entities.name
inc3.entities_incident_entities
inc3.notes
inc3.product_categories
inc21 = Incident.find 21
inc21.notes
Contact.last
Contacts.where(contactable_id: 2)
Contact.where(contactable_id: 2)
Contact.last
Technology.last
Entity.find(564)
e = _
e.target_lists
e
e.target_lists.size
e.reload
e.target_lists
reload!
'Entity::EntityTable'.constantize
'::Entity::EntityTable'.constantize
exit
'::Entity::EntityTable'.constantize
'TargetListItem::EntityTable'.constantize
Product.last
t = Technology.first
t.products
RoyaltyReportItem.count
Technology.first
Technology.first.update_attributes(certification_required: true)
t = Technology.first
t.products
t.royalty_report_items
t.products.includes(:royalty_report_items).pluck(:id)
t.products.includes(:royalty_report_items).uniq.pluck(:id)
ids = _
Product.where.not(id: ids)
p = _.first
p.royalty_reports
p.royalty_report_items
t.products.includes(:royalty_report_items).where("royalty_report_item.id": nil).uniq.pluck(:id)
t.products.includes(:royalty_report_items).where("royalty_report_items.id": nil).uniq.pluck(:id)
ids = _
Product.where(id: ids)
p = _.first
p.royalty_report_items
Product.where.not(id: ids)
p2 = _.first
p2.royalty_report_items
t.products.includes(:royalty_report_items).where("royalty_report_items.id": nil).uniq.pluck(:id).count
edit -t
products
edit -t
RoyaltyReportItem.first(1000).map(&:technology)
edit -t
t.products.includes(:royalty_report_items).where("royalty_report_items.id": nil).uniq.pluck(:id).count
Product
p = Product.last
p.royalty_report_items
t.products.includes(:royalty_report_items).where("royalty_report_items.id": nil).uniq.pluck(:id).count
edit -t
products
products.count
ids = products
ids = products.sort
Product.first 1000
Product.first(100).last
p = _
p.royalty_report_items
Product.where(id: ids).last
Product.where(id: ids).first
p = _
p.royalty_report_items
Product.find(id: ids.sample).royalty_report_item_ids
Product.find(ids.sample).royalty_report_item_ids
p = Product.last
p = Product.last.entity
p
p.product_status
Product.first
p = _
p.product_status
Product.includes(:technologies, royalty_report_items: :royalty_reports)
Product.includes(:technologies, royalty_report_items: :royalty_report).to_sql
Product.includes(:technologies, royalty_report_items: :royalty_report).to_sql.count
Product.includes(:technologies, royalty_report_items: :royalty_report).count
Product.joins <<-SQL
      left outer join royalty_report_items on products.id=royalty_report_items.product_id
      left outer join royalty_reports on royalty_reports.id=royalty_report_items.royalty_report_id
    SQL
Product.joins <<-SQL
      left outer join royalty_report_items on products.id=royalty_report_items.product_id
      left outer join royalty_reports on royalty_reports.id=royalty_report_items.royalty_report_id
    SQL.count
Product.joins <<-SQL
      left outer join royalty_report_items on products.id=royalty_report_items.product_id
      left outer join royalty_reports on royalty_reports.id=royalty_report_items.royalty_report_id
    SQL
products = _
products.count
Product.includes(:technologies, royalty_report_items: :royalty_report).count
products.uniq.count
edit -t
Product.includes(royalty_report_items: :royalty_report).class
edit -t
products.class
products.first
Product.includes(:technologies)
Product.includes(:technologies).where(      .where('technologies.certification_required': true)
Product.includes(:technologies).where('technologies.certification_required': true).size
edit -t
products = _
products.size
products.count
products.uniq.count
products.map(&:end_of_life_at)
Product.where('products.end_of_life_at IS NULL OR products.end_of_life_at > ?', Date.today.1.year_ago)
Date.today.1.year.ago
1.year_ago
1.year
Date.today - 1.year
Product.where('products.end_of_life_at IS NULL OR products.end_of_life_at > ?', Date.today - 1.year)
Product.where('products.end_of_life_at IS NULL OR products.end_of_life_at > ?', Date.today - 1.year).size
Product.where('products.end_of_life_at IS NULL OR products.end_of_life_at > ?', Date.today - 1.year).count
Product.active(Date.today - 2.years)
Product.active(Date.today - 2.years).count
p = Product.find 4864
p.send('Q3 14'.parameterize.underscore)
Technology.where(certification_required: true).map &:name
Technology.where(certification_required: true).each { |tech| tech.update_attributes(certification_required: false) }
Technology.find(9).update_attributes(certification_required: true)
t = Technology.find(9)_
t = Technology.find(9)
t.products
Product.first.technologies << t
Technology.first
Technology.first.update_attributes(certification_required: true)
Technology.first.update_attributes(certification_required: false)
Technology.first.update_attributes(certification_required: true)
DeveloperKey.last
DeveloperKey.last.secret
DeveloperKey.last.access_id
DeveloperKey.last
DeveloperKey.first.access_id
DeveloperKey.first.secret
Contract.last
Contract.last.termination_date
Contract.last.application_date
Contract.last.suspension_date
details
Contract.last.details
Contract.technologies
Contract.last.technologies
i%(:email, :phone)
i%(:email :phone)
i%(:email, :phone)
DeveloperKey.first.access_id
DeveloperKey.first.secret
DeveloperKey.first
DeveloperKeys.where(access_id: 'a69914df-6311-48ac-bd99-a435886c2f33')
DeveloperKey.where(access_id: 'a69914df-6311-48ac-bd99-a435886c2f33')
%i(email phone fax name)
edit -t
reponse.first
response.first
response
edit -t
response.first
response.as_json.symbolize_keys
edit -t
response.as_json.symbolize_keys
i%(email phone fax name)
%i(email phone fax name)
find_fields = _
contacts = []
edit -t
contacts.each do |c|
  contact = nil
c = contacts.first
find_fields.each do |field, index|
  p find_fields[0..index]
end
find_fields.each do |field, index|
  puts index
end
exit
DeveloperKey.last
DeveloperKey.last.access_id
DeveloperKey.last.secret
DeveloperKey.last
u = User.find 12
u.attributes
DeveloperKey.last.access_id
DeveloperKey.last.secret
DeveloperKey.last.access_id
DeveloperKey.last
DeveloperKey.last.access_id
DeveloperKey.last.secret
AUTH_HEADER_PATTERN = /APIAuth ([^:]+):(.+)$/
auth_header = "APIAuth 34f7cc0c-431c-446c-b3c4-99b0abc65388:98RuyQSHY2MQBf1zy0L1DMSQHA0="
AUTH_HEADER_PATTERN.match(auth_header)
OpenSSL::Digest.new('sha1')
DeveloperKey.last.access_id
DeveloperKey.last.secret
RoyaltyReport.find 6
rr = _
rr.accepted_at
rr.accepted_at.to_s
rr.start_date
rr.start_date.class
rr.accepted_at.class
rr.accepted_at.to_date
Date.parse rr.accepted_at
Date.parse rr.accepted_at.to_s
DeveloperKey.last.access_id
DeveloperKey.last.secret
Enity.last
Entity.last
Entity.last 2
e = Entity.last
e.contacts
con = e.contacts.last
con.update_attributes email: nil
DeveloperKey.last.access_id
DeveloperKey.last.secret
e = Entity.last
con = e.contacts.last
con = e.contacts
e = Entity.last
e.contacts
e = Entity.last
e.contacts
e = Entity.last
e.contacts
e.reload
e = Entity.last
e.contacts
e = Entity.last
e.contacts
e.addresses
Address.all
DeveloperKey.last.access_id
DeveloperKey.last.secret
Entity.last
Entity.last.attributes
Entity.last
Entity.last.notes
DeveloperKey.last.access_id
DeveloperKey.last.secret
Entity.last
DeveloperKey.last.access_id
DeveloperKey.last.secret
Entity.last
DeveloperKey.last.access_id
DeveloperKey.last.secret
ProductCategory.last
Technology.last
Technology.all
Technology.first.update_attributes(technology_num: 'ALPHA')
Product.last
ProductTechnology.where('product_id > ?', Product.last.id).size
ProductTechnology.where('product_id > ?', Product.last.id).destroy_all
ProductTechnology.where.not(product_id: Product.ids).size
ProductTechnology.where.not(product_id: Product.ids).destory_all
ProductTechnology.where.not(product_id: Product.ids).destroy_all
p = Product.last
p.sticker_transactions
p.sticker_balance_units
p.sticker_balance_avg_usage
p.sticker_balance_avg_purchase
Array(nil)
Array(nil).each do |t| t.smoethin end
e = Entity.find 4
e.contacts
c = _
c = _.last
c.fields
c.attributes
reload!
c = Contact.create()
c.errors
EntityType.tableize
::EntityType.tablize
EntityType.to_s.tableize
EntityType.table_name
TargetListItem.last
TargetListItem.last.converted
TargetListItem.first
tli = _
tli.actions
tli.reminders
Reminder
Action.last
Reminder.last
TargetListItem.find 549
tli = _
tli.reminders
tli.follow_ups
Action.last
tli.actions
TargetListItem.select { |tli| tli.reminders.size > 0 }
TargetList.find 1
Entity.find 550
tli = TargetListItem.find 532
tli.reminders
tli.reminders.destroy_all
tli.reminders.each { |r| r.destroy }
tli.reminders
tli.reload
tli.reminders
Action.last
tli = TargetListItem.find 549
tli.actions
tli.reminders
tli
tli.find 539
tli= TargetListItem.find 539
tli.reminders
tli.actions
Action.last
Action.last.reminder
Reminder.last
Reminder.last.action.actionable
tli.actions
tli.reload
tli.actions
tli.reminders
Reminder.last.action.actionable
tli.actions
tli
Reminder.last.action.actionable
tli = TargetListItem.find 549
tli.reminders
Reminder.find 259
r = _
r.action
ActionType
r
Reminder.last
Reminder.last.action_type
Reminder.find 259
r = _
Reminder.last
Reminder.last.action_type
Status.all
Reminder.closed.last
Reminder.closed.last 3
a = Action.find 332
a.reminder
Reminder
Reminder.last
Reminder.last.resolution
Incident.last.incident_num
Incident.last.incident_num.class
f = 1234.56
String(f)
Comment.last
Action.last
exit
Comment.last
c = _
c.actionable_type
c.action_id
c.action
c.document
CommentDocument.last
CommentDocument.to_json
Action.last
Actoin.last
Action.last
a = _
Comment.last
c = Comment.last
c.actionable_id
exit
c = Comment.last
c.actionable_id
c.actionable_type
Discussion
DiscussionDocumnet
DiscussionDocument
Object.const_get(DiscussionDocument.to_s.sub(/Document/, ''))
Attachment.last
Attachment.last.action.actionable
Attachment.last.action
Attachment.first
reload!
Apartment.tenant_names
Task.last
Comment.last
c = 
c = Comment.find 16
c.update_attributes body: "Find this comment"
c = Comment.last
c.actionable_type
exit
Action.last.actionable_type.class
Comment.count
Incident.find 1
i = _
i.comments
exit
c = Comment.last
c.action_actionable_id
c.action_actionable_type
c
c.reload
c.actionable_id
c.actionable_type
c.last
c
action = Action.last
action.actionable.actionable
Document.update_all!
CommentDocument.update_all!
reload!
CommentDocument.update_all!
c = Contact.last
c = Contact.find 1080
'Entity'.constantize
Object.const_get('Entity')
TargetList
Contact.where(contactable_id: nil)
" janedoe@email"
name = _
name.squish
name " janedoe@email 3330000"
name = " janedoe@email 3330000"
name.squish
reload!
Contact.count
ContactDocument.count
relaod!
reload!
relaod!
reload!
ContactDocument.update_all!
reload!
ContactDocument.update_all!
reload!
ContactDocument.update_all!
reload!
ContactDocument.update_all!
reload!
ContactDocument.update_all!
reload!
ContactDocument.update_all!
reload!
ContactDocument.update_all!
reload!
ContactDocument.update_all!
Contact.where(contactable_type: nil)
Contact.where(contactable_type: nil).count
Contact.where(contactable_type: nil).destroy_all
Contact.count
reload!
ContactDocument.update_all!
reload!
ContactDocument.update_all!
reload!
ContactDocument.update_all!
Contact.includes(:contactable).where(contactable_type: 'Entity').where("entities.deprecated": 0).count
Contact.includes(:entity).where(contactable_type: 'Entity').where("entities.deprecated": 0).count
Contact.where(contactable_type: "Entity").count
Contact.where(contactable_type: "Entity").joins(:entities).where("entities.deprecated": false)
Contact.where(contactable_type: "Entity").joins(:entities).where("entities.deprecated": false).count
Contact.where(contactable_type: "Entity").joins(:contactable).where("entities.deprecated": false).count
reload!
Contact.includes(:entity).where("entities.depecrated": false).count
Contact.includes(:entity).where(entities: {deprecated: 0}).count
reload!
Contact.includes(:entity).where(entities: {deprecated: 0}).count
reload!
Contact.includes(:entity).where(entities: {deprecated: 0}).count
reload!
Contact.includes(:entity).where(entities: {deprecated: 0}).count
reload!
Contact.includes(:entity).where(entities: {deprecated: 0}).count
Contact.includes(:entity).where(entities: {deprecated: 1}).count
Contact.where(contactable_id: nil)
Entity.deprecated
Entity.where(deprecated: true)
Entity.where(deprecated: true).ids
Contact.where(contactable_id: 744)
Entity.find 744
Entity.ids.max
Contact.includes(:entity).where("entities.id > #{Entity.ids.max}")
Contact.includes(:entity).where("entities.id = 744")
Contact.where(contactable_type: 'Entity').where("contactable_id > #{Entity.ids.max}")
Contact.where(contactable_type: 'Entity').where("contactable_id > #{Entity.ids.max}").destroy_all
reload!
ContactDocument.update_all!
RoyaltyReport.last
CommentDocument.update_all
reload!
CommentDocument.update_all!
reload!
CommentDocument.update_all!
reload!
CommentDocument.update_all!
reload!
CommentDocument.update_all!
reload!
CommentDocument.update_all!
ContactDocument.update_all!
c = Contact.find_by email: 'daniel.jimenez@domoti-sas.com'
ContactDocument.update_all!
reload!
ContactDocument.update_all!
Contact.find_by email: 'jim.novack@dynatec.es'
Contact.where "email like '%novack%'
Contact.where "email like '%novack%'"
Contact.where "name like '%jim%'"
Contact.where "email like '%jim%'"
exit
Contact.where "email like '%jim%'"
Contact.where "email like '%novack%'"
Contact.where "email like '%jim%'"
Contact.where.not(conactable_id: Entity.ids & TargetList.ids)
Contact.where.not(conactable_id: Entity.ids & TargetList.ids).size
Contact.where.not(contactable_id: Entity.ids & TargetList.ids).size
Contact.where.not(contactable_id: Entity.ids & TargetList.ids & Training.ids & TargetListItem.ids)
Entity.find 22
Contact.where.not(contactable_id: [Entity.ids + TargetList.ids + Training.ids + TargetListItem.ids].flatten.uniq)
Entity.find 68749
Contact.where.not(contactable_id: [Entity.ids + TargetList.ids + Training.ids + TargetListItem.ids].flatten.uniq).destroy_all
exit
reload!
Figaro.env.es_index
exit
ContactDocument.update_all!
"   ".blank?
exit
ContactDocument.update_all!
CommenttDocument.update_all!
CommentDocument.update_all!
"#{nil}"
nil += "hello"
Contact.where(name: nil, email: nil)
query = Contact.where(name: nil, email: nil).load
query.order name: :asc
query = Contact.all.load
query.order name: :asc
query.order name: :desc
exit
User.first
User.first.roles
Roles.all
Role.all
User.first.roles << Role.first
User.first.user_types
User.first.user_type
reload!
User.first.user_type_id
Redis.current
puts Redis.current
p Redis.current
User.first
Entity.first
Entity.first.to_json
MutiJson.load _, symbolize_keys: true
e = Entity.first.to_json
MultiJson.load e, symbolize_keys: true
mjson = MultiJson.load e, symbolize_keys: true
mjson.class
e.class
Redis.current
SharedCache.fetch("entity_fuzzy_matches")
SharedCache.fetch("entity_fuzzy_matches") {}
full_key = "#{Rails.env}:shared_cache:#{Apartment::Tenant.current}:#{key}"
key = "entity_fuzzy_matches"
full_key = "#{Rails.env}:shared_cache:#{Apartment::Tenant.current}:#{key}"
connection = Redis.current
connection.get(full_key)
connection
Redis.new
Redis.current
reload!
exit
key = "entity_fuzzy_matches"
full_key = "#{Rails.env}:shared_cache:#{Apartment::Tenant.current}:#{key}"
connection.get(full_key)
connection = Redis.current
exit
Redis.current
Rails.cache
connection.get(full_key)
connection = Redis.current
connection.get(full_key)
full_key = "#{Rails.env}:shared_cache:#{Apartment::Tenant.current}:#{key}"
key = "entity_fuzzy_matches"
full_key = "#{Rails.env}:shared_cache:#{Apartment::Tenant.current}:#{key}"
connection.get(full_key)
Rails.cache
exit
key = "entity_fuzzy_matches"
full_key = "#{Rails.env}:shared_cache:#{Apartment::Tenant.current}:#{key}"
connection = Redis.current
connection.get(full_key)
Rails.cache
connection.class
connection.clear
Rails.cache.clear
Rails.cache
Rails.cache.clear
Rails.cache.del(full_key)
Rails.cache.delete(full_key)
connection.get(full_key)
connection.get(full_key).clear
connection.get(full_key)
connection.get(full_key).reload
connection.get(full_key).class
connection.get(full_key)
Rails.cache.fetch('entity_fuzzy_matches')
connection = Redis.current
connection.clear
connection.clear_cache
SharedCache.clear_cache!("entity_fuzzy_matcher")
connection.get(full_key)
exit
Entity.count
Entity.businesses.count
edit -t
entities.length
SharedCache.clear_cache!("entity_fuzzy_matcher")
connection.get(full_key)
connection = Redis.current
key = "entity_fuzzy_matches"
full_key = "#{Rails.env}:shared_cache:#{Apartment::Tenant.current}:#{key}"
connection.get(full_key)
SharedCache
SharedCache.clear_cache!("entity_fuzzy_matcher")
connection.get(full_key)
Incident.find 94
i = _
i.name
i.reload
i.name
i.name ? 't' : 'f'
i.name.present?
i.name.blank?
Comment.first
Comment.find 18
Comment.last
Comment.all.tough
Comment.all.touch
Contact.where(contactable_id: nil)
Contact.where(contactable_id: nil).count
Contact.where(contactable_id: nil).first
name = Contact.where(contactable_id: nil).first.name
Contact.where name: name
name = Contact.where(contactable_id: nil).last.name
Contact.where name: name
name = Contact.where(contactable_id: nil).where.not(name: nil).last.name
Contact.where name: name
u = User.internal.first
Task.where(assigned_to_user_id: u.id).count
exit
Task.first
Task.where(status_id: nil).first
Task.first
t = _
t.status
t.status_id
Status.find 1
Status.all
Reminder.includes(:action).where(actions: {actionable_type: 'Project'})
Reminder.includes(:action).where(actions: {actionable_type: 'Project'}).first.action
Reminder.includes(:action).where(actions: {actionable_type: 'Project'}).last.action
Reminder.includes(:action).where(actions: {actionable_type: 'Project'}).last.action.actionable
Action.includes(:actionable).first.actionable
Action.includes(:actionable).where.not(actionable_id: nil).actionable
Action.includes(:actionable).where.not(actionable_id: nil).first.actionable
reload!
Action.includes(:actionable).where.not(actionable_id: nil).first.actionable
Action.includes(:project).where.not(actionable_type: 'Project').first.actionable
Action
Action.includes(:project).first.actionable
Product.pluck(:current_sticker_balance)
'   '.nil?
''.nil?
exit
StickerTransaction.last
p = Product.find 1163
p.sticker_transactions
p.sticker_transactions.where(void: false).sum(:qty)
t = p.sticker_transactions.last
t.void = true
t.save
p.reload
p.sticker_transactions.where(void: false).sum(:qty)
p.sticker_transactions.where(void: false)
0 || 'hey'
t
t.update :void, false
t.void = false
t.save
p.reload
p.sticker_transactions.where(transaction_type: 'DEDUCTION', void: false).sum(:qty)
p.sticker_transactions.where(transaction_type: 'DEDUCTION', void: false).pluck('avg(qty)'
p.sticker_transactions.where(transaction_type: 'DEDUCTION', void: false).pluck('avg(qty)')
p.sticker_transactions.where(transaction_type: 'DEDUCTION', void: false).pluck('avg(qty)')[0]
p.sticker_transactions.where(transaction_type: 'DEDUCTION', void: false).pluck('avg(qty)')[0].to_f
p.sticker_transactions.where(transaction_type: 'DEDUCTION', void: false).pluck('avg(qty)')[0].to_i
p.sticker_transactions.where(transaction_type: 'DEDUCTION', void: false).average(:qty)
p.sticker_transactions.where(transaction_type: 'DEDUCTION', void: false).average(:qty).to_i
nil.to_i
products = Product.includes(:royalty_report_items).where(has_sticker: true)
products.sum(:current_sticker_balance)
User.find(nil)
User.find('1')
Product.count
UnknownProduct.count
UnknownProduct.last
product.last
p = Product.find_by model: 'RTL8198C'
p = UnknownProduct.find_by model: 'RTL8198C'
UnknownProduct.all.destroy_all
p = Product.last
p.manufacturer
gst
Product.count
Product.last
UnknownProduct.count
p = Product.find_by model: 'RTL8198C'
reload!
p = Product.find_by model: 'rtl8198c'
p = Product.new
p.valid?
p.errors
p.errors.full_messages
exit
Entity.businesses.includes(
  :country,
  :managers,
  :alternate_names,
  :technologies,
  :contacts,
  :entity_types,
  :addresses,
  royalty_reports:[
    :royalty_report_items
  ],
  messages: [
    :email_template,
    :attachment,
  ],
  contract_technologies: [
    :entity_status,
    :technology
  ]
)
edit -t
_.size
Entity.connection
edit -t
_.size
edit -t
exit
edit -t
reload!
realoa!
reload!
exit
Entity.businesses.joins(:country).first
Entity.businesses.joins(:country).first.country
Entity.businesses.includes(:technologies, :country).where("LOWER(entities.name) LIKE :query OR LOWER(technologies.name) LIKE :query OR LOWER(countries.name) LIKE :query", query: "%#{global}%")
Entity.businesses.includes(:technologies, :country).where("LOWER(entities.name) LIKE :query OR LOWER(technologies.name) LIKE :query OR LOWER(countries.name) LIKE :query", query: "%global%")
Entity.businesses.includes(:technologies, :country).where("LOWER(entities.name) LIKE :query OR LOWER(technologies.name) LIKE :query OR LOWER(countries.name) LIKE :query", query: "%global%").where_values
Entity.unscoped.includes(:technologies, :country).where('entities.name': 'global').where('technologies.name': 'global').where('countries.name': 'global').where_values
values = _
values.reduce(:or)
Entity.where(_)
Entity.arel_table
tbl = _
ContractTechnology.last
ContractTechnology.last.entity_status
Entity.status
Entity
ContractTechnology.last.entity_status
Entity.includes(contract_technolgies: :technology).where('technologies.name': '180')
Entity.includes(contract_technolgies: :technology).where('technologies.name': '180').size
Entity.includes(contract_technologies: :technology).where('technologies.name': '180').size
Entity.includes(:technologies).first(10).each {|e| p e.technology_names}
Entity.includes(contract_technologies: :technology).first(10).each {|e| p e.technology_names}
Entity.includes(contract_technologies: :technologies).first(10).each {|e| p e.technology_names}
Entity.includes(:technologies).first(10).each {|e| p e.technology_names}
p = Project.find 6
exit
p = Project.find 6
p.status
Status.all
p.status_id = 1
p.save
p.created_by_user
User.all
p.created_by_user_id = 5
p.save
Project
reload!
p = Project.find 6
p.pinned_project_users
Project.permissions
p = Project.find 6
p.permissions
p.permissions(User.find 5)
u = User.find 5
u.projects
u.projects.ids
u.projects.pluck :id
p.permissions(User.find 5)
u.projects.pluck :id
u.projects.map{|p| p.status.name}
Ability.new u
Ability
Ability.first
Reminder.includes(action: :actionable)
Reminder.includes(action: :actionable).first.action.actionable.name
Reminder.includes(action: :actionable).last.action.actionable.name
Task.joins(:assigned_to_user, :milestone, :status, :project, :created_by_user).where(assigned_to_user: 5)
Task.joins(:assigned_to_user, :milestone, :status, :project, :created_by_user).where(assigned_to_user_id: 5)
Task.joins(:assigned_to_user, :milestone, :status, :project, :created_by_user).where(assigned_to_user_id: 5).size
Task.joins(:assigned_to_user, :milestone, :status, :project, :created_by_user).where(assigned_to_user_id: 5).map &:summary
Task
Task.joins(:assigned_to_user, :milestone, :status, :project, :created_by_user).where(assigned_to_user_id: 5).map &:description
Entity.first
Entity.find 11
_.destroy
Entity.where name: "OBJECT:NULL"
Project.where name: "OBJECT:NULL"
AccrualReport.last
AccrualReport.last.update_attributes(archived_at: nil, archived_by_user_id: nil)
AccrualPeriod.last
AccrualPeriod.first
Product.last
p = _
p.entity
Products.group(:entity_id).count
Product.group(:entity_id).count
Entity.find 2
exit
e = Entity.find 992
e.products.count
Product.reportable(e)
Product.reportable(e).size
e = Entity.find 1094
e.technologies
e.technology_ids
tech_ids = _
e.technology_ids = [19, 16]
tli = TargetListItem.find 1170
exit
tli = TargetListItem.find 1170
tli.target_list.name
tli.target?
tli
tli.converted
tli.converted?
TargetListItem.where(converted: true)
tli = TargetListItem.find 523
tli.converted
i = Incident.find 94
i.incident_entities
i
i.products
i.products.first.entity
IncidentEntities.last
IncidentEntity.last
ie = _
ie.entity
i
i.entities
Incident.includes(:incident_entities, :products).where("incident_entities.entity_id = ? OR products.entity_id = ?", entity_id)
Incident.includes(:incident_entities, :products).where("incident_entities.entity_id = ? OR products.entity_id = ?", 9)
Incident.includes(:incident_entities, :products).where("incident_entities.entity_id = :id OR products.entity_id = :id", id: 9)
Incident.includes(:incident_entities, :products).where("incident_entities.entity_id = :id OR products.entity_id = :id", id: 8)
Incident.includes(:incident_entities, :products, :unknown_products).where("incident_entities.entity_id = :id OR products.entity_id = :id OR unknown_products.entity_id = :id", id: 8)
User.last
exit
Incident.count
exit
RoyaltyReport.count
RoyaltyReportItem.count
RoyaltyReportItem.last
RoyaltyReportItem.unit_price
RoyaltyReportItem.last.unit_price
Entity.royalty_reporting.count
reload!
Entity.royalty_reporting.count
Entity.royalty_reporting.includes(royalty_report_items: [:product])
Entity.royalty_reporting.includes(royalty_report_items: [:product]).count
Entity.royalty_reporting.includes(royalty_reports: [royalty_report_items: [:product]]).count
Entity.royalty_reporting.first
e = Entity.find(2)
e.royalty_reports.includes(:royalty_report_items).where('royalty_report_items
RoyaltyReportItem
e.royaly_reports.last(2)a
e.royaly_reports.last(2)
e.royalty_reports.last(2)
e.royalty_reports.includes(:royalty_report_items).last(2)
RoyaltyReport
RoyaltyReportItem
edit -t
last2 = last_2_reports(e.id)
last2.first.royalty_report_items.each do |rr1|
  rri2 = last2.second.royalty_report_items.where(product_id: rr1.product_id, technology_id: rr1.technology_id)
  puts rr2.count
end
last2.first.royalty_report_items.each do |rr1|
  rri2 = last2.second.royalty_report_items.where(product_id: rr1.product_id, technology_id: rr1.technology_id)
  puts rri2.count
end
last2.first.royalty_report_items.each do |rri1|
edit -t
edi -t
edit -t
RoyaltyReportItem
edit -t
RoyaltyReport.last
RoyaltyReport.last.quarter
RoyaltyReportItem
RoyaltyReport.where(entity_id: Entity.royalty_reporting.ids)
RoyaltyReport.where(entity_id: Entity.royalty_reporting.pluck(:id).references(:entities))
RoyaltyReport.where(entity_id: Entity.royalty_reporting.pluck(:id))
RoyaltyReport.where(entity_id: Entity.royalty_reporting.pluck(:id)).count
RoyaltyReport.where(entity_id: Entity.royalty_reporting.uniq.pluck(:id)).count
RoyaltyReport.where(entity_id: Entity.royalty_reporting.uniq.pluck(:id)).group(:entity_id).count
edit -t
rrs = _
rr.size
rrs.size
rrs.first
reload!
Entity.royalty_reporting.includes(:recent_royalty_reports).group(:id)
Entity.royalty_reporting.includes(:recent_royalty_reports).group(:id).count
Entity.royalty_reporting.includes(:recent_royalty_reports).group("entities.id").count
Entity.royalty_reporting.includes(:recent_royalty_reports).group("entities.id")
Entity.royalty_reporting.includes(recent_royalty_reports: :royalty_report_items).group("entities.id")
Entity.royalty_reporting.includes(recent_royalty_reports: :royalty_report_items).group("entities.id").each do |e|
  e.royalty_report_items.size
end
e.royalty_reports.size
Entity.royalty_reporting.includes(recent_royalty_reports: :royalty_report_items).group("entities.id").each do |e|
  e.royalty_reports.size
end
Entity.royalty_reporting.includes(recent_royalty_reports: :royalty_report_items).group("entities.id").each do |e|
  puts e.royalty_reports.size
end
Entity.royalty_reporting.includes(recent_royalty_reports: :royalty_report_items).each do |e|
  puts e.royalty_reports.size
end
Entity.royalty_reporting.includes(recent_royalty_reports: :royalty_report_items).each do |e|
  puts e.royalty_reports.size
end
Entity.royalty_reporting.includes(recent_royalty_reports: :royalty_report_items).load.each do |e|
  puts e.royalty_reports.size
end
exit
Entity.royalty_reporting.joins(recent_royalty_reports: :royalty_report_items).load.each do |e|
  puts e.royalty_reports.size
end
Entity.royalty_reporting.joins(recent_royalty_reports: :royalty_report_items).load.each do |e|
  puts e.recent_royalty_reports.size
end
Entity.royalty_reporting.includes(recent_royalty_reports: :royalty_report_items).each do |e|
hash = Entity.royalty_reporting.includes(recent_royalty_reports: :royalty_report_items).each_with_object({}) do |e, h|
  h[e.id] << e.royalty_reports
end
hash = Entity.royalty_reporting.includes(recent_royalty_reports: :royalty_report_items).each_with_object({}) do |e, h|
  h[e.id] << e.recent_royalty_reports
end
reload!
hash = Entity.royalty_reporting.includes(recent_royalty_reports: :royalty_report_items).each_with_object(Hash.new { |h, k| h[k] = [] }) do |e, h|
  h[e.id] << e.recent_royalty_reports
end
reload!
hash = Entity.royalty_reporting.includes(recent_royalty_reports: :royalty_report_items).each_with_object(Hash.new { |h, k| h[k] = [] }) do |e, h|
  h[e.id] << e.recent_royalty_reports
end
reload!
hash = Entity.royalty_reporting.includes(recent_royalty_reports: :royalty_report_items).each_with_object(Hash.new { |h, k| h[k] = [] }) do |e, h|
  h[e.id] << e.recent_royalty_reports
end
reload!
hash = Entity.royalty_reporting.includes(recent_royalty_reports: :royalty_report_items).each_with_object(Hash.new { |h, k| h[k] = [] }) do |e, h|
  h[e.id] << e.recent_royalty_reports
end
reload!
hash = Entity.royalty_reporting.includes(recent_royalty_reports: :royalty_report_items).each_with_object(Hash.new { |h, k| h[k] = [] }) do |e, h|
  h[e.id] << e.recent_royalty_reports
end
hash = Entity.royalty_reporting.includes(recent_royalty_reports: :royalty_report_items).each_with_object(Hash.new { |h, k| h[k] = [] }) do |e, h|
RoyaltyReport.includes(:royalty_report_items)
RoyaltyReport.includes(:royalty_report_items).
where(entity_id: 2)
RoyaltyReport.includes(:royalty_report_items).
where(entity_id: 2).
order(id: :desc).limit(2)
def reporting_entity_ids
  Entity.royalty_reporting.ids
end
def last_2_reports(entity_id)
  RoyaltyReport.includes(:royalty_report_items)
  .where(entity_id: entity_id)
  .order(id: :desc).limit(2)
end
edit -t
RoyaltyReportItem
edit -t
def last_2_reports(entity_id)
  RoyaltyReport.includes(:royalty_report_items)
  .where(entity_id: entity_id)
  .order('royalty_reports.id': :desc).limit(2)
end
edit -t
run_it
edit -t
run_it
RoyaltyReportItem
edit -t
run
edit -t
run
edit -t
cd run
cd :run
ls
cd
:run.to_s
run
cd run
cd :run
show-source
cd
show-source run
edit -t
run
RoyaltyReportItem.where(id: [2861, 3004])
rr1, rr2 = _
rr1.unit_price.to_f
rr2.unit_price.to_f
rr2.product_id
rr1.product_id
show-source run
id = RoyaltyReport.lsat
id = RoyaltyReport.last
id = RoyaltyReport.last.id
rep1, rep2 = RoyaltyReport.where(id: id)
rep1
rep2
edit -t
edit-t
edit -t
run
edit -t
run
Lead.all.select {|l| l.files.count > 0}
Project.all.select {|l| l.files.count > 0}
edit -t
run
edit -t
exit
Entity.royalty_reporting
Entity.royalty_reporting.count
edit -t
run
RoyaltyReportItems.size
RoyaltyReportItem.size
RoyaltyReportItem.count
RoyaltyReport.count
edit -t
run
rr = RoyaltyReport.find(176)
rr.royalty_report_items.size
rr.royalty_report_items.group(:product_id, :technology_id)
rr.royalty_report_items.index_by(&:product_id)
_.length
rr.royalty_report_items.count
indexed = rr.royalty_report_items.index_by(&:product_id)
index.first.length
indexed = rr.royalty_report_items.all.index_by(&:product_id)
indexed = rr.royalty_report_items.group_by(:product_id)
indexed = rr.royalty_report_items.all.group_by(:product_id)
indexed = rr.royalty_report_items.load.group_by(:product_id)
indexed = rr.royalty_report_items.load.group_by(&:product_id)
indexed = rr.royalty_report_items.load.group_by {|rri| [rri.product_id, rri.technology_id]}
groups = _
rr
rr.entity.royalty_reports.last(2).first
prev = _
indexed = rr.royalty_report_items.load.group_by {|rri| [rri.product_id, rri.technology_id]}
report = rr
report.royalty_report_items.load.group_by {|rri| [rri.product_id, rri.technology_id]}.map(&:exchanged_unit_price).reduce(&:+)
report.royalty_report_items.load.group_by {|rri| [rri.product_id, rri.technology_id]} #.map(&:exchanged_unit_price).reduce(&:+)
report.first
hash = report.royalty_report_items.load.group_by {|rri| [rri.product_id, rri.technology_id]} #.map(&:exchanged_unit_price).reduce(&:+)
hash.first
hash = report.royalty_report_items.load.group_by {|rri| [rri.product_id, rri.technology_id]}.map{|k,v| v.map(&:exchanged_unit_price).reduce(&:+) }
hash.count
hash
hash = report.royalty_report_items.load.group_by {|rri| [rri.product_id, rri.technology_id]}
hash = _
hash[3622, 15].lenght
hash[3622, 15].length
hash[[3622, 15]].length
hash.each {|k,v| puts v.length}
sums = {}
hash
hash.each { |k, v| sums[k] = v.reduce(&:exchanged_unit_price) }
hash.each { |k, v| sums[k] = v.map(&:exchanged_unit_price).reduce(&:+) }
sums
sums.map(&:to_f)
sums.values.map(&:to_f)
edit -t
rr.entity_id
id = _
last_2_reports(id)
edit -t
reports
edit -t
run
edit -t
run
0.next
1.next
num = 1
num.next
num.next!
edit -t
run
a, b = []
a
b
b.present?
Cloud.current
edit -t
run
last_2_reports(2)
prev, curr = last_2_reports(2)
prev.values.first(2)
a, b = prev.values.first(2).map(&:exchanged_unit_price)
prev
prev.values
edit -t
prev, curr = last_2_reports(2)
edit -t
prev, curr = last_2_reports(2)
edit -t
run
prev, curr = last_2_reports(2)
prev.values.first(2)
prev.values.first(2).flatten
a, b = prev.values.first(2).flatten.map &:exchanged_unit_price
a
b
a == b
edit -t
prev, curr = last_2_reports(2)
prev
prev.values.first
prev, curr = last_2_reports(2)
last_2_reports(2)
last_2_reports(2).flatten
[].present?
prev, curr = last_2_reports(2)
curr
curr.present
curr.present?
curr
edit -t
prev, curr = last_2_reports(2)
cur
curr
edit -t
ReportingPeriodService.quarter_name(ReportingPeriodService(Date.today))
require 'reporting_period_service.rb'
ReportingPeriodService.quarter_name(ReportingPeriodService(Date.today))
ReportingPeriodService.quarter_name(Date.today.financial_quarter)
Date.today.financial_quarter
prev
edit -t
prev, curr = last_2_reports(2)
prev[:report]
prev[:report].accepted_at
prev[:report].accepted_at.finacial_quarter
Date.new(prev[:report].accepted_at).finacial_quarter
prev[:report].accepted_at.to_date.finacial_quarter
Date.today.fincancial_quarter
Date.today.financial_quarter
Date.today.class
prev[:report].accepted_at.to_date.class
date = prev[:report].accepted_at.to_date
date.financial_quarter
date = prev[:report].accepted_at.to_date.financial_quarter
now
now = Time.now
(Time.now - now).seconds
(Time.now - now)
SignalCategory.all
exit
RvxSignalGenerator.last
date_now = DateTime.now.to_s
txn_id = Digest::MD5.hexdigest date_now
RvxSignalGenerators::RoyaltyReportRateChange.new(28, txn_id).perform
RvxSignalGenerators::RoyaltyReportRateChange.new.perform(28, txn_id)
reload!
RvxSignalGenerators::RoyaltyReportRateChange.new.perform(28, txn_id)
exit
txn_id = Digest::MD5.hexdigest date_now
date_now = DateTime.now.to_s
txn_id = Digest::MD5.hexdigest date_now
RvxSignalGenerators::RoyaltyReportRateChange.new.perform(28, txn_id)
reload!
date_now = DateTime.now.to_s
txn_id = Digest::MD5.hexdigest date_now
RvxSignalGenerators::RoyaltyReportRateChange.new.perform(28, txn_id)
exit
before = RvxSignal.count
Signal.count
before = RvxSignal.count
date_now = DateTime.now.to_s
txn_id = Digest::MD5.hexdigest date_now
RvxSignalGenerators::RoyaltyReportRateChange.new.perform(28, txn_id)
exit
date_now = DateTime.now.to_s
txn_id = Digest::MD5.hexdigest date_now
RvxSignalGenerators::RoyaltyReportRateChange.new.perform(28, txn_id)
RvxSignal.count
RoyaltyReport
RvxSignal.last
RvxSignal.where(txn_id: RvxSignal.last.txn_id)
RvxSignal.where(txn_id: RvxSignal.last.txn_id).count
RvxSignal.where(txn_id: RvxSignal.last.txn_id).destroy_all
exit
date_now = DateTime.now.to_s
txn_id = Digest::MD5.hexdigest date_now
RvxSignalGenerators::RoyaltyReportRateChange.new.perform(28, txn_id)
Entity.first
Entity.find 2
e = _
e.entity_types
e.entity_type_ids
e = Entity.find 2
e.entity_type_ids
exit
date_now = DateTime.now.to_s
txn_id = Digest::MD5.hexdigest date_now
RvxSignalGenerators::RoyaltyReportRateChange.new.perform(28, txn_id)
exit
date_now = DateTime.now.to_s
txn_id = Digest::MD5.hexdigest date_now
RvxSignalGenerators::RoyaltyReportRateChange.new.perform(28, txn_id)
RvxSignal.last
RvxSignal.where(txn_id: RvxSignal.last.txn_id).select {|sig| sig.target_entity_id != sig.product_entity_id }
sigs = _
sigs.lst
sigs.last
Entity.find 2
SignalGroup
SignalGroup.all
RoyaltyReportItem
CostReport.last
CostReport.last.accrual_period
CostReport.last.accrual_period_id
CostReport
Product.find 1163
p = _
p.has_sticker = true
:w
p.save
StickerTransaction.last
StickerTransaction.last.transaction_amount
StickerTransaction.last.transaction_amount.to_f
Product.find 3
Product.find(3).has_sticker = true
_.save
p = Product.find(3)
p.has_sticker = true
p.save
Product.find_by model: 'aaa123'
p = Product.find(3)
ProductCategory.find 13
ProductCategory.find_by: name: 'Sound bar'
ProductCategory.find_by name: 'Sound bar'
UnknownProduct.find 3
Product == Product
Prodcut
p
p.is_a? Product
RoyaltyReport.last.start_date
RoyaltyReport.last.start_date.class
RoyaltyReport.where(accepted_at: nil)
rr = RoyaltyReport.last
rr.start_date
rr.end_date
rr.start_date.financial_quarter
`git show --pretty=%H`
`git rev-parse --short HEAD`
commit = `git rev-parse --short HEAD`
EntitiesIndex
EntitiesIndex.class_methods
EntitiesIndex.methods
EntitiesIndex.possible_names('Jon')
EntitiesIndex.possible_names('Jon Co')
reload!
EntitiesIndex.possible_names('Jon Co')
EntitiesIndex.possible_names('Jon Intl')
EntitiesIndex.possible_names(3.14)
EntitiesIndex.new
EntitiesIndex.new.class
EntitiesIndex.new
ind = _
ind.entities
ind.as_josn
ind.as_json
CostReport.last
report = )
report = CostReport.last
report.currency
report.currency.name
report.currency = nil
report.currency.name
report.currency.try(:name)
report.amount
Monetize.parse(report.amount, nil)
Monetize.parse(report.amount, nil).to_f
TargetListItemIdentifiedProduct.where(product: nil)
TargetListItemIdentifiedProduct.where(product_id: nil)
tli = _.first
CostReport.last
CostReport.last.start_date
CostReport.last.end_date
AccrualReport.last.accrual_period.name
exit
Product.find 105462
xit
exit
Figaro.env.rds_api_url
RdsSearch.rds_product_search(['HDMI'])
operator 'AND'
operator = 'AND'
terms = {keywords: 'HDMI'}
query = Product.generate_query_by_keywords(terms, operator, {})
products = []
search_result = Product.search(query, size: 400).page(1).results
search_result = Product.search(query, size: 400).page(1).response
search_result = Product.search(query, size: 400).page(1).response['products']
search_result = Product.search(query, size: 400).page(1).response
query
query
search_result = Elasticsearch::Model.search(query, [Product], size: per_page).page(params[:page]).results
search_result = Elasticsearch::Model.search(query, [Product], size: per_page).page(1).results
search_result = Elasticsearch::Model.search(query, [Product], size: 400).page(1).results
search_result = Product.search(query, size: 400).page(1).results
search_result.map { |p| p.id if p.type == 'products' }.compact
ids = _
Product.includes(:sellers, :scrape_product_sellers, :scrapes, :images, :screenshots).where(id: ids)
search_result.total
products = Product.includes(:sellers, :scrape_product_sellers, :scrapes, :images, :screenshots).where(id: ids)
render products, meta: { total: search_result.total, current_page: search_result.current_page }, meta_key: :pagination_info
RdsSearch.rds_product_search(['HDMI'])
terms = {keywords: 'HDMI'}
products = Product.includes(:sellers, :scrape_product_sellers, :scrapes, :images, :screenshots).where(id: ids)
Technology.first
t = _
t.send(keyword_type)
t.send('strong_keywords')
RdsSearch.rds_product_search('HDMI')
reload!
RdsSearch.rds_product_search('HDMI')
response = _
JSON.parse(response)['products']
respone
response
response.map(|hp| Hashie::Mash.new(hp) }
response.map {|hp| Hashie::Mash.new(hp) }
royalty_products = RoyaltyReport.includes(:products).flat_map(&:products).uniq(&:id).map {|p| Hashie::Mash.new(brand: p.brand, model: p. model) }
rds_products = response.map {|hp| Hashie::Mash.new(hp) }
rp = reported_products.first
rp = royalty_products.first
reported_products = rds_products.select {|p| royalty_products.include? Hashie::Mash.new(brand: p.brand, model: p.model)}
reported_products.count
royalty_products.count
rds_products.count
response = RdsSearch.rds_product_search(['HDMI'])
response = RdsSearch.rds_product_search(['HDMI', 'SONY'])
response = RdsSearch.rds_product_search(['HDMI'])
response = RdsSearch.rds_product_search(['HDMI', 'SONY'])
Product.find 80657
Product.find 80657
p = Product.find 80657
p.details
p.detail
p.details
p.product_identifiers
p.categories
response = RdsSearch.rds_product_search('HDMI,SONY')
response = RdsSearch.rds_product_search('HDMI,SONY', nil, nil, 'any')
response = RdsSearch.rds_product_search('HDMI,SONY')
reload!
response = RdsSearch.rds_product_search('HDMI,SONY')
reload!
response = RdsSearch.rds_product_search('HDMI,SONY')
Product.first
response = RdsSearch.rds_product_search('HDMI,SONY')
relaod!
reload!
response = RdsSearch.rds_product_search('HDMI,SONY')
reload!
response = RdsSearch.rds_product_search('HDMI,SONY')
response = RdsSearch.rds_product_search('HDMI')
response = RdsSearch.rds_product_search('Sony')
params = { keywords: 'HDMI' }
response = RestClient.get 'https://rvx:1Admin11@data.ruvixx.com/api/v1/products', params: params
hash = JSON.parse(response)
reload!
response = RestClient.get 'https://rvx:1Admin11@data.ruvixx.com/api/v1/products', params: params
hash = JSON.parse(response)['products']
hash_products.map {|hp| Hashie::Mash.new(hp) }
hash.map {|hp| Hashie::Mash.new(hp) }
date_now = DateTime.now.to_s
txn_id = Digest::MD5.hexdigest date_now
RvxSignalGenerator::ProductFoundByKeyword.new.perform(1, txn_id)
RvxSignalGenerators::ProductFoundByKeyword.new.perform(1, txn_id)
exit
date_now = DateTime.now.to_s
txn_id = Digest::MD5.hexdigest date_now
RvxSignalGenerators::ProductFoundByKeyword.new.perform(1, txn_id)
SignalGeneratorLog.last
SignalGeneratorLog.all
response = RestClient.get 'https://rvx:1Admin11@data.ruvixx.com/api/v1/products', params: params
response = RdsSearch.rds_product_search('HDMI,SONY')
response = RdsSearch.rds_product_search('HDMI,SONY', 'any')
response = RdsSearch.rds_product_search('HDMI,SONY', nil, nil, 'any')
reload!
response = RdsSearch.rds_product_search('HDMI,SONY', nil, nil, 'any')
response = RdsSearch.rds_product_search('HDMI,SONY', nil, nil, 'all')
response.count
RestClient::GatewayTimeout
response = RdsSearch.rds_product_search('HDMI,SONY', nil, nil, 'any')
exit
exit
p = Product.last
brand = p.brand
p.update_attributes brand: 'Jonathan'
p.update_attributes brand: 'Jon'
s.reload
exit
rows = 1762 + 11613 - 2
rows * 1
rows / 1024
Product.skip_callback(:update, :after, :update_document)
Product.connection
Product.skip_callback(:update, :after, :update_document)
p = Product.last
p.update_attributes brand: 'Jonathan'
Product.set_callback(:update, :after, :update_document)
Product.skip_callback(:update, :after, :update_document)
Product.set_callback(:update, :after, :update_document)
p.reload!
p.reload
p.update_attributes brand: 'Jon'
100 / 20
exit
Identifer.asin.first
Identifier.asin.first
asin = _
Scrape.where source_id: 1
_.last
s = _
s.products
s.products.count
reload!
s.products.count
Product.connection
exit
Scrape.where source_id: 1
s = _.last
s.products.size
s.other_products.size
s.other_products.sample
op = _
op.identifiers
op.details
gtins = Identifier.where(uniq_id: ["00646444705639"])
Product.count
Product.includes(:identifiers).first(1000).select {|p| p.identifiers.size > 1 }
products = _
p = products.last
p.identifiers
OtherProduct.includes(:identifiers).merge(Identifier.asin).where('identifiers.uniq_id': 'B009X14QDY')
other_products = OtherProduct.includes(:identifiers).first(1000).select {|p| p.identifiers.size > 1 }
Product.includes(:identifiers).merge(Identifier.asin).where('identifiers.uniq_id': 'B009X14QDY')
SourceCategory.first
cat = SourceCategory.first
category = SourceCategory.first
source = Source.first
source = Source.find 1
category = source.source_categories.first
category.parse_url
url = _
URI.escape(url, '|')
url.host
url = category.url
url.host
url = URI.escape(url, '|')
url.host
edi t-t
edit -t
exit
"D3300 DX-format Digital SLR Kit with 18-55mm DX Vibration Reduction II and 55-200mm DX Vibration Reduction II Zoom Lenses and Case EN-EL14a Rechargeable Li-ion Battery MH-24 Quick Charger UC-E17 USB Cable EG-CP14 Audio Video Cable DK-25 Rubber Eyecup AN-DC3 Camera Strap BF-1B Body Cap NikonView NX CD-ROM LC-52 Front Lens Cap LF-4 Rear Lens Cap LC-52 Front Lens Cap LF-4 Rear Lens Cap".length
exit
Project.find 14
p = _
p.signals
Project.first(1000).select {|p| p.signals.size > 0}
p = Project.find 15
p.signals
p.signals.size
signal = RvxSignal.find 52905
signal.incidents
signal.incident
p
p.statsus
p.status
p.revenue_opportunities
p.close
signal.reload
signal.status
signal.closed_at
RvxSignal.close(project: p)
signal.reload
signal.closed_at
RvxSignalGenerator.last
RvxSignal.find 67666
sig = _
reload!
i = Incident.find 95
i.name
i.name.gsub(/^(Incident #\d+)\s?-?\s?/, '')
reload
reload!
i.name
i.name = i.name
i.save
i.name
i.name.gsub(/^(Incident ##{i.id})\s?-?\s?/, '')
reload!
i.name
i.reload
i.name = i.name
i.save
i.name
exit
i = Incident.find 95
i.name
i.update_attributes(name: "Incident #95")
i.name
i.name = "Incident #95"
i.save
i.reload
i.name
reload!
i.reload!
i.reload
i.name
i.save
i.name
exit
i = Incident.find 95
i.name
i.save
i.name
i.attributes['name
i.attributes['name']
i.reload
i.attributes['name']
reload!
i = Incident.find 95
i.name
i.name = 'Incident #95'
i.save
reload!
i = Incident.find 95
i.name
i.name = ''
i.save
i.name
i.attributes['name']
i.name = 'Incident #95'
i.save
i.attributes['name']
i.name.gsub!(/^(Incident ##{self.id})\s?-?\s?/, '')
i.name.gsub!(/^(Incident ##{i.id})\s?-?\s?/, '')
i.name
i.name.gsub!(/^(Incident ##{i.id})\s?-?\s?/, '')
i.name.gsub!(/^(Incident ##{i.id})\s?-?(\s)?/, '')
strs = ["Incident ##{i.id} - ", "Incident ##{i.id}"]
strs.each {|str| i.name.gsub(str, '') }
i.name
strs.each {|str| i.name.gsub!(str, '') }
i.name
i.name = nil
i.save
i.name = nil
i.save
i.reload
i.name
i.name = nil
i.save
i.name
i.reload
i.name
i.reload
i.name
i.attributes['name']
exit
amaz_cats = SourceCategory.where(source_id: 3)
amaz_cats.first
amaz_cats.first.url
Product.count
Scrape.last.destroy
exit
nil < 1
nil.to_i
Product.last
p = )
p = Product.last
p.scrapes
p.identifiers
amaz_scrapes = Scrape.where(source_id: 3)
s = amaz_scrapes.last
s.products.last
s = amaz_scrapes.last(2).first
s.products.last
p = _
p.details
g_scrape = Scrape.last
gp = g_scrape.products.first
gp.detail
p
p.scrapes
gp.scrapes
gp.model
p.model
p.detail
gp.detail
gp.details
g_scrape
gp.details.where(scrape_id: g_scrape.id).last
gp
Scrape.last.products.first(10).map do |p|
  p.details.where(scrape_id: Scrape.last.id)
end
Scrape.last.products.first(10).map do |p|
  p.details.where(scrape_id: Scrape.last.id)
end.first
Scrape.last.products.first(10).map do |p|
  p.details.where(scrape_id: Scrape.last.id)
end.last
detail = _
detail
detail.prodcut
detail.product
_.first
detail = _
detail.product
p = _
p.detail
ap = amaz_scrapes.last.products.sample
ap.detail
ap
SourceCategory.where(source_id: 3).first
cat = _
c = SourceCategory.where(source_id: 3).first
c.url
Capybara::Session.new(:poltergeist, default_wait_time: 60, time_out: 60)
session = _
ActivityLog
Product.find_by(model: 'Retevis H-777')
p = _
p.detail
p.details
p.detail.attribute_keys
Detail.find(246089)
ProductDetail.find(246089)
detail = _
p = detail.product
p.url
p.original_url
detail
text = "Retevis H-777 2 way radio walkie takies is widely used in camping, skiing, keeping in contact with your children in theme parks, and various outdoor activities Of course it is also the prefect way for the following industry:Service industry(Hotel,Restaurant,Supermarket,etc);Security,Property Management;Police,Constrution Sites,Transportation;Warehouse, Ports, Logistic Industry etc"
name = text.strip
value = text.gsub(name, '')
text = "If you need more powerful and longer distance 2 way radio,please consider Retevis RT 1 2 Way Radio.It is 10w and 3600 mAh battery."
text = "Retevis H-777 2 way radio walkie takies is widely used in camping, skiing, keeping in contact with your children in theme parks, and various outdoor activities Of course it is also the prefect way for the following industry:Service industry(Hotel,Restaurant,Supermarket,etc);Security,Property Management;Police,Constrution Sites,Transportation;Warehouse, Ports, Logistic Industry etc"
name = text.delete(':')
exit
ProductDetail.find(246089)
p = detail.product
p = ProductDetail.find(246089).product
p.detail
Scrape.where(source_id: 3).last
scrape = _
p.category
p.categories
p.name
p.original_url
ProductDetailJob.new.perform(scrape.id, 1, p.original_url)
AmazonProductDetailJob.new.perform(scrape.id, 1, p.original_url)
p.detal
p.detail
AmazonProductDetailJob.new.perform(scrape.id, 1, p.original_url)
reload!
AmazonProductDetailJob.new.perform(scrape.id, 1, p.original_url)
exit
p = ProductDetail.find(246089).product
s = Scrape.where(source_id: 3).last
AmazonProductDetailJob.new.perform(s.id, 1, p.original_url)
p.reload
p.detail
reload!
exit
p = ProductDetail.find(246089).product
s = Scrape.where(source_id: 3).last
AmazonProductDetailJob.new.perform(s.id, 1, p.original_url)
p.detail
p.reload
p.detail
name = detail.data_hash[:General].first.key
name = p.detail.data_hash[:General].first.key
name = p.detail.data_hash[:General]
name = p.detail.data_hash[:General].keys.first
name.to_s
name = name.to_s
value = p.detail.data_hash[:General].values.first.to_s
name == value
value.gsub(name, '')
exit
p = ProductDetail.find(246089).product
s = Scrape.where(source_id: 3).last
AmazonProductDetailJob.new.perform(s.id, 1, p.original_url)
exit
p = ProductDetail.find(246089).product
AmazonProductDetailJob.new.perform(s.id, 1, p.original_url)
s = Scrape.where(source_id: 3).last
AmazonProductDetailJob.new.perform(s.id, 1, p.original_url)
name = "Retevis H-777 2 way radio walkie takies is widely used in camping, skiing, keeping in contact with your children in theme parks, and various outdoor activities Of course it is also the prefect way for the following industry:Service industry(Hotel,Restaurant,Supermarket,etc);Security,Property Management;Police,Constrution Sites,Transportation;Warehouse, Ports, Logistic Industry etc"
text = "Retevis H-777 2 way radio walkie takies is widely used in camping, skiing, keeping in contact with your children in theme parks, and various outdoor activities Of course it is also the prefect way for the following industry:Service industry(Hotel,Restaurant,Supermarket,etc);Security,Property Management;Police,Constrution Sites,Transportation;Warehouse, Ports, Logistic Industry etc"
text.gsub(name, "").gsub(/^(\s+):, '').strip
text.gsub(name, "").gsub(/^(\s+):/, '').strip
text
value = text.gsub(name, "").gsub(/^(\s+):/, '').strip
value.blank?
exit
p = ProductDetail.find(246089).product
s = Scrape.where(source_id: 3).last
AmazonProductDetailJob.new.perform(s.id, 1, p.original_url)
exit
p = ProductDetail.find(246089).product
s = Scrape.where(source_id: 3).last
AmazonProductDetailJob.new.perform(s.id, 1, p.original_url)
reload!
p.reload
p.detail
p.details.size
p.detail.last
p.detail
p.details
p.detail
exit
p = ProductDetail.find(246089).product
s = Scrape.where(source_id: 3).last
AmazonProductDetailJob.new.perform(s.id, 1, p.original_url)
p.reload
p.detail
edit -t
exit
p = ProductDetail.find(246089).product
s = Scrape.where(source_id: 3).last
AmazonProductDetailJob.new.perform(s.id, 1, p.original_url)
exit
p = ProductDetail.find(246089).product
s = Scrape.where(source_id: 3).last
AmazonProductDetailJob.new.perform(s.id, 1, p.original_url)
exit
p = ProductDetail.find(246089).product
s = Scrape.where(source_id: 3).last
AmazonProductDetailJob.new.perform(s.id, 1, p.original_url)
Scrape.where(source_id: 1).first
s = )
s = Scrape.where(source_id: 1).first
s.products + s.other_products
s.products.size + s.other_products.size
s
(s.products.size + s.other_products.size) / s.duration
(s.products.size + s.other_products.size).to_f / s.duration
s
s.duration / 60
(s.products.size + s.other_products.size).to_f / s.duration / 60
(s.products.size + s.other_products.size).to_f / (s.duration / 60)
p.detail
p.product_description
RvxSignal.size
RvxSignal.count
RvxSignal.select(:id, :sig, :created_at)
RvxSignal.select(:id, :sig, :created_at).load.group_by {|s| s.sig }
O[q
  :q
:q
jj
exit
nil.to_s
p
p.name.gsub('', '%%%')
'' || 2
!!'' || 2
exit
edit -t
total = results.reduce(:+)
total / results.length
72 * 2000
_ / 15
_ / 7
/ 60
1371 / 60
exit
5 * 60
1000 / 60
Scrape.where(source_id: 3).last
Scrape.where(source_id: 3).last(2).first
s = _
s.other_products.sample
op = _
op.url
op = s.other_products.sample
AmazonProductDetailJob.new.perform(s.id, 1, op.url)
s
op.categories
AmazonProductDetailJob.new.perform(s.id, 6, op.url)
exit
op = OtherProduct.find 465661
s = Scrape.where(source_id: 3).last(2).first
AmazonProductDetailJob.new.perform(s.id, 6, op.url)
op.detail.detail_hash
op.detail
op.brand
op.model
op.reload
OtherProduct.last
op
op.detail
exit
op = OtherProduct.find 465661
op.details
s = Scrape.where(source_id: 3).last(2).first
AmazonProductDetailJob.new.perform(s.id, 6, op.url)
op
op.reload
op.detail
op
p = Product.where(brand: 'VIZIO', model: 'm70-c3')
p.detail
p = p.last
p.detail
url = 'http://www.amazon.com/Amazon-Fire-Phone-32GB-Unlocked/dp/B00OC0USA6/ref=lp_7072561011_1_1?s=wireless&ie=UTF8&qid=1440037139&sr=1-1'
exit
url = 'http://www.amazon.com/Amazon-Fire-Phone-32GB-Unlocked/dp/B00OC0USA6/ref=lp_7072561011_1_1?s=wireless&ie=UTF8&qid=1440037139&sr=1-1'
s = Scrape.where(source_id: 3).last(2).first
AmazonProductDetailJob.new.perform(s.id, 1, op.url)
AmazonProductDetailJob.new.perform(s.id, 1, url)
OtherProductDetail
OtherProductDetail.last
s
exit
s = Scrape.where(source_id: 3).last(2).first
url = 'http://www.amazon.com/HDMI-Cloner-need-Capture-streaming-videos/dp/B00TF9MCXU/ref=sr_1_13/176-3964668-0357537'
AmazonProductDetailJob.new.perform(s.id, 1, url)
op = OtherProduct.last
exit
s = Scrape.where(source_id: 3).last(2).first
url = 'http://www.amazon.com/HDMI-Cloner-need-Capture-streaming-videos/dp/B00TF9MCXU/ref=sr_1_13/176-3964668-0357537'
AmazonProductDetailJob.new.perform(s.id, 1, url)
exit
s = Scrape.last
s
s.reload
s
s.reload
ProductManual
ProductManual.last
pm = ProductManual.last
pm.scrape_id = 3
pm.save
pm.reload
pm.scrape
exit
90 * 400000
t = _
t / 60 / 60 / 24
t / 60 / 60 / 24 / (7 * 15)
p
p = Product.all.sample
p.detail
p = Product.all.sample
p.detail
p = Product.find 148811
p.detail
p.details
Sidekiq::Queue.new('product_listing_queue').size
Sidekiq::Queue.new('product_listing_queue').clear
exit
CapybaraPhantomJs
require 'capybara_phantomjs'
new_session
@session
session = new_session
session.driver
p
p = Product.last
p.detail
p.url
p.original_url
p.product_description
product.last.id
Product.last.id
p
p.product_description
Product.where.not(product_description: nil).first
Product.includes(scrape: :scoure).where.not(product_description: nil).where(source_id: 3).first
Product.includes(scrape: :scoure).where.not(product_description: nil).where('scrapes.source_id': 3).first
s = Scrape.where(source_id: 3).last
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(s.id, 1, url)
p
ProductDetail.last
p = ProductDetail.last.product
p.product_description
AmazonProductDetailJob.new.perform(s.id, 1, url)
exit
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(128, 1, url)
exit
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(128, 1, url)
exit
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(128, 1, url)
exit
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(128, 1, url)
exit
AmazonProductDetailJob.new.perform(128, 1, url)
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(128, 1, url)
p = ProductDetail.last.product
p.detail
p.product_description
exit
p = ProductDetail.last.product
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(128, 1, url)
exit
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(128, 1, url)
exit
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(128, 1, url)
p = ProductDetail.last
p = ProductDetail.last.product
p
exit
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(128, 1, url)
exit
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(128, 1, url)
exit
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(128, 1, url)
exit
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(128, 1, url)
exit
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(128, 1, url)
4 * 400000
secs = _
500000 / 4
_ / 60
2083 / 24
exit
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(128, 1, url)
exit
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(128, 1, url)
exit
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(128, 1, url)
exit
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(128, 1, url)
exit
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(128, 1, url)
exit
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(128, 1, url)
exit
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(128, 1, url)
exit
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(128, 1, url)
exit
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(128, 1, url)
exit
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(128, 1, url)
p = ProductDetail.last.product
exit
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(128, 1, url)
p = ProductDetail.last.product
p.product_description
exit
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(128, 1, url)
exit
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(128, 1, url)
p
id = 148811
ProductDetail.last.product
p = _
p.reload
text = "Product Description Package Quantity: 1 | Style Name: Black Product Description The SBG6580 SURFboard Wi-Fi Cable Modem Gateway enables the delivery of innovative ultra-broadband data and multimedia services as well as high-bandwidth home networking. Designed for seamless mobility, Motorola’s SBG6580 is a fully integrated all-in-one home networking solution that combines the functionality of a DOCSIS/EuroDOCSIS 3.0 cable modem, four-port 10/100/1000 Ethernet switch with advanced firewall, and an 802.11n Wi-Fi access point in a sleek, stylish package for the sophisticated consumer. It’s the perfect networking solution for the home, home office, or small business, allowing users to create a custom network to share a single ultra-broadband connection, files, and networked peripherals using wired or Wi-Fi connectivity. Cost-effective, efficient, and secure, the SBG6580 enables users to maximize the potential of their existing resources, while benefiting from next generation high-bandwidth services.Frequency range: DOCSIS and EuroDOCSIS 108 to 1002 MHz (edge to edge). From the Manufacturer Ultra-fast Internet and Wireless N at your fingertips – NEW lightning fast technology lets you surf up to 8x faster than traditional cable modems. The Motorola SURFboard SBG6580 eXtreme Wireless Cable Modem Gateway unleashes the next wave of broadband technology at exceptional speeds with high-performance Wireless N networking and a 4-port Gigabit Ethernet router. The SBG6580 is up to 8 times faster than DOCSIS® 2.0 broadband cable modems making it capable of reaching data rates up to 300 Mbps. That makes gaming, shopping, downloading, working, high-quality voice and video conferencing, and peer-to-peer networking applications — far more realistic, faster, and efficient than ever before. In addition to the lightning fast speeds, the SBG6580 includes an 802.11n wireless access point with dual band capabilities, as well as a 4-port Gigabit Ethernet router. The SBG6580 delivers your complete personal media experience, at lightning-fast broadband speed. So what are you waiting for? Motorola SURFboard® eXtreme Wireless Cable Modem Gateway Easy to Install, Activate, and Use – Includes a Wi-Fi Wizard and a Wi-Fi pairing button. Delivers a realistic gaming experience wirelessly – watch as games come to life Next Generation DOCSIS® 3.0; backwards compatible to DOCSIS 2.0 Up to 8x faster than DOCSIS 2.0 cable modems Enhance your interactive gaming experience Stream HD video throughout your network – wired or wirelessly Easy to install and simple to use with Motorola’s setup wizard and Wi-Fi pairing button Secure and protect your data and privacy with a built-in, commercial-grade firewall Watch as games come to life. Highlights Capable of downloading up to 8000 times faster than 56k analog phone modems Capable of downloading up to 8 times faster than DOCSIS 2.0 broadband cable modems DOCSIS® 3.0 Certified Enables a myriad of advanced multimedia services: - Access real-time online games; new heights in realism and speed - wirelessly Faster music downloads; one song or the entire album Share digital photos, home movies and other multimedia effortlessly over the Internet or over your Wi-Fi network Download high-definition movies to your PC or watch as streaming media Windows®, Macintosh®, and UNIX® compatible Supports Advanced Encryption Services for data, as activated by your operator Features a four-port 10/100/1000Base-T Gigabit Port for incredible wired network speeds: enjoy lag-free network gaming and faster file transfers Wireless N Networking with Dual Band Wi-Fi Intuitive, easy to read front panel operational status LEDs Use your current Internet Browser No additional software required Slim design saves desk space 1-year limited warranty What’s in the Box Inside the Box - Motorola SBG6580 SURFboard eXtreme Wireless Cable Modem Gateway,Motorola’s Wi-Fi Wizard & Installation CD-ROM,Quick start guide,Ethernet cable,Power adapter and cord,Install sheet,Warranty card if(typeof P !== 'undefined' && typeof P.when !== 'undefined') { P.when('product-description-fix').execute(function(productDescription){ productDescription.fixTableIssue(); }); }
"
text
text.length
exit
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(128, 1, url)
id = 148811
p = Product.find(id)
exit
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(128, 1, url)
exit
SourceCategory.where(source_id: 3, test_process: true)
Category.find 72
Category.find 112
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(128, 1, url)
p = Product.new
p.valid?
exit
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(128, 1, url)
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(128, 1, url)
exit
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(128, 1, url)
p = ProductDetail.last.product
exit
'_attributes'.length
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
Product
Product.connection
Prodcut
Product
Product.attributes
p
p = ProductDetail.last.product
p
p.
p
p.details
exit
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(128, 1, url)
p = ProductDetail.last.product
p.product_description = nil
p.save
p
AmazonProductDetailJob.new.perform(128, 1, url)
exit
p = ProductDetail.last.product
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(128, 1, url)
exit
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(128, 1, url)
30 * 7
150 * 7
500000 * 100
_ / 60 / 60 / 24
_ / 7
_ / 30
150 * 30
150 * 7
600 / 7
600 - 7 - 3
590 / 7
1 * 7 + 3 + 15
600 - 25
_ / 7
exit
processes = Sidekiq::ProcessSet.new
processes.count
exit
OtherProduct.all.each do |op|
  p op.name
end
31536000 / 60
_ / 60
_ / 24
OtherProduct.all.each do |op|
  p op.name
end
reload!
OtherProduct.all.each do |op|
  p op.name
end
ActiveRecord::Base.establish_connection
OtherProduct.all.each do |op|
  p op.name
end
exit
OtherProduct.all.each do |op|
model = 0
OtherProduct.all.each do |op|
  puts model += 1
end
brand
model
OtherProduct.count
Sidekiq.options
ENV['WEB_CONCURRENCY']
exit
s = Scrape.last
def stats(s)
  p = s.products.size
  op = s.other_products.size
  total = p + op
  time = (s.duration || Time.now - s.created_at).to_f / 60
  rate = total / time
  puts "id: #{s.id} Source: #{s.source_id} Ratio: #{p}/#{op} Total: #{total} Time: #{time.round(2)}mins Rate: #{rate.round(2)} created: #{s.created_at.strftime("%m/%d/%Y")}"
  puts "id: #{s.id} Source: #{s.source_id} Total: #{total} Rate: #{rate.round(2)} created: #{s.created_at.strftime("%m/%d/%Y")}"
end
stats s
Sidekiq::Queues.new('product_detail_queue')
Sidekiq::Queue.new('product_detail_queue')
Sidekiq::Queue.new('product_detail_queue').clear
Sidekiq::Queue.new('upload_asset_queue').clear
Sidekiq::Queue.new('product_manual_review_queue').clear
Sidekiq::QueueList
Sidekiq::Queue
Sidekiq::Queue.list
Sidekiq::Queue.all
Sidekiq::Queue.all.each {|q| q.count}
Sidekiq::Queue.new('upload_asset_queue').clear
Sidekiq::Queue.new('product_detail_queue').clear
Sidekiq::Queue.new('sellers_queue').clear
Sidekiq::Queue.new('upload_asset_queue').clear
Sidekiq::Queue.new('product_detail_queue').clear
Sidekiq::Queue.new('sellers_queue').clear
Sidekiq::Queue.new('upload_asset_queue').clear
Sidekiq::Queue.new('sellers_queue').clear
Sidekiq::Queue.new('product_detail_queue').clear
Sidekiq::Queue.new('upload_asset_queue').clear
Sidekiq::Queue.new('product_manual_review_queue').clear
Sidekiq::Queue.new('upload_asset_queue').clear
Sidekiq::Queue.new('product_manual_review_queue').clear
Sidekiq::Queue.new('upload_asset_queue').clear
Sidekiq::Queue.new('product_manual_review_queue').clear
Sidekiq::Queue.new('product_detail_queue').clear
Sidekiq::Queue.new('upload_asset_queue').clear
Sidekiq::Queue.new('product_manual_review_queue').clear
Sidekiq::Queue.new('product_detail_queue').clear
Sidekiq::Queue.new('upload_asset_queue').clear
Sidekiq::Queue.new('product_manual_review_queue').clear
Sidekiq::Queue.new('upload_asset_queue').clear
Sidekiq::Queue.new('product_manual_review_queue').clear
Sidekiq::Queue.new('upload_asset_queue').clear
Sidekiq::Queue.new('product_manual_review_queue').clear
Sidekiq::Queue.new('upload_asset_queue').clear
Sidekiq::Queue.new('product_manual_review_queue').clear
Sidekiq::Queue.new('upload_asset_queue').clear
Sidekiq::Queue.new('product_manual_review_queue').clear
Sidekiq::Queue.new('upload_asset_queue').clear
Sidekiq::Queue.new('product_manual_review_queue').clear
s
s = Scrape.last
exit
s = Scrape.last
stats s
def stats(s)
  p = s.products.size
  op = s.other_products.size
  total = p + op
  time = (s.duration || Time.now - s.created_at).to_f / 60
  rate = total / time
  puts "id: #{s.id} Source: #{s.source_id} Ratio: #{p}/#{op} Total: #{total} Time: #{time.round(2)}mins Rate: #{rate.round(2)} created: #{s.created_at.strftime("%m/%d/%Y")}"
  puts "id: #{s.id} Source: #{s.source_id} Total: #{total} Rate: #{rate.round(2)} created: #{s.created_at.strftime("%m/%d/%Y")}"
end
stats s
exit
op = OtherProduct.last
op.detail
detail = op.detail
detail.attributes
detail
exit
detail = OtherProductDetail.last
detail.product_description = {:"Product Description" => "BlahBlahBlahBlahBlahBlahBlahBlahBlahBlahBlahBlahBlahBlahBlahBlahBlahBlahBlahBlah"}
detail.save
detail.reload
exit
ProductDetail.where.not(product_features: nil)
detail = _.first
detail
detail.data_hash
detail.data_hash[:product_description] = {:"Product Description" => "BlahBlahBlahBlahBlahBlahBlahBlahBlahBlahBlahBlahBlahBlahBlahBlahBlahBlahBlahBlah"}
detail.save
detail
detail.data_hash
detail.data_hash[:General]
detail.data_hash.keys
detail.data_hash.detail(:product_description)
detail.data_hash.delete(:product_description)
detail.save
detail.data_hash[:"Product Description"] = {:"Product Description" => "BlahBlahBlahBlahBlahBlahBlahBlahBlahBlahBlahBlahBlahBlahBlahBlahBlahBlahBlahBlah"}
detail.save
detail.data_hash.keys
exit
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(128, 1, url)
exit
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(128, 1, url)
exit
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(128, 1, url)
{}.blank?
exit
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(128, 1, url)
exit
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(128, 1, url)
ProductDetail.last
p = _.product
AmazonProductDetailJob.new.perform(128, 1, url)
ProductDetail.last
exit
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(128, 1, url)
ProductDetail.last
exit
url = 'http://www.amazon.com/gp/product/B00SMLK2KW/'
AmazonProductDetailJob.new.perform(128, 1, url)
p = ProductDetail.last.product
p.detail
detail2 = ProductDetail.last(2).first
detail
p.detail
det
detail2
p.detail
exit
url = 'http://www.amazon.com/gp/product/B00SMLK2KW/'
AmazonProductDetailJob.new.perform(128, 1, url)
p = ProductDetail.last.product
p.detail
p.images
p.images.map(&:original_url)
exit
url = 'http://www.amazon.com/gp/product/B00SMLK2KW/'
AmazonProductDetailJob.new.perform(128, 1, url)
p = ProductDetail.last.product
p.detail
exit
url = 'http://www.amazon.com/gp/product/B00SMLK2KW/'
AmazonProductDetailJob.new.perform(128, 1, url)
exit
url = 'http://www.amazon.com/gp/product/B00SMLK2KW/'
AmazonProductDetailJob.new.perform(128, 1, url)
p = ProductDetail.last.product
p.screeshots.last
p.screenshots.last
ProductScreenshot.last
ProductScreenshot.last.download_url
ProductScreenshot.last.attachment
ProductScreenshot.last.attachment.url
exit
url = 'http://www.amazon.com/gp/product/B00SMLK2KW/'
AmazonProductDetailJob.new.perform(128, 1, url)
p = ProductDetail.last.product
p.detail
exit
url = 'http://www.amazon.com/gp/product/B00SMLK2KW/'
AmazonProductDetailJob.new.perform(128, 1, url)
ProductDetail.last
exit
url = 'http://www.amazon.com/gp/product/B00SMLK2KW/'
AmazonProductDetailJob.new.perform(128, 1, url)
ProductDetail.last
exit
url = 'http://www.amazon.com/gp/product/B00SMLK2KW/'
AmazonProductDetailJob.new.perform(128, 1, url)
exit
url = 'http://www.amazon.com/gp/product/B00SMLK2KW/'
AmazonProductDetailJob.new.perform(128, 1, url)
ProductDetail.last
"https:".starts_with?(['http', https'])
"https:".starts_with?(['http', 'https'])
"https:".starts_with?(*['http', 'https'])
exit
urls = ['http', 'https']
url = "/lasdfjalskfj"
visit
require 'capybara_phantomjs'
visit_page('amazon.com', false)
session = new_session(false)
CapybaraPhantomJs.new_sesson
CapybaraPhantomJs.new_session
include CapybaraPhantomJs
session = new_session
session.visit_page('amazon.com', false)
exit
i = ProductImage.last
ProductImage.find_by(product: i.product, original_url: i.original_url)
p = i.product
url = i.original_url
ProductImage.find_by(product: p, original_url: url)
ProductImage.find_or_create_by(product: p, original_url: url)
exit
i = ProductImage.last
p = i.product
url = i.original_url
ProductImage.find_or_create_by(product: p, original_url: url)
exit
i = ProductImage.last
p = i.product
url = i.original_url
ProductImage.find_or_create_by(product: p, original_url: url)
exit
i = ProductImage.last
p = i.product
url = i.original_url
ProductImage.find_or_create_by(product: p, original_url: url)
s = Scrape.last
op = s.other_products.last
op
s
ScrapeOtherProduct.find_by(scrape_id: s.id, other_product_id: op.id)
ScrapeOtherProduct.count
exit
url = ;'http://www.amazon.com/gp/product/B00SMLK2KW/'
url = 'http://www.amazon.com/gp/product/B00SMLK2KW/'
s = Scrape.last
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDetail.last
AmazonProductDetailJob.new.perform(129, 1, url)
exit
url = 'http://www.amazon.com/gp/product/B00SMLK2KW/'
AmazonProductDetailJob.new.perform(129, 1, url)
exit
AmazonProductDetailJob.new.perform(129, 1, url)
url = 'http://www.amazon.com/gp/product/B00SMLK2KW/'
AmazonProductDetailJob.new.perform(129, 1, url)
exit
url = 'http://www.amazon.com/gp/product/B00SMLK2KW/'
AmazonProductDetailJob.new.perform(129, 1, url)
exit
url = 'http://www.amazon.com/gp/product/B00SMLK2KW/'
AmazonProductDetailJob.new.perform(129, 1, url)
exit
AmazonProductDetailJob.new.perform(129, 1, url)
url = 'http://www.amazon.com/gp/product/B00SMLK2KW/'
AmazonProductDetailJob.new.perform(129, 1, url)
exit
url = 'http://www.amazon.com/gp/product/B00SMLK2KW/'
AmazonProductDetailJob.new.perform(129, 1, url)
exit
ProductDetail.last
url = 'http://www.amazon.com/gp/product/B00SMLK2KW/'
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDetail.last
url = "http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/"
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDetail.last
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDetail.last
exit
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDetail.last
require 'nokogiri'
uri = URI(url)
body = Net::HTTP.get(uri)
url = 'http://tutorials.jumpstartlab.com/topics/scraping-with-capybara.html'
uri = URI(url)
body = Net::HTTP.get(uri)
exit
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDetail.last
exit
rails c
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(129, 1, url)
iframe
iframe.find
desc = iframe.css("#productDescription")
desc = iframe.css("#productDescription").text
c
continue
require 'open-uri'
url
page = Nokogiri::HTML(open(url))
require 'restclient
'
require 'restclient'
page = Nokogiri::HTML(RestClient.get(url))
AmazonProductDetailJob.new.perform(129, 1, url)
iframe
html
html = @session.evaluate_script("window.ProductDescriptionIframeResize.doc.body.innerHTML")
@session
html = @session.evaluate_script("window.ProductDescriptionIframeResize.doc.body.innerHTML")
continue
url
AmazonProductDetailJob.new.perform(129, 1, url)
exit
ScrapeUrl.first(100).map &:url
ScrapeUrl.where(scrape_id: 129).first(100).map &:url
ScrapeUrl.first(100).map &:url
ScrapeUrl.where(scrape_id: 129).map &:url
urls = _
urls.count
urls.uniq.count
url = urls.first
url = urls.first =~ /^[\/]{6}/
url = urls.first =~ /^\/{6}/
url = urls.first.gsub(/[ref=].+$/, '')
url = urls.first.gsub(/(ref=).+$/, '')
url = urls.first.gsub(/\/(ref=).+$/, '')
url = urls.first.gsub(/(ref=).+$/, '')
url
urls.first
url = 'http://www.amazon.com/1byone-Amplified-HDTV-Antenna-Performance/dp/B00IF70T4M/'
url = url.gsub(/(ref=).+$/, '')
urls.map {|url| url.gsub(/(ref=).+$/, '')}
urls.map {|url| url.gsub(/(ref=).+$/, '')}.uniq.count
400 - 326
74.0 / 400
ScrapeUrl
s = Scrape.where(source_id: 3).first
urls = ScrapeUrl.where(scrape_id: s.id).pluck(:url)
urls.count
urls.map {|url| url.gsub(/(ref=).+$/, '')}.uniq.count
(1834049 - _).to_f / 1834049
1834049 - 1525986
tot = _
tot * 60
_ / 60 / 60
/ 24
5134 / 24
tot
tot * 60
_ / (7 * 15) / 60
_ / 60
tot
ScrapeUrl.all.select(:url_type).uniq(:url_type)
urls = ScrapeUrl.where(scrape_id: s.id, url_type: "Product").pluck(:url)
urls.map {|url| url.gsub(/(ref=).+$/, '')}.uniq.count
urls.count
exit
AmazonProductDetailJob.new.perform(129, 1, url)
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(129, 1, url)
html
desc_el = iframe.css('#productDescription')
desc_el.css('h3.productDescriptionSource')
el = desc_el.css('h3.productDescriptionSource').first
iframe
Poltergeist::Capybara::HTML(iframe)
Capybara::HTML(iframe)
Capybara::Node::Simple.new(iframe)
node = _
node.find('#productDescription')
desc_el = _
desc_hash
within desc_el do
  all('h3.productDescriptionSource').each do |source_el|
    source = source_el.text.strip
    text = source_el.first(:xpath, "../div[@class='productDescriptionWrapper']").text.strip
    if source.blank?
      desc_hash = text
    else
      desc_hash[source.to_sym] = text
    end
    # @session.all('img').map do |image|
    #   save_image(scrape_id, product, src: image[:src])
    # end
  end
  desc_hash
end
html
iframe
desc_el
el = desc_el.css('h3.productDescriptionSource')
desc_el = iframe.css('#productDescription')
el = desc_el.css('h3.productDescriptionSource')
el.next_sibling
el = desc_el.css('h3.productDescriptionSource').first
el.next_sibling
el.css('+ productDescriptionWrapper')
el.next
el = desc_el.css('.productDescriptionWrapper').first
el.previous
desc_el
el = desc_el.css('.productDescriptionWrapper').first
text_els = desc_el.css('.productDescriptionWrapper')
source_els = desc_el.css('h3.productDescriptionSource')
text_els.first.text
source_els.first
source_els.first.next
source_els.first.next.next
html
source_els
source_els.first
source_els.first.next
source_els.first.next.next
source_els.first.next
desc_el = iframe.css('#productDescription > *')
source_els = desc_el.css('h3.productDescriptionSource')
text_els = desc_el.css('.productDescriptionWrapper')
source = source_els.first
source.next
iframe = Nokogiri::HTML(html) { |c| c.noblanks }
desc_el = iframe.css('#productDescription')
source_els = desc_el.css('h3.productDescriptionSource')
text_els = desc_el.css('.productDescriptionWrapper')
source = source_els.first
source.next
source = source_els.first
source.path
desc_el.css(source.path+'/next-sibling')
desc_el.css(source.path + /../div[@class='productDescriptionWrapper']))
desc_el.css(source.path + /../div[@class='productDescriptionWrapper'])
desc_el.css(source.path + "/../div[@class='productDescriptionWrapper']")
desc_el.css(source.path + "../div[@class='productDescriptionWrapper']")
desc_el.css(source.path + "/./../div[@class='productDescriptionWrapper']")
source.methods
source.next.text
source.next.text.blankl?
source.next.text.blank?
source.next.text.blank? ? source.next.next : source.next
within desc_el do
  puts 'hi'
end
within desc_el.path do 
  puts 'hi'
end
desc_el
desc_el.class
desc_el.count
desc_el.first.path
within desc_el.first.path do
  puts 'hi'
end
within :xpath, desc_el.first.path do
  puts 'hi'
end
desc_el
desc_el.first
desc_el = desc_el.first
desc_el.class
desc_el.xpath('./*')
desc_el = desc_el.xpath('./*')
desc_el.text
source_els = desc_el.css('h3.productDescriptionSource')
source = source_els.first
source.next
desc_el = desc_el.children.select(&:element?)
source_els = desc_el.css('h3.productDescriptionSource')
desc_el.first
iframe = Nokogiri::HTML(html) { |c| c.noblanks }
iframe.clas
iframe.class
iframe.path
iframe.css('#productDescription')
desc_el.class
desc_el = iframe.css('#productDescription')
desc_el.class
desc_el.each { |node| desc_el.delete(node) unless node.element? }
desc_el
source_els = desc_el.css('h3.productDescriptionSource')
source = _.first
source.text
iframe.class
iframe
sources = iframe.css('h3.productDescriptionSource')
content = iframe.css('div.productDescriptionWrapper')
contents = iframe.css('div.productDescriptionWrapper')
contents.first.text
contents.first.text.strip
sources.first.text.strip
sourcesmapped = sources.map {|s| s.text.strip}
contentsmapped = contents.map {|s| s.text.strip}
iframe.css('img')
iframe.css('img').map(&:href)
iframe.css('img').first
iframe.css('img').first.value
iframe.css('img').first.src
iframe.xpath('//')
iframe.xpath('/')
iframe.xpath('/img/@src').first
iframe.xpath('/img/@src')
iframe.xpath('//img/@src')
iframe.xpath('//img/@src').count
iframe.xpath('//img/@src').map &:value
jobn
job.nil?
jon.undefined?
exit
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.perform(129, 1, url)
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDetail.last
exit
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDetail.last
exit
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(129, 1, url)
exit
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDetail.last
exit
url = 'http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/'
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDetail.last
url = 'http://www.amazon.com/gp/product/B00SMLK2KW/'
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDescription.last
ProductDetail.last
AmazonProductDetailJob.new.perform(129, 1, url)
exit
url = 'http://www.amazon.com/gp/product/B00SMLK2KW/'
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDetail.last
exit
url = 'http://www.amazon.com/gp/product/B00SMLK2KW/'
AmazonProductDetailJob.new.perform(129, 1, url)
exit
url = 'http://www.amazon.com/gp/product/B00SMLK2KW/'
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDetail.last
exit
url = 'http://www.amazon.com/gp/product/B00SMLK2KW/'
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDetail.last
exit
url = 'http://www.amazon.com/gp/product/B00SMLK2KW/'
AmazonProductDetailJob.new.perform(129, 1, url)
exit
ProductDetail.last
url = 'http://www.amazon.com/gp/product/B00SMLK2KW/'
AmazonProductDetailJob.new.perform(129, 1, url)
url = "http://www.amazon.com/dp/B00C7Z9ZAW/ref=psdc_3743521_t2_B00SMLK2KW"
AmazonProductDetailJob.new.perform(129, 1, url)
html = @session.evaluate_script("window.ProductDescriptionIframeResize.doc.body.innerHTML")
html
@session.screenshot_and_open_image
@session.save_and_open_image
@session.save_screenshot
@session
@session.class
@session.methods
save_and_open_image
exit
@session.save_screenshot('/Users/jonathan', :full => true)
@session.save_screenshot('/Users/jonathan/screenshot.png', :full => true)
exit
ScrapeUrl.where(scrape: 129).first 100
exit
url = http://www.amazon.com/dp/B00W7C7M3M?psc=1
url = "http://www.amazon.com/dp/B00W7C7M3M?psc=1"
exit
url = "http://www.amazon.com/dp/B00W7C7M3M?psc=1"
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDetail.last
exit
url = "http://www.amazon.com/dp/B00W7C7M3M?psc=1"
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDteail.last
ProductDetail.last
exit
url = "http://www.amazon.com/dp/B00W7C7M3M?psc=1"
AmazonProductDetailJob.new.perform(129, 1, url)
exit
url = "http://www.amazon.com/dp/B00W7C7M3M?psc=1"
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDetail.last
url = "http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/"
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDetail.last
url = "http://www.amazon.com/dp/B00C7Z9ZAW/ref=psdc_3743521_t2_B00SMLK2KW"
AmazonProductDetailJob.new.perform(129, 1, url)
url = "http://www.amazon.com/KINGSOLAR-trade-Generation-Motorhome-Campervan/dp/B00R8SMVC2/ref=sr_1_1?ie=UTF8&qid=1456792571&sr=8-1&keywords=sunpower"
AmazonProductDetailJob.new.perform(129, 1, url)
url = 'http://www.amazon.com/KINGSOLAR-trade-Generation-Motorhome-Campervan/dp/B00R8SMVC2/'
AmazonProductDetailJob.new.perform(129, 1, url)
exit
url = 'http://www.amazon.com/KINGSOLAR-trade-Generation-Motorhome-Campervan/dp/B00R8SMVC2/'
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDetail.last
url = "http://www.amazon.com/dp/B00C7Z9ZAW/ref=psdc_3743521_t2_B00SMLK2KW"
AmazonProductDetailJob.new.perform(129, 1, url)
Product Detail
ProductDetail.last
url
url = 'http://www.amazon.com/KINGSOLAR-trade-Generation-Motorhome-Campervan/dp/B00R8SMVC2/'
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDetail.last
url = "http://www.amazon.com/iPad-Air-Keyboard-Case-Satisfaction/dp/B00JJ2PR80/ref=sr_1_1/184-8252132-0405715"
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDetail.last
url = "http://www.amazon.com/iPad-Air-Keyboard-Case-Satisfaction/dp/B00JJ2PR80/ref=sr_1_1/184-8252132-0405715"
AmazonProductDetailJob.new.perform(129, 1, url)
s = Scrape.last
s = Scrape.last(2).first
s = Scrape.last(3).first
s = Scrape.last(4).first
s = Scrape.last(10).first
s = Scrape.last(0).first
s = Scrape.last(9).first
s = Scrape.where(source_id: 3).first
urls = ScrapeUrl.where(scrape_id: s.id)
exit
urls = ScrapeUrl.where(scrape_id: s.id).pluck(:url)
s = Scrape.where(source_id: 3).first
urls = ScrapeUrl.where(scrape_id: s.id).pluck(:url)
urls.size
urls.uniq.size
total = urls.size
uniq = urls.uniq.size
total - uniq
urls.map! {|url| url.gsub(/(ref=).+$/, '') }
urls
total = urls.size
uniq = urls.uniq.size
total - uniq
mazon.com/EVGA-Supporting-1920x1200-Resolutions-100-U3-UV39-KR/dp/B008BUIKD0/",
"http://www.amazon.com/Hauppauge-Gaming-Definition-Capture-Device/dp/B008ZT8QKO/",
"http://www.amazon.com/AVerMedia-Extreme-Tuners-Capture-GC550/dp/B00Y3U01RU/",
"http://www.amazon.com/1byone-OUS00-0563-Antenna-Performance-Coaxial/dp/B00RFLGJLG/",
"http://www.amazon.com/HDMI-Cloner-need-Capture-streaming-videos/dp/B00TF9MCXU/",
"http://www.amazon.com/Antenna-SHAKE-HCD-SHAKE-model-receivers/dp/B015L28BYS/",
"http://www.amazon.com/Hauppauge-Colossus-Express-Internal-HD-PVR/dp/B00WMJMDG6/",
"http://www.amazon.com/AverMedia-ExtremeCap-Capture-Uncompressed-CV710/dp/B00G9QTUVE/",
"http://www.amazon.com/ADS-B-1090MHz-Band-pass-SMA-Filter/dp/B010GBQXK8/",
"http://www.amazon.com/Amplified-HDTV-Antenna-Reception-Range/dp/B010VDH4Y0/",
"http://www.amazon.com/Hauppauge-1540-Rocket-1080p-Recorder/dp/B00GEBVEI6/",
"http://www.amazon.com/Authentic-NA-771-15-6-Inch-SMA-Female-BaoFeng/dp/B00KC4PWQQ/",
total - uniq
url = "http://www.amazon.com/dp/B00Y3U01RU"
AmazonProductDetailJob.new.perform(129, 1, url)
url = "http://www.amazon.com/Elgato-Systems-Capture-Definition-Recorder/dp/B00840353W/ref=sr_1_1?s=pc&ie=UTF8&qid=1456795812&sr=1-1"
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDetail.last
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDetail.last
exit
url = "http://www.amazon.com/Philips-433227-Equivalent-SlimStyle-Dimmable/dp/B00I134ORI/ref=sr_1_2?s=hi&ie=UTF8&qid=1456796035&sr=1-2"
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDetail.last
exit
url = http://www.amazon.com/Elgato-Systems-Capture-Definition-Recorder/dp/B00840353W/ref=sr_1_1?s=pc&ie=UTF8&qid=1456795812&sr=1-1
url = "http://www.amazon.com/Elgato-Systems-Capture-Definition-Recorder/dp/B00840353W/ref=sr_1_1?s=pc&ie=UTF8&qid=1456795812&sr=1-1"
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDetail.last
exit
url = "http://www.amazon.com/Elgato-Systems-Capture-Definition-Recorder/dp/B00840353W/ref=sr_1_1?s=pc&ie=UTF8&qid=1456795812&sr=1-1"
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDetail.last
exit
url = "http://www.amazon.com/Elgato-Systems-Capture-Definition-Recorder/dp/B00840353W/ref=sr_1_1?s=pc&ie=UTF8&qid=1456795812&sr=1-1"
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDetail.last
exit
DateTime.now.milliseconds
Time.now
Time.now.milliseconds
Time.now.to_i
exit
url = "http://www.amazon.com/Elgato-Systems-Capture-Definition-Recorder/dp/B00840353W/ref=sr_1_1?s=pc&ie=UTF8&qid=1456795812&sr=1-1"
AmazonProductDetailJob.new.perform(129, 1, url)
exit
url = "http://www.amazon.com/Elgato-Systems-Capture-Definition-Recorder/dp/B00840353W/ref=sr_1_1?s=pc&ie=UTF8&qid=1456795812&sr=1-1"
AmazonProductDetailJob.new.perform(129, 1, url)
ProductScreenshot.last
ProductScreenshot.last.attachment.url
exit
url = "http://www.amazon.com/Elgato-Systems-Capture-Definition-Recorder/dp/B00840353W/ref=sr_1_1?s=pc&ie=UTF8&qid=1456795812&sr=1-1"
url = "http://www.amazon.com/Elgato-Systems-Capture-Definition-Recorder/dp/B00840353W/"
AmazonProductDetailJob.new.perform(129, 1, url)
exit
url = "http://www.amazon.com/Elgato-Systems-Capture-Definition-Recorder/dp/B00840353W/"
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDetail.last
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDetail.last
ProductScreenshot.last
ProductScreenshot.last.attachment.url
ProductDetail.last
exit
url = "http://www.amazon.com/Elgato-Systems-Capture-Definition-Recorder/dp/B00840353W/"
AmazonProductDetailJob.new.perform(129, 1, url)
exit
url = "http://www.amazon.com/Elgato-Systems-Capture-Definition-Recorder/dp/B00840353W/"
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDetail.last
ProductDeail.last
ProductDetail.last
{}.blank?
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDetail.last
exit
url = "http://www.amazon.com/Elgato-Systems-Capture-Definition-Recorder/dp/B00840353W/"
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDetail.last
exit
url = "http://www.amazon.com/Elgato-Systems-Capture-Definition-Recorder/dp/B00840353W/"
AmazonProductDetailJob.new.perform(129, 1, url)
exit
url = "http://www.amazon.com/Elgato-Systems-Capture-Definition-Recorder/dp/B00840353W/"
AmazonProductDetailJob.new.perform(129, 1, url)
exit
AmazonProductDetailJob.new.perform(129, 1, url)
url = "http://www.amazon.com/Elgato-Systems-Capture-Definition-Recorder/dp/B00840353W/"
AmazonProductDetailJob.new.perform(129, 1, url)
url
exit
url = "http://www.amazon.com/Elgato-Systems-Capture-Definition-Recorder/dp/B00840353W/"
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDetail.last
exit
url = "http://www.amazon.com/Elgato-Systems-Capture-Definition-Recorder/dp/B00840353W/"
AmazonProductDetailJob.new.perform(129, 1, url)
exit
url = "http://www.amazon.com/Elgato-Systems-Capture-Definition-Recorder/dp/B00840353W/"
AmazonProductDetailJob.new.perform(129, 1, url)
exit
AmazonProductDetailJob.new.perform(129, 1, url)
url = "http://www.amazon.com/Elgato-Systems-Capture-Definition-Recorder/dp/B00840353W/"
AmazonProductDetailJob.new.perform(129, 1, url)
exit
url = "http://www.amazon.com/Elgato-Systems-Capture-Definition-Recorder/dp/B00840353W/"
AmazonProductDetailJob.new.perform(129, 1, url)
exit
url = "http://www.amazon.com/Elgato-Systems-Capture-Definition-Recorder/dp/B00840353W/"
AmazonProductDetailJob.new.perform(129, 1, url)
exit
url = "http://www.amazon.com/Elgato-Systems-Capture-Definition-Recorder/dp/B00840353W/"
AmazonProductDetailJob.new.perform(129, 1, url)
url = "http://www.amazon.com/dp/B00C7Z9ZAW/ref=psdc_3743521_t2_B00SMLK2KW"
AmazonProductDetailJob.new.perform(129, 1, url)
exit
url = "http://www.amazon.com/Elgato-Systems-Capture-Definition-Recorder/dp/B00840353W"
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDetail.last
Product.last
Product.last.original_url
ScrapeProductSeller.last
op = OtherProduct
op.detail
op = OtherProduct.last
op.detail
url = "http://www.amazon.com/Elgato-Systems-Capture-Definition-Recorder/dp/B00840353W"
AmazonProductDetailJob.new.perform(129, 1, url)
exit
url = "http://www.amazon.com/Elgato-Systems-Capture-Definition-Recorder/dp/B00840353W"
AmazonProductDetailJob.new.perform(129, 1, url)
exit
url = "http://www.amazon.com/Elgato-Systems-Capture-Definition-Recorder/dp/B00840353W"
AmazonProductDetailJob.new.perform(129, 1, url)
desc_hash
detail.data_hash
n
s
step
setp
step
detail
detail.save
detail
step
continue
ProductDetail.last
ProductDetail.last 2
exit
url = "http://www.amazon.com/Elgato-Systems-Capture-Definition-Recorder/dp/B00840353W"
AmazonProductDetailJob.new.perform(129, 1, url)
exit
url = "http://www.amazon.com/Elgato-Systems-Capture-Definition-Recorder/dp/B00840353W"
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDetail.last
exit
url = "http://www.amazon.com/Elgato-Systems-Capture-Definition-Recorder/dp/B00840353W"
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDetail.last
exit
url = "http://www.amazon.com/Elgato-Systems-Capture-Definition-Recorder/dp/B00840353W"
AmazonProductDetailJob.new.perform(129, 1, url)
exit
url = "http://www.amazon.com/Elgato-Systems-Capture-Definition-Recorder/dp/B00840353W"
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDetail
ProductDetail.last
exit
url = "http://www.amazon.com/Elgato-Systems-Capture-Definition-Recorder/dp/B00840353W"
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDetail.last
url = "http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/"
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDetail.last
exit
url = "http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/"
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDetail.last
exit
url = "http://www.amazon.com/ARRIS-SURFboard-SBG6580-Docsis-Router/dp/B0040IUI46/"
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDetail.last
exit
url = "http://www.amazon.com/dp/B00C7Z9ZAW/ref=psdc_3743521_t2_B00SMLK2KW"
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDetail.last
url = "http://www.amazon.com/Philips-433227-Equivalent-SlimStyle-Dimmable/dp/B00I134ORI/ref=sr_1_2?s=hi&ie=UTF8&qid=1456796035&sr=1-2"
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDetail.last
exit
OtherProductDetail.last
opd = OtherProductDetail.last
opd.attributes
def do_it(*args)
exit
OtherProductDetail.last
opd = _
opd
opd.product_description = {"Product Description": 'Blah Blah Blah'}
opd.save
opd.reload
opd.attributes
exit
OtherProductDetail.last
opd = _
opd
opd.attributes
exit
url = "http://www.amazon.com/dp/B00C7Z9ZAW/ref=psdc_3743521_t2_B00SMLK2KW"
AmazonProductDetailJob.new.perform(129, 1, url)
exit
url = "http://www.amazon.com/dp/B00C7Z9ZAW/ref=psdc_3743521_t2_B00SMLK2KW"
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDetail.last
exit
url = "http://www.amazon.com/dp/B00C7Z9ZAW/ref=psdc_3743521_t2_B00SMLK2KW"
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDetail.last
exit
url = "http://www.amazon.com/dp/B00C7Z9ZAW/ref=psdc_3743521_t2_B00SMLK2KW"
AmazonProductDetailJob.new.perform(129, 1, url)
ProductDetail.last
exit
img = Asset.new(original_url: ' http://images.amazon.com/images/G/01/electronics/detail-page/t3.jpg'
img = Asset.new(original_url: ' http://images.amazon.com/images/G/01/electronics/detail-page/t3.jpg')
img.valid?
img.errors.full_messages
img.assetable_id = Product.last.id
img.valid?
img.errors.full_messages
img.assetable = Product.last
img.errors.full_messages
img.valid?
img.errors.full_messages
img
img.save
exit
img = Asset.new(original_url: ' http://images.amazon.com/images/G/01/electronics/detail-page/t3.jpg', assetable: Product.last)
img.valid?
img.save
img
name = 'iPad Air & iPad Air 2 Keyboard Case - Alpatronix KX130 iPad Air Keyboard Case with Bluetooth Removable Folio Wireless ABS Keyboard, Detachable Vegan Leather Case & Tablet Stand. 100% Satisfaction Guarantee for Apple iPad Air Keyboard Case - (Black)'
name.include?('Alpatronix')
0.2 * 500000
exit
op = OtherProduct.last
op.detail
op.as_indexed_json
p = Product.last
p.detail
p.as_indexed_json
exit
op = OtherProduct.last
op.as_indexed_json
s = Scrape.last
Scrape.last 10
s = Scrape.find 123
scrape_id = 123
OtherProduct.includes(:scrapes).where('scrapes.id': scrape_id).find_in_batches do |other_products|
  OtherProduct.bulk_index(other_products, {})
end
exit
Identfier.find_by(uniq_id: 'B00ZOI31K4'_)
Identfier.find_by(uniq_id: 'B00ZOI31K4')
Identifier.find_by(uniq_id: 'B00ZOI31K4')
id = _
id.other_products
op = _.first
op.detail
''.present?
Sidekiq::Queues
Sidekiq::Queue
Sidekiq::Queue.all
Sidekiq::Queue.each {|q| q.clear}
Sidekiq::Queue.all.each {|q| q.clear}
Sidekiq::Queue.all.first
Sidekiq::Queue.all
Sidekiq::Queue.new('product_detail_queue').clear
exit
s = Scrape.find 123
OtherProduct.includes(:scrapes).where('scrapes.id': s.id).find_in_batches do |other_products|
  OtherProduct.bulk_index(other_products, {})
end
s = Scrape.last
s
def stats(s)
  p = s.products.size
  op = s.other_products.size
  total = p + op
  time = (s.duration || Time.now - s.created_at).to_f / 60
  rate = total / time
  puts "id: #{s.id} Source: #{s.source_id} Ratio: #{p}/#{op} Total: #{total} Time: #{time.round(2)}mins Rate: #{rate.round(2)} created: #{s.created_at.strftime("%m/%d/%Y")}"
  # puts "id: #{s.id} Source: #{s.source_id} Total: #{total} Rate: #{rate.round(2)} created: #{s.created_at.strftime("%m/%d/%Y")}"
end
stats s
stats s.reload
stats s
Sidekiq::Queue.new('product_detail_queue').clear
Sidekiq::Queue.new('product_detail_queue').clearexit
exit
Sidekiq::Queue.new("product_detail_queue").clear
stats 
stats s
s = Scrape.last
s.stats
stats s
def stats(s)
  p = s.products.size
  op = s.other_products.size
  total = p + op
  time = (s.duration || Time.now - s.created_at).to_f / 60
  rate = total / time
  puts "id: #{s.id} Source: #{s.source_id} Ratio: #{p}/#{op} Total: #{total} Time: #{time.round(2)}mins Rate: #{rate.round(2)} created: #{s.created_at.strftime("%m/%d/%Y")}"
  # puts "id: #{s.id} Source: #{s.source_id} Total: #{total} Rate: #{rate.round(2)} created: #{s.created_at.strftime("%m/%d/%Y")}"
end
stats s
ScrapeProductSellers.where(scrape_id: s.id)
ScrapeProductSeller.where(scrape_id: s.id)
urls = ScrapeProductSeller.where(scrape_id: s.id).map &:original_url
urls
urls.size
urls.uniq.size
s.products.includes(:scrape_product_sellers)
products = _
products.map {|p| p.scrape_product_sellers.last.original_url }
s
urls = products.map {|p| p.scrape_product_sellers.last.original_url }
urls.size
urls.uniq
ScrapeProductSeller
products.map {|p| p.scrape_product_sellers.map(&:seller_url) }
products.flat_map {|p| p.scrape_product_sellers.map(&:seller_url) }
products.flat_map {|p| p.scrape_product_sellers.map(&:seller_url) }.compact
urls.size
s
s.reload
OtherProduct.includes(:scrapes).where('scrapes.id': s.id).find_in_batches do |other_products|
  OtherProduct.bulk_index(other_products, {})
end
Product.includes(:scrapes).where('scrapes.id': s.id).find_in_batches do |other_products|
  Product.bulk_index(other_products, {})
end
s = Scrape.last
stats s
s.other_products.map &:url
stats s
Product.search('HDMI')
Product.search('HDMI').results
Product.search('HDMI').records
Product.search "LED"
Product.search("LED").count
Product.search("LED").response
Product.search("HDMI").response
q
Product.search("local dimming").response
xtqqqq
exit
Product.search("local dimming").response
p = Product.find 9934
p.detail
OtherProduct.last.detail
p.detail
Product.last.detail
hash[:General] ||= {}
hash
hash = {}
exit
OtherProduct.last
op = )
op = OtherProduct.last
op.detail
op.as_indexed_json
op.detail.data_hash
Product.first.detail
p = Product.find 9934
p.as_indexed_json
p.detail
exit
op = OtherProduct.last
op.as_indexed_json
op.detail
op.detail[:"Feature Bullets"]
op.detail.data_hash[:"Feature Bullets"]
op.detail.data_hash[:"Feature Bullets"].to_s
op.detail.data_hash.fatten
op.detail.data_hash.class
op.detail.data_hash
op.detail.data_hash.class
op.detail.data_hash.map do |k, v|
  if v.is_a? Hash
    v
  elsif v.is_a? Array
    v.join(' ')
  else
    v
  end
end
mapped = _
injected = mapped.inject({}) {|a, e| a.merge(e || {}) }
op.detail.data_hash.map do |k, v|
  if v.is_a? Hash
    v
    if v.is_a? Array
op.detail.data_hash.map do |k, v|
  case v.class
  when Hash
    v
  when
op.detail.data_hash.map do |k, v|
  case v.class
  when Hash
    v
  when Array
    {k => v.join(', ')}
  else
    {k => v}
  end
end
mapped = )
exit
p = Product.last
p.detail
p.as_indexed_json
'asdfasdf'.join(',')
exit
p = Product.last
p.as_indexed_json
p.detail
p.as_indexed_json
exit
p = Product.last
p.as_indexed_json
s = Scrape.last
s.reload
stats s
exit
s = Scrape.last 2
s = Scrape.last 3
s = _.first
s.duration / 60
s.duration / 60 / 60
s = Scrape.find 4
ScrapeUrl
exit
s = Scrape.last
p = products.includes(:detail).where.not('product_details.from_mfg': nil).sample
products = s.products
products.size
s = Scrape.last 4
s = Scrape.find 138
s = Scrape.find 139
s = Scrape.find 138
products = s.products
p = products.includes(:detail).where.not('product_details.from_mfg': nil).sample
p.id
p
p.detail
p
p.detail
exit
s = Scrape.find 138
prods = s.products
details = s.products.includes(:detail).map(&:detail)
details
ids = details.map(&:id)
def perform(scrape_id, detail_id)
  product_detail = ProductDetail.find(detail_id)
  data_hash = product_detail.data_hash
  product_desc = product_detail.product_description
  from_mfg = product_detail.from_mfg
  # return unless data_hash
  [data_hash, product_desc, from_mfg].each do |attr|
    next if attr.nil?
    process_attribute(attr, product_detail)
  end
  product_detail.update_attribute :processed, true
end
def process_attribute(attr, product_detail)
  return unless attr.is_a? Hash
  attr.each do |key, val|
    unless val.is_a? Hash
      create_product_attribute(product_detail, key, val)
      next
    end
    val.each do |name, value|
      create_product_attribute(product_detail, name, value, key)
    end
  end
end
def create_product_attribute(detail, name, value, category = nil)
  key = ProductAttributeKey.transaction do
    ProductAttributeKey.find_or_create_by name: name, category: category
  end
  ProductAttributeValue
  .find_or_initialize_by( \
    attribut        attribut        attribut        attribut il        attribut        attribut       lu        
edit -t
ids
s.id
ids.each {|id| perform(s.id, id) }
p
p = 63207
p = Product.find 63207
p.detail
exit
xit
exit
Product.write_to_excel(Product.dump_all_product_info(Scrape.last.products.ids, Scrape.last.id))
s = Scrape.last
Product.write_to_excel(Product.dump_all_product_info(Scrape.last.products.ids, Scrape.last.id))
exit
Product.write_to_excel(Product.dump_all_product_info(Scrape.last.products.ids, Scrape.last.id))
reload
exit
Product.write_to_excel(Product.dump_all_product_info(Scrape.last.products.ids, Scrape.last.id))
exit
Product.write_to_excel(Product.dump_all_product_info(Scrape.last.products.ids, Scrape.last.id))
exit
s.last
s = Scrape.last
s = Scrape.where(source_id: 3).last
exit
s = Scrape.where(source_id: 3).last
exit
s = Scrape.where(source_id: 3).last
def stats(s)
  p = s.products.size
  op = s.other_products.size
  total = p + op
  time = (s.duration || Time.now - s.created_at).to_f / 60
  rate = total / time
  full_scrape = (500000 / rate / 60 / 24)
  puts "id: #{s.id} Source: #{s.source_id} Ratio: #{p}/#{op} Total: #{total} Time: #{time.round(2)}mins Rate: #{rate.round(2)} created: #{s.created_at.strftime("%m/%d/%Y")}"
  puts "#{full_scrape.round(2)} days for full scrape"
  # puts "id: #{s.id} Source: #{s.source_id} Total: #{total} Rate: #{rate.round(2)} created: #{s.created_at.strftime("%m/%d/%Y")}"
end
stats s
s
s.products.size
s.other_products.size
s
Product.write_to_excel(Product.dump_all_product_info(s.prouducts.ids, s.id))
Product.write_to_excel(Product.dump_all_product_info(s.products.ids, s.id))
exit
p = Scrape.last.products.sample
exit
p = Scrape.last.products.sample
p.detail
edit -t
hash
exit
s = Scrape.where(source_id: 3).last
Category.all
exit
Category.all
s
s = Scrape.where(source_id: 3).last
s.products.includes(:categories).where('categories.id': [109, 110, 111, 112]).size
exit
s = Scrape.where(source_id: 3).last
Product.write_to_excel(Product.dump_all_product_info(s.products.ids, s.id))
exit
s = Scrape.where(source_id: 3).last
Product.write_to_excel(Product.dump_all_product_info(s.products.ids, s.id))
exit
s = Scrape.where(source_id: 3).last
Product.write_to_excel(Product.dump_all_product_info(s.products.ids, s.id))
exit
s = Scrape.where(source_id: 3).last
Product.write_to_excel(Product.dump_all_product_info(s.products.ids, s.id))
exit
s = Scrape.where(source_id: 3).last
Product.write_to_excel(Product.dump_all_product_info(s.products.ids, s.id))
exit
s = Scrape.where(source_id: 3).last
Product.dump_all_product_info(s.products.ids, s.id)
hsqq:helpOBOOOBOBOOAOAOOAOB
as
exit
s = Scrape.where(source_id: 3).last
hash = Product.dump_all_product_info(s.products.ids, s.id)
hash.select {|el| el[:category_name] == "Solar Panels"}
hash.select {|el| el[:category_name] == "Hardware"}
h1 = hash.select {|el| el[:category_name] == "Hardware"}.first
h2 = hash.first
[h1, h2]
exit
s = Scrape.where(source_id: 3).last
Product.dump_all_product_info(s.products.ids.first(100), s.id)
Product.write_to_excel(Product.dump_all_product_info(s.products.ids.first(100), s.id))
exit
s = Scrape.where(source_id: 3).last
Product.write_to_excel(Product.dump_all_product_info(s.products.ids.first(100), s.id))
ProductDetail.last
ProductDetail.where.not(product_description: nil).last
ProductDetail.where.not(from_mfg: nil).last
exit
s = Scrape.where(source_id: 3).last
Product.write_to_excel(Product.dump_all_product_info(s.products.ids.first(100), s.id))
exit
s = Scrape.where(source_id: 3).last
Product.write_to_excel(Product.dump_all_product_info(s.products.ids.first(100), s.id))
false.to_s
exit
s = Scrape.where(source_id: 3).last
Product.write_to_excel(Product.dump_all_product_info(s.products.ids, s.id))
exit
p = Product.find ]
p = Product.find 822
p.detail
p = Product.includes(:detail).where.not('details.product_description':  nil).first
p = Product.includes(:detail).where.not('product_details.product_description':  nil).first
p.detail
p.id
Product.search('sunpower').response
Product.search('sunpower').results
p = Product.includes(:detail).where.not('product_details.from_mfg':  nil).first
p = Product.includes(:detail).where.not('product_details.from_mfg':  nil).last
p.id
exit
s = Scrape.where(source_id: 1).last
def stats(s)
  p = s.products.size
  op = s.other_products.size
  total = p + op
  time = (s.duration || Time.now - s.created_at).to_f / 60
  rate = total / time
  full_scrape = (500000 / rate / 60 / 24)
  puts "Scrape id: #{s.id} Source: #{s.source_id}"
  puts "Ratio: #{p}/#{op} Total: #{total}"
  puts "#{((p.to_f / total) * 100).round(2)} percent Products"
  puts "Time: #{time.round(2)}mins Rate: #{rate.round(2)} created: #{s.created_at.strftime("%m/%d/%Y")}"
  puts "#{full_scrape.round(2)} days for full scrape"
  puts "id: #{s.id} Source: #{s.source_id} Total: #{total} Rate: #{rate.round(2)} created: #{s.created_at.strftime("%m/%d/%Y")}"
end
stats s
Product.write_to_excel(Product.dump_all_product_info(s.products.ids, s.id))
s.products.where(product_description: nil).size
s.products.where.not(product_description: nil).size
s
s.products.first
p = )
p = s.products.first
exit
s = Scrape.where(source_id: 1).last
def stats(s)
  p = s.products.size
  op = s.other_products.size
  total = p + op
  time = (s.duration || Time.now - s.created_at).to_f / 60
  rate = total / time
  full_scrape = (500000 / rate / 60 / 24)
  puts "Scrape id: #{s.id} Source: #{s.source_id}"
  puts "Ratio: #{p}/#{op} Total: #{total}"
  puts "#{((p.to_f / total) * 100).round(2)} percent Products"
  puts "Time: #{time.round(2)}mins Rate: #{rate.round(2)} created: #{s.created_at.strftime("%m/%d/%Y")}"
  puts "#{full_scrape.round(2)} days for full scrape"
  puts "id: #{s.id} Source: #{s.source_id} Total: #{total} Rate: #{rate.round(2)} created: #{s.created_at.strftime("%m/%d/%Y")}"
end
stats s
id = Identifier.gtin.find_by(uniq_id: '05060068483004')
exit
s = Scrape.where(source_id: 1).last
id = Identifier.gtin.find_by(uniq_id: '05060068483004')
OtherProduct.find_by(model: 'EKG-88S')
Product.find_by(model: 'EKG-88S')
Product.has_models('EKG-88S')
exit
Product.has_models('EKG-88S')
exit
Product.has_models('EKG-88S')
AmazonProductDetailJob.new.perform(scrape.id, 1, p.original_url)
url = 'https://www.google.com/shopping/product/15449194099032446207'
s = Scrape.where(source_id: 1)
s = Scrape.where(source_id: 1).last
p = Product.find 3
p.source_by_scrape_time
p.source_scrapes(3)
exit
Product.where(brand: 'R')
Product.where(model: 'SG-125SC')
_.last.original_url
exit
Product.where(model: 'SG-125SC').destroy_all
s = Scrape.where(source_id: 3).last
url = 'http://www.amazon.com/GTSUN-50Pcs-125x125MM-Multiple-Monocrystalline/dp/B00C9S0D7Q/'
AmazonProductDetailJob.new.perform(s.id, 1, url)
Category.where(name: 'Hardware')
AmazonProductDetailJob.new.perform(s.id, 110, url)
p = Product.last
p.detail
OtherProduct.last
url = 'http://www.amazon.com/Lifetime-Warranty-Protector-InaRock-Tempered/dp/B00VHSY99W'
AmazonProductDetailJob.new.perform(s.id, 1, url)
Product.last
p = _
p = Product.last
p = OtherProduct.last
exit
url = 'http://www.amazon.com/RISING-50Pcs-125x125MM-Solar-Monocrystalline/dp/B0111WYM1S/'
s = Scrape.where(source_id: 3)
s = Scrape.where(source_id: 3).last
AmazonProductDetailJob.new.perform(s.id, 1, url)
p = Product.last
op = OtherProduct.last
exit
s = Scrape.where(source_id: 3).last
url = 'http://www.amazon.com/RISING-50Pcs-125x125MM-Solar-Monocrystalline/dp/B0111WYM1S/'
AmazonProductDetailJob.new.perform(s.id, 1, url)
op = OtherProduct.last
op = OtherProduct.last(20
op = OtherProduct.last(2)
op.destroy
op = OtherProduct.last.destroy
exit
url = 'http://www.amazon.com/RISING-50Pcs-125x125MM-Solar-Monocrystalline/dp/B0111WYM1S/'
s = Scrape.where(source_id: 3).last
AmazonProductDetailJob.new.perform(s.id, 1, url)
exit
url = 'http://www.amazon.com/RISING-50Pcs-125x125MM-Solar-Monocrystalline/dp/B0111WYM1S/'
s = Scrape.where(source_id: 3).last
AmazonProductDetailJob.new.perform(s.id, 1, url)
exit
url = 'http://www.amazon.com/RISING-50Pcs-125x125MM-Solar-Monocrystalline/dp/B0111WYM1S/'
s = Scrape.where(source_id: 3).last
AmazonProductDetailJob.new.perform(s.id, 1, url)
exit
url = 'http://www.amazon.com/RISING-50Pcs-125x125MM-Solar-Monocrystalline/dp/B0111WYM1S/'
s = Scrape.where(source_id: 3).last
AmazonProductDetailJob.new.perform(s.id, 1, url)
exit
url = 'http://www.amazon.com/RISING-50Pcs-125x125MM-Solar-Monocrystalline/dp/B0111WYM1S/'
s = Scrape.where(source_id: 3).last
AmazonProductDetailJob.new.perform(s.id, 1, url)
exit
url = 'http://www.amazon.com/RISING-50Pcs-125x125MM-Solar-Monocrystalline/dp/B0111WYM1S/'
s = Scrape.where(source_id: 3).last
AmazonProductDetailJob.new.perform(s.id, 1, url)
name = RISING 50Pcs 125x125MM 5x5 Mono Solar Cells High Power For DIY Solar Panel 2.8W/Pcs Monocrystalline Solar Cell Good
name = 'RISING 50Pcs 125x125MM 5x5 Mono Solar Cells High Power For DIY Solar Panel 2.8W/Pcs Monocrystalline Solar Cell Good'
brand = 'Rising'
def get_brand(brand_string, product_name)
  return if [brand_string, product_name].any?(&:blank?)
  substr = substring_match([brand_string, product_name])
  brand_string if (substr && brand_string.include?(substr))
rescue Capybara::ElementNotFound
  return nil
end
brand
name
get_brand(brand, name)
def substring_match(list)
  return if list.empty? || list.any?(&:blank?)
  character_matches = []
  list.map(&:downcase).min.split('').each_with_index do |char, index|
    break unless list.map { |m| m[index] }.all? { |m| m == char }
    character_matches.push(char)
  end
  character_matches.join.strip if character_matches.any?
end
get_brand(brand, name)
brand
name
def substring_match(list)
  return if list.empty? || list.any?(&:blank?)
  character_matches = []
  list.map(&:downcase).max.split('').each_with_index do |char, index|
    break unless list.map { |m| m[index] }.all? { |m| m == char }
    character_matches.push(char)
  end
  character_matches.join.strip if character_matches.any?
end
get_brand(brand, name)
brand
name
def get_brand(product_name, brand_string)
  return if [brand_string, product_name].any?(&:blank?)
  product_name.downcase!
  brand = brand_string.downcase
  substr = substring_match([brand, product_name])
  if product_name.starts_with?(brand) || (substr && brand.include?(substr))
    brand_string
  end
end
get_brand(brand, name)
def get_brand(brand_string, product_name)
  return if [brand_string, product_name].any?(&:blank?)
  product_name.downcase!
  brand = brand_string.downcase
  substr = substring_match([brand, product_name])
  if product_name.starts_with?(brand) || (substr && brand.include?(substr))
    brand_string
  end
end
get_brand(brand, name)
brand
get_brand(brand, '')
brand_string)
Rising
********************************************************************************
product_name
RISING 50Pcs 125x125MM 5x5 Mono Solar Cells High Power For DIY Solar Panel 2.8W/Pcs Monocrystalline Solar Cell Good
brand = 'Rising'
name = 'RISING 50Pcs 125x125MM 5x5 Mono Solar Cells High Power For DIY Solar Panel 2.8W/Pcs Monocrystalline Solar Cell Good'
get_brand(brand, name)
name = 'RISING MFG 50Pcs 125x125MM 5x5 Mono Solar Cells High Power For DIY Solar Panel 2.8W/Pcs Monocrystalline Solar Cell Good'
get_brand(brand, name)
brand = 'Rising MFG'
get_brand(brand, name)
name = 'RISING 50Pcs 125x125MM 5x5 Mono Solar Cells High Power For DIY Solar Panel 2.8W/Pcs Monocrystalline Solar Cell Good'
get_brand(brand, name)
exit
url = 'http://www.amazon.com/RISING-50Pcs-125x125MM-Solar-Monocrystalline/dp/B0111WYM1S/'
s = Scrape.where(source_id: 3).last
AmazonProductDetailJob.new.perform(s.id, 1, url)hafsd
exit
url = 'http://www.amazon.com/RISING-50Pcs-125x125MM-Solar-Monocrystalline/dp/B0111WYM1S/'
s = Scrape.where(source_id: 3).last
AmazonProductDetailJob.new.perform(s.id, 1, url)hafsd
AmazonProductDetailJob.new.perform(s.id, 1, url)
p = Product.last
get_brand(brand, '')
brand = 'Rising MFG'
get_brand(brand, '')
get_brand(brand, brand + ' something')
def get_brand(brand_string, product_name)
  return if [brand_string, product_name].any?(&:blank?)
  brand, name = brand_string.downcase, product_name.downcase
  substr = substring_match([brand, name])
  if name.starts_with?(brand) || (substr && brand.include?(substr))
    brand_string
  end
end
get_brand(brand, brand + ' something')
def substring_match(list)
  return if list.empty? || list.any?(&:blank?)
  character_matches = []
  list.map(&:downcase).max.split('').each_with_index do |char, index|
    break unless list.map { |m| m[index] }.all? { |m| m == char }
    character_matches.push(char)
  end
  character_matches.join.strip if character_matches.any?
end
get_brand(brand, brand + ' something')
brand
name = brand + ' something
name = brand + ' something'
name
brand
get_brand(brand, name)
substring_match(['abc123', 'abc12'])
substring_match(['abc123', 'abc12', 'abc120'])
exit
AmazonProductDetailJob.new.perform(s.id, 1, url)hafsd
s = Scrape.where(source_id: 3)
s = Scrape.where(source_id: 3).last
url = 'http://www.amazon.com/LB1-High-Performance-Efficient-Monocrystalline/dp/B01AKWS8PQ/'
AmazonProductDetailJob.new.perform(s.id, 1, url)hafsd
AmazonProductDetailJob.new.perform(s.id, 1, url)
exit
AmazonProductDetailJob.new.perform(s.id, 1, url)
s = Scrape.where(source_id: 3).last
url = 'http://www.amazon.com/LB1-High-Performance-Efficient-Monocrystalline/dp/B01AKWS8PQ/'
AmazonProductDetailJob.new.perform(s.id, 1, url)
p = Product.last
op = OtherProduct.last
p = ProductDetail.find(269421).product
p.identifiers
exit
s = Scrape.where(source_id: 1)
s = Scrape.where(source_id: 1).last
url = 'https://www.google.com/shopping/product/1185020369203678244'
GoogleProductDetailJob.new.perform(s.id, 94, url)
p = Product.find 279
exit
OtherProducts.all.each { |op| puts op.name }
OtherProduct.all.each { |op| puts op.name }
exit
Sidekiq::Queues.all
Sidekiq::Queue.all
s = Scrape.last
def stats(*scrapes)
  scrapes.each do |s|
    s.reload
    p = s.products.size
    op = s.other_products.size
    total = p + op
    time = (s.duration || Time.now - s.created_at).to_f / 60
    rate = total / time
    full_scrape = (500000 / rate / 60 / 24)
    puts "Scrape id: #{s.id} Source: #{s.source_id}"
    puts "Ratio: #{p}/#{op} Total: #{total}"
    puts "#{((p.to_f / total) * 100).round(2)} percent Products"
    puts "Time: #{time.round(2)}mins Rate: #{rate.round(2)} created: #{s.created_at.strftime("%m/%d/%Y")}"
    puts "#{full_scrape.round(2)} days for full scrape"
    puts "id: #{s.id} Source: #{s.source_id} Total: #{total} Rate: #{rate.round(2)} created: #{s.created_at.strftime("%m/%d/%Y")}"
    puts '=============================='
  end
  nil
end
stats s
s.products.includes(:detail).where('product_details.product_description': nil)
s.products.includes(:detail).where('product_details.product_description': nil).map { |p| p.detail.product_description }
s.products.includes(:detail).where.not('product_details.product_description': nil).map { |p| p.detail.product_description }
s.products.includes(:detail).where.not('product_details.product_description': nil).map { |p| p.detail.created_at }
s.products.includes(:detail).where.not('product_details.product_description': nil).map { |p| p.detail.updated_at }
exit
s = Scrape.last
ProductDetail.where(scrape_id: s.id)
ProductDetail.where(scrape_id: s.id).size
ProductDetail.where(scrape_id: s.id).where.not(product_description: nil).size
ProductDetail.where(scrape_id: s.id).where(product_description: nil).size
redis = Redis::Namespace.new('low_priority', redis: Redis.new(url: ENV['REDIS_URL']))
redis.namespace
redis.
p redis
puts redis
redis.methods - 9.methods
redis.redis
redis.client
redis.methods - 9.methods
redis.keys
redis.multi
redis.pipelined
redis.type
s = Scrape.last
stats
def stats(*scrapes)
  scrapes.each do |s|
    s.reload
    p = s.products.size
    op = s.other_products.size
    total = p + op
    time = (s.duration || Time.now - s.created_at).to_f / 60
    rate = total / time
    full_scrape = (500000 / rate / 60 / 24)
    puts "Scrape id: #{s.id} Source: #{s.source_id}"
    puts "Ratio: #{p}/#{op} Total: #{total}"
    puts "#{((p.to_f / total) * 100).round(2)} percent Products"
    puts "Time: #{time.round(2)}mins Rate: #{rate.round(2)} created: #{s.created_at.strftime("%m/%d/%Y")}"
    puts "#{full_scrape.round(2)} days for full scrape"
    puts "id: #{s.id} Source: #{s.source_id} Total: #{total} Rate: #{rate.round(2)} created: #{s.created_at.strftime("%m/%d/%Y")}"
    puts '=============================='
  end
  nil
end
stats s
s
s.reload
s = Scrape.last
stats s
s.reload
s.running = false
s = Scrape.where(running: true)
s = Scrape.last
s.products.sample
s.detail
p = s.products.sample
p.detail
p.original_url
url = 'http://www.amazon.com/Elgato-Systems-Capture-Definition-Recorder/dp/B00840353W'
cid = 1
Sidekiq::Queue.new('product_detail_queue').clear
Scrape.last
!!nil
!!1
total_enqueued_default_jobs = Sidekiq::Shards.map { Sidekiq::Queue.new.size }.inject(:+)
total_enqueued_default_jobs
Sidekiq::Shards.all
Sidekiq::Shards.any?
total_enqueued_default_jobs = Sidekiq::Shards.map { Sidekiq::Queue.new.size }
exit
Sidekiq::Shards.shards
Sidekiq::Shards.shards.first
Sidekiq::Shards.shards.first.warning
bs = Sidekiq::BatchSet.new
bs.each {|s| p s.bid }
bs.each {|s| puts s.bid }
url
scrape_di, cat_id, url = 143, 11, "http://www.amazon.com/Elgato-Systems-Capture-Definition-Recorder/dp/B00840353W/"
url
AmazonProductDetailJob.new.perform(scrape_id, cat_id, url)
s_id, c_id, url = 143, 11, "http://www.amazon.com/Elgato-Systems-Capture-Definition-Recorder/dp/B00840353W/"
AmazonProductDetailJob.new.perform(s_id, c_id, url)
exit
s_id, c_id, url = 143, 11, "http://www.amazon.com/Elgato-Systems-Capture-Definition-Recorder/dp/B00840353W/"
AmazonProductDetailJob.new.perform(s_id, c_id, url)
exit
s = Scrape.last
def stats(*scrapes)
  scrapes.each do |s|
    s.reload
    p = s.products.size
    op = s.other_products.size
    total = p + op
    time = (s.duration || Time.now - s.created_at).to_f / 60
    rate = total / time
    full_scrape = (500000 / rate / 60 / 24)
    puts "Scrape id: #{s.id} Source: #{s.source_id}"
    puts "Ratio: #{p}/#{op} Total: #{total}"
    puts "#{((p.to_f / total) * 100).round(2)} percent Products"
    puts "Time: #{time.round(2)}mins Rate: #{rate.round(2)} created: #{s.created_at.strftime("%m/%d/%Y")}"
    puts "#{full_scrape.round(2)} days for full scrape"
    puts "id: #{s.id} Source: #{s.source_id} Total: #{total} Rate: #{rate.round(2)} created: #{s.created_at.strftime("%m/%d/%Y")}"
    puts '=============================='
  end
  nil
end
stats s
s = Scrape.last
stats s
stast s
stats s
Sidekiq::Shards.shards
id = Identifier.asin(uniq_id: 'B007H4VT7A')
id = Identifier.asin.find(uniq_id: 'B007H4VT7A')
id = Identifier.asin.where(uniq_id: 'B007H4VT7A')
id.products
id = id.first
i.products
id.products
p = _.first
p.detail
id
p.reload
p.detail
p.original_url
p.details
p.details.where.not(product_description: nil)
Scrape.last
p.details.where.not(product_description: nil)
s = Scrape.last
stats s
s = Scrape.last
stats s
37 / 49
s.reload
s = Scrape.last
stats s
p = Product.find 150019
p.detail
p.url
p.original_url
s
s.products.includes(:detail).where.not(product_description: nil).size
s.products.includes(:detail).where.(product_description: nil).size
s.products.includes(:detail).where(product_description: nil).size
1.1 * 1.1.
1.1 * 1.1
1.2 * 1.1
1.3 * 1.3
_ * _
s = Scrape.last
stats s
s.products.includes(:detail).where.not(product_description: nil)
s.products.includes(:detail).where.not(product_description: nil).size
36 / 81.to_f
exit
s = Scrape.last
def stats(s)
  s.reload
  p = s.products.size
  op = s.other_products.size
  total = p + op
  time = (s.duration || Time.now - s.created_at).to_f / 60
  rate = total / time
  full_scrape = (500000 / rate / 60 / 24)
  puts "Scrape id: #{s.id} Source: #{s.source_id}"
  puts "Ratio: #{p}/#{op} Total: #{total}"
  puts "#{((p.to_f / total) * 100).round(2)} percent Products"
  puts "Time: #{time.round(2)}mins Rate: #{rate.round(2)} created: #{s.created_at.strftime("%m/%d/%Y")}"
  puts "#{full_scrape.round(2)} days for full scrape"
  puts "id: #{s.id} Source: #{s.source_id} Total: #{total} Rate: #{rate.round(2)} created: #{s.created_at.strftime("%m/%d/%Y")}"
  puts '=============================='
  nil
end
stats s
s.reload
s.products.include(:detail).where.not(product_description: nil).size
s.products.includes(:detail).where.not(product_description: nil).size
s.products.includes(:detail).where(product_description: nil).size
s = Scrape.last
s.products.size
s.products.includes(:detail).where('product_details.product_description': nil).size
ProductDetail
s.id
ProductDetail.where(scrape_id: 147).size
ProductDetail.where(scrape_id: 147).where(product_description: nil).size
ProductDetail.where(scrape_id: 147).where.not(product_description: nil).size
46.to_f / 51
46.to_f / 97
0 || 1
sleep 1
3.times do
  3.nil? ? (sleep 1) : break
end
3.times do
  nil.nil? ? (sleep 1) : break
end
s = Scrape.last
stats s
s = Scrape.last
ProductDetail.where(scrape_id: s.id).where(product_description: nil).size
s.products.first
p.detail
p = s.products.first
p.detail
ProductDetail.where(scrape_id: s.id).where(product_description: nil).sample.product
ProductDetail.where(scrape_id: s.id).where(product_description: nil).sample
p = ProductDetail.where(scrape_id: s.id).where(product_description: nil).sample.product
p.screenshots.last
p.screenshots.last.attachment.ur
p.screenshots.last.attachment.url
s.reload
p.screenshots.last.attachment.url
p.screenshots.last
p.screenshots
ProductScreenshot.last
ProductScreenshot.last.attachment.url
ProductScreenshot.last
ProductScreenshot.last.screenshot
ProductScreenshot.last.screenshot.url
Nokogiri::HTML('<html><head></head><body></body></html>')
nodes = _
nodes.text
nodes.blank?
nodes
nodes.text
nodes.class
Nokogiri::HTML('')
Nokogiri::HTML('').text
Nokogiri::HTML(nil).text
Nokogiri::HTML(nil)
Sidekiq::Queue.new('product_detail_job').clear
Sidekiq::Queue.new('product_detail_queue').clear
exit
Sidekiq::Queue.new('product_detail_queue').clear
exit
Sidekiq::Queue.new('product_manual_search_queue').clear
Sidekiq::Queue.new('upload_asset_queue').clear
s = Scrape.last
exit
p_ids = [147800, 390041, 470479, 460428, 147429, 460501,147513,147460,147548]
d_ids = [270127,477944,477943,477942,270125
d_ids = [270127,477944,477943,477942,270125,477941,270124,270123,270122]
ProductDetail.where(id: p_ids).pluck(:product_description)
10 * 1.1
1 * 1.1
ProductDetail.where(id: p_ids).pluck(:product_description)
Product.where(id: p_ids)
Product.where(id: p_ids).includes(:detail).where.now('product_details.product_description': nil).size
Product.where(id: p_ids).includes(:detail).where.not('product_details.product_description': nil).size
Product.where(id: p_ids).includes(:detail).where.not('product_details.product_description': nil).reload.size
exit
s = Scrape.last
def stats(s)
  s.reload
  p = s.products.size
  op = s.other_products.size
  total = p + op
  time = (s.duration || Time.now - s.created_at).to_f / 60
  rate = total / time
  full_scrape = (500000 / rate / 60 / 24)
  puts "Scrape id: #{s.id} Source: #{s.source_id}"
  puts "Ratio: #{p}/#{op} Total: #{total}"
  puts "#{((p.to_f / total) * 100).round(2)} percent Products"
  puts "Time: #{time.round(2)}mins Rate: #{rate.round(2)} created: #{s.created_at.strftime("%m/%d/%Y")}"
  puts "#{full_scrape.round(2)} days for full scrape"
  puts "id: #{s.id} Source: #{s.source_id} Total: #{total} Rate: #{rate.round(2)} created: #{s.created_at.strftime("%m/%d/%Y")}"
  puts '=============================='
  nil
end
stats s
ProductDetail.where(product_description: nil, scrape_id: s.id)
ProductDetail.where(product_description: nil, scrape_id: s.id).size
ProductDetail.where(scrape_id: s.id).size
['10', 3].max
[10, 3].max
5.times {|i| puts i}
(1..10).each {|i| puts i}
rand(5)
rand(6)
rand(5)+1
stats s
s.reload
ProductDetail.where(product_description: nil, scrape_id: s.id).size
puts ProductDetail.where(scrape_id: s.id).size; puts ProductDetail.where(product_description: nil, scrape_id: s.id).size
ProductDetail.where(scrape_id: s.id).where.not(product_description: nil)
Sidekiq::Queue.new('product_detail_queue').clear
puts ProductDetail.where(scrape_id: s.id).size; puts ProductDetail.where(product_description: nil, scrape_id: s.id).size
ProductDetail.where(product_description: nil, scrape_id: s.id).size
puts ProductDetail.where(scrape_id: s.id).size; puts ProductDetail.where(product_description: nil, scrape_id: s.id).size
stats s
ProductDetail.where(scrape_id: s.id).where.not(product_description: nil)
s.reload
stats s
ProductDetail.where(scrape_id: s.id).where.not(product_description: nil)
ProductDetail.where(scrape_id: s.id).where.not(product_description: nil).size
exit
Sidekiq::Queue.new('product_manual_search_queue').clear
Sidekiq::Queue.new('upload_asset_queue').clear
%Q{$("#scrollElement").prop("scrollTop", 1000000).trigger('scroll')}
Capybara::Session
session = _
session = Capybara::Session.new
exit
pd = ProductDetail.find 270165
ProductDetail.where(scrape_id: 149).size
s = Scrape.find 149
ProductDetail.where(scrape_id: 149).where(product_description: nil)
ProductDetail.where(scrape_id: 149).where(product_description: nil).size
ProductDetail.where(scrape_id: 149).size
ProductDetail.where(scrape_id: 149).where(product_description: nil).size
pd = ProductDetail.find 270139
ProductDetail.where(scrape_id: 149).where(product_description: nil).size
ProductDetail.where(scrape_id: 149).where.not(product_description: nil).size
ProductDetail.where(scrape_id: 149).where(product_description: nil).size
ProductDetail.where(scrape_id: 149).where.not(product_description: nil).reload.size
ProductDetail.where(scrape_id: 149).where(product_description: nil).reload.size
exit
ProductDetail.where(scrape_id: 149).where(product_description: nil).reload.size
ProductDetail.where(scrape_id: 149).where.not(product_description: nil).reload.size
s.products.count
s = Scrape.find 149
s.products.count
25.to_f / 54
ProductDetail.where(scrape_id: 149).where.not(product_description: nil).reload.size.to_f / 54
ProductDetail.where(scrape_id: 149).where.not(product_description: nil).reload.size
pd = ProductDetail.where(scrape_id: 149).where.not(product_description: nil).sample
h = {}
h.merge {desc: 'balaalfd'}
h.merge { :desc => 'balaalfd'}
h
h.merge({ :desc => 'balaalfd')
h.merge({ :desc => 'balaalfd'})
h
h = h.merge({ :desc => 'balaalfd'})
ProductDetail.where(scrape_id: 149).where.not(product_description: nil).reload.size
ProductDetail.where(scrape_id: 149).where.not(product_description: nil).reload.size.to_f / 54
ProductDetail.where(scrape_id: 149).where.not(product_description: nil).sample
pd = _
pd.product.original_url
ProductDetail.where(scrape_id: 149).where.not(product_description: nil).reload.size.to_f / 54
ProductDetail.where(scrape_id: 149).where.not(product_description: nil).reload.size
5746.6 / 60 / 24
15 - _
s
stats s
ScrapeProductSeller.last.url
ScrapeProductSeller.last.seller_url
ScrapeProductSeller.where.not(seller_url: nil).last.seller_url
ScrapeProductSeller.where.not(seller_url: nil).sample.seller_url
ScrapeProductSeller.includes(:scrape).where(scrape_id: Scrape.last.id).pluck(:seller_url)
exit
s = Scrape.last
ProductDetail.where(scrape_id: s.id).where.not(product_description: nil).reload.size
ProductDetail.where(scrape_id: s.id).where(product_description: nil).reload.size
def stats(s)
  s.reload
  p = s.products.size
  op = s.other_products.size
  total = p + op
  time = (s.duration || Time.now - s.created_at).to_f / 60
  rate = total / time
  full_scrape = (500000 / rate / 60 / 24)
  puts "Scrape id: #{s.id} Source: #{s.source_id}"
  puts "Ratio: #{p}/#{op} Total: #{total}"
  puts "#{((p.to_f / total) * 100).round(2)} percent Products"
  puts "Time: #{time.round(2)}mins Rate: #{rate.round(2)} created: #{s.created_at.strftime("%m/%d/%Y")}"
  puts "#{full_scrape.round(2)} days for full scrape"
  puts "id: #{s.id} Source: #{s.source_id} Total: #{total} Rate: #{rate.round(2)} created: #{s.created_at.strftime("%m/%d/%Y")}"
  puts '=============================='
  nil
end
stats s
ProductDetail.where(scrape_id: s.id).where.not(product_description: nil).reload.first
ProductDetail.where(scrape_id: s.id).where.not(product_description: nil).reload.first.url
s = Scrape.last
ProductDetail.where(scrape_id: s.id).where.not(product_description: nil).reload.size
ProductDetail.where(scrape_id: s.id).where(product_description: nil).reload.size
Sidekiq::Queue.new('product_manual_search').clear
ProductDetail.where(scrape_id: s.id).size
17 / 63.to_f
[1] * 3
([5] * 3)
([5] * 3).map {|el, i| el + i }
([5] * 3).map_with_index {|el, i| el + i }
([5] * 3).each_with_index.map {|el, i| el + i }
([5] * 3).each_with_index.map {|el, i| el * i }
([5] * 3).each_with_index.map {|el, i| el * i + 1 }
([5] * 3).each_with_index.map {|el, i| el * i.to_f/2 }
([5] * 3).each_with_index.map {|el, i| el * (i + 1).to_f/2 }
([5] * 3).each_with_index.map {|el, i| el * (i + 1) }
([5] * 3).each_with_index.map {|el, i| el * (i + 1).to_f/2 }
([5] * 3).each_with_index.map {|el, i| el * (i + 1).to_f/2) }
([5] * 3).each_with_index.map {|el, i| el * ((i + 1).to_f/2) }
([5] * 3).each_with_index.map {|el, i| el * ((i * 1.5)) }
([5] * 3).each_with_index.map {|el, i| el * ((i+1 * 1.5)) }
([5] * 3).each_with_index.map {|el, i| el * ((i+1 * 1.25)) }
([3] * 3).each_with_index.map {|el, i| el * ((i+1 * 1.25)) }
([4] * 3).each_with_index.map {|el, i| el * ((i+1 * 1.25)) }
([4] * 5).each_with_index.map {|el, i| el * ((i+1 * 1.25)) }
([4] * 5).each_with_index.map {|el, i| [(el * (i+1 * 1.25)), 10].min }
([4] * 5).each_with_index.map {|el, i| [(el * (i+1), 10].min }
([4] * 5).each_with_index.map {|el, i| [el * (i+1), 10].min }
([5] * 5).each_with_index.map {|el, i| [el * (i+1), 10].min }
([5] * 5).each_with_index.map {|el, i| [el + (i+1), 10].min }
([5] * 10).each_with_index.map {|el, i| [el + (i+1), 10].min }
([5] * 10).each_with_index.map {|el, i| [el + (i+1), 15].min }
([5] * 10).each_with_index.map {|el, i| [el * (i+1), 15].min }
([5] * 10).each_with_index.map {|el, i| [el * (i * 1.2), 15].min }
([5] * 10).each_with_index.map {|el, i| [el * (i * 0.8), 15].min }
([5] * 10).each_with_index.map {|el, i| [el * (i * 1.12), 15].min }
([5] * 10).each_with_index.map {|el, i| [el * 1.12), 15].min }
([5] * 10).each_with_index.map {|el, i| [el * 1.12, 15].min }
Sidekiq::Queue.new('product_manual_search').clear
Sidekiq::Queue.new('product_manual_search_queue').clear
stats s
s
s.reload
ProductDetail.where(scrape_id: s.id).where.not(product_description: nil).reload.size
ProductDetail.where(scrape: s).size
25 / 76
25.to_f / 76
ProductDetail.where(scrape: s).size
ProductDetail.where(scrape_id: s.id).where.not(product_description: nil).reload.size
31 / 84
31.to_f / 84
ProductDetail.where(scrape_id: s.id).where.not(product_description: nil).reload.sample
p = _.product
p.id
Sidekiq::Queue.new('product_manual_search_queue').clear
Sidekiq::Queue.new('product_detail_queue').clear
Sidekiq::Queue.new('product_manual_search_queue').clear
p.id
p.detail
ProductDetail.where(scrape_id: s.id).where.not(product_description: nil).reload.sample
p2 = _.product
p2.id
p2.detail
p.detail
p2.asin
p2.asins
p2.detail
s.detail
p.detail
p.detail.attributes
pd = p.detail
detail.product_description
pd.product_description
p2.detail
pd2 = p2.detail
pd2.product_description
pd2
pd
ProductDetail.where(scrape_id: s.id).where.not(product_description: nil).reload.sample
ProductDetail.where(scrape_id: s.id).where.not(product_description: nil).reload.size
s = Scrape.last
ProductDetail.where(scrape_id: s.id).where.not(product_description: nil).reload.size
ProductDetail.where(scrape_id: s.id).where.not(product_description: nil).reload.sample
ProductDetail.where(scrape_id: s.id).where.not(product_description: nil).reload.first
ProductDetail.where(scrape_id: s.id).where.not(product_description: nil).reload.second
ProductDetail.where(scrape_id: s.id).where.not(product_description: nil).reload.last
ProductDetail.where(scrape_id: s.id).where.not(product_description: nil).reload.secondq
ProductDetail.where(scrape_id: s.id).where.not(product_description: nil).reload.first
exit
ProductDetail.where(scrape_id: s.id).where.not(product_description: nil).reload
s = Scrape.last
ProductDetail.where(scrape_id: s.id).where.not(product_description: nil).reload
it
s = Scrape.last
ProductDetail.where(scrape_id: s.id).where.not(product_description: nil).reload.size
ProductDetail.where(scrape_id: s.id).where.not(product_description: nil).reload.last
ProductDetail.where(scrape_id: s.id).where.not(product_description: nil).reload.last(4).map(&:prodcut_description)
ProductDetail.where(scrape_id: s.id).where.not(product_description: nil).reload.last(4).map(:&prodcut_description)
ProductDetail.where(scrape_id: s.id).where.not(product_description: nil).reload.last(4).map(:&product_description)
ProductDetail.where(scrape_id: s.id).where.not(product_description: nil).reload.last(4).map(&:product_description)
s.products.size
ProductDetail.where(scrape_id: s.id).where.not(product_description: nil).reload.size
10 / 25
10 / 35.to_f
ProductDetail.where(scrape_id: s.id).where.not(product_description: nil).reload.last
Sidekiq::Queue.new('product_manual_search_queue').clear
Sidekiq::Queue.new('product_listing_queue').clear
Seller
ScrapeProductSeller.where(scrape_id: Scrape.last.id)
ScrapeProductSeller.where(scrape_id: Scrape.last.id).where.not(seller_id: nil)
ScrapeProductSeller.joins(:seller).select('sellers.name').where(scrape_id: Scrape.last.id).where.not(seller_id: nil)
ScrapeProductSeller.where(scrape_id: Scrape.last.id).where.not(seller_id: nil)
ids = ScrapeProductSeller.where(scrape_id: Scrape.last.id).where.not(seller_id: nil).pluck(:seller_id)
ids.count
Seller.where(id: ids).pluck(:name)
s
s.products.first
p = _
p.sellers
p.sellers.count
s
s.products.first
p = _
p.detail
p.id
s.products.first
p.sellers
s.products.size
prods = s.products.select {|p| p.scrape_product_sellers.where(scrape: s) }
p = prods.first
prods = s.products.select {|p| p.scrape_product_sellers.where(scrape: s).size > 10 }
prods.first
p = _
p.sellers.size
exit
str = 'asdfasdf.jpg\asdfas'
str.match(/\.jpg(\S*) (.*)/)
str.match(/\s+\.jpg(\S*) (.*)/)
str.match(/\s+\.jpg(.*)/)
str.match(/\.jpg(.*)/)
data = _
data[0]
data[1]
str.gsub(data[1], '')
url = URI.parse('https://images-na.ssl-images-amazon.com/images/G/01/videogames/detail-page/B003IU02HK.PT04.jpg\u{201d}')
url = '\u{201d}'
url = "https://images-na.ssl-images-amazon.com/images/G/01/videogames/detail-page/B003IU02HK.PT04.jpg”"
encoding_options = {
  :invalid           => :replace,  # Replace invalid byte sequences
  :undef             => :replace,  # Replace anything not defined in ASCII
  :replace           => '',        # Use a blank for those replacements
:universal_newline => true }
url
url.encode(Encoding.find('ASCII'), encoding_options)
URI.encode('https://images-na.ssl-images-amazon.com/images/G/01/videogames/detail-page/B003IU02HK.PT04.jpg/u{201d}')
exit
URL.encode 'www.google.com'
URI.encode 'www.google.com'
exit
ProductListing.where(scrape_id: 1, url: null)
ProductListing.where(scrape_id: 1, url: nil)
ProductListing.where(scrape_id: 1)
exit
p = Product.find 1
p.original_Url
p.original_rrl
p.original_url
ProductListing
ProductListing.size
ProductListing.count
ScrapeProductSeller.where(scrape_id: 1, product_id: 1)
ScrapeProductSeller.where(scrape_id: 1, product_id: 1).size
ScrapeProductSeller.group(:original_url).count
ScrapeProductSeller.group(:url).count
ScrapeProductSeller.group(:original_url).count
ei
xxtei
ScrapeProductSeller.uniq(:original_url).count
Product.count
ScrapeProductSeller
p = Product.sample
p = Product.all.sample
p.scrape_product_seller.first.original_url
p.scrape_product_sellers.first.original_url
p.scrape_product_sellers.first
sps_id, scrape_id, product_id = _.id, 4, 21131
ProductDetail.where(scrape_id: scrape_id, product_id: product_id)
ProductListing.where(scrape_id: scrape_id, product_id: product_id)
ProductListing.where(scrape_id: scrape_id, product_id: product_id).count
sps
sps = ScrapeProductSeller.find(sps_id)
ScrapeProductSellers.where(Scrape.find_by(source: 3))
sps = ScrapeProductSeller.where(Scrape.find_by(source: 3))
sps.count
sps = ScrapeProductSeller.where(scrape_id: Scrape.find_by(source: 3))
sps.uniq(:scrape_id, :product_id, :orinal_url).size
sps.uniq(:orinal_url).size
sps.size
ScrapeProductSeller.first
ScrapeProductSeller.all.uniq(:product_id).first(100)
s_id, p_id, url = 1, 3, 'https://www.google.com/shopping/product/17347559800266076962'
listing = ProductListing.where(scrape_id: s_id, product_id: p_id).size
listing = ProductListing.where(scrape_id: s_id, product_id: p_id)
uri = URI.parse(url)
url = 'https://www.google.com/shopping/product/5875203508966579527?sa=X&ved=0ahUKEwiEjojxtbfLAhVT0mMKHSMkDnIQrhIIggE'
uri = URI.parse(url)
uri.path
url
url = 'http://www.amazon.com/gp/product/B00M1NEUKK/ref=s9_simh_gw_g23_i3_r?ie=UTF8&fpl=fresh&pf_rd_m=ATVPDKIKX0DER&pf_rd_s=desktop-3&pf_rd_r=0T6AD89PZWF63C9N24WF&pf_rd_t=36701&pf_rd_p=2084660942&pf_rd_i=desktop'
uri = URI.parse(url)
uri.path
Scrape.all.uniq(:source_id)
Scrape.all.group(:source_id)
s = Scrape.where(source_id: 11)
p = s.products.first
s = Scrape.where(source_id: 11).first
p = s.products.first
p.product_detail
p.product_listings
p
p.listings
s
p.listings.where(scrape_id: 16)
p.scrape_product_sellers.where(scrape_id: 16)
p
s_id, p_id, url = 1, 3, 'https://www.google.com/shopping/product/17347559800266076962'
listing = ProductListing.where(scrape_id: s_id, product_id: p_id).size
listing = ProductListing.where(scrape_id: s_id, product_id: p_id)
p = Product.find p_id
p.listings
p.listings.count
p.listings.order(id: :desc)
p.listings.where(scrape_id: 4).order(id: :desc)
p.original_url
p.scrape_product_sellers.last.original_url
url = '          
url = "https://www.google.com/shopping/product/17347559800266076962"
url = url + "?asldfkja=alsdfkajs"
uri = URI.parse(url)
uri.path
uri.host + uri.path
uri.scheme + uri.host + uri.path
uri.scheme +'://'+ uri.host + uri.path
uri.scheme + "://" + uri.host + uri.path
uri.root
uri.query
uri.port
url = 'http://www.amazon.com/gp/product/B00M1NEUKK/ref=s9_simh_gw_g23_i3_r?ie=UTF8&fpl=fresh&pf_rd_m=ATVPDKIKX0DER&pf_rd_s=desktop-3&pf_rd_r=0T6AD89PZWF63C9N24WF&pf_rd_t=36701&pf_rd_p=2084660942&pf_rd_i=desktop'
url.gsub(/ref=.+/, '')
ScrapeProductSeller.where(scrape_id: 4).first(100).map(&:original_url)
s = Scrape.where(source_id: 2).lasts
s = Scrape.where(source_id: 2).last
s = Scrape.where(source_id: 2).first
s.products.sample
alip = _
sps = ScrapeProductSeller.where(product_id: alip, scrape_id: s.id)
url2 = 'http://www.alibaba.com/product-detail/43-Inch-FHD-LED-TV-43L73F-_60208032786.html?spm=a2700.7724838.0.0.adlWtC'
uri2 = URI.parse(url2)
#{uri2.scheme}://#{uri2.host}#{uri2.path}"
"#{uri2.scheme}://#{uri2.host}#{uri2.path}"
Scrape.first
s = _
s.products
ProductListing.where(scrape_id: s.id)
ProductListing.where(scrape_id: s.id).size
p
p.scrape_ids
ScrapeProductSellers.where(scrape_id: 1, product_id: p.id).order(:desc)
ScrapeProductSeller.where(scrape_id: 1, product_id: p.id).order(:desc)
ScrapeProductSeller.where(scrape_id: 1, product_id: p.id).order(:desc).size
sps = ScrapeProductSeller.where(scrape_id: 1, product_id: p.id).order(:desc)
sps = ScrapeProductSeller.where(scrape_id: 1, product_id: p.id).order(:desc).first
sps = ScrapeProductSeller.where(scrape_id: 1, product_id: p.id).order(id: :desc).first
sps = ScrapeProductSeller.where(scrape_id: 1, product_id: p.id).order(id: :desc).size
sps = ScrapeProductSeller.where(scrape_id: 1, product_id: p.id).order(id: :desc)
ProductListing.where(scrape_id: s.id).size
exit
url = 'http://www.alibaba.com/trade/search?fsb=y&IndexArea=product_en&CatId=&SearchText=led+tv'
url.gsub(/?.+$/, '')
url.gsub(/\?.+$/, '')
url = 'http://www.alibaba.com/product-detail/43-Inch-FHD-LED-TV-43L73F-_60208032786.html?spm=a2700.7724838.0.0.adlWtC'
p = Product.find 6810
p.scrape_product_sellers.where(scrape_id: 1).map(&:original_url)
p.details
p.listings
p.listings.where(scrape_id: 1)
p.scrape_ids
p.listings
p.scrape_product_sellers.where(scrape_id: 1).map(&:original_url)
p.scrape_product_sellers.where(scrape_id: 1)
reload1
reload!
ScrapeProductSellers.where(scrape_id: 1).size
ScrapeProductSeller.where(scrape_id: 1).size
ProductListing.where(scrape_id: 1)
ProductListing.where(scrape_id: 1).size
5042 - 3786
ProductListing.where.not(original_url: nil)
ProductListing.where(url: nil, scrape_id: 1)
ProductListing.where(scrape_id: 1).size
Scrape.find(1).products.size
Scrape.where.not(id: 1).each.products.size
sum = 0
Products.size
Product.size
Product.count
151000 / 5000
10 * 30
300 / 60
Scrape.where(source_id: 1).second
ProductListing.where(scrape_id: 3)
ProductListing.where(scrape_id: 3).size
ProductAttributeKey.count
ProductAttributeValue.count
ProductAttributeValue.destroy_all
ProductAttributeValue.delete_all
ProductAttributeKey.delete_all
ProductDetail.count
ProductListing.count
Scrape.find 4
ProductListing.where(scrape_id: 4).last.id
Scrape.where(id: 4).last
s = _
s.products.last
p = _
p.detail
p.listing
p.listings
ScrapeProductSellers.where(scrape_id: 4, product_id: 242536)
ScrapeProductSeller.where(scrape_id: 4, product_id: 242536)
before = 229179
ProductListing.count
before - _
ProductList.where(url: '')
ProductListing.where(url: '')
ProductListing.where(url: '').size
ProductListing.count
ProductListing.where(url: '').update_all(url: nil)
ProductListing.where(url: '').size
ProductListing.where(url: '').update_all(url: nil)
ProductListing.where(url: '').size
ProductListing.where(url: nil).size
ProductListing.where(url: 'NULL-3')
ScrapeProductSeller.where(original_url: nil)
ScrapeProductSeller.where(original_url: nil).size
exit
ProductListing.where(url: '')
ProductListing.where(url: nil)
exit
ScrapeProductSeller.where(scrape_id: 1, original_url: nil)
p = Product.find 6810
p.original_url
ScrapeProductSeller.where(scrape_id: 1, product: p)
ScrapeProductSeller.where(scrape_id: 1, product: p).pluck(:original_url)
seller = Seller.find 3774
seller.listings
Product.count
ScrapeProductSeller.includes(scrape: :source).where(original_url: nil).map {|sps| sps.scrape.source_id}.uniq
ScrapeProductSeller.includes(scrape: :source).where(original_url: nil).size
exit
ScrapeProductSeller.where(scrape_id: 1, original_url: nil)
p = Product.find 6810
p.original_url
p.scrape_product_sellers
ScrapeProductSellers.where(scrape_id: 1).size
ScrapeProductSeller.where(scrape_id: 1).size
ScrapeProductSeller.where(scrape_id: 1, original_url: nil).size
ScrapeProductSeller.where(scrape_id: 1, original_url: nil).pluck(:product_id)
ScrapeProductSeller.where(scrape_id: 2, original_url: nil).pluck(:product_id)
ScrapeProductSeller.where(scrape_id: 3, original_url: nil).pluck(:product_id)
ScrapeProductSeller.where(scrape_id: 3, original_url: nil).pluck(:product_id).size
ScrapeProductSeller.where(scrape_id: 3, original_url: nil).pluck(:product_id).uniq.size
ScrapeProductSeller.where(scrape_id: 1, original_url: nil).uniq.size
ScrapeProductSeller.where(scrape_id: 1, original_url: nil).pluck(:product_id).uniq.size
ScrapeProductSeller.where(scrape_id: 1, original_url: nil).pluck(:product_id).size
ScrapeProductSeller.where(scrape_id: 1, original_url: nil).pluck(:product_id).uniq
ScrapeProductSeller.where(scrape_id: 1, original_url: nil).pluck(:product_id).uniq.sort
ScrapeProductSeller.where(scrape_id: 3, original_url: nil).pluck(:product_id).uniq.sort
q
ScrapeProductSeller.where(original_url: nil).pluck(:product_id).size
ScrapeProductSeller.where(original_url: nil).pluck(:product_id).uniq.size
ScrapeProductSeller.where(scrape_id: 3, original_url: nil).pluck(:product_id).uniq.size
ScrapeProductSeller.where(scrape_id: 4, original_url: nil).pluck(:product_id).uniq.size
ScrapeProductSeller.where(scrape_id: 5, original_url: nil).pluck(:product_id).uniq.size
ScrapeProductSeller.includes(scrape: :source).where(original_url: nil).where('sources.id': [1,2,3,4]).pluck(:product_id).uniq.size
ScrapeProductSeller.includes(scrape: :source).where(original_url: nil).where('sources.id': [1,2,3,4]).destroy_all
ScrapeProductSeller.includes(scrape: :source).where(original_url: nil).where('sources.id': [1,2,3,4]).pluck(:product_id).uniq.size
ProductAttributeValue.delete_all
ProductAttributeKey.delete_all
ScrapeProductSeller.includes(scrape: :source).where(original_url: nil).where('sources.id': [1,2,3,4]).pluck(:product_id).uniq.size
ScrapeProductSeller.where(url: nil)
ScrapeProductSeller.where(original_url: nil).size
exit
url = 'https://www.google.com/shopping/product/7444589549292806470/sp'
url.length
ProductListing.find(12407)
listing = _
listing1 = ProductListing.where(url: url)
url = 'https://www.google.com/shopping/product/7444589549292806470/specs'
listing1 = ProductListing.where(url: url)
listing1 = ProductListing.where(url: url).first
listing
listing1
p1, p2 = Product.find([281, 6659)
p1, p2 = Product.find([281, 6659])
url
SourceProductSeller
ScrapeProductSeller
ScrapeProductSellers.where.not(product_source_path: nil).first(100).map(&:product_source_path)
ScrapeProductSeller.where.not(product_source_path: nil).first(100).map(&:product_source_path)
Seller
ScrapeProductSeller
ProductListing.where.not(contact_name: nil).first
dqhttps://www.google.com/aclk?sa=l&ai=CCt9INyTjVpDKAseI-gOBxpLwBoaVnt4IhuGaiMMChaXM-dQCCAkQAiDezc8eKBRgyfb4hsijoBmgAbjSme4DyAEHqgQqT9C_0BnMcb1RbSyvqBDwiiICqMey4QFUQ12npocBo0VLznwRx0WRpyUWwAUFoAYmgAewreYRiAcBkAcCqAemvhvYBwHgEq6ox8mI9ev9bw&sig=AOD64_3sTcX4C53Qp1UCe8BiwzwkX5I1og&ctype=5&clui=16&q=&ved=0ahUKEwjljuK6trnLAhVT3GMKHbr0CogQ2CkIgAMwAQ&adurl=http://www.gogreensolar.com/products/7000w-diy-solar-panel-kit-grid-tie-inverter
url = 'https://www.google.com/aclk?sa=l&ai=CCt9INyTjVpDKAseI-gOBxpLwBoaVnt4IhuGaiMMChaXM-dQCCAkQAiDezc8eKBRgyfb4hsijoBmgAbjSme4DyAEHqgQqT9C_0BnMcb1RbSyvqBDwiiICqMey4QFUQ12npocBo0VLznwRx0WRpyUWwAUFoAYmgAewreYRiAcBkAcCqAemvhvYBwHgEq6ox8mI9ev9bw&sig=AOD64_3sTcX4C53Qp1UCe8BiwzwkX5I1og&ctype=5&clui=16&q=&ved=0ahUKEwjljuK6trnLAhVT3GMKHbr0CogQ2CkIgAMwAQ&adurl=http://www.gogreensolar.com/products/7000w-diy-solar-panel-kit-grid-tie-inverter'
p1, p2 = Product.find([281, 6659)
p1, p2 = Product.find([281, 6659])
p1.scrape_product_sellers.map(&:id) & p2.scrape_product_sellers.map(&:id)
url
p1.listing
p1.scrape_product_sellers.last
p1.scrape_product_sellers.first
p1.scrape_product_sellers..where(scrape_id: 3).first
p1.scrape_product_sellers..where(scrape_id: 3).last
p1.scrape_product_sellers.where(scrape_id: 3).last
url = p1.scrape_product_sellers.where(scrape_id: 3).last.original_url
ProductListing.find_by(scrape_id: 3, url: url)
ProductListing.find_by(scrape_id: 3, url: 'jon.com')
OtherProductDetail.last(100).first
op = _.other_product
ScrapeProductSeller.where(scrape_id: 3).size
ProductListing.where(scrape_id: 3).size
Scrape.find(3).num_products
ProductListing.where(scrape_id: 3, url: nil).first
ProductListing.where(scrape_id: 3, url: nil).size
ProductListing.where(scrape_id: 3, url: nil).pluck(:url)
ProductListing.where(scrape_id: 3, url: nil).pluck(:product_id)
ProductListing.where(scrape_id: 3, url: nil).pluck(:product_id).uniq
p_ids = _
ProductListing.where(scrape_id: 3, product_id: p_ids).where.not(url: nil).size
Scrape.where(scrape_id: 4).products.last
Scrape.find(4).products.last
100 / 2
90.to_f / 2
90
Scrape.find(4).products.size
(_ * 90).to_f / 60 / 60
(68169.to_f / 90) / 60 / 60
.21 * 60
0.21 * 60
Asset
ProductImage
ProductImage.last
ProductScreenshot.last
ProductManual.last
opsOtherProduct.first 100
others = OtherProduct.first 100
op = others.sample
ops = Scrape.first.other_products
ops.size
urls = ops.pluck(:url).select { |op| op.url.include?('specs') }
urls = ops.pluck(:url).select { |url| url.include?('specs') }
urls.size
urls = ops.pluck(:url).select { |url| url.include?('shopping/product') }
urls.siz
urls.size
ops.size
op_ids = ops.where("other_products.url like '%/shopping/product/%'").ids
op_ids.size
op_ids.uniq.size
Scrape.find(4).products.last.id
Scrape.last.id
Scrape.find 122
1738 / 60
ProductListing.all.count
Product.count
OtherProduct.count
ScrapeOtherProductSeller.first
OtherProduct.first
OtherProduct
OtherProduct.first
op = _
op.details
op.other_product_details
op.details
op
op = Scrape.where(source_id: 1).last.other_products.first
s = Scrape.first
s.other_products.where.not("other_products.url like '%shopping/product%'").size
s.other_products.where("other_products.url like '%shopping/product%'").size
[1,2].includes 2
[1,2].includes? 2
[1,2].include? 2
[1,2].include? 3
s
s.other_products.first
op = _
op.scrape_other_product_sellers
s.other_products.includes(:scrapes).map {|op| op.scrape_ids}
s.other_products.includes(:scrapes).map {|op| op.scrapes.map(&:source_id)}
sources = _
sources.flatten
sources.flatten.uniq
s.other_products.where("other_products.url like '%/shopping/product/%'").size
s.other_products.where.not("other_products.url like '%/shopping/product/%'").size
s.other_products.where.not("other_products.url like '%/shopping/product/%'")
others = _
others.first
op = _
op.detail
others.map {|op| op.details.map(&:id) }
_.flatten
s.other_products.includes(:scrape_product_sellers).where.not("other_products.url like '%shopping/product/%'")
s.other_products.includes(:scrape_product_sellers).where.not("other_products.url like '%shopping/product/%'").size
s.other_products.includes(:scrape_product_sellers).where.not("other_products.url like '%shopping/product/%'").first
s.other_products.includes(:scrape_other_product_sellers).where.not("other_products.url like '%shopping/product/%'").first
op = _
op.scrape_other_product_sellers
s.other_products.includes(:scrape_other_product_sellers).where.not("other_products.url like '%shopping/product/%'").size
s.other_products.includes(:scrape_other_product_sellers).where.not("other_products.url like '%shopping/product/%'").first
s.other_products.includes(:scrape_other_product_sellers).where.not("other_products.url like '%shopping/product/%'").destroy_all
s.other_products.includes(:scrape_other_product_sellers).where.not("other_products.url like '%shopping/product/%'").last
s.other_products.includes(:scrape_other_product_sellers).where.not("other_products.url like '%shopping/product/%'")
s = Scrape.find 4
op = s.other_products.first
op.url
op.original_url
op.scrape_product_sellers
op.scrape_ohter_product_sellers
op.scrape_other_product_sellers
OtherProduct.count
ScrapeProductSeller.count
Product.count
ScrapeProductSeller
ScrapeProductSeller.first
Seller
ScrapeProductSeller.where.not(seller_url: nil).first
ScrapeProductSeller.first
ProductListing.first
ScrapeProductSeller.first
listing = PRoductListing.find 1
listing = ProductListing.find 1
ScrapeProductSeller.where(scrape_id: 1, original_url: listing.original_url)
listing = ProductListing.last
ScrapeProductSeller.where(scrape_id: 138, original_url: listing.original_url)
op
op.detail
op.details
op.details.size
op.details.where(scrape_id: 4).size
op.details.where(scrape_id: 11).size
op.details
op.details.pluck(:scrape_id)
op.details.where(scrape_id: 109).size
op.detail
op.detail.attributes
exit
s = Scrape.find 4
op.details
op = s.other_products.first
op.details
op.detail.attributes
attrs = op.detail.attributes
attrs.merge(url: 'www.jonathan.com')
attrs.merge('url' => 'www.jonathan.com')
ProductListing.size
ProductListing.count
s = Scrape.first
op = s.other_products.first
op.details.where(scrape_id: 1).last
attrs = op.details.where(scrape_id: 1).last.delete(:other_product_id)
attrs = op.details.where(scrape_id: 1).last.attributes.delete(:other_product_id)
op
op.details
op.details.where(scrape_id: 1)
attrs = op.details.where(scrape_id: 1).last
attrs = op.details.where(scrape_id: 1).last.attributes
attrs = op.details.where(scrape_id: 1).last.attributes.delete('other_product_id')
attrs = op.details.where(scrape_id: 1).last.attributes.except('other_product_id')
attrs = op.details.where(scrape_id: 1).last.attributes.except(:id, :other_product_id)
attrs = op.details.where(scrape_id: 1).last.attributes.except('id', 'other_product_id')
attrs = op.details.where(scrape_id: 1).last.attributes.except('id', 'other_product_id', 'created_at', 'updated_at')
Scrape.find 2
s = _
ops = s.other_products.first(10)
op = ops.first
op.details.where(scrape_id: s.id)
OtherProductDetail.last(100)
s = Scrape.find 4
s.other_products.last
s.other_products.last.id
ProductList.last
ProductListing.last
ProductScreenshot
ProductScreenshot.last
Assetable.first
Asset.first
img = _
img.assetable_ids
img.products
op
op.screenshots
Asset.all.group(:assetable_type).uniq
Asset.all.pluck(:assetable_type).uniq
Asset.all.select('DISTINCT(assetable_type)')
p
op
op.categories
op.category_ids
op
op.scrape_other_product_sellers
op.scrape_other_product_sellers.map {|ops| ops.attributes.except('id', 'other_product_id', 'updated_at') }
p = s.products.first
p.scrape_product_sellers.map {|ops| ops.attributes.except('id', 'product_id', 'updated_at') }
op.scrape_other_product_sellers
op.scrape_other_product_sellers.map {|ops| ops.attributes.except('id', 'other_product_id', 'updated_at') }
p.scrape_product_sellers.map {|ops| ops.attributes.except('id', 'product_id', 'updated_at') }
op.scrape_other_product_sellers.map {|ops| ops.attributes.except('id', 'other_product_id', 'updated_at').merge('listing_id' => 1) }
p.categories
p
p.category_products
Category.find 4
Scrape.first.products.last.id
Listing.first
l = )
l = Listing.first
l.categories
CategoryListing.first
l = Listing.find 235
l.categories
CategoryListing.third
Category.find 5
CategoryListing.where.not(source_category_id: nil).first
17520 / 60 / 60
ScrapeOtherProductSeller.where.not(other_product_source_path: nil)
OtherProduct.last.id
l = Listing.last
l.categories
p = Product.last
p.listings
exit
l = Listing.first
l.categories
CategoryListing.first
l = Listing.find 235
l.categories
l.category_ids
op.category_ids
op = OtherProduct.first
op.category_ids
l.category_ids & op.category_ids
l.category_ids | op.category_ids
op = OtherProduct.find 3
op.category_ids
l = Listing.find 269297
l.category_ids
op.id
op.category_ids
l.category_ids
l.category_ids | op.category_ids
l.categories | op.categories
(l.categories | op.categories).class
(l.categories | op.categories).size
ProductIdentifier
Identifier.count
exit
Listing.count
Listing.last.id
ListingIdentifier.count
ProductIdentifier.count
OtherProductIdentifier.count
exit
OtherProduct.find 81139
op = _
op.scrape_ids
other_product = OtherProduct.includes(:details, :scrape_other_product_sellers)
exit
op_id = 90587
s_id = 4
OtherProduct.includes(:details, :scrape_other_product_sellers).where('other_product_details.scrape_id': s_id, 'scrape_other_product_sellers.scrape_id': s_id).find(op_id)
op = OtherProduct.find 90587
op.other_product_details
op.details
OtherProduct.includes(:details, :scrape_other_product_sellers)
Listing.where(product_id: nil).first
l = _
l.sellers
l.name
l
url = 'https://www.google.com/shopping/product/12770536777640250214/specs'
op = OtherProduct.where(original_url: url)
op.name
op = op.first
op = OtherProduct.where(original_url: url).first
OtherProduct
op = OtherProduct.where(url: url).first
op.name
l
l.scrapes
op.sellers
op.scrape_product_sellers
listing.where(product_id: nil).where.not(name: nil)
Listing.where(product_id: nil).where.not(name: nil)
363900 / 24
363900.0 / 24 / 60
_ / 60
349600 / 30 / 60 / 60
l.id
CategoryListing
CategoryListing.where(listing_id: l.id)
CategoryListing.where.not(source_category_id: nil)
CategoryProduct.where.not(source_category_id: nil)
p = Product.find 8
p.listing.first
p.listings.first
l = _
cp = CategoryProduct.where.not(source_category_id: nil).where(product_id: 8).first
l.product_id
l.category_ids
l
l.category_listings
CategoryOtherProduct.where.not(source_category_id: nil).size
CategoryListing.where.not(source_category_id: nil).size
cp = CategoryProduct.where.not(source_category_id: nil).where(product_id: 8).first
cp.attributes.except(:id, :product_id)
params = _
cp.attributes.except('id', 'product_id')
params = _
cl = l.category_listings.where(category_id: 7)
l.product_id
l.category_listing.create(params)
l.category_listings.create(params)
l.categories
l.category_ids
l.category_listings
p = Product.first
p.categories
l = Listing.where(product_id: 1).first
l.categories
op
op.category_other_products
CategoryProduct
Sidekiq.queue.new('default').clear
Sidekiq::Queue.new('default').clear
cp
l
l.data_hash
exit
l = Listing.last
l.url
l.url.length
1.bytes
223421.bytes
767 - 4
760 / 4
160 * 3
exit
Listing.last
CategoryListing
CategoryListing.first
Product.count
exit
Sidekiq::Queue.new('product_attribute_queue').clear
Sidekiq::Queue.new('sellers_queue').clear
Sidekiq::Queue.new('update_scrape_counter_queue').clear
Sidekiq::Queue.new('upload_asset_queue').clear
AttributeValue.count
ProductAttributeValue
ProductAttributeValue.connection
ProductAttributeValue.count
ProductAttributeKey.count
l = Listing.last
l.attributes
exit
l = Listing.last
l.last
l.attributes
l
exit
l = Listing.last
l.identifiers
l = Listing.first
l.categories
l.source_categories
l.category_listings
Product.last.id
l = Listing.find 1768
q
exit
reload!
p = Product.first
p.listing
l = _
l.screenshots
Listing.where(url: nil).size
Listing.size
Listing.count
p = Product.first
p.scrape_product_sellers
p.scrape_product_sellers.last
p_id = p.id
s_id = 1
product = p
original_url = product.scrape_product_sellers.where(scrape_id: s_id, product_id: p_id).where.not(original_url: nil).last.try(:original_url)
ScrapeProductSeller.where(original_url: nil).size
ScrapeProductSeller.where(original_url: nil).map(&:scrape_id).uniq
ScrapeListingSeller.first
l = Listing.first
Listing.where(url: nil).size
p.listings
s = Scrape.find(110)
ScrapeOtherProductSeller.where(original_url: nil).first
ScrapeProductSeller.where(original_url: nil)
ScrapeProductSeller.where(original_url: nil).first
ScrapeProductSeller.count
ScrapeProductSeller.where(original_url: nil).size
ScrapeOtherProductSeller.where(original_url: nil).size
ScrapeOtherProductSeller.count
ScrapeOtherProductSeller.where(original_url: nil).first 10
OtherProduct
OtherProduct.where(url: nil)
op = OtherProduct.first
op.original_url
op.url
op
op.url
ProductDetail.count + OtherProductDetail.count
Listing.count + OtherProductDetail.count
p.listings
p.categories
exit
p = Product.first
p.sellers
p.sellers.count
exit
id = Identifier.first
id.products
id
exit
id = Identifier.first
id.products
exit
id = Identifier.first
id.products
id.products.pluck(:name)
id.products.size
exit
Listing.count
Listing.last
Listing.first
p = Product.find 43532
p.listings.count
ScrapeListingSeller.last
l = Listing.find(104743)
l.product
p = _
p.id
Scrape.find(4).products.last.id
Scrape.find(4).products.first.id
Listing.where.not(product_id: nil).first(100)
575500 / 6 / 60 / 60
575500 / 20 / 60 / 60
575500 / 40 / 60 / 60
Listing.where.not(product_id: nil).first(100)
Listing.find_by(url: 'http://www.amazon.com/gp/product/B00E5Z3R6A')
id = Identifier.find_by(uniq_id: 'B00E5Z3R6A')
id.products
p = _
p.models
p.scrapes
p = p.first
p.scrapes
Listing.count
Product.count
exit
ProductListing.first
exit
Listing.first
l = _
l.products
exit
l = Listing.first
l.products
exit
p = Product.first(1235).last
p.listings
p.listings.size
p.listings.count
p.listings.first.assets
p.listings.first.screenshot
p.listings.first.screenshots
p.listings.first.images
l = p.listings.last
Listing.where.not(product_id: nil).last.id
Product.last.id
l = Listing.last
p = Product.last
p.listings
p.listings.count
p.listings
l = Listing.last
l.products
Listing.where(url: nil).size
Listing.count
exit
ProductListing.last
Listing.last.id
Listing.last
ProductListing.last
ProductListing.delete_all
Listing.last.id
exit
p = Product.first(1513).last
p = _
p.listings
p = Product.first(12513).last
p.listings
p.listings.where(scrape_id: 4)
p.listings.where(scrape_id: 4).last
ScrapeProductSeller.where(original_url: nil).size
ScrapeProductSeller.count
75113.to_f / _
exit
Sidekiq::Queue.new('default').size
Sidekiq::Queue.new('default').clear
Sidekiq::Queue.new('default').size
exit
q  = Sidekiq::Queue.new('default')
q.size
q.first
q.first.class
q.first.klass
q.each {|job| job.delete if job.klass == "AssignUniqueIdentifierJob" }
q.size
80000 / 2
80000 / 2 / 60
80000 / 2 / 60 / 60
80000 / 2 / 60 / 60 / 6
80000 / 4 / 60 / 60
Listing.last
Listing.where.not(url: nil)
Listing.where.not(url: nil).last
Listing.where(scrape_id: 4).where.not(url: nil).last
l = _
l.original_url
l.url
l.products
p = _.first
p.scrape_product_sellers.last
l = Listing.where(scrape_id: 4).where.not(url: nil).last
l.url
Listing.where(scrape_id: 1).where.not(url: nil).last
l.url
l = Listing.where(scrape_id: 1).where.not(url: nil).last
Images
Image
Asset.first(100).pluck(:type).uniq
Asset.first(100).map(&:type).uniq
Asset.first(10000).map(&:type).uniq
Asset.first(100000).map(&:type).uniq
ProductImage.size
ProductImage.count
ProductScreenshot.count
5000000 / 1000
* 4
5000 * 4
listing = Listing.find_by(url: 'http://www.amazon.com/Ematic-EM608VIDC-3-Inch-Digital-Charcoal/dp/B003FVTP9I')
url = 'http://www.amazon.com/GearIt-10-Pack-Ethernet-Cable-Patch/dp/B00ZJX3Z8C/'
l = Listing.find_by(url: url)
Listing.find_by(scrape_id: 4, url: url)
Listing.find_by(scrape_id: 1, url: url)
Listing.where(url: nil).where.not(original_url: nil).size
Listing.where.not(original_url: nil).size
Listing.where.not(url: nil).size
l = Listing.find_or_initialize_by(scrape_id: 1, url: url)
l.new_record?
l = Listing.find_or_initialize_by(scrape_id: 4, url: url)
l.new_record?
l.category_ids
l.category_ids << 2
l.categories
l.category_ids
l.category_ids << 2
l.save
l.category_ids
CategoryListing
CategoryListing.find_or_create(listing_id: l.id, category_id: 2)
CategoryListing.find_or_create_by(listing_id: l.id, category_id: 2)
l.reload
l.category_ids
l.categories
cl = CategoryListing.find_or_create(listing_id: l.id, category_id: 2)
cl = CategoryListing.find_or_create_by(listing_id: l.id, category_id: 2)
cl.destroy
c = Category.find(2)
c = Category.where.not(source_id: nil).sample
l.categories << c
l.reload
c.id
l.category_ids
l.categories
c = Category.where.not(source_id: nil, parent_id: nil).sample
c = Category.where.not(parent_id: nil).sample
c.id
l.categories << c
l.reload
c.parent
l.category_ids
c.id
l.categories
CategoryListing.find_by(listing_id: l.id, category_id: 1)
CategoryListing.where(listing_id: 3, category_id: 4)
SourceCategory.first
CategoryListing.where(source_category_id: nil)
SourceCategory.where(source_id: 4, category_id: 1)
SourceCategory.where(source_id: 4, category_id: 2)
SourceCategory.where(source_id: 3, category_id: 3)
SourceCategory.where(source_id: 3)
SourceCategory.where(source_id: 3).size
SourceCategory.where(source_id: 1).size
s = Source.first 4
source = Source.find 3
cats = source.categories
cats = source.source_categories
SourceCategory.where(source_id: 4, category_id: 1)
CategoryListing.where(listing_id: 3, category_id: 4)
l = Listing.find_or_initialize_by(scrape_id: 1, url: url)
l.new_listing?
l.new_record?
data = {:name => 'Jonathan'}
l
l.update_attrubtes data_hash: data
l.update_attrbutes data_hash: data
l.update_attributes data_hash: data
l.destroy
ps = [1,2,3,4]
l = 1
pls = ps.map {|p| {product_id: p, listing_id: l}}
listing
listing = Listing.last
listing
gtin = Identifier.gtin.last
listing.identifiers
listing.identifiers << gtin
listing.reload
listing.identifiers
product = Product.last
listing.prodcuts
listing.products
product
listing.products << prodcut
listing.products << product
listing.reload
listing.products
product.reload
product.listings
listing.products.last
ProductListing.find_by(product_id: product.id, listing_id: listing.id)
ProductListing.find_by(product_id: product.id, listing_id: listing.id).destroy
p
l
p
listing
listing.call(:products) << p
listing = Listing.first
product = Product.last
listing.send(:products)
listing.send(:products) << product
exit
Scrape.first.listings
listings.size
s = Scrape.first
s.listings
s.listings.size
s.products.size
exit
s = Scrape.first
s.listings.size
s.products.size
exit
s = Scrape.first
s.listings.size
s.products.size
ScrapeListingSeller.size
ScrapeListingSeller.count
Listing.count
ScrapeListingSeller.all.uniq(:seller_id).size
ProductScreenshot.first
exit
ListinScreenshot.first
ListingScreenshot.first
Asset.where(type: 'ProductScreenshot').first
exit
Asset.where(type: 'ProductScreenshot').first
exit
ProductScreenshot.first
ProductScreenshot.count
ProductScreenshot.all.pluck(:url).uniq.size
ProductScreenshot.pluck(:url).limit(100).uniq.size
ProductScreenshot.limit(100).pluck(:url).uniq
urls = _
lisitngs = Listing.where(url: urls)
listings.count
listings.size
lisitngs = Listing.where(url: urls)
listings.class
listings = Listing.where(url: urls)
listings.size
listings.where(scrape_id: 1)
screenshots = ProductScreenshot.limit(100)
screenshots.map(&:scrape_id)
screenshots.map(&:scrape_id).uniq
l = Listing.first
listing.screenshots
l.screenshots
ScrapeProductSeller.first
ScrapeProductSeller.where.not(product_source_path: nil).first
exit
Listing.where("data_hash like '%THX%'").size
OtherProductDetail.where("data_hash like '%thx%'").size
OtherProductDetail.where("data_hash like '%THX%'").size
OtherProductDetail.where("data_hash like '%THX%'")
Listing.where("data_hash like '%THX%'").size
Listing.where("data_hash like '%THX%'")
Listing.where("data_hash like '%THX%'").size
Listing.where("data_hash like '%THX%'").pluck(:name, :data_hash)
eixt
exit
Listing.where("data_hash like '%THX%'").pluck(:name, :data_hash)
Listing.where("data_hash like '%THX%'")
p = Product.find 95362
p.original_url
p.scrape_product_sellrs
p.scrape_product_sellers
p.scrapes
s = Scrape.where(source_id: 1).last
s = Scrape.where(source_id: 1).last(10)
s = Scrape.where(source_id: 1).last(45)
s = Scrape.find 3
p = s.products.first
p.sellers
p.scrape_product_sellers
p.sellers.count
Source.find(1).scrape_ids
ids = _
Listing.where(scrape_id: ids).where("data_hash like '%product_features%'")
Listing.where(url: 'https://www.google.com/shopping/product/11416968227283457572/specs')
Listing.where(url: 'http://www.google.com/shopping/product/11416968227283457572/specs')
Listing.where("data_hash.product_description == data_hash.product_features")
Listing.where("data_hash.product_description == data_hash.product_features").size
Listing.where("data_hash.product_description = data_hash.product_features").size
Listing.where("product_description = product_features").size
exit
Product.count
OtherProduct.count
ScrapeProductSellers.count
exit
Sidekiq::Queue.new('default').size
Sidekiq::Queue.new('default').clear
exit
Sidekiq::Queue.new('product_listing_queue').size
Sidekiq::Queue.new('product_listing_queue').clear
s = Scrape.last
def stats(s)
  s.reload
  p = s.products.size
  l = s.listings.size
  time = (s.duration || Time.now - s.created_at).to_f / 60
  rate = l / time
  full_scrape = (500000 / rate / 60 / 24)
  puts "Scrape id: #{s.id} Source: #{s.source_id}"
  puts "Ratio: #{p}/#{l} products/listing"
  puts "#{((p.to_f / l) * 100).round(2)} percent found Products"
  puts "Time: #{time.round(2)}mins Rate: #{rate.round(2)} created: #{s.created_at.strftime("%m/%d/%Y")}"
  puts "#{full_scrape.round(2)} days for full scrape"
  puts '=============================='
  nil
end
stats s
def stats(s)
  s.reload
  p = s.listings.inject(:+) {|l| l.products.size}
  l = s.listings.size
  time = (s.duration || Time.now - s.created_at).to_f / 60
  rate = l / time
  full_scrape = (500000 / rate / 60 / 24)
  puts "Scrape id: #{s.id} Source: #{s.source_id}"
  puts "Ratio: #{p}/#{l} products/listing"
  puts "#{((p.to_f / l) * 100).round(2)} percent found Products"
  puts "Time: #{time.round(2)}mins Rate: #{rate.round(2)} created: #{s.created_at.strftime("%m/%d/%Y")}"
  puts "#{full_scrape.round(2)} days for full scrape"
  puts '=============================='
  nil
end
stats s
(1..3).each {|i| puts i}
stats s
s.listings.map(&:product_ids).uniq.size
s.listings.size
def stats(s)
  s.reload
  p = s.listings.map(&:product_ids).uniq.size
  l = s.listings.size
  time = (s.duration || Time.now - s.created_at).to_f / 60
  rate = l / time
  full_scrape = (500000 / rate / 60 / 24)
  puts "Scrape id: #{s.id} Source: #{s.source_id}"
  puts "Ratio: #{p}/#{l} products/listing"
  puts "#{((p.to_f / l) * 100).round(2)} percent found Products"
  puts "Time: #{time.round(2)}mins Rate: #{rate.round(2)} created: #{s.created_at.strftime("%m/%d/%Y")}"
  puts "#{full_scrape.round(2)} days for full scrape"
  puts '=============================='
  nil
end
stats s
(5..1).each {|i| puts i}
(5..1).to_a.each {|i| puts i}
(1..5).to_a.each {|i| puts i}
(1..5).each {|i| puts i}
(1..5).reverse.each {|i| puts i}
(1..5).to_a.reverse.each {|i| puts i}
sleep 0.5; puts 'hi'
sleep 0.74; puts 'hi'
0.6 * 5
0.55 * 5
url = 'http://www.amazon.com/VIZIO-M65-C1-65-Inch-Ultra-Smart/dp/B00T63YUDK/'
Listing.find_by url: url
url.size
l = Listing.find_by url: url
orig_url = l.original_url
name = l.name
data = l.data_hash
l.update_attributes(url: url, original_url: orig_url, name: name, detail_hash: data)
l.update_attributes(url: url, original_url: orig_url, name: name, data_hash: data)
l = Listing.find_or_initialize_by scrape_id: 1, url: url
l.class
l.new_record?
l = Listing.find_by scrape_id: 1, url: url
l.exists?
l.persisted?
l = Listing.find_by scrape_id: 3, url: url
l = Listing.find_by scrape_id: 1, url: url
l.present?
l = Listing.find_by scrape_id: 3, url: url
l.present?
l = Listing.find_or_initialize_by scrape_id: 3, url: url
l.persited?
l.persisted?
l = Listing.find_or_initialize_by scrape_id: 1, url: url
l.persisted?
Listing.last
l = _
l.products
s.products
l
l.images
l.images.size
l.screenshots
l.screenshots.size
Listings.where.not(product_description: nil).size
Listing.where.not(product_description: nil).size
Listing.count
18 / 51.to_f
url = 'http://www.amazon.com/VTech-CS6719-2-Waiting-Cordless-Handsets/dp/B00FRSYS12/'
url.length
Listing.find_by(url: url)
Listing.find_by(url: 'http://www.amazon.com/VTech-CS6719-2-Waiting-Cordless-Handsets')
url = 'http://www.amazon.com/ARRIS-SURFboard-SB6141-DOCSIS-Cable/dp/B00AJHDZSI/'
Listing.find_by(url: url)
url = 'http://www.amazon.com/ARRIS-SURFboard-SB6141-DOCSIS-Cable/dp/B00AJHDZSI'
Listing.find_by(url: url)
stats s
exit
url = 'http://www.amazon.com/ARRIS-SURFboard-SB6141-DOCSIS-Cable/dp/B00AJHDZSI/'
Listing.find_by(url: url)
Listing.first
Listing.last
l = _
l.url
url = 'http://www.amazon.com/TP-LINK-Archer-C7-Wireless-1300Mbps/dp/B00BUSDVBQ/ref=lp_300189_1_1/189-0252078-0471362?s=pc&ie=UTF8&qid=1458153318&sr=1-1'
Listing.find_by(url: url)
l = _
l.id
Listing.each do |l|
  l.update_attributes url: l.url.gsub(/(ref=).+$/, '')
end
Listing.all.each do |l|
  l.update_attributes url: l.url.gsub(/(ref=).+$/, '')
end
Listing.all.each do |l|
  begin
    l.update_attributes url: l.url.gsub(/(ref=).+$/, '')
  rescue
Listing.all.each do |l|
  begin
    l.update_attributes url: l.url.gsub(/(ref=).+$/, '')
  rescue ActiveRecord::RecordNotUnique
    l.destroy
  end
end
Listing.all.map(&:url)
l = Listing.first
l
l.identifiers
Identifier.count
ListingIdentifier
ListingIdentifier.count
l = Listing.last
l.identifiers
Identifier.first
l = Listing.first
id = Identifier.asin.find_by(uniq_id: 'B00INEG8W4')
id.listings
l.data_hash
id
l.identifiers << id
l.reload
l.identifiers
Listing.all.map(|l| l.identifiers.size < 1)
Listing.all.map {|l| l.identifiers.size < 1}
Listing.all.select {|l| l.identifiers.size < 1}
Listing.all.select {|l| l.identifiers.size < 1}.map(&:name)
Product.all.map(&:model).uniq
Product.all.map(&:model).uniq.size
Product.count
Product.all.map(&:brand).uniq.size
Product.all.map(&:brand)
Asset.find 741
s = Scrape.last
Scrape
exit
manual = ProductManual.first
manual.product
manual
manual.text
manual.product.listings
manual.product.listing
manual.product.listings
manual.assetable
Assetable
Asset
Asset.where(type: 'ProductManual')
Asset.where(type: 'ProductManual').map(&:assetable_type)
Asset.where(assetable_type: 'Listing')
Asset.where(type: 'ProductManual', assetable_type: 'Listing').map(&:assetable_type)
Asset.where(type: 'ProductManual', assetable_type: 'Listing')
Asset.where(type: 'ProductManual', assetable_type: 'Listing').each do |pm|
  pm.update_attributes assetable_type: 'Product', assetable: pm.assetable.products.first
end
Asset.where(type: 'ProductManual', assetable_type: 'Listing')
ProductManual.first
ProductManual.first.product
s = Scrape.last
stats s
def stats(s)
  s.reload
  p = s.listings.map(&:product_ids).uniq.size
  l = s.listings.size
  time = (s.duration || Time.now - s.created_at).to_f / 60
  rate = l / time
  full_scrape = (500000 / rate / 60 / 24)
  puts "Scrape id: #{s.id} Source: #{s.source_id}"
  puts "Ratio: #{p}/#{l} products/listing"
  puts "#{((p.to_f / l) * 100).round(2)} percent found Products"
  puts "Time: #{time.round(2)}mins Rate: #{rate.round(2)} created: #{s.created_at.strftime("%m/%d/%Y")}"
  puts "#{full_scrape.round(2)} days for full scrape"
  puts '=============================='
  nil
end
stats s
ProductManual.find 911
pm = _
pm.text
text = _
ManualText.last
exit
pm = ProductManual.find 911
pm.product.brand
pm.product.model
ENV['LIMIT_MANUALS']
pm
pm.destroy
pm = ProductManual.all.sample
pm.product
p.manuals
p = Product.find 13
p.manuals
Asset.TYPES
Asset.where(type: 'ProductManual')
Asset.where(type: 'ProductManual').map(&:assetable_type)
Asset.where(type: 'ProductManual').map(&:assetable_type).uniq
s
s = Scrape.last
s.scrape_listing_sellers.size
Listing.count
s.scrape_listing_sellers.distinct.count :listing_id
s.scrape_listing_sellers.pluck(:listing_id)
s.scrape_listing_sellers.pluck(:listing_id).uniq.count
Listing.count
Scrape.listings.count
s.listings.size
Product.includes(:listings).where('listings.scrape_id': 1).size
Product.count
exit
s = Scrape.last
s.products.size
Product.count
s.listings.size
Listing.count
s.sellers.size
ScrapeListingSeller.count
ScrapeListingSeller.all.map(&:seller_id)
ScrapeListingSeller.all.map(&:seller_id).size
ScrapeListingSeller.all.map(&:seller_id).uniq.size
Product
p = Product.all.sample
p.as_indexed_json
exit
s = Scrape.last
s.sellers
s.products
s.scrape_listing_sellers.size
s.scrape_listing_sellers.count('distinct seller_id')
s.scrape_listing_sellers.count('distinct listing_id')
s
s.running?
s.running
s.visible?
s.running?
s.attributes['num_sellers']
s
exit
Sidekiq::Queue.new('product_attribute_queue').clear
Sidekiq::Queue.new('product_detail_queue').clear
Sidekiq::Queue.new('product_manual_search_queue').clear
Sidekiq::Queue.new('sellers_queue').clear
exit
s = Scrape.last
s.num_products
def stats(s)
  s.reload
  p = s.products.size
  op = s.other_products.size
  total = p + op
  time = (s.duration || Time.now - s.created_at).to_f / 60
  rate = total / time
  full_scrape = (500000 / rate / 60 / 24)
  puts "Scrape id: #{s.id} Source: #{s.source_id}"
  puts "Ratio: #{p}/#{op} Total: #{total}"
  puts "#{((p.to_f / total) * 100).round(2)} percent Products"
  puts "Time: #{time.round(2)}mins Rate: #{rate.round(2)} created: #{s.created_at.strftime("%m/%d/%Y")}"
  puts "#{full_scrape.round(2)} days for full scrape"
  puts "id: #{s.id} Source: #{s.source_id} Total: #{total} Rate: #{rate.round(2)} created: #{s.created_at.strftime("%m/%d/%Y")}"
  puts '=============================='
  nil
end
def stats(s)
  s.reload
  p = s.listings.map(&:product_ids).uniq.size
  l = s.listings.size
  time = (s.duration || Time.now - s.created_at).to_f / 60
  rate = l / time
  full_scrape = (500000 / rate / 60 / 24)
  puts "Scrape id: #{s.id} Source: #{s.source_id}"
  puts "Ratio: #{p}/#{l} products/listing"
  puts "#{((p.to_f / l) * 100).round(2)} percent found Products"
  puts "Time: #{time.round(2)}mins Rate: #{rate.round(2)} created: #{s.created_at.strftime("%m/%d/%Y")}"
  puts "#{full_scrape.round(2)} days for full scrape"
  puts '=============================='
  nil
end
stats s
Sidekiq::Queue.new('product_listing_queue').clear
stats s
key = ProductAttributeKey.find_by(name: 'Teilenummer')
stats s
s.products.size
stats s
s.listings.size
ProductListing.includes(:listings).where('listings.scrape_id': s.id).size
ProductListing.includes(:listing).where('listings.scrape_id': s.id).size
ProductListing.includes(:listing).where('listings.scrape_id': s.id).count('distinct product_id')
ProductListing.includes(:listing).where('listings.scrape_id': s.id).count('distinct product_listings.product_id')
Product.count
stats s
stats Scrape.first
Sidekiq::Queue.new('product_detail_queue').clear
Seller.last 100
s.listings.map(&:seller_id).uniq.count
s.products
s.num_products
s.sellers
exit
s = Scrape.last
s.products.size
s.sellers.size
s.listings.size
s.products.uniq.size
def stats(s)
  s.reload
  p = s.listings.map(&:product_ids).uniq.size
  l = s.listings.size
  time = (s.duration || Time.now - s.created_at).to_f / 60
  rate = l / time
  full_scrape = (500000 / rate / 60 / 24)
  puts "Scrape id: #{s.id} Source: #{s.source_id}"
  puts "Ratio: #{p}/#{l} products/listing"
  puts "#{((p.to_f / l) * 100).round(2)} percent found Products"
  puts "Time: #{time.round(2)}mins Rate: #{rate.round(2)} created: #{s.created_at.strftime("%m/%d/%Y")}"
  puts "#{full_scrape.round(2)} days for full scrape"
  puts '=============================='
  nil
end
stats s
s.listings.map(&:product_ids)
s.listings.map(&:product_ids).flatten.uniq.size
def stats(s)
  s.reload
  p = s.listings.map(&:product_ids).flatten.uniq.size
  l = s.listings.size
  time = (s.duration || Time.now - s.created_at).to_f / 60
  rate = l / time
  full_scrape = (500000 / rate / 60 / 24)
  puts "Scrape id: #{s.id} Source: #{s.source_id}"
  puts "Ratio: #{p}/#{l} products/listing"
  puts "#{((p.to_f / l) * 100).round(2)} percent found Products"
  puts "Time: #{time.round(2)}mins Rate: #{rate.round(2)} created: #{s.created_at.strftime("%m/%d/%Y")}"
  puts "#{full_scrape.round(2)} days for full scrape"
  puts '=============================='
  nil
end
stats s
s.listings.select {
s.listings.select {|l| l.product_ids.empty?}
listings = _
listings.size
def stats(s)
  s.reload
  p = s.listings.map(&:product_ids).flatten.uniq.size
  l = s.listings.size
  no_product = s.listings.select{|l| l.product_ids.empty?}.size
  time = (s.duration || Time.now - s.created_at).to_f / 60
  rate = l / time
  full_scrape = (500000 / rate / 60 / 24)
  puts "Scrape id: #{s.id} Source: #{s.source_id}"
  puts "Ratio: #{p}/#{l} products/listing"
  puts "Listings without product: #{no_product}"
  puts "#{((p.to_f / l) * 100).round(2)} percent found Products"
  puts "Time: #{time.round(2)}mins Rate: #{rate.round(2)} created: #{s.created_at.strftime("%m/%d/%Y")}"
  puts "#{full_scrape.round(2)} days for full scrape"
  puts '=============================='
  nil
end
stats s
s.sellers.first 100
s.scrape_listing_sellers
url = '/aclk?sa=l&ai=C8_EOuObpVufRE4aF-gPP1bugDdiRpvYFwKbgrJgBgIiTzaUCCAkQAiDWzs8eKBlgyfb4hsijoBmgAaKP8PgDyAEHqgQZT9BOdHqfT3Sfvx4rMIPE6nFIJeaCLBWy-8AFBaAGJoAH0Lf5J4gHAZAHAqgHpr4b2AcB4BKjp7KiiMXpkSA&sig=AOD64_3QBgB6TVmqZDNgNZ7xkK8IskcS7A&ctype=5&clui=3&q=&ved=0ahUKEwjI5YbmqMbLAhVCy2MKHXLwCD0Q2ikIMg&adurl=http://walmart.com/ip/36126031%3Fwmlspartner%3Dwlpa%26selectedSellerId%3D0%26adid%3D22222222227024264742%26wl0%3D%26wl1%3Dg%26wl2%3Dc%26wl3%3D40882806512%26wl4%3D%26wl5%3Dpla%26wl6%3D78813381632%26veh%3Dsem'
CGI.parse(url)['/url?q'].first
CGI.parse(url)['adurl'].first
CGI.parse(url)['adurl'].first.gsub(/\?.+$/, '')
ScrapeListingSeller.last
ScrapeListingSeller.last.id
id = 5600
sls_id = 5600
url
ScrapeListingSeller.where.not(seller_url: nil)
url
ScrapeListingSeller.last
sls_id = ScrapeListingSeller.last.id
sls_id = ScrapeListingSeller.find(sls_id + 1)
Seller.find(2052)
url = '/aclk?sa=l&amp;ai=CUS5O4e3pVpX6CY2q-gOFoLUIn9XqngjPh4bd8wGM1_n5bwgJEAEg1s7PHigZYMn2-IbIo6AZoAHX--X9A8gBB6oEHE_QSMhxvqsAsScAfxiU4raYs7NxpjPN5JkumvPABQWgBiaAB5GEmgKIBwGQBwKoB6a-G9gHAeASud3Hx4DjxP9t&amp;sig=AOD64_0doAOa7ZzLdDMpHaFIHo1eOheH4w&amp;ctype=5&amp;clui=1&amp;q=&amp;ved=0ahUKEwiE7IHQr8bLAhUW8GMKHbN7AdoQ2ikICg&amp;adurl=http://clickserve.dartsearch.net/link/click%3Flid%3D92700006802738515%26ds_s_kwgid%3D58700000446459106%26ds_e_adid%3D65340778863%26ds_e_product_group_id%3D30052084620%26ds_e_product_id%3D095_MMS015282088%26ds_e_product_merchant_id%3D8400729%26ds_e_product_country%3DUS%26ds_e_product_language%3Den%26ds_e_product_channel%3Donline%26ds_e_product_store_id%3D%26ds_e_ad_type%3Dpla%26ds_s_inventory_feed_id%3D97700000001021517%26ds_url_v%3D2%26ds_dest_url%3Dhttp://link.mercent.com/redirect.ashx%3Fmr:merchantID%3DEFollett%26mr:trackingCode%3D1AE08503-5982-E411-B04B-90E2BA285E75%26mr:targetUrl%3Dhttp://www.bkstr.com/ProductDisplay%253furlRequestType%253dBase%2526catalogId%253d10001%2526productId%253d73132232%2526errorViewName%253dProductDisplayErrorView%2526langId%253d-1%2526storeId%253d10210%2526demoKey%253dd%2526cm_mmc%253dRisePaidSearch-_-ComparativeShopping-_-Google-_-Store%25252095%2526source%253dAdWords%2526adcampaigngroup%253dGMShopping_General%2526adcampaign%253dnull%2526adgroup%253dnull%26mr:device%3Dc%26mr:adType%3Dplaonline%26mr:ad%3D65340778863%26mr:keyword%3D%26mr:match%3D%26mr:tid%3Dpla-30052084620%26mr:ploc%3D9031942%26mr:iloc%3D%26mr:store%3D%26mr:filter%3D30052084620'
CGI.parse(url)['/url?q'].first
CGI.parse(url)['adurl'].first
sls_id = ScrapeListingSeller.last.id
ScrapeListingSeller.last
sls_id
ScrapeListingSeller.last
sls_id
ScrapeListingSeller.where('id > 6489').where(seller_url: nil).size
Sidekiq::Queue.new('product_manuall_search_queue').clear
Sidekiq::Queue.new('product_manual_search_queue').clear
s = Scrape.last
s.running = false
s.save
s.reload
s.num_products
s.products
s.products.size
158 - 139
19000 / 8
139000 / 2375
139000 / 2375 / 24
exit
OtherProduct
OtherProduct.connection
OtherProduct
OtherProduct.count
OtherProduct.first
op = _
op.details
op.listings
op.id
OtherProduct.count
OtherProduct.last
ScrapeOtherProduct.count
ScrapeOtherProduct.first
OtherProduct.destroy_all
OtherProductSeller.count
ScrapeOtherProduct.count
exit
l = Listing.first(10235).last
l = _
l.scrape_listing_sellers.size
l.scrape_listing_sellers.first
l.scrape_listing_sellers.where(scrape_id: 1).first
ScrapeListingSeller.where(scrape_id: 1)
arr = [1,2,3,4]
arr.delete(2)
arr
Listing
Listing.find(arr)
Listing.find(arr).map(&:url)
Listing.find(arr).map(&:class)
p
p = Product.first
p = Product.includes(:scrape_product_sellers).find(1)
p.scrape_product_sellers
ScrapeProductSellers
p.scrape_listing_sellers
exit
p = Product.first
p.scrape_product_sellers
ScrapeProductSellers.first
ScrapeProductSeller.first
ScrapeListingSeller.first
l = Listing.find 3
l.scrape_listing_sellers
l.scrape_listing_sellers.size
Listing.all.first {|l| l.scrape_listing_sellers.size > 1}
Listing.all.first {|l| l.product_id && l.scrape_listing_sellers.size > 1}
Listing.all.first {|l| l.product_id.present? && l.scrape_listing_sellers.size > 1}
Listing.all.select {|l| l.product_id.present? && l.scrape_listing_sellers.size > 1}
Listing.all.select {|l| l.scrape_listing_sellers.size > 1}
_.select {|l| l.product_id.present? }
Listing.all.select {|l| l.scrape_listing_sellers.size > 1}
l = Listing.find 3
l = Listing.find 2
l.scrape_listing_sellers
l.scrape_listing_sellers.size
l = Listing.includes(:scrape_listing_sellers).find(2)
l
l.scrape_listing_sellers.size
l.scrape_listing_sellers
l.scrape_listing_sellers.map(&:id)
ids = [329, 330, 331, 332, 333]
ScrapeListingSeller.find(ids).each(&:delete)
l.scrape_listing_sellers
l.scrape_listing_sellers.size
l.scrape_listing_sellers.reload
l.scrape_listing_sellers.size
l.scrape_listing_sellers.where(scrape_id: 1)
Listing.count
Product.coutn
Product.count
Listing.find(1,3,4,5)
Listing.find(1,3,4,5).class
Listing.find(1).class
Listing.where(id: 1).each {}
Listing.find(1).each {}
100000 / 80
1250 / 60
exit
Scrape.last
Scrape.listings.size
s = Scrape.last
s.listings
s.listings.size
s.products
s.sellers
s.sellers.size
s.sellers.uniq.size
id = Identifier.last
id.products
id
p = Product.last
p.identifiers
exit
FactoryGirl.create :product, :with_scrape
exit
FactoryGirl.create :product, :with_scrape
exit
p = FactoryGirl.create(:product, :with_scrape)
p.valid?
p.scrapes
p.listings
exit
p = Product.first
p.as_indexed_json
exit
p = Product.first
p.as_indexed_json
p.scrapes
exit
p = Product.first
p.as_indexed_json
exit
p.as_indexed_json
p = Product.first
p.as_indexed_json
Product.search_by('', scrape_id: 1)
Product.search_by('', scrape_id: 1).results.total
Products.count
Product.count
Product.import
reload!
exit
Product.import
exit
Product.import
exit
Product.import
exit
Product.search_by('', scrape_id: 1).results.total
Product.delete_index!
Product.create_index!
Product.import
Product.search_by('', scrape_id: 1).results.total
Product.search_by('', scrape_id: 1).results
Product.search_by('', scrape_id: 1).response
Scrape.find(1).products.size
Scrape.find 1
s = _
s.listings
s.listings.size
Product.search_by('', scrape_id: 1).results.total
Product.search_by('', scrape_id: 1).response
exit
Product.delete_index
Product.delete_index!
Product.import refresh: true, force: true
Product.search_by('', scrape_id: 1).response
Product.search_by('', scrape_id: 1).results.total
exit
s = FactoryGirl.create(:scrape)
listings = FactoryGirl.create_list(:listing, 10, scrape: s)
s.listings
products = FactoryGirl.create_list(:product, 10)
sellers = FactoryGirl.create_list(:seller, 10)
sellers.count
products.count
listings.count
listings.each_with_index do |l, idx|
  FactoryGirl.create(:product_listing, product: products[idx], listing: l)
  FactoryGirl.create(:scrape_listing_seller, scrape: s, listing: l, seller: sellers[idx])
end
s.products
s.products.count
Product.import
Product.search_by('', scrape_id: s.id).results.total
s.products
s.listings
p = products.first
p.as_indexed_json
p.index_document
Product.search_by('', scrape_id: s.id).results.total
Product.prepare_records(products)
Product.search_by(nil, scrape_id: s.id).results.total
products.each(&:update_document)
Product.search_by('', scrape_id: s.id).results.total
p = products.first
p.update_document
p.valid?
p
p.listigns
p.listings
p.as_indexed_json
p.__elasticsearch__.index_document
exit
s = FactoryGirl.create(:scrape)
listings = FactoryGirl.create_list(:listing, 10, scrape: s)
products = FactoryGirl.create_list(:product, 10)
itkqq
s = FactoryGirl.create(:scrape)jjasldkfj
exit
s = FactoryGirl.create(:scrape)
listings = FactoryGirl.create_list(:listing, 10, scrape: s)
products = FactoryGirl.create_list(:product, 10)
sellers = FactoryGirl.create_list(:seller, 10)
listings.each_with_index do |l, idx|
  FactoryGirl.create(:product_listing, product: products[idx], listing: l)
  FactoryGirl.create(:scrape_listing_seller, scrape: s, listing: l, seller: sellers[idx])
end
s.products.count
Product.import
products.last.id
Product.search_by('', scrape_id: s.id).results.total
p = products.last
p.__elasticsearch__.index_document
Product.delete_index!
Product.create_index!
Product.search_by('', scrape_id: 1).results.total
Product.import
Product.search_by('', scrape_id: 1).results.total
Product.search_by('', scrape_id: s.id).results.total
l = listings.first
l.valid?
l.slice(:product_features)
exit
s = FactoryGirl.create(:scrape)
listings = FactoryGirl.create_list(:listing, 10, scrape: s)
products = FactoryGirl.create_list(:product, 10)
sellers = FactoryGirl.create_list(:seller, 10)
listings.each_with_index do |l, idx|
  FactoryGirl.create(:product_listing, product: products[idx], listing: l)
  FactoryGirl.create(:scrape_listing_seller, scrape: s, listing: l, seller: sellers[idx])
end
s.products
Product.delete_index!
Product.create_index!
p = products.first
p.as_indexed_json
p.as_indexed_json['listings']['product_features']
p.as_indexed_json.class
p.as_indexed_json['listings'][0]['product_features']
p.as_indexed_json
p.as_indexed_json['listings'][0]['product_description']
p.as_indexed_json['listings'][0]['product_features']
reload!
p.reload
p.as_indexed_json['listings'][0]['product_features']
exit
products = FactoryGirl.create_list(:product, 10)
listings = FactoryGirl.create_list(:listing, 10, scrape: s)
s = FactoryGirl.create(:scrape)
listings = FactoryGirl.create_list(:listing, 10, scrape: s)
sellers = FactoryGirl.create_list(:seller, 10)
edit -t
s.products.size
p = products.first
p.as_indexed_json
p.__elasticsearch__.index_document
Product.delete_index!
Product.create_index!
Product.import
Product.search_by('', scrape_id: 1).results.total
Product.search_by('', scrape_id: s.id).results.total
listings.first
names = Faker::Lorem.words(2)
Faker::Commerce.product_name.split(' ')
_ | names
category = FactoryGirl.create(:category)
names
brand = 'FamousBrand'
text = 'abcdefghij'
manual = ProductManual.create status: 1
manual.text = text
manual.text
manual.manual_text
ManualText
ManualText.create(product_manual_id: manual.id, text: text)
manual.text
manual.manual_text
manual
manual = ProductManual.create status: 1
manual.valid?
manual.errors
products.first.manuals << manual
manual
ManualText.create(product_manual_id: manual.id, text: text)
manual.text
manual.confirmed
manual.status
product.manuals.confirmed
p = products.first
p.manuals.confirmed
edit -t
manual.__elasticsearch__.index_document parent: manual.assetable_id
category.products.count
category.products.map { |p| p.__elasticsearch__.index_document(refresh: true) }
Product.search_by(names.join(' '), scrape_id: scrape.id, category_ids: [category.id]).results.total
Product.search_by('', scrape_id: scrape.id, category_ids: [category.id]).results.total
reload
reload!
Product.delete_index!
Product.create_index!
Product.import
Product.search_by('', scrape_id: scrape.id, category_ids: [category.id]).results.total
Product.search_by(names.join(' '), scrape_id: scrape.id, category_ids: [category.id]).results.total
names
reload!
Product.search_by(names.join(' '), scrape_id: scrape.id, category_ids: [category.id]).results.total
reload!
Product.search_by(names.join(' '), scrape_id: scrape.id, category_ids: [category.id]).results.total
Product.search_by(names.join(' '), scrape_id: scrape.id, category_ids: [category.id]).results
Product.search_by(names.join(' '), scrape_id: scrape.id, category_ids: [category.id]).response
listings.map &:name
named_listings.map &:name
Product.search_by(names.join(' '), scrape_id: scrape.id, category_ids: [category.id]).response.ids
Product.search_by(names.join(' '), scrape_id: scrape.id, category_ids: [category.id]).response
Product.search_by(names.join(' '), scrape_id: scrape.id, category_ids: [category.id]).products
Product.search_by(names.join(' '), 'AND', scrape_id: scrape.id, category_ids: [category.id]).response
Product.search_by(names.join(' '), 'AND', scrape_id: scrape.id, category_ids: [category.id])
Product.search_by(names.join(' '), {scrape_id: scrape.id, category_ids: [category.id]}, 1, nil, 'AND")
Product.search_by(names.join(' '), {scrape_id: scrape.id, category_ids: [category.id]}, 1, nil, 'AND')
Product.search_by(names.join(' '), {scrape_id: scrape.id, category_ids: [category.id]}, 1, nil, 'AND").results.total
Product.search_by(names.join(' '), {scrape_id: scrape.id, category_ids: [category.id]}, 1, nil, 'AND').results.total
Product.search_by(names.join(' '), {scrape_id: scrape.id, category_ids: [category.id]}, 1, nil, 'AND').results
Product.search_by(names.join(' '), {scrape_id: scrape.id, category_ids: [category.id]}, 1, nil, 'OR').results
Product.search_by(names.join(' '), {scrape_id: scrape.id, category_ids: [category.id]}, 1, nil, 'OR').results.total
named_listings.size
branded_listings.size
Product.search_by(brand, {scrape_id: scrape.id, category_ids: [category.id]}, 1, nil, 'OR').results.total
Product.search_by(brand, {scrape_id: scrape.id, category_ids: [category.id]}).results.total
Product.search_by('', {scrape_id: scrape.id, category_ids: [category.id]}).results.total
products.flat_map {|p| p.listings.map(&:name)}
(named_products | braned_products | products).flat_map {|p| p.listings.map(&:name)}
(named_products | branded_products | products).flat_map {|p| p.listings.map(&:name)}
branded_products.map(&:model)
category.listings
category.listings.count
category.listings.uniq.count
exit
Product.delete_index@
Product.delete_index!
Product.create_index!
edit -t
terms = { keywords: names.join(' ') }
operator = 'OR'
filters = {scrape_id: scrape.id, category_ids: [category.id]}
query = Product.generate_query_by_keywords(terms, operator, filters)
query
listings = (named_listings | branded_listings)
listings.map(&:name)
query
Product.search(query).results.total
terms = { keywords: brand }
operator = 'OR'
filters = {scrape_id: scrape.id, category_ids: [category.id]}
query = Product.generate_query_by_keywords(terms, operator, filters)
query
Product.search(query).results.total
terms = { brand: brand }
operator = 'OR'
filters = {scrape_id: scrape.id, category_ids: [category.id]}
query = Product.generate_query_by_keywords(terms, operator, filters)
Product.search(query).results.total
query
terms = { brand: brand }
operator = 'AND'
filters = {scrape_id: scrape.id, category_ids: [category.id]}
query = Product.generate_query_by_keywords(terms, operator, filters)
Product.search(query).results.total
query
query = Product.generate_query_by_keywords(terms, operator, filters)        terms = { keywords: brand, brand: brand }
operator = 'AND'
filters = {scrape_id: scrape.id, category_ids: [category.id]}
query = Product.generate_query_by_keywords(terms, operator, filters)
terms = { keywords: brand, brand: brand }
operator = 'AND'
filters = {scrape_id: scrape.id, category_ids: [category.id]}
query = Product.generate_query_by_keywords(terms, operator, filters)
Product.search(query).results.total
query
terms = { keywords: '', brand: brand }
operator = 'AND'
filters = {scrape_id: scrape.id, category_ids: [category.id]}
query = Product.generate_query_by_keywords(terms, operator, filters)
Product.search(query).results.total
terms = { keywords: '', brand: brand }
operator = 'OR'
filters = {scrape_id: scrape.id, category_ids: [category.id]}
query = Product.generate_query_by_keywords(terms, operator, filters)
Product.search(query).results.total
terms = { keywords: brand, brand: brand }
operator = 'OR'
filters = {scrape_id: scrape.id, category_ids: [category.id]}
query = Product.generate_query_by_keywords(terms, operator, filters)
Product.search(query).results.total
terms = { keywords: names.join(' ') }
operator = 'AND'
filters = {scrape_id: scrape.id, category_ids: [category.id]}
query = Product.generate_query_by_keywords(terms, operator, filters)
Product.search(query).results.total
terms = { keywords: names.join(' ') }
operator = 'OR'
filters = {scrape_id: scrape.id, category_ids: [category.id]}
query = Product.generate_query_by_keywords(terms, operator, filters)
Product.search(query).results.total
Faker::Lorem.words(2)
names = Faker::Lorem.words(2).join(' ')
Faker::Commerce.product_name.split(' ')
_ | names
_ | [names]
listings
listings.map(&:name)
names = Faker::Lorem.words(2)
[Faker::Commerce.product_name,  names.join(' ')].shuffle.join(' ')
terms = { keywords: short_name, brand: brand }
operator = 'AND'
filters = { scrape_id: scrape.id, category_ids: [category.id] }
query = Product.generate_query_by_keywords(terms, operator, filters)
brand
terms = { keywords: short_name, brand: brand }
operator = 'AND'
filters = { scrape_id: scrape.id, category_ids: [category.id] }
query = Product.generate_query_by_keywords(terms, operator, filters)
brand
terms
terms = { keywords: short_name, brand: brand }
operator = 'AND'
filters = { scrape_id: scrape.id, category_ids: [category.id] }
query = Product.generate_query_by_keywords(terms, operator, filters)
terms
terms = { keywords: short_name, brand: brand }
short_name = "et in"
terms = { keywords: short_name, brand: brand }
operator = 'AND'
filters = { scrape_id: scrape.id, category_ids: [category.id] }
query = Product.generate_query_by_keywords(terms, operator, filters)
Product.search(query).results
Product.search(query).results.size
Product.search(query).response
exit
[].nil?
exit
args = terms: {keywords: ''}, filters: {scrape_id: scrape.id}
args = {terms: {keywords: ''}, filters: {scrape_id: scrape.id}}
args = {terms: {keywords: ''}, filters: {scrape_id: 9}}
Product.generate_query_by_keywords(terms: {keywords: ''}, filters: {scrape_id: 9})
reload!
Product.generate_query_by_keywords(terms: {keywords: ''}, filters: {scrape_id: 9})
reload!
Product.generate_query_by_keywords(terms: {keywords: ''}, filters: {scrape_id: 9})
reload!
Product.search_by(terms: {keywords: ''}, filters: {scrape_id: 9})
_.as_json
reload!
Product.search_by(terms: {keywords: ''}, filters: {scrape_id: 9})
reload!
Product.connection
Product.search_by(terms: {keywords: ''}, filters: {scrape_id: 9})
Product.search_by(terms: {keywords: ''}, filters: {scrape_id: 9}).results.size
exit
Product.search_by(terms: {keywords: ''}, filters: {scrape_id: 9}).results.size
Product.search_by(terms: {keywords: ''}, filters: {scrape_id: 9})
reload!
Product.search_by(terms: {keywords: ''}, filters: {scrape_id: 9})
exit
Product.search_by(terms: {keywords: ''}, filters: {scrape_id: 9})
Product.connection
Product.search_by(terms: {keywords: ''}, filters: {scrape_id: 9}).results.size
Product.search_by(terms: {keywords: ''}, filters: {scrape_id: 9}).results
Product.search_by(terms: {keywords: ''}, filters: {categories_id: 1}).results
Product.search_by(terms: {keywords: ''}, filters: {category_ids: [1]}).results
Product.search_by(terms: {keywords: ''}, filters: {category_ids: [1]})
Product.search_by(terms: {keywords: ''}, filters: {category_ids: [1]}).size
reload!
Product.search_by(terms: {keywords: ''}, filters: {category_ids: [1]}).size
Product.search_by(terms: {keywords: ''}, filters: {category_ids: [1]})
Product.search_by(terms: {keywords: ''}, filters: {scrape_id: 1})
reload!
Product.search_by(terms: {keywords: ''}, filters: {scrape_id: 1})
Product.search_by(terms: {keywords: ''}, filters: {scrape_id: 1}).response
Product.search_by(terms: {keywords: ''}, filters: {scrape_id: 1})
reload!
Product.search_by(terms: {keywords: ''}, filters: {scrape_id: 1})
Product.search_by(terms: {keywords: ''}, filters: {scrape_id: 1}).results.size
eixt
exit
Product.search_by(terms: {keywords: ''}, filters: {scrape_id: 1}).results.size
['', 'hello'].join(' ')
:W
terms = {keywords: '', brand: 'brand'}
terms.values.reject(&:blank?).blank?
terms[:keywords].blank?
exit
Scrapes.where(scrape_id: nil).last
Scrape.where(scrape_id: nil).last
Scrape.where(id: nil).last
exit
p = Product.first
p.listings.flat_map {|l| l.scrape_listing_sellers}
p.listings.flat_map {|l| l.scrape_listing_sellers}.uniq
p.listings.flat_map {|l| l.scrape_listing_sellers}.uniq.size
p.listings.flat_map {|l| l.scrape_listing_sellers}.size
p.listings.flat_map {|l| l.scrape_listing_sellers}.uniq.size
p.listings.flat_map {|l| l.scrape_listing_sellers}.uniq
Scrape.first.name
Scrape.lastt.name
Scrape.last.name
Scrape.last.source_name
Listing.includes(:attribute_keys).order(:desc).find_by(scrape_id: scrape_id)
Listing.includes(:attribute_keys).order(:desc).find_by(scrape_id: 1)
Listing.includes(:attribute_keys).order(id: :desc).find_by(scrape_id: 1)
s = Scrape.first
s.listings.reject {|l| l.product_ids.blank?}
s.listings.includes(:products).reject {|l| l.product_ids.blank?}
s.listings.select {|l| l.product_ids.blank? }
s.listings.select {|l| l.product_ids.blank? }.size
s.listings.select {|l| l.product_ids.blank? }
s.listings.includes(:products).select {|l| l.product_ids.blank? }
s.listings.includes(:products).select {|l| l.product_ids.blank? }.size
s.listings.includes(:products).where.not('products.id': nil).size
s.listings.includes(:products).where.not('products.id': nil).uniq.size
s.listings.includes(:products).where.not('products.id': nil).uniq.('listings.id')
s.listings.includes(:products).where.not('products.id': nil).uniq.(:id)
s.listings.includes(:products).where.not('products.id': nil).uniq(:id)
s.listings.includes(:products).where.not('products.id': nil).uniq(:id).size
s.listings.includes(:products).where.not('products.id': nil).uniq(:id).map(&:product_ids)
s.listings.size
s.listings.includes(:products).where.not('products.id': nil).uniq(:id).map(&:product_ids)
s.listings.includes(:products).where.not('products.id': nil).uniq(:id).map(&:product_ids).size
s.listings.size
s.listings.includes(:products).where('products.id': nil).uniq(:id).map(&:product_ids).size
s.listings.includes(:products).select {|l| l.product_ids.blank? }.size
s.listings.includes(:products).reject {|l| l.product_ids.blank? }.size
s.listings.includes(:products).where('products.id': nil).uniq(:id).map(&:product_ids).size
s.listings.includes(:products).where.not('products.id': nil).uniq(:id).map(&:product_ids).size
scrape.listings.includes(:products).where.not('products.id': nil).uniq(:id)
s.listings.includes(:products).where.not('products.id': nil).uniq(:id)
s.listings.includes(:products).where.not('products.id': nil).uniq.pluck(:id)
s.listings.includes(:products).where.not('products.id': nil).uniq.pluck(:id).size
Listings.includes(:products, sources: :source_type).where('products.id': nil).where('source_types.verified': false).size
Listing.includes(:products, sources: :source_type).where('products.id': nil).where('source_types.verified': false).size
Listing.count
Product.count
625 - 419
Listing.includes(:products, :sources).where('sources.id': 1, 'products.id': nil).size
reload!
source
source = Source.first
source.products.size
source = Source.find(1)
source.products.size
source = Source.find(3)
source.products.size
source
source.listings
source.listings.first.products
source.listings.includes(:products).where('products.id': nil).uniq.size
source.listings.includes(:products).where.not('products.id': nil).uniq.size
source.listings.size
exit
scrapes = FactoryGirl.create_list :scrape, 10
scrapes.size
52.times { FactoryGirl.create_list(:listing, 52, scrape: scrapes.sample) }
scrapes.map {|s| s.listings.size}
52.times { FactoryGirl.create(:listing, scrape: scrapes.sample) }
scrapes.map {|s| s.listings.size}
exit
scrapes = FactoryGirl.create_list :scrape, 10
scrapes.map {|s| s.listings.size}
52.times { FactoryGirl.create(:listing, scrape: scrapes.sample) }
scrapes.map {|s| s.listings.size}
scrapes.reload!
scrapes.map(&:reload)
scrapes.map {|s| s.listings.size}
Listing
.includes(:products, sources: :source_type)
.where('products.id': nil)
.where('source_types.verified': false)
.size
edit -t
exit
Source.destroy_all
FactoryGirl.create :source
SourceTypes.all
SourceType.all
source
source = Source.last
listing = FactoryGirl.create(:listing, scrape: FactoryGirl.create(:scrape))
listing.source
listing.scrape.source
listing.scrape.source.verified?
p = Product.first
keywords = nil
brand = 'Fitbit'
type = 'product'
exit
terms = {keywords: nil.to_s, brand: 'Fitbit'}
Product.search_by(terms: terms, operator: 'AND', filters: {})
Product.search_by(terms: terms, operator: 'AND', filters: {}).results
Product.search_by(terms: terms, operator: 'AND', filters: {}).response
operator = 'AND'
filters = {}
query = Product.generate_query_by_keywords(terms, operator, filters)
Product.search(query, size: 10)
Product.search(query, size: 10).results
Product.search(query, size: 10).response
p = Product.where(brand: 'Fitbit')
p = Product.where(brand: 'Fitbit').size
terms = {keywords: nil.to_s, brand: 'Fitbit'}
Product.search_by(terms: terms, operator: 'AND', filters: {}).results
Product.search_by(terms: terms, operator: 'AND', filters: {}).results.map {|p| p.id}
terms
terms = {:keywords=>"", :brand=>"Fitbit"}
operator: 'AND'
operator = 'AND'
Product.search_by(terms: terms, operator: 'AND', filters: {}).results.size
Product.import
Product.search_by(terms: terms, operator: 'AND', filters: {}).results.size
Product.where(brand: 'Apple').count
Product.count
Listing.count
ids = Product.flat_map {|p| p.listings.pluck(:id)}.uniq
ids = Product.all.flat_map {|p| p.listings.pluck(:id)}.uniq
ids
Listing.where.not(id: ids).size
p = Product.find 16
p.listings
scrape.first
Scrape.first
s = +
s = Scrape.first
s.products.sampel
s.products.sample
Product.where(brand: 'Sony').count
p = Product.find 23
p.listings
p.listings.first
l = _
l.screenshots
no_screenshots = s.listings.select {|l| l.screenshots.empty? }
no_screenshots.size
s.listings.size
no_screenshots.first
Listing.where(data_hash: nil)
exit
scrape = Scrape.first
scrape = Scrape.last
scrape
product_count = scrape.products.size
seller_count = scrape.sellers.size
Product.count
scrape.update_attributes num_sellers: seller_count, num_products: product_count
scrape.reload
scrape.products.pluck(:product_id).uniq
Product
scrape.products.pluck(:product_id).uniq
scrape.products.pluck(:id).uniq
scrape.products.pluck(:id).uniq == scrape.products.pluck(:product_id).uniq
p = Product.first
p.product_id
scrape.products.first.product_id
scrape.products.sample.product_id
pids = scrape.products.pluck(:id).uniq
pids = scrape.products.uniq.pluck(:id)
pids = scrape.products.pluck(:id).uniq
ids
pids = scrape.products.pluck(:id).uniq
pids
pids.count
Category.joins(:products).where('products.id': pids).distinct.pluck(:name)
Category.joins(:products).where('products.id': pids).pluck(:name).uniq
results = model.search_by(terms: { keywords: '' }, filters: { scrape_id: 1,  })
results = Product.search_by(terms: { keywords: '' }, filters: { scrape_id: 1,  })
results.current_page
results = Product.search_by(terms: { keywords: '' }, filters: { scrape_id: 1,  }).results
results.current_page
results = Product.search_by(terms: { keywords: '' }, filters: { scrape_id: 1,  }).results.size
results = Product.search_by(terms: { keywords: '' }, filters: { scrape_id: 1 }).results.size
results = Product.search_by(terms: { keywords: '' }, filters: { scrape_id: 1 }, opt: {size: 400}).results
results = Product.search_by(terms: { keywords: '' }, filters: { scrape_id: 1 }, opts: {size: 400}).results
results = Product.search_by(terms: { keywords: '' }, filters: { scrape_id: 1 }, opts: {size: 400}).results.size
results = Product.search_by(terms: { keywords: '' }, filters: { scrape_id: 1 }, opts: {size: 400}).page(1)
results = Product.search_by(terms: { keywords: '' }, filters: { scrape_id: 1 }, opts: {size: 400}).page(1).size
results = Product.search_by(terms: { keywords: '' }, filters: { scrape_id: 1 }, opts: {size: 400})
results.current_page
results.next_page
results = Product.search_by(terms: { keywords: '' }, filters: { scrape_id: 1 }, opts: {size: 400})
results = Product.search_by(terms: { keywords: '' }, filters: { scrape_id: 1 }, opts: {size: 400}).page(1)
results = Product.search_by(terms: { keywords: '' }, filters: { scrape_id: 1 }, opts: {size: 400}).page(1).results
results = Product.search_by(terms: { keywords: '' }, filters: { scrape_id: 1 }, opts: {size: 400}).page(1).response
exit
Scrape.last
Scrape.count
Scrape.where(source_id: 4)
Source.find 4
Source.find 2
exit
SourceCategories
SourceCategory.all
SourceCategory.where(source_id: 2)
SourceCategory.where(source_id: 2).count
exit
Sidekiq::Queue.new('product_listing_queue').clear
Listing.where(name: nil).size
exit
ConvertProductToListingJob.new.perform(1,1)
p = Scrape.first.products.first
exit
Sidekiq::Queue.new('product_detail_queue').clear
Sidekiq::Queue.new('product_manual_search_queue').clear
Sidekiq::Queue.new('upload_asset_queue').clear
edit -t
text = "<img src="http://g01.s.alicdn.com/kf/HTB1SWXEGpXXXXccXFXXq6xXFXXXf/200026846/HTB1SWXEGpXXXXccXFXXq6xXFXXXf.jpg" alt="0.6 inch LED Alarm Clock Radio Use for Desk,Bedside,Kitchen and hotel room" ori-width="980" ori-height="589">"
text = '<img src="http://g01.s.alicdn.com/kf/HTB1SWXEGpXXXXccXFXXq6xXFXXXf/200026846/HTB1SWXEGpXXXXccXFXXq6xXFXXXf.jpg" alt="0.6 inch LED Alarm Clock Radio Use for Desk,Bedside,Kitchen and hotel room" ori-width="980" ori-height="589">'
text.gsub(/^.+src\=/, '')
text.gsub(/^.+src\=\"/, '')
text.gsub(/^.+src\="/, '')
Nokogiri::HTML(text) { |config| config.noblanks }
img = _
img[:src}
img[:src]
img
img.xpath('//img/@src').map(&:value)
exit
img = 'http://g01.s.alicdn.com/kf/HTB1SWXEGpXXXXccXFXXq6xXFXXXf/200026846/HTB1SWXEGpXXXXccXFXXq6xXFXXXf.jpg'
s = Scrape.last
Scrape
Scrape.last
Scrape.first
eixt
exit
SourceCategory.where(url: 'http://www.alibaba.com/catalogs/products/CID100000323/1z6S')
Category.find 3
url1 = 'http://www.alibaba.com/products/F0/monocrystalline_solar_cells_and_panels/1.html'
url2 = 'http://www.alibaba.com/catalogs/products/CID4499/1'
uri1 = URI.parse(url1)
uri2 = URI.parse(url2)
current_page = uri1.path.split('/').last.scan(/\d+/)
current_page = uri2.path.split('/').last.scan(/\d+/)
uri.path
uri2.path
uri1.path
uri1.path.ends_with?('html')
uri2.path.ends_with?('html')
uri1.path
next_link = "#{uri.scheme}://#{uri.host}#{uri.path.gsub(/\d+\.html/, "#{next_page}.html")}"
next_link = "#{uri1.scheme}://#{uri1.host}#{uri1.path.gsub(/\d+\.html/, "#{next_page}.html")}"
next_page = 2
next_link = "#{uri1.scheme}://#{uri1.host}#{uri1.path.gsub(/\d+\.html/, "#{next_page}.html")}"
uri2.path.ends_with?('html')
uri1.path.ends_with?('html')
next_link = "#{uri.scheme}://#{uri.host}#{uri.path.gsub(/\d+\z/, "#{next_page}.html")}"
next_link = "#{uri2.scheme}://#{uri2.host}#{uri2.path.gsub(/\d+\z/, "#{next_page}.html")}"
uri1
uri1[0]
a.undefined
a.undefined?
a
a.nil?
'eee'.ends_with?('a')
exit
String
String.max
exit
4294967296.bytes
4294967296.bytes / 1000000000
rake -T
exit
Sidekiq::Queue.new('product_detail_queue').size
Sidekiq::Queue.new('product_detail_queue').clear
s = Scrape.last
Product.count
exit
SourceCategory.count
SourceCategory.last
SourceCategory.where(source_id: 2).last
SourceCategory.where(source_id: 2).first 10
SourceCategory.where(source_id: 2).first
url = _.url
uri = URI.parse(url)
scheme, host, path = uri.scheme, uri.host, uri.path
current_page = path.split('/').last.scan(/\d+/)
next_page = current_page ? current_page.to_i + 1 : 2
current_page = path.split('/').last.scan(/\d+/).first
next_page = current_page ? current_page.to_i + 1 : 2
if uri.path.ends_with?('html')
  next_link = "#{scheme}://#{host}#{path.gsub(/\d+\.html/, "#{next_page}.html")}"
else
  next_link = "#{scheme}://#{host}#{path.gsub(/\d+\z/, "#{next_page}")}"
end
next_link
exit
SourceCategory.where(source_id: 2).first 20
SourceCategory.where(source_id: 2).find(17)
url = SourceCategory.where(source_id: 2).find(17).url
uri = URI.parse url
scheme, host, path = uri.scheme, uri.host, uri.path
current_page = path.split('/').last.scan(/\d+/).first
next_page = current_page ? current_page.to_i + 1 : 2
if uri.path.ends_with?('html')
  next_link = "#{scheme}://#{host}#{path.gsub(/\d+\.html/, "#{next_page}.html")}"
else
  next_link = "#{scheme}://#{host}#{path.gsub(/\d+\z/, "#{next_page}")}"
end
next_link
Scrape.count
s = Scrape.last
exit
Sidekiq::Queue.new('product_detail_queue').new
Sidekiq::Queue.new('product_detail_queue').size
Sidekiq::Queue.new('product_detail_queue').clear
Sidekiq::Queue.new('product_listing_queue').clear
s = Scrape.last
s.update running: false
s.destroy
SourceCategory.where(source_id: 2, test_process: true)
SourceCategory.where(source_id: 2).last
SourceCategory.where(source_id: 2).last.update(test_process: true)
SourceCategory.where(test_process: true)
s = Scrape.last
s.update running: false
s.destroy
reload!
SourceCategory.where(test_process: true)
SourceCategory.find(407)
sc = _
sc.category
SourceCategory.all.map {|sc| sc.category.name}
SourceCategory.all.select {|sc| sc.category.nil?}
cats = _
cats.size
SourceCategory.count
Category.last
Category.count
Category.all.pluck(:id, :name)
Category.where(name: 'Solar Panels')
s = Scrape.last
Category
Category.first
Category.create(name: 'Solar Panels')
SourceCategory.last
SourceCategory.last(4)
SourceCategory.where(test_process: true)
sc = _.last
cat = Category.last
c = Category.last
sc.category = c
sc.save
sc.category.parent
s = Scrape.last
SourceCategory
SourceCategory.first
s
s.reload
exit
Category.last
SourceCategory.count
SourceCategory.where(category: nil).size
SourceCategory.where(category: nil)
exit
SourceCategory.count
edit -t
categories.size
edit -t
gsus_links.size
gsus_links
gsus_links.first
categories.first
c = categories.first
c.split('>')
exit
SourceCategory.count
Category.last
SourceCategory.last
Source.all
Category.last
SourceCategory.where(source_id: 2)
Category.last
SourceCategory.last 3
Category.where(source_id: nil)
Category.where(source_id: nil).size
Category.count
SourceCategory.last 3
SourceCategory.where(category_id: nil)
SourceCategory.where(source_id: 2, category_id: nil)
edit -t
alius_links
edit -t
links
links.size
links[5]
categories = %w(
  Consumer Electronics
  Consumer Electronics > TV & Video
  Consumer Electronics > TV & Video > Video
  Consumer Electronics > TV & Video > Video > Televisions
  Consumer Electronics > TV & Video > Video > Televisions > High Definition
  Consumer Electronics > TV & Video > Video > Televisions > 1080p
  Consumer Electronics > TV & Video > Video > Televisions > 4k
  Consumer Electronics > TV & Video > Video > Televisions > 720p
  Consumer Electronics > TV & Video > Video > Televisions > Under 32 inch
  Consumer Electronics > TV & Video > Video > Televisions > 32-40 inch
  Consumer Electronics > TV & Video > Video > Televisions > 40-48 inch
  Consumer Electronics > TV & Video > Video > Televisions > 48-55 inch
  Consumer Electronics > TV & Video > Video > Televisions > 55-65 inch
  Consumer Electronics > TV & Video > Video > Televisions > Over 65 inch
  Consumer Electronics > TV & Video > Video> DVD & Blu-ray Players (VCD)
  Consumer Electronics > TV & Video > Video > Digital Video Recorders
edit -t
Category.where(name: 'Consumer Electronics')
Category.count
Category.all.pluck(:name)
Category.where(name: 'Computer & Devices')
edit -t
gsus_links.size
edit -t
categories.size
Category.last
SourceCategory.last 3
SourceCategory.where(test_only: true).size
edit -t
alius_links.first
links = _
links.split('\,')
SourceCategory.count
SourceCategory.where(category: nil).size
s = Scrape.last
Category.first 5
SourceCategory.where(source_id: 2)
s = Scrape.last
CategoryProduct.first
SourceCategory.find_by(407)
Category
Category.first
SourceCategory
ProductCategory
CategoryProduct
CategoryProduct.first
cp = _
old = [1,2,3,4,5]
new = [1,3,4,6,7,8]
old
new
old - new
CategoryOtherProduct.where(source_category_id: 407)
SourceCategory.last
SourceCategory.last.category
sc = SourceCategory.find 821
sc.category
sc.url
sc
SourceCategory.count
Category.count
SourceCategory.where(url: nil)
SourceCategory.where(category: nil)
SourceCategory.where(source: nil)
Category.count
num
num = Category.count
num * 3
SourceCategory.count
SourceCategory.where(source_id: 2).map {|sc| sc.category.name}
SourceCategory.count
SourceCategory.where(source_id: 2).map {|sc| sc.category.name}
names = _
names.count
names.uniq.count
SourceCategory.where(source_id: 1).map {|sc| sc.category.name}
names = _
names.count
names.uniq.count
SourceCategory.count
ali = 79
gsus = 87
SourceCategory.where(source_id: 3).map {|sc| sc.category.name}.count
amazus = 87
SourceCategory.where(source_id: 4).map {|sc| sc.category.name}.count
gsuk = 82
gsus + gsuk + amazus + ali
SourceCategory.count
SourceCategory.first
SourceCategory.last
SourceCategory.first
SourceCategory.where(source_id: 1, category_id: 1)
SourceCategory.where(source_id: 2, category_id: 1)
SourceCategory.count
SourceCategory.first
SourceCategory.last
SourceCategory.count
SourceCategory.first
SourceCategory.last
exit
s = Scrape.last
Product.count
exit
Product.count
OtherProduct.count
exit
edit -t
blocked.size
blocked.size.sort
blocked = blocked.sort
blocked.uniq.size
blocked.size
blocked.first
blocked.first.split('.')
blocked.first.split('.').take(3)
blocked.first.split('.').take(3).join('.')
subnets = blocked.map {|ip| ip.split('.').take(3).join('.')}
subnets.count
subnets.uniq
blocked.uniq.size
edit -t
edit it
edit -t
list.size
available = list.map {|ip| ip.split('.').take(3).join('.')}
available.size
available.uniq
available.uniq.size
exit
Product.last
Category.first
Category.count
SourceCategory.where(test_process: true)
redirect = 'http://www.amazon.com/b/ref=dp_bc_4?ie=UTF8&node=2236628011'
url = 'http://www.amazon.com/b/ref=dp_bc_4?ie=UTF8&node=2236628011'
url == redirect
(redirect =~ /#{url}/)
("http://www.amazon.com/b/ref=dp_bc_4?ie=UTF8&node=2236628011" =~ "http://www.amazon.com/b/ref=dp_bc_4?ie=UTF8&node=2236628011").nil?
("http://www.amazon.com/b/ref=dp_bc_4?ie=UTF8&node=2236628011" =~ /http://www.amazon.com/b/ref=dp_bc_4?ie=UTF8&node=2236628011/).nil?
("http://www.amazon.com/b/ref=dp_bc_4?ie=UTF8&node=2236628011" =~ /"http://www.amazon.com/b/ref=dp_bc_4?ie=UTF8&node=2236628011/").nil?
("http://www.amazon.com/b/ref=dp_bc_4?ie=UTF8&node=2236628011" =~ /"http://www.amazon.com/b/ref=dp_bc_4?ie=UTF8&node=2236628011"/).nil?
("http://www.amazon.com/b/ref=dp_bc_4?ie=UTF8&node=2236628011" =~ /#{url}/).nil?
("http://www.amazon.com/b/ref=dp_bc_4?ie=UTF8&node=2236628011" =~ /url/).nil?
("http://www.amazon.com/b/ref=dp_bc_4?ie=UTF8&node=2236628011" =~ /http:\/\/www.amazon.com\/b\/ref=dp_bc_4\?ie=UTF8&node=2236628011\/).nil?
redirect.include? "http://www.amazon.com/b/ref=dp_bc_4?ie=UTF8&node=2236628011"
redirect
(redirect =~ /#{redirect}/)
(redirect =~ /#{redirect}/sig)
(redirect =~ /#{redirect}/si)
(redirect =~ /#{redirect}/i)
redirect.include? redirect
'x' =~ /x/
/#{redirect}/ =~ redirect
redirect =~ redirect
redirect
redirect =~ /http:\/\/www\.amazon\.com\/b\/ref=dp_bc_4\?ie=UTF8&node=2236628011/
redirect =~ /#{redirect.to_s}/
Regexp.new redirect
redirect =~ _
redirect =~ Regexp.new redirect
redirect =~ (Regexp.new redirect)
URI.parse(redirect)
uri = _
Regexp.new "http://www.amazon.com/b/ref=dp_bc_4?ie=UTF8&node=2236628011"
Product.last
Product.count
Listing.count
Product.count
Listing.count
Listing.last
Listing.all.map(&:data_hash)
Listing.all.map(&:product_description)
Listing.all.map(&:url)
listing.last.images
Listing.last.images
(5..1).to_a.reverse.each do |i|
  puts i
end
(1..5).to_a.reverse.each do |i|
  puts i
end
10 * 0.2
W
s = Scrape.last
s.reload
s.products.size
s.listings.size
48 / 3
exit
s = Scrape.last
s.listings
s.listings.count
s.products.count
s.reload
'http://www.amazon.com/s/ref=sr_pg_14/176-3771968-6843860?rh=n%3A2972638011%2Cn%3A%213238155011%2Cn%3A552808%2Cn%3A3236381%2Cn%3A2236628011&page=14&ie=UTF8&qid=1459203735&spIA=B01AHHEPG0,B016P4TZCS,B01BLTY3KG,B01AHHEQEG,B00HK2S3NC,B00FE349JQ,B008DGSQHM,B008DGQI22,B0169NP164,B01686NJYI,B0169NOXZY,B00BJE7K18,B004FWW44Y,B00DCCOSV0,B01BBCVIBK,B00RAIW70Q,B0169NP3YE,B018K1XPUC,B004FOGL0K,B00MU09DL0,B00L1UPKSK,B00BF449L6,B017TPAB0C,B01BBCVIAQ,B00FE45FD4,B00F9EO5OO,B0049H0NNY,B01BER4UT4,B013G3ZEX0,B00OQ0CAW6,B00FF0HBH0,B01AHHENG2,B00JML23X0'
ProductScreenshot.last
ProductScreenshot.last.attachment.url
s = Scrape.last
s.products.count
first = Scrape.first
first.products.count
(1..1)
(1..1).each {|i| puts i}
(1..0).each {|i| puts i}
s = Scrape.last
s.products
s.products.size
s.listings.size
Listing.where("data_hash like '%maxeon%'").size
Listing.where("data_hash like '%sunpower%'").size
Listing.where("data_hash like '%Sunpower%'").size
Listing.where("data_hash like '%sunpower%'").size
Listing.where("data_hash ilike '%sunpower%'").size
Listing.where("data_hash like '%sunpower cell%'").size
Listing.where("data_hash like '%sunpower%cell%'").size
Listing.where("data_hash like ?", '%sunpower%').size
Listing.where("data_hash like ? OR product_description like ? OR from_mfg like ?", '%sunpower%').size
Listing.where("data_hash like ? OR product_description like ? OR from_mfg like ?", ['%sunpower%']).size
Listing.where("data_hash like :q OR product_description like :q OR from_mfg like :q", q: '%sunpower%').size
Listing.where("data_hash like :q OR product_description like :q OR from_mfg like :q", q: '%sunpower panel%').size
Listing.where("data_hash like :q OR product_description like :q OR from_mfg like :q", q: '%sunpower cell%').size
Listing.where("data_hash like :q OR product_description like :q OR from_mfg like :q", q: '%sunpower%panel%').size
Listing.where("data_hash like :q OR product_description like :q OR from_mfg like :q", q: '%sunpower%cell%').size
ids = Listing.where("data_hash like :q OR product_description like :q OR from_mfg like :q", q: '%sunpower%cell%').ids
ids |= Listing.where("data_hash like :q OR product_description like :q OR from_mfg like :q", q: '%sunpower%panel%').ids
ids |= Listing.where("data_hash like :q OR product_description like :q OR from_mfg like :q", q: '%sunpower%module%').ids
ids |= Listing.where("data_hash like :q OR product_description like :q OR from_mfg like :q", q: '%module%sunpower%').ids
ids |= Listing.where("data_hash like :q OR product_description like :q OR from_mfg like :q", q: '%cell%sunpower%').ids
ids |= Listing.where("data_hash like :q OR product_description like :q OR from_mfg like :q", q: '%panel%sunpower%').ids
Listing.where(id: ids).flat_map(&:product_ids)
p_ids = _.uniq
p.name
ids |= Listing.where("data_hash like :q OR product_description like :q OR from_mfg like :q OR name like :q", q: '%panel%sunpower%').ids
ids |= Listing.where("data_hash like :q OR product_description like :q OR from_mfg like :q OR name like :q", q: '%module%sunpower%').ids
s.listings.size
s.products.size
s.products.last
p = _
p.listings
p.listings.size
p.listings.last
Listing.where(product_description: nil).size
Listing.where.not(product_description: nil).size
Listing.where.not(product_description: nil).first
Listing.where.not(product_description: nil).map(&:created_at)
Listing.where.not(product_description: nil).map(&:created_at).sort
Listing.where.not(product_description: nil).map(&:id)
Listing.last
Listing.where.not(product_description: nil).map(&:id)
ids = Listing.where.not(product_description: nil).map(&:id)
ids.last
325 - 98
ids = Listing.where.not(product_description: nil).map(&:id)
335 - 98
ids = Listing.where.not(product_description: nil).map(&:data_hash)
ids = Listing.where.not(product_description: nil).map(&:product_description)
ids = Listing.all.map(&:product_description)
ids = Listing.all.pluck(:id, :product_description)
s.listings.count
s.products.count
Sidekiq::Queue.new('product_detail_queue').size
Sidekiq::Queue.new('product_detail_queue').clear
s.sellers
s.sellers.count
ProductImage.where(scrape_id: s.id).size
s.listings.count
1500 / 360
360 * 4
1575.0 / 360
p = Product.last
p.images
s.sellers.count
s.scrape_listing_sellers.size
s.scrape_listing_sellers.last.listing_id
Listing.last.id
Sidekiq::Queue.new('sellers_queue').clear
s
s.reload
s.products.count
s.products.last
Product.find_by(brand: 'Allpowers', brand: '130x150')
Product.find_by(brand: 'Allpowers', model: '130x150')
p = _
p.url
p.original_Url
p.original_url
p.listings.last.url
url = 'http://ufdcimages.uflib.ufl.edu/UF/00/02/61/02/00447/11-08-2013.pdf'
url.include?('.gov')
url.include?('.edu')
ProductManual.last
pm = _
pm.product
"\"WindyNation\" AND \"SOL-030P-01\" manual filetype:pdf"
"\"WindyNation\" AND \"SOL-030P-01\" manual filetype:pdf".to_s
puts "\"WindyNation\" AND \"SOL-030P-01\" manual filetype:pdf"
p.listings.last.name
p = pm.product
p.listings.last.name
pm = ProductManual.last
pm.product
pm = ProductManual.last
pm.product
ProductManual.last 20
ProductManual.last(20).map {|pm| [pm.product.brand, pm.product.model, pm.original_url]}
edit -t
s.reload
ProductManual.last
pm = _
pm.product
pm = ProductManual.last
pm.product
pm = ProductManual
pm = ProductManual.last
pms = ProductManual.last(20)
ProductManual.last(20).map {|pm| [pm.product.brand, pm.product.model, pm.original_url]}
p = Product.find_by(model: '7666')
p.listings.last.url
'abc'.to_i
ProductManual.last(20).map {|pm| [pm.product.brand, pm.product.model, pm.original_url]}
ProductManual.where(confirmed: true)
ProductManual.where(confirmed: true).size
ProductManual.where(status: :confirmed).size
ProductManual.where(status: :confirmed)
ProductManual.first
ProductManual.where(status: 1)
ProductManual.where(status: 1).size
confirmed = ProductManual.confirmed
confirmed = ProductManual.confirmed.first.original_url
pm = ProductManual.confirmed.first
pm.product
pm.text
pm = ProductManual.confirmed.last
pm.text
Product.delete_index
Product.delete_index!
OtherProduct.delete_index!
Product.import
terms = { keywords: 'sunpower cell' }
operator = 'AND'
filters = {}
@products = []
per_page = params[:per_page] || 10
per_page = 500
page = 1
search_result = Product.search_by(terms: terms, operator: operator, filters: filters, opts: {size: per_page}).page(page).results
search_result = Product.search_by(terms: terms, operator: operator, filters: filters, opts: {size: per_page}).page(page).response
search_result = Product.search_by(terms: terms, operator: operator, filters: filters, opts: {size: per_page}).page(page).results
search_result = Product.search_by(terms: terms, operator: operator, filters: filters, opts: {size: per_page}).page(page).results.count
search_result = Product.search_by(terms: terms, operator: operator, filters: filters, opts: {size: per_page}).page(page).results.size
ProductManual.import
search_result = Product.search_by(terms: terms, operator: operator, filters: filters, opts: {size: per_page}).page(page).results.size
y _
search_result = Product.search_by(terms: terms, operator: operator, filters: filters, opts: {size: per_page}).page(page).results.size
ProductManual.find_by(original_url: 'http://mission.sfgov.org/OCA_BID_ATTACHMENTS/FA24425.pdf')
pm = _
pm.product
exit
ScrapeProductSeller
ScrapeProductSeller.connection
ScrapeProductSeller
ProductDetail
Listing
exit
ProductDetail
exit
exi
exit
scrape = Scrape.first
scrape.products.first
p = _
p_id = p.id
s_id = 1
scrape
Listing.first
product.includes(:scrape_product_sellers, :listings).find(p_id)
product = Product.includes(:scrape_product_sellers, :listings).find(p_id)
reload!
product = Product.includes(:scrape_product_sellers, :listings).find(p_id)
product.nil?
original_sps = product.scrape_product_sellers.where(scrape_id: s_id, product_id: p_id).where('original_url IS NOT NULL OR product_source_path IS NOT NULL').first
original_url = original_sps.try(:original_url) || original_sps.try(:product_source_path)
if original_url.nil?
  product.scrape_product_sellers.where(scrape_id: s_id).each(&:destroy)
end
url = case scrape.source_id
when 1, 2, 4
  original_url.gsub(/\?.+/, '')
else
  original_url
end
listing = Listing.where.not(url: nil).find_by(scrape_id: s_id, url: url)
listings = product.listings.where(scrape_id: s_id)
Listing.count
Listing.where(url: nil).size
listings = product.listings.where(scrape_id: s_id)
if listing
else
  listing = listings.first
original_url
url
if listing
else
  listing = listings.first
  listing.update_columns(url: url, original_url: original_url, name: product.name)
end
Listing.first
l_id = listing.id
ids = listings.ids.reject {|id| id == l_id }
Listing.where(id: ids).each(&:delete)
assets = Asset.where(assetable_id: p_id, assetable_type: 'Product', scrape_id: s_id).where.not(type: 'ProductManual')
assets.size
assets
listing
Listing.second
Listing.first
Asset.where(assetable_id: p_id, assetable_type: 'Product', scrape_id: s_id).where.not(type: 'ProductManual')
Asset.where(assetable_id: p_id, assetable_type: 'Product', scrape_id: s_id).where.not(type: 'ProductManual').update_all(assetable_id: l_id, assetable_type: 'Listing')
Listing.first.images
Listing.first.assets
Asset.where(assetable_type: 'Listing')
CategoryProduct.first
edit -t
listing.sellers
listing.sellers.map(&:name).uniq.size
listing.sellers.map(&:name).size
edit -t
listing.categories
edit -t
CategoryListing.all
listing.categoreis
listing.categories
edit -t
listing.categories
CategoryListings
CategoryListing.all
listing.reload
listing.categories
Listing.first
Listing.last
l = _
p = l.product
reload!
l = Listing.last
l.product
OtherProductDetail.last
OtherProductDetail.where("data_hash like '%mnumber%'").first
OtherProductDetail.where("data_hash like '%number%'").first
OtherProductDetail.where("data_hash like '%number%'").last
Scrape.find 159
ProductDetail.where("data_hash like '%number%'").last
Listing.where("data_hash like '%number%'").last
Listing.where("data_hash like '%Brand%'").last
if false
  puts 'lllasdf'
end
Listing.where("data_hash like '%Brand Name%'").last
Scrape.find 158
Listing.where("data_hash like '%Brand Name%'").forst
Listing.where("data_hash like '%Brand Name%'").first
Listing.connection
Listing.where("data_hash like '%Brand Name%'").first
exit
Listing.where("data_hash like '%Brand Name%'").first
Scrape.find 2
Listing.where("data_hash like '%Brand Name%'").first
l = _
l = Listing.first
Listing.where(url: l.url).pluck(&:id, :created_at)
Listing.where(url: l.url).pluck(:id, :created_at)
l = Listing.lat
l = Listing.last
Asset.first
ProductImage.first
ProductManual.first
ListingAsset.first
Listing.images
Listing.first.images
ListingAsset.find_or_create(asset_id: 1, listing_id: 1)
exit
ListingAsset.find_or_create(asset_id: 1, listing_id: 1)
ListingAsset.find_or_create_by(asset_id: 1, listing_id: 1)
l = Listing.first
l.images
reload!
l = Listing.first
l.images
exit
ListingAsset.first
l = _
l.images
l = Listing.first
l.listing_assets
l.images
reload!
l.reload
l.images
exit
l = Listing.first
l.images
exit
l = Listing.first
l.listing_assets
l.assets
listing.images
l.images
reload!
exit
l = Listing.first
l.assets
Asset.first
l.screenshots
exitxit
exit
l = Listing.first
l.assets
l.screenshots
l.images
ProductManual.first
p_id = Product.first
s_id = Scrape.first
Asset.where(assetable_id: p_id, assetable_type: 'Product', scrape_id: s_id).each do |asset|
  ListingAsset.find_or_create_by(listing_id: l_id, asset_id: asset.id)
end
l.assets
Asset.where(assetable_id: l.id, assetable_type: 'Listing', scrape_id: s_id).each do |asset|
Asset.where(assetable_id: l.id, assetable_type: 'Listing', scrape_id: s_id)
Asset.where(assetable_id: l.id, assetable_type: 'Listing', scrape_id: s_id).size
Asset.where(assetable_id: l.id, assetable_type: 'Listing', scrape_id: s_id).each do |asset|
  ListingAsset.find_or_create_by(listing_id: l_id, asset_id: asset.id)
end
Asset.where(assetable_id: l.id, assetable_type: 'Listing', scrape_id: s_id).each do |asset|
  ListingAsset.find_or_create_by(listing_id: l.id, asset_id: asset.id)
end
l.screenshots
l.images
.manuals
l.manuals
Asset.where(assetable_id: l.id, assetable_type: 'Listing', scrape_id: s_id)
l.reload
l.images
img = ProductImage.first
img.listings
OtherProduct.first
op = _
op = OtherProduct.last
op.details
op.scrape_other_product_sellers
op.sellers
OtherProduct.last(10)
_.map(&:sellers)
op = OtherProduct.first
op.name
op.details
ProductScreenshot.where(assetable_type: 'Product').first 10
ProductScreenshot.where(scrape_id: Scrape.where(source_id: 1).last.id, assetable_type: 'Product').first 10
ProductScreenshot.where(scrape_id: Scrape.where(source_id: 1).last.id, assetable_type: 'Product').uniq(:url).first 10
ProductScreenshot.where(scrape_id: Scrape.where(source_id: 3).last.id, assetable_type: 'Product').first 10
ProductScreenshot.where(scrape_id: Scrape.where(source_id: 3).first.id, assetable_type: 'Product').first 10
ProductScreenshot.where(scrape_id: Scrape.where(source_id: 1).last.id, assetable_type: 'Product').uniq(:url).first 10
ProductImage.where(scrape_id: 4).first 20
p = Product.find 8834
p.images
ProductImage.where(scrape_id: 4).first 20
p = Product.find 8835
p.images
p.images.size
p.images.map &:original_url
p.images.uniq(:original_url)
p.images.distinct(:original_url)
p.images.distinct('original_url')
p.images.select(distinct('original_url'))
p.images.select('distinct(original_url)')
p.images.uniq(&:original_url)
p.reload
p.images.uniq(&:original_url)
p.images.uniq(&:original_url).size
Product.includes(:images).find_in_batches do |products|
p_ids = []
Product.includes(:images).find_in_batches do |products|
  products.each do |p|
    p_ids << p.id if (p.images.size > p.images.uniq(&:original_url)
      end
Product.includes(:images).find_in_batches do |products|
  products.each do |p|
    p_ids << p.id if (p.images.size > p.images.uniq(&:original_url))
  end
  break unless p_ids.blank?
end
Product.includes(:images).find_in_batches do |products|
  products.each do |p|
    p_ids << p.id if (p.images.size > p.images.uniq(&:original_url).size)
  end
  break unless p_ids.blank?
end
p_ids
exit
(5.to_f / 12 * 100).round(2)
Listing.first
Product.first
exit
1911 - 544
_ / 2
1367.0 / 2
exit
Product.first
rake db:migrate
exit
s = Scrape.first
p = Product.create(brand: 'Brand1', model: 'model-x')
p.manuals.size
exit
p = Product.first
p.manuals.size
reload!
p = Product.first
p.manuals.size
reload!
p = Product.first
p.manuals.size
p.manuals.create(original_url: 'http://www.st.com/web/en/resource/technical/document/datasheet/CD00166044.pdf')
p.manuals
pm = p.manuals.first
reload!
p = Product.first
pm = p.manuals.first
p.manuals << pm
reload
reload!
p = Product.first
pm = p.manuals.first
reload!
p = Product.first
pm = p.manuals.first
reload!
p = Product.first
pm = p.manuals.first
ProductProductManual.find_or_create_by(product: p, manual: pm)
exit
p = Product.first
pm = p.manuals.first
p.manuals.create(original_url: 'http://www.st.com/web/en/resource/technical/document/datasheet/CD00166044.pdf')
pm = p.manuals.first
ProductProductManual.find_or_create_by(product: p, manual: pm)
p = Product.first
pm = p.manuals.first
p.manuals.create(original_url: 'http://www.st.com/web/en/resource/technical/document/datasheet/CD00166044.pdf')
p.manuals
ProductManual.count
pm = p.manuals.first
ProductProductManual.find_or_create_by(product: p, manual: pm)
ProductProductManual.size
ProductProductManual.count
pm
pm.valid?
pm.errors
pm.destroy
pm = ProductManual.first
ProductProductManual.find_or_create_by(product: p, manual: pm)
pm = p.manuals.first
pm = p.manuals.last
exit
pm = ProductManual.first
pm.products
p = Product.first
p.manuals
p.manuals.first
p.manuals.count
exit
p.manuals
p = Product.first
pm = ProductManual.first
ProductProductManual.confirmed
ProductProductManual.need_review
reload!
ProductProductManual.need_review
reload!
ProductProductManual.need_review
exut
exit
ProductProductManual.need_review
ProductProductManual.includes(:manual).where("assets.title is NULL OR assets.title = '' OR product_product_manuals.status IS NULL").size
ProductProductManual.includes(:manual).where("product_manual.title is NULL OR assets.title = '' OR product_product_manuals.status IS NULL").size
ProductProductManual.includes(:manual).where("assets.title is NULL OR assets.title = '' OR product_product_manuals.status IS NULL").to_sql
ProductProductManual.includes(:manual).where("assets.title": nil).size
ProductProductManual.includes(:manual).where("`assets`.`title` is NULL OR `assets`.`title` = '' OR product_product_manuals.status IS NULL").to_sql
ProductProductManual.includes(:manual).where("`assets`.`title` is NULL OR `assets`.`title` = '' OR product_product_manuals.status IS NULL").size
ProductProductManual.includes(:manual).where("`assets`.`title` is NULL OR `assets`.`title` = '' OR product_product_manuals.status IS NULL")
ProductProductManual.need_review
ProductProductManual.confirmed
exit
ProductProductManual.not(:confirmed)
exit
ProductProductManual.confirmed
ProductProductManual.not(:confirmed)
pm = ProductProductManual.first
pm = ProductManual.first
ppm = ProductProductManual.find_or_create_by(product: Product.first, manual: pm)
ppm
ppm.update status: :confirmed
ppm.reload
ProductProductManual.confirmed
ProductProductManual.not(:confirmed)
exit
ProductProductManual.not(:confirmed)
ProductProductManual.needs_review
exit
ProductProductManual.needs_review
exit
ProductProductManual.needs_review
reload!
ProductProductManual.needs_review
reload!
ProductProductManual.needs_review
reload!
ProductProductManual.needs_review
reload!
ProductProductManual.needs_review
ProductProductManual.needs_review.size
reload!
ProductProductManual.needs_review
ProductProductManual.needs_review.to_sql
reload!
ProductProductManual.needs_review
reload!
ProductProductManual.needs_review
reload!
ProductProductManual.needs_review
ProductProductManual.needs_review.size
reload!
ProductProductManual.needs_review.size
reload!
ProductProductManual.needs_review.size
reload!
ProductProductManual.needs_review.size
exit
ProductProductManual.needs_review.size
exit
ProductProductManual.needs_review.size
ppms = ProductProductManual.arel_table
manuals = ProductManual.arel_table
ppms.join(manuals).on(ppms[:asset_id].eq(manuals[:id])).where(ppms[:status].eq(nil).or(manuals[:title].eq(nil).or(manuals[:title].eq('')))).to_sql
ppms.join(manuals).on(ppms[:asset_id].eq(manuals[:id])).where(ppms[:status].eq(nil).or(manuals[:title].eq(nil).or(manuals[:title].eq(''))))
ppms.join(manuals).on(ppms[:asset_id].eq(manuals[:id])).where(ppms[:status].eq(nil).or(manuals[:title].eq(nil).or(manuals[:title].eq('')))).to_sql
sql = _
ProductProductManual.select(sql)
ProductProductManual.select(sql).size
ProductProductManual.where(sql)
ProductProductManual.where(sql).siq
ProductProductManual.where(sql).size
ProductProductManual.select(sql)
ProductProductManual.select(sql).size
ProductProductManual.select(sql).first
sql
sql = ppms.join(manuals, Arel::Nodes::OuterJoin).on(ppms[:asset_id].eq(manuals[:id])).where(ppms[:status].eq(nil).or(manuals[:title].eq(nil).or(manuals[:title].eq(''))))
ProductProductManual.where(sql)
sql = ppms.join(manuals, Arel::Nodes::OuterJoin).on(ppms[:asset_id].eq(manuals[:id])).where(ppms[:status].eq(nil).or(manuals[:title].eq(nil).or(manuals[:title].eq('')))).to_sql
ProductProductManual.select(sql).size
ProductProductManual.select(sql)
sql
sql = ppms.join(manuals).on(ppms[:asset_id].eq(manuals[:id])).where(ppms[:status].eq(nil).or(manuals[:title].eq(nil).or(manuals[:title].eq('')))).to_sql
ProductProductManual.find_by_sql(sql)
sql
sql = ppms.project(ppms.join(manuals).on(ppms[:asset_id].eq(manuals[:id])).where(ppms[:status].eq(nil).or(manuals[:title].eq(nil).or(manuals[:title].eq('')))).to_sql)
sql = ppms.project(ppms.join(manuals).on(ppms[:asset_id].eq(manuals[:id])).where(ppms[:status].eq(nil).or(manuals[:title].eq(nil).or(manuals[:title].eq('')))).to_sql).to_sql
ProductProductManual.find_by_sql(sql)
sql = ppms.join(manuals).on(ppms[:asset_id].eq(manuals[:id])).where(ppms[:status].eq(nil).or(manuals[:title].eq(nil).or(manuals[:title].eq('')))).to_sql
"SELECT * FROM `product_product_manuals` INNER JOIN `assets` ON `product_product_manuals`.`asset_id` = `assets`.`id` WHERE (`product_product_manuals`.`status` IS NULL OR (`assets`.`title` IS NULL OR `assets`.`title` = ''))"
sql = _
ProductProductManual.find_by_sql(sql)
sql = ppms.project(Arel.star).join(manuals).on(ppms[:asset_id].eq(manuals[:id])).where(ppms[:status].eq(nil).or(manuals[:title].eq(nil).or(manuals[:title].eq('')))).to_sql
ProductProductManual.find_by_sql(sql)
ProductManual.first
pm = _
ProductManual.count
pm.update status: :confirmed
ProductProductManual.find_by_sql(sql)
pm.update title: "Some Title"
ProductProductManual.find_by_sql(sql)
reload!
ProductProductManual.needs_review
reload!
ProductProductManual.needs_review
pm.update title: ""
ProductProductManual.needs_review
reload!
ProductProductManual.needs_review
pm.update status: :deleted
ProductProductManual.needs_review
pm
pm.update title: "Some Title"
ProductProductManual.needs_review
pm.update status: :confirmed
reload!
pm.update title: "Some Title"
exit
pm = ProductManual.first
pm.update status: :confirmed
ProductProductManual.needs_review
pm.update title: ""
ProductProductManual.needs_review
reload!
ProductProductManual.needs_review
pm
pm.update system_confirmed: true
ProductProductManual.needs_review
pm
pm.update title: 'some title'
ProductProductManual.needs_review
pm.update title: nil
ProductProductManual.needs_review
Asset.where(title: [nil, '']).size
exit
ProductProductManual.needs_review
ppm = _.first
pm = _.product
ppm
pm = _.manual
pm.title = 'Something'
ppm
ProductProductManual.needs_review
reload!
ProductProductManual.needs_review
reload!
ProductProductManual.needs_review
ppm = ProductProductManual.needs_review.first
pm = ppm.manual
pm
pm.update system_confirmed: false
ProductProductManual.needs_review
reload
reload!
ProductProductManual.needs_review
reload!
ProductProductManual.needs_review
reload!
ProductProductManual.needs_review
pm = ProductManual.first
pm.update system_confirmed: true
ProductProductManual.needs_review
reload!
ProductProductManual.needs_review
reload!
ProductProductManual.needs_review
pm.reload
ppm.reload
ppm.update system_confirmed: true
ProductProductManual.needs_review
ppm.manual.title
reload!
ProductProductManual.needs_review
pm.reload
pm.update title: 'Something'
ProductProductManual.needs_review
reload!
ProductProductManual.needs_review
reload!
ProductProductManual.needs_review
pm.update title: ''
ProductProductManual.needs_review
ppm.relaod
ppm.reload
ppm.system_confirmed: false
ppm.update system_confirmed: false
ProductProductManual.needs_review
reload!
ProductProductManual.needs_review
reload!
ProductProductManual.needs_review
ppm.reload
ppm.update status: nil
ProductProductManual.needs_review
pm.reload
pm.update title: "Sometitle"
ProductProductManual.needs_review
:w
ProductProductManual.not(:needs_review)
reload!
ProductProductManual.needs_review
pm.reload
pm.update title: nil
ProductProductManual.needs_review
ppm.reload
ppm.update status: 1
ProductProductManual.needs_review
ppm.update status: 0
ProductProductManual.needs_review
ProductProductManual.needs_review.where_values
reload!
ProductProductManual.needs_review.where_values
ProductProductManual.need_review.where_values
ProductProductManual.not(:need_review)
ProductProductManual.where.not(id: ProductProductManual.need_review.ids)
ProductProductManual.confirmed
ppm = ProductProductManual.includes(:manual).find(1)
manual = ppm.manual
ppm.reload
ppm.product
ppm.manual
reload
reload!
Listing.first
reload!
pm.reload
pm.listings
exit
ppm = ProductProductManual
ppm = ProductProductManual.connection
ppm = ProductProductManual
ppm = ProductProductManual.first
ppm.update status: 1
ppm.confirmed?
exit
ppm = ProductProductManual.first
ppm.confirmed?
ppm.update status: 0
ppm.confirmed?
exit
Product.first
Product.includes(:sellers, :sources, :scrapes, :images, :screenshots).where(id: pids)
Product.includes(:sellers, :sources, :scrapes, :images, :screenshots).first
l = Listing.first
require 'factory_girl_rails'
reload!
exit
def stats(s)
  s.reload
  p = s.listings.map(&:product_ids).flatten.uniq.size
  l = s.listings.size
  no_product = s.listings.select{|l| l.product_ids.empty?}.size
  time = (s.duration || Time.now - s.created_at).to_f / 60
  rate = l / time
  full_scrape = (500000 / rate / 60 / 24)
  puts "Scrape id: #{s.id} Source: #{s.source_id}"
  puts "Ratio: #{p}/#{l} products/listing"
  puts "Listings without product: #{no_product}"
  puts "#{((p.to_f / l) * 100).round(2)} percent found Products"
  puts "Time: #{time.round(2)}mins Rate: #{rate.round(2)} created: #{s.created_at.strftime("%m/%d/%Y")}"
  puts "#{full_scrape.round(2)} days for full scrape"
  puts '=============================='
  nil
end
s = Scrape.last
stats s
Sidekiq::Queue.new('product_listing_queue').size
stats s
p = Product.first
p.listings
p.screenshots
p.images
ProductImage.count
p.reload
p.images
p = Product.count
stats s
def stats(s)
  s = Scrape.includes(:listings, :products).find s.id
  p = s.listings.map(&:product_ids).flatten.uniq.size
  l = s.listings.size
  no_product = s.listings.select{|l| l.product_ids.empty?}.size
  time = (s.duration || Time.now - s.created_at).to_f / 60
  rate = l / time
  full_scrape = (500000 / rate / 60 / 24)
  puts "Scrape id: #{s.id} Source: #{s.source_id}"
  puts "Ratio: #{p}/#{l} products/listing"
  puts "Listings without product: #{no_product}"
  puts "#{((p.to_f / l) * 100).round(2)} percent found Products"
  puts "Time: #{time.round(2)}mins Rate: #{rate.round(2)} created: #{s.created_at.strftime("%m/%d/%Y")}"
  puts "#{full_scrape.round(2)} days for full scrape"
  puts '=============================='
  nil
end
stats s
l = Listing.last
Listing.count
Listing.count('Distinct(url)')
Listing.all.pluck(:url).uniq.size
Listing.count
urls = Listing.all.pluck(:url)
dutch = urls.select {|url| url.include?('google.de')}
Listing.where("name like '%Wacom Intuos Draw Small%'").size
urls = Listing.all.pluck(:url).uniq.size
Listing.count
urls = Listing.all.pluck(:url).select {|url| url.include?('google.de')}.uniq.size
Listing.count
urls = Listing.all.pluck(:url).select {|url| url.include?('google.de')}.uniq.size
Listing.count
Asset.count
Asset.first
screenshot.attachment.url
Asset.first.attachment.url
Asset.first.listings
Asset.first.listings.count
Asset.first.listings
l = _
l.identifiers
l = l.first
l.identifiers
p = Product.find_by(model: 'MH-110584')
pm = ProductManual.first
pm.product
pm.products
p = _.first
p.manuals
pm
pm.product_product_manuals
p.model
p.brand
pm
pm = ProductManual.last
pm.product
pm
pm.product
pm.reload
pm.product
pm.products
pm.url
pm.original_url
pm.products
xit
exit
pm = ProductManual.last
pm.products
pm.original_url
pm
p = pm.products.first
p.images
ProductManual.all.pluck(:title)
ProductManual.all.pluck(:title).compact
ProductManual.all.pluck(:url).select {|url| url.downcase.include?('catalog')}
ProductManual.all.pluck(:original_url).select {|url| url.downcase.include?('catalog')}
Asset.find 335
pm = ProductManual.last
pm.products.first
pm.original_url
pm = ProductManual.last
pm = ProductManual.last 5
pm = _.first
pm.products.first
pm.original_url
pm = _.first
pm = ProductManual.last
pm.products.first
pm = ProductManual.last
pm.products
p = _.first
p.manuals
l = Listing.find 88
l.products
l.identifier
l.identifiers
pm = ProductManual.last
ProductManual.count
Product.count
Asset.find 345
pm = ProductManual.last
pm.products
ProductManual.all.select {|pm| pm.products.size > 1}
pm = _.first
pm.products.size
pm.products
Listing.count
Product.count
gsus = Source.find 1
gsus.categories.size
gsus.source_categories.size
ProductManual.find 1
ProductProductManual.first
ProductProductManual.where(status: :confirmed).size
ProductProductManual.where(status: status[:confirmed]).size
ProductProductManual.where(status: statuses[:confirmed]).size
ProductProductManual.where(status: 1).size
ProductProductManual.where(status: 1).last
ppm = _
ppm.manual
ppm.product
ProductProductManual.confirmed
ProductProductManual.confirmed.last
ppm = ProductProductManual.confirmed.last
p = ppm.product
pm = ppm.manual
ProductProductManual.confirmed.size
ProductProductManual.confirmed.pluck(:asset_id).uniq.size
ppm = ProductProductManual.find 36
p = ppm.product
pm = ppm.manual
ProductManual.all.select {|pm| %w(price list).all? {|w| pm.original_url.downcase.include?(w)} }
ProductManual.all.select {|pm| %w(price list).all? {|w| pm.original_url.downcase.include?(w)} }.size
ProductManual.all.select {|pm| %w(price list).all? {|w| pm.original_url.downcase.include?(w)} }
ProductManual.all.select {|pm| %w(catalog).all? {|w| pm.original_url.downcase.include?(w)} }
ProductManual.all.select {|pm| %w(catalog).all? {|w| pm.original_url.downcase.include?(w)} }.size
ProductManual.count
10 / 88.to)f
10 / 88.to_f
url = "BlahBAL"
url.downcase
url
url.downcase!
url
listing = Listing.last
l = _
l.categories
l.url
Listing.where("url like '%2227264358349%'").size
l
l.listings_matching_identifiers
l.identifiers
l = Listing.include(:identifiers).all.first {|l| l.identifiers.size > 1}
l = Listing.includes(:identifiers).all.first {|l| l.identifiers.size > 1}
l
l.identifier
l.identifiers
l.listings_matching_identifiers
p = Product.find 147
p.listings
l = _.first
l.listings_matching_identifiers
l.scrapes
l.scrape_listing_sellers
l.scrape_listing_sellers.szie
l.scrape_listing_sellers.size
l.scrape_listing_sellers
l.sellers
l.sellers.size
l.scrape_listing_sellers.size
l.scrape_listing_sellers.distinct('seller_id')
l.scrape_listing_sellers.uniq(:seller_id)
l.scrape_listing_sellers.uniq_by(:seller_id)
l.scrape_listing_sellers.uniq_by(&:seller_id)
l.scrape_listing_sellers.select(:seller_id).distinct
l.scrape_listing_sellers.select('Distinct(seller_id), select *').distinct
p
p.images.first
p.images.first.url
p.images.first.original_url
p.images.first.attachment.url
p.listings.last
l.images
l.screenshots.size
l.screenshots
l.screenshots.map{|img| img.attachment.url }
l.manuals
ProductManual.first
ProductManual.confirmed
ProductProductManual.confirmed.last
ppm = _
p = ppm.product
l = p.listings.first
l.manuals
l.reload!
l.reload
l.listing_assets
l.listing_assets.map(&:asset)
edit -t
l.reload
l.manuals
l.products.first
p = _
p.manuals
l.manuals.first.attachment.url
exit
l = Listing.last
l.repeat_scrapes
exit
l = Listing.last
l.repeat_scrapes
reload!
exit
l = Listing.last
l.repeat_scrapes
reload!
l = Listing.last
l.repeat_scrapes
exit
l = Listing.last
url = l.url
Digest::MD5.hexdigest url
url = l.original_url
l.sellers.first
l.scrape_listing_sellers
url = l.scrape_listing_sellers.first.seller_url
url = l.scrape_listing_sellers.find(1743)
url = l.scrape_listing_sellers.find(1743).seller_url
Digest::MD5.hexdigest url
exit
l = Listing.first
url_md5 = Digest::MD5.hexdigest l.url
l
l.url
l = Listing.first
l.url
exit
l = Listing.first
l.url
url_md5 = Digest::MD5.hexdigest l.url
l_url = ListingUrl.find_or_create_by(url_md5: url_md5) do |listing_url|
  listing_url.url = l.url
end
l_url.listings << l
l.reload
l.listing_url
exit
ListingUrl.count
Listing.count
lurl = ListingUrl.all.sample
lurl.scrapes
l = Listing.first
l.scrapes
l.listings
eixt
exit
l = Listing.first
l.listings
l.reload
l.listings
exit
l = Listing.first
l.listings
exit
l = Listin.last
l = Listing.last
l.as_indexed_json
l_url = ListingUrl.includes(:listings).find_by(url_md5: url_md5).where('listings.scrape_id': scrape_id)
scrape_id = 2
url = l.url
url_md5 = Digest::MD5.hexdigest(url)
l_url = ListingUrl.includes(:listings).find_by(url_md5: url_md5).where('listings.scrape_id': scrape_id)
l_url = ListingUrl.includes(:listings).where('listings.scrape_id': scrape_id).find_by(url_md5: url_md5)
l_url
l_url = ListingUrl.includes(:listings).find_by(url_md5: url_md5)
l_url.listings.where(scrape_id: scrape_id)
l_url = ListingUrl.includes(:listings).find_by(url_md5: url_md5)
l_url.listings.where(scrape_id: 2)
l_url.listings.where(scrape_id: 1)
l_url
l
l_url << l
l_url.listings << l
l_url.listings.siz
l_url.listings.size
l_url.listings.uniq.size
l_url.listing_ids
l.reload
l.listing_url
l.listing_urls
l_url.listing_ids
l.reload
l_url.reload
l.listing_urls
l_url.listing_ids
l_url.listings << l
l_url.listings.count
l_url.listings << l
l_url.listings.count
l.listings_matching_identifiers
l.listings_matching_urls
l.listings_matching_url
exit
s = Scrape.last
s.listings.last
l_url = ListingUrl.first
l_url.listings.find_by(scrape_id: 2)
url = 'https://www.google.de/shopping/product/11850082154367815'
detail_url = url + 'specs'
detail_url = url + '/specs'
Listing.find_by(url: detail_url)
exit
ManualText.first
ManualText.last
mt = _
mt.text.length
mt.text.squish.length
mt.text.squish
mt.text.squish.length
mt.text.squish.length - mt.text.length
exit
Youm
Yomu
pm = ProductManual.first
pm = ProductManual.first.attachment.url
url = ProductManual.first.attachment.url
content = Yomu.new(url)
content.text
content.text.squish
ManualText.where(text: nil)
pdf = Yomu.new(url)
pdf.text
SourceCategory.find 175
Listing.where("url like '%google.de%').first.url
Listing.where("url like '%google.de%'").first.url
Yomu.new url
exit
ListingUrl.count
ListingUrl.last
l_url = _
l_url.listings
Listing.find_by(url: l_url.url)
l = _
l_url.listings << l
l_url.reload
l_url.listings
Listing.where(url: l_url.url).size
orig = l.original_url
Listing.create_with(original_url: 'bahafasdas').find_or_create_by(scrape_id: l.scrape_id, url: l.url)
l.reload
Listing.find_or_create_by(scrape_id: l.scrape_id, url: l.url) do |listing|
  listings.original_url = 'asdlgkjasdgasdgas'
end
l.reload
ProductProdcutManual.confirmed
ProductProductManual.confirmed
ProductProductManual.confirmed.size
ProductManual.size
ProductManual.count
exit
listing_url = ListingUrl.first
listing_url.listings
listing_url.listings.find_by(scrape_id: 2)
listing_url.listings.find_by(scrape_id: 4)
scrape_id = 2
listing = listing_url.nil? ? nil : listing_url.listings.find_by(scrape_id: scrape_id)
listing
listing.present?
listing.data_hash.present? && listing.screenshots.count > 0
listing = Listing.last
listing.listing_url
l_url = _
l_url.listings
l_url.listings.size
l_url
Listing.count
Listing.last
listing.url
s = Scrape.last
Listing.find_by(scrape_id: 2, url: 'https://www.google.com/shopping/product/10465836192486734814/specs')
listing = Listing.find_by(scrape_id: 2, url: 'https://www.google.com/shopping/product/10465836192486734814/specs')
listing.screenshots
Products.last
Product.last
ProductProductManual.last
ppm = _
ppm = ProductProductManual.last
pm = ProductManual.last
ppm = ProductProductManual.last
ppm.product
ppm.manual
pm
url = 'http://162.220.52.187/samsung/samsung-manual-un32eh4003.pdf'
URI.parse(url)
uri = _
uri.path
uri.domain
uri.host
url =~ Resolv::IPv4::Regex
url
URI.host
uri.host
uri.host =~ Resolv::IPv4::Regex
host = '162.220.52.260'
host =~ Resolv::IPv4::Regex
0 == true
0 ? true : false
host
host =~ Resolv::IPv6::Regex
uri.host =~ Resolv::IPv4::Regex
uri.host =~ Resolv::IPv6::Regex
url
host = URI.parse(url).host
(host =~ Resolv::IPv4::Regex) || (host =~ Resolv::IPv4::Regex) ? true: false
Reslov::IPv6::Regex
Resolv::IPv6::Regex
(host =~ Resolv::IPv4::Regex) || (host =~ Resolv::IPv6::Regex) ? true: false
def is_ip?(url)
  host = URI.parse(url).host
  (host =~ Resolv::IPv4::Regex) || (host =~ Resolv::IPv6::Regex) ? true: false
end
ProductManual.all.select {|pm| is_ip?(pm.original_url)}
ProductManual.all.select {|pm| is_ip?(pm.original_url)}.size
pm = ProductManual.last
pm.attachment.url
ppm = ProductProductManual.find 105
pm = ppm.manual
pm.attachment.url
pm.attachment = pm.original_url
ppm = ProductProductManual.find 137
ppm.product
ppm.manual
pm = ppm.manual
pm.attachment = pm.original_url
ppm.product
pm
pm.attachment.url
url = _
pdf = Yomu.new url
pdf.text.squish
url = ProductManual.all.select {|pm| pm.attachment.url.include?('.png')}.first
url = ProductManual.all.select {|pm| pm.attachment.url.include?('.png')}.first.attachment.url
Yomu.new (url)
Yomu.new (pm.original_url)
pdf = Yomu.new (pm.original_url)
pdf.text
pm = ProductManual.all.select {|pm| pm.attachment.url.include?('.png')}.first
pm.original_url
pdf = Yomu.new (pm.original_url)
pdf.text
ManualText.last
ppm = ProductProductManual.last
ppm = ProductProductManual.order_by(:updated_at).last
ppm = ProductProductManual.all.order_by(:updated_at).last
ppm = ProductProductManual.all.order(:updated_at).last
ppm = ProductProductManual.all.order(:updated_at).first
ppm = ProductProductManual.all.order(:updated_at).last
ppm.manual
ppm.product
pm = ppm.manual
pm.text
pm
ppm
pm
manual = ppm.manual
pdf_content = manual.text.downcase
pdf_content.include?(product.model.downcase) && pdf_content.include?(product.brand.downcase)
product = ppm.product
pdf_content.include?(product.model.downcase) && pdf_content.include?(product.brand.downcase)
ppm.confirmed
ppm.confirmed?
ProductProductManual.confirmed
pdf_content.index(/Regexp.escape(product.brand).+Regexp.escape(product.model)/)
ProductManual.all.map(&:original_url)
pm = ProductManual.find_by(original_url: 'https://panasonic.ca/viewing/ALL/KX-TGA401C/OI/TGA401C_PNQX2391ZA/TGA401C_PNQX2391ZA.pdf')
pm.product_product_manuals
ppm = pm.product_product_manuals.first
ppm.product
ppm.manual
product = ppm.product
manual = ppm.manual
pdf_content = manual.text.downcase
pdf_content.index(/Regexp.escape(product.brand).+Regexp.escape(product.model)/)
pdf_content.include?(product.model.downcase) && pdf_content.include?(product.brand.downcase)
pdf_content = manual.text.downcase.squish
product.model
pdf_content.index(/Regexp.escape(product.brand).+Regexp.escape(product.model)/)
pdf_content.index(/Regexp.escape(product.brand).+?Regexp.escape(product.model)/)
pdf_content.index(/Regexp.escape(product.brand)/)
Regexp.escape(product.brand)
pdf_content.index(/Regexp.escape(product.brand)/)
pdf_content.index(/#{Regexp.escape(product.brand)}/)
pdf_content.index(/Panasonic/)
pdf_content =~ /#{Regexp.escape(product.brand)}/
pdf_content =~ /Panasonic/
pdf_content.include?(/Panasonic/)
pdf_content.include?('Panasonic')
pdf_content
pdf_content.include?('panasonic')
pdf_content =~ /#{Regexp.escape(product.brand.downcase)}/
pdf_content =~ /#{Regexp.escape(product.brand.downcase)}.+#{Regexp.escape(product.model.downcase)}/
pdf_content.index(/Panasonic/)
pdf_content.index(/Regexp.escape(product.brand.downcase).+?Regexp.escape(product.model.downcase)/)
pdf_content.index(/#{Regexp.escape(product.brand.downcase)}.+?#{Regexp.escape(product.model.downcase)}/)
pdf_content.index(/Panasonic/)
ProductProductManual.last
ProductProductManual.last.manual
ProductProductManual.last(5).map(&:manual)
pdf_content.index(/user\smanual/)
pdf_content
manual.original_url
pdf_content.index(/installation\smanual/)
pdf_content =~ (/user\s(manual|guide)/)
pdf_content =~ (/installation\s(manual|guide)/)
pdf_content =~ (/(product|technical)\s(specifications|specs)/)
pdf_content[_..pdf_content.length-1]
product
pdf_content =~ (/#{Regexp.escape(product.brand.downcase)}.+#{Regexp.escape(product.model.downcase)}/) ? true : false
pdf_content =~ (/#{Regexp.escape(product.brand.downcase)}\s#{Regexp.escape(product.model.downcase)}/) ? true : false
pdf_content =~ (/#{Regexp.escape(product.brand.downcase)}.+#{Regexp.escape(product.model.downcase)}/) ? true : false
model = "some model i have"
Regexp.escape(model)
Regexp.escape('sera omis dans ce manuel')
pdf_content =~ (/_/)
exit
Listing.last
l = _
l.reload
Listing.where(scrape_id: 3)
listing = Listing.where(scrape_id: 3).first
listing.url
listing.reload
listing.reload.product_description
Listing.where(scrape_id: 3).where.not(product_description: nil).size
Listing.where(scrape_id: 3).where.not(product_description: nil)
listing = Listing.find_by(scrape_id: 3, url: 'http://www.amazon.com/Transformer-Touch-Laptop-T100TA-H1-GR-500GB/dp/B00IVLHJ2M')
listing.product_description
s = Scrape.last
s.products
Listing.where(scrape_id: 3).where.not(product_description: nil)
Listing.where(scrape_id: 3).where.not(product_description: nil).size
Listing.where(scrape_id: 3).where(product_description: nil).size
Listing.where(scrape_id: 3).where.not(product_description: nil).size
Listing.where(scrape_id: 3).map(&:product_description)
Listing.where(scrape_id: 3).first
Listing.where(scrape_id: 3).first.url
Listing.find_by(scrape_id: 3, url: 'http://www.amazon.com/AmazonBasics-High-Speed-HDMI-Cable-Standard/dp/B014I8SSD0')
l = _
l.product_description
l.listing_url
l.reload
l.categories
l.original_url
s.reload
s.sellers
s.sellers.size
l
l.listings_matching_url
Listing.where(url: l.url).size
listing_url = ListingUrl.last
listing_url.listings
listing_url.reload
listing_url.listings
Listing.find_by(url: 'https://www.amazon.com/NEEWER%C2%AE-Dimmable-Digital-Camcorder-Panasonic/dp/B004TJ6JH6')
listing_url = ListingUrl.first
listing_url.listings
ListingUrl.all.select {|lurl| lurl.listings.size == 0 }
s.listings.size
Product.where(brand: 'neewer')
url = 'https://www.amazon.com/NEEWER%C2%AE-Dimmable-Digital-Camcorder-Panasonic/dp/B004TJ6JH6'
detail_url = url
url_md5 = Digest::MD5.hexdigest(detail_url)
listing_url = ListingUrl.find_or_create_by(url_md5: url_md5) do |l_url|
  l_url.url = detail_url
end
listing = listing_url.nil? ? nil : listing_url.listings.find_by(scrape_id: scrape_id)
scrape_id = 3
listing = listing_url.nil? ? nil : listing_url.listings.find_by(scrape_id: scrape_id)
def listing_complete?(listing)
  listing.data_hash.present? && listing.product_description.present? && listing.screenshots.size > 0
end
if listing.present? && listing_complete?(listing)
  puts 'asdlfkjasdl;fjasdl;fkjasd'
end
url
visited = 'https://www.amazon.com/NEEWER%C2%AE-Dimmable-Digital-Camcorder-Panasonic/dp/B004TJ6JH6'
visted = url
visited = 'https://www.amazon.com/NEEWER%C2%AE-Dimmable-Digital-Camcorder-Panasonic/dp/B004TJ6JH6'
visted == url
Sellers.count
Seller.count
Product.count
Listing.count
def stats(s)
  s = Scrape.includes(:listings, :products).find s.id
  p = s.listings.map(&:product_ids).flatten.uniq.size
  l = s.listings.size
  no_product = s.listings.select { |l| l.product_ids.empty? }.size
  time = (s.duration || Time.now - s.created_at).to_f / 60
  rate = l / time
  full_scrape = (500000 / rate / 60 / 24)
  puts "Scrape id: #{s.id} Source: #{s.source_id}"
  puts "Ratio: #{p}/#{l} products/listing"
  puts "Listings without product: #{no_product}"
  puts "#{((p.to_f / l) * 100).round(2)} percent found Products"
  puts "Time: #{time.round(2)}mins Rate: #{rate.round(2)} created: #{s.created_at.strftime("%m/%d/%Y")}"
  puts "#{full_scrape.round(2)} days for full scrape"
  puts '=============================='
  nil
end
s = Scrape.last
stats s
s = Scrape.last
stats s
s.listings.count
s.products.count
s.sellers.count
Sellers.count
Seller.count
ScrapeListingSeller.count
exit
ppm = ProductProductManual.last
ppm.product
ppm.manual
pm = ProductManual.last
pm.product_product_manuals
img = Asset.find 1858
pdf = Yomu.new _.original_url
pdf.text
Listing.includes(:manuals).first
p = Product.last
p.images
Listing.includes(:images).last
ListingAsset.last
l = ListingAsset.last
l.images
la = ListingAsset.last
la.listing
l = _
la
la.asset
l
l.manuals
Product.includes(:images).last
Listing.includes(:images).last
Listing.includes(:images).last.to_sql
Listing.includes(:images).limit(1).to_sql
exit
Listing.includes(:images).last
exit
Listing.includes(:images).last
Listing.includes(:images, :manuals, :screenshots).last
l = Listing.includes(:images, :manuals, :screenshots).last
l.images
l.images.map(&:type)
exit
p = Product.last
p.images
p = Product.includes(:images).last
exit
l = Listing.includes(:manuals).last
l.manuals
exit
ManualText.where(text: nil)
l = Listing.last
Digest::MD5.hexdigest(l.data_hash)
Digest::MD5.hexdigest(l.data_hash.to_s)
l.data_hash
l.data_hash.to_s
Digest::MD5.hexdigest(l.data_hash.to_s)
l.data_hash
l.data_hash[:General]
arr = [1,2,3]
arr += [2,3,5,6]
arr
arr.uniq
arr
arr = [1,2,3]
arr |= [2,3,5,6]
arr
ProductManual.to_s
ProductManual.to_s.tableize
Asset
ProductManual.last
pm = )
ProductManual.last
ProductManual.create
l_url = ListingUrl.first
l_url.listing_ids
id = 102
l_url.listing_ids = [1]
l =Listing.find 102
l.listing_url
l_url.listing_ids |= [102]
l.reload
l.listing_url
exit
ProductManual.where(assetable_type: 'Product').count
exit
l = Listing.first
l.manuals
ProductManual.first
pm = _
pm.lisitngs
pm.listings
l = Listing.find 2
l.manuals.size
l.manuals.first.product_product_manual
l.manuals.first.product_product_manuals
Listing.includes(:products, :sellers, :scrapes, :categories, { product_product_manuals: :manuals }).where('product_product_manuals.status': 1).first 2
Listing.includes(:products, :sellers, :scrapes, :categories, manuals: :product_product_manuals[C }).where('product_product_manuals.status': 1).first 2
Listing.includes(:products, :sellers, :scrapes, :categories, manuals: :product_product_manuals).where('product_product_manuals.status': 1).first 2
Listing.includes(:products, :sellers, :scrapes, :categories, {manuals: :product_product_manuals}).where('product_product_manuals.status': 1).first 2
Listing.includes( {manuals: :product_product_manuals}).where('product_product_manuals.status': 1).first 2
Listing.includes( {manuals: :product_product_manuals}).first 2
l.manuals.first.product_product_manuals.first
exit
ProductManual.confirmed
pms = ProductManual.where(assetable_type: 'Product').select('original_url')
pms = ProductManual.select('original_url')
pms = ProductManual.select('original_url').size
pms = ProductManual.select('original_url').uniq
pms = ProductManual.select('original_url').uniq.size
ProductProductManual.size
ProductProductManual.count
pm = ProductManual.select('id', 'original_url').first
pm.id
pm.url
exit
str = 'Hardware > Power & Electrical Supplies > Solar Panels'
category = Category.find_by name: 'Solar Panels')
category = Category.find_by name: 'Solar Panels'
category = Category.find_by name: str
str.split('>')
str.split('>').map(&:strip)
name = str.split('>').map(&:strip).last
category = Category.find_by name: name
Category.all.map(&:parent_id)
Category.where(parent_id: nil)
upc = '632423076098'
upc.length
upc2 = '632423076098'
upc == upc2
@keys = [
  :product_name, :product_description, :ruvixx_category, :product_category, :product_url,
  :original_url, :brand, :model, :mpn, :alternate_models, :product_features, :contact_name,
  :seller_name, :seller_url, :currency_code, :min_price, :max_price, :gtin, :asin, :upc, :sku, :product_images
]
@keys.length
@keys.sort
@keys
Asset
Asset.first
exit
Listing.import
Listing.search_by({keywords: ['HDMI']}, 'AND')
Listing.search_by({keywords: ['HDMI']}, 'AND', {}, {size: 1}).page(1).results
result = _
result.map {|l| l.id }
data = { :General => { a: 'something', :'' => '' }}
hash = data
data[:General].delete(:'')
data
Money.new(39.99, 'USD')
price = _
price.to_f
Money.new(39._9, 'USD')
price = Money.new(39_99, 'USD')
price.to_f
Monetize.parse("USD 100.50")
s = Scrape.last
ProductImage.find_by(original_url: 'https://www.ebluejay.com/img/account/b/a/bahiasolar/1/bd7c992.jpg')
s = Scrape.last
s.listings
s.listings.count
l = s.listings.first
l
l.categories
s.listings.destroy_all
s.destroy
ScrapeListingSeller.first
ScrapeListingSeller.first.min_price
ScrapeListingSeller.first.min_price.to_f
l = Listing.last
l.images
l.id
s = Scrape.last
s.listings.count
s.listings.destroy_all
s.destroy
Listing.initialize
l = Listing.new
l.valid?
l.errors.full_messages
reload!
ProductImage.last
s = Scrape.last
s.listings.count
s.images
s.listings.first
l = _
l.images
l.destroy
s.destroy
s = Scrape.last
s.listings.destroy_all
s.destroy
l = Scrape.last.listings.last
l.images
s = Scrape.last
s.listings.size
s.listings.destroy_all
s.destroy
s = Scrape.last
s.destroy
s.listings.size
s.listings.destroy_all
s.destroy
s = Scrape.last
s.listings.destroy_all
s.destroy
s = Scrape.last
s.listings.destroy_all
s.destroy
s = Scrape.last
s.listings.destroy_all
s.destroy
s = Scrape.last
s.listings.destroy_all
s.destroy
s = Scrape.last
l = Listing.find 321
l = _
l.url
s = Scrape.last
s.listings.size
s = Scrape.last
s.listings.destroy_all
s.destroy
ListingUrl.find 
ListingUrl.find_by url: 'https://www.ebluejay.com/item/5572140'
ListingUrl.last
s = Scrape.last
s.listings.destroy_all
s.destroy
ListingUrl.last
s = Scrape.last
s.listings.destroy_all
s.destroy
ListingUrl.last
ListingUrl.last.destroy
ListingUrl.last
ListingUrl.last.destroy
ListingUrl.last
url = "http://www.bonanza.com/listings/Solar-panel-Cell-50-Pcs-2-8W-125mm-Flexible-Maxeon-Sunpower-C60-Monocrystalline/290859447"
url = url.gsub(/\?.+$/)
ListingUrl.last.destroy
ListingUrl.last
ListingUrl.last.destroy
ListingUrl.last
ListingUrl.last.destroy
ListingUrl.last
ListingUrl.last.destroy
ListingUrl.last
s = Scrape.last
s.listings.destroy_all
s.destroy
s = Scrape.last
s.listings.destroy_all
s.destroy
s = Scrape.last
ListingUrl.last
ListingUrl.last 2
l = Listing.last
l.url
l.products
Product.where(model: nil).destroy_all
s = Scrape.last
s.images
s.listings.each do |l|
  p l.images
end
s.listings.each do |l|
  l.images.each do |img|
    img.update original_url: img.original_url.strip
  end
end
ProductImage.select {|img| img.original_url.starts_with(' ') }
ProductImage.select {|img| img.original_url.starts_with?(' ') }
url = ""
url = "https://s3.amazonaws.com/bonanzleimages/afu/images/2762/0767/75/__12.jpg\u{a0}"
encoding_options = {
  :invalid           => :replace,  # Replace invalid byte sequences
  :undef             => :replace,  # Replace anything not defined in ASCII
  :replace           => '',        # Use a blank for those replacements
  :universal_newline => true       # Always break lines with \n
}
url
url = "https://s3.amazonaws.com/bonanzleimages/afu/images/2762/0767/75/__12.jpg\u{a0}"
url.encode(Encoding.find('ASCII'), encoding_options)
url
Asset.count
Asset.each do |a|
  a.original_url.encode(Encoding.find('ASCII'), encoding_options)
end
Asset.find_each do |a|
  a.original_url.encode(Encoding.find('ASCII'), encoding_options)
Asset.find_each do |a|
  a.update original_url: a.original_url.encode(Encoding.find('ASCII'), encoding_options)
end
Asset.find_each do |a|
  next if a.original_url.nil?
  a.update original_url: a.original_url.encode(Encoding.find('ASCII'), encoding_options)
end
Asset.find_each do |a|
  next if a.original_url.nil?
  a.update original_url: a.original_url.encode(Encoding.find('ASCII'), encoding_options).strip
end
url = " http://www.dhresource.com/0x0s/f2-albu-g3-M01-A9-23-rBVaHVa8NvOAaiioAAIcs10XoC8365.jpg/Promotion%20solar%20panel%20with%20frame%20110W"
url.encode(Encoding.find('ASCII'), encoding_options).strip
exit
p = Product.all.sample
p.last_scrape
p.scrapes
p.listings
p = Product.first
p.scrapes
p.last_scrape
p.last_scrape.created_at.strftime('%F')
.listings.map do |listing|
{
  source_name: listing.scrape.source_name,
  url: listing.url
}
end.uniq
edit -t
p
edit -t
p.images.to_a.compact.map { |a| a.image.url }
p.screenshots.to_a.compact.map { |a| a.screenshot.url }
object = p
sls_map = {}
scrape_listing_sellers = object.listings.includes(:scrape_listing_sellers).flat_map { |l| l.scrape_listing_sellers.includes(:seller) }.uniq
scrape_listing_sellers.each do |sls|
  next if sls.seller.nil? && sls.min_price.nil?
  seller = sls.seller || Seller.new(name: "NO SELLER")
  sls_map[seller] = sls_map.key?(seller) ?
  [sls_map[seller], sls].max_by(&:updated_at)
  :
  sls
end
scrape_listing_sellers.each do |sls|
  next if sls.seller.nil? && sls.min_price.nil?
  seller = sls.seller || Seller.new(name: "NO SELLER")
  sls_map[seller] = sls_map.key?(seller) ?
  [sls_map[seller], sls].max_by(&:updated_at)
  :
  sls
end
edit -t
sellers
object.source_by_scrape_time.map do |source|
  source
  .as_json(only: [:id, :name])
  .merge(scrapes: object.source_scrapes(source.id).map do |scrape|
      scrape
      .as_json(only: :id)
      .merge scrape_time: scrape.created_at.strftime('%F')
  end)
end
edit -t
object.source_details
edit -t
ppm = product_product_manuals.find 276
ppm = ProductProductManual.find 276
ppm = ProductManual.find_by asset_id: 276
ppm = ProductProductManual.find_by asset_id: 276
ppm.as_json(only: [:id, :status])
edit -t
ProductUpdateRequest.find_by(product: object, status: nil).present?
p.id
p.images.where(scrape_id: 1)
p.images
scrape_id: 1
scrape_id = 1
edit -t
source_product_images = p.listings.where(scrape_id: scrape_id).map(&:images).map do |i|
edit -t
i = listings.where(scrape_id: scrape_id).map(&:images).first
i = p.listings.where(scrape_id: scrape_id).map(&:images).first
edit -t
edit -0t
edit -t
l = p.listings.first
l.scrape_listing_sellers
l.scrape_listing_sellers.pluck(:min_price, :max_price)
l.scrape_listing_sellers.pluck(:min_price, :max_price).flatten.minmax
l.scrape_listing_sellers.pluck(:min_price, :max_price).flat_map(&:to_f).minmax
l.scrape_listing_sellers.pluck(:min_price, :max_price).flat_map
l.scrape_listing_sellers.pluck(:min_price, :max_price)
l.scrape_listing_sellers.pluck(:min_price, :max_price).flatten
l.scrape_listing_sellers.pluck(:min_price, :max_price).flatten.map(&:to_f).minmax
l.scrape_listing_sellers.pluck(:min_price, :max_price).compact.flatten.map(&:to_f).minmax
l.scrape_listing_sellers.pluck(:min_price, :max_price).flatten.map(&:to_f).reject(&:zero?).minmax
edit -t
l.scrape
l.scrape.source
p = Product.find 4
p.name
exit
Listing.import
exit
Listing.import
exit
Listing.import
t
Product.import
Scrape.find 3
Scrape.last
Scrape.first 3
Listing.where.not(scrape_id: [1,2,3]).destroy_all
Product.import
search_result = Listing.search_by(terms, operator, filters, { size: per_page }).page(page).results
lids = search_result.map { |l| l.id }.compact
operator = 'AND'
terms = { keywords: 'hdmi' }
filters = {}
filters[:scrape_id] = params[:scrape_id] if params[:scrape_id].present?
filters = {}
search_result = Listing.search_by(terms, operator, filters, { size: per_page }).page(page).results
search_result = Listing.search_by(terms, operator, filters, { size: 400 }).page(1).results
lids = search_result.map { |l| l.id }.compact
_.size
Listing.count
search_result.map
search_result.map {|l| l}
search_result.flat_map {|l| l[products].map {|p| p.id} }
search_result.flat_map {|l| l.products.map {|p| p.id} }
search_result.flat_map {|l| l.products.map {|p| p.id} }.size
search_result.flat_map {|l| l.products.map {|p| p.id} }.uniq
search_result.flat_map {|l| l.products.map {|p| p.id} }.uniq.size
Listing.import
search_result = Listing.search_by(terms, operator, filters, { size: per_page }).page(page).results
search_result = Listing.search_by(terms, operator, filters, { size: 400 }).page(1).results
search_result.flat_map {|l| l.products.map {|p| p.id} }.uniq.size
Listing.import
exit
Listing.import
operator = 'AND'
terms = { keywords: 'HDMI' }
per_page = 10
page = 1
filters = {}
search_result = Listing.search_by(terms, operator, filters, { size: per_page }).page(page).results
pids = search_result.flat_map {|l| l.products.map {|p| p.id} }.uniq.size
Product.import
search_result = Product.search_by(terms, operator, filters, { size: per_page }).page(page).results
pids = search_result.map {|p| p.id }.uniq.size
pids = search_result.map {|p| p.id }
Product.find(269)
p = _
p.listing
p.listings
p.listings.first.url
pids = search_result.map {|p| p.id }.uniq.size
search_result = Product.search_by(terms, operator, filters, { size: 1000 }).page(page).results
pids = search_result.map {|p| p.id }.uniq.size
Listings.includes(:products).where('products.id': pids).ids
Listing.includes(:products).where('products.id': pids).size
pids = search_result.map {|p| p.id }.uniq
Listing.includes(:products).where('products.id': pids).size
Listing.includes(:products).where('products.id': pids)
listing_search_result = Listing.search_by(terms, operator, filters, { size: 1000 }).page(page).results
lids = listing_search_result.map {|p| p.id }
lids = listing_search_result.map {|p| p.id }.size
lids = listing_search_result.flat_map {|l| l.products.map {|p| p.id }}.uniq
lids = listing_search_result.flat_map {|l| l.products.map {|p| p.id }}.uniq.size
exit
Product.import
lids = listing_search_result.flat_map {|l| l.products.map {|p| p.id }}.uniq
listing_search_result = Listing.search_by(terms, operator, filters, { size: 1000 }).page(page).results
filters = {}
operator = 'AND'
terms = { keywords: 'hdmi' }
listing_search_result = Listing.search_by(terms, operator, filters, { size: 1000 }).page(page).results
listing_search_result = Listing.search_by(terms, operator, filters, { size: 1000 }).page(1).results
lids = listing_search_result.flat_map {|l| l.products.map {|p| p.id }}.uniq
lids.size
Product.import
Listing.import
terms = { keywords: 'hdmi' }
operator = 'AND'
filters = {}
page = 1
listing_search_result = Listing.search_by(terms, operator, filters, { size: 1000 }).page(1).results
lids = listing_search_result.flat_map {|l| l.products.map {|p| p.id }}.uniq
lids.size
product_search_result = Product.search_by(terms, operator, filters, { size: 1000 }).page(1).results
exit
terms = { keywords: 'hdmi' }
operator = 'AND'
filters = {}
page = 1
listing_search_result = Listing.search_by(terms, operator, filters, { size: 1000 }).page(1).results
lids = listing_search_result.flat_map {|l| l.products.map {|p| p.id }}.uniq
lids.size
product_search_result = Product.search_by(terms, operator, filters, { size: 1000 }).page(1).results
pids = product_search_result.map {|p| p.id }.uniq
pids.size
lids.size
lids = listing_search_result.flat_map {|l| l }
pids.first
lids.first
lids = listing_search_result.flat_map {|l| l.products.map {|p| p.id }}.uniq
lids.first
pids.first
lids - pids
pids.size
pids.first
pids = pids.map(&:to_i)
lids - pids
p = Product.find 89
p.as_indexed_json
p.manuals
ProductListing.where(product: p)
pl = _.first
l = Listing.find 69
l.manuals
p.manuals
listing.manuals
l.manuals.size
ppm = ProductProductManual.where(product: p)
l.manuals
l.manuals.confirmed
pm = ProductManual.find 328
pm.listings
pm.products
p
p.screenshots
p = Product.find 2
p.sellers.size
p.sellers.uniq.size
p = Product.find 2
p.listings
p.listings.size
p.listings.map(&:scrape_listing_sellers)
p.listings.map { |l| l.scrape_listing_sellers.min_price }
p.listings.map { |l| l.scrape_listing_sellers.map(&:min_price) }
prices  = p.listings.flat_map { |l| l.scrape_listing_sellers.map(&:min_price) }
prices.first
prices.any?(&:nil?)
prices
prices.minmax
min, max = _
min
min.to_s
max.to_s
ScrapeListingSeller.first
Currency.find 1
price = '4339.99'
text = '$1,579'
min =  text.delete('^0-9.'),
min =  text.delete('^0-9.')
price_text = '$3,299.99'
price =     {
  min: price_text.delete('^0-9.'),
  currency: price_text.delete('0-9., ')
}
sls = ScrapeListingSeller.first
sls.update min_price price[:min].to_f
sls.update min_price: price[:min].to_f
sls.min_price
sls.min_price_cents
sls.min_price_cents.to_s
ProductImage.first
Listing.import
exit
Listing.import
exit
Listing.import
Product.import
exit
s = Source.last
s.create_at
s.created_at
s.created_at.format("F"
)
s.created_at.format("F")
s.created_at.format("%b")
s.created_at.strftime("%b")
s.created_at.strftime("%b%-d")
s.created_at.strftime("%b %-d, %Y")
s
s.attributes
attrs = s.attributes
Scrape.where.not(id: [1,2,3,4]).destroy
Scrape.where.not(id: [1,2,3,4]).destroy_all
attrs
attrs.delete('id')
attrs
attrs.delete(:created_at, :updated_at)
attrs.delete('created_at', 'updated_at')
attrs.delete('created_at')
attrs.delete('updated_at')
attrs.delete('pretty_name')
attrs
s = Source.create(attrs)
exit
attrs = {"name"=>"hand scrape",
  "domain"=>nil,
  "job_name"=>nil,
  "stack_id"=>nil,
  "source_type_id"=>2,
  "upload_url"=>
  "https://rvx-rds-dev.s3.amazonaws.com/import/1459999842404-tshb7cpm8ih-2485b566c65541649da4c9cfbef1e553/KL_SunPower+-+Google+REQUIREMENTS+20160321+complete.xlsx.zip",
"file_hash"=>"a58dd51ae70ff5445eb0e02eaa1ae37d"}
attrs
s = Source.create(attrs)
exit
Scrape.last 2
Source.last 2
_.destroy_all
Scrape.last(2).destroy_all
Scrape.last(2).each(&:destroy)
Source.last(2).each(&:destroy)
Source.last 2
Source.last(2).each(&:destroy)
Source.last 2
Scrape.last
operator = 'AND'
terms = { keywords: 'led' }
p = Product.first
p.listings
eixt
exit
Product.import
exit
Product.import
Listing.where.not(product_description: nil)
Listing.where.not(product_description: nil).where(scrape_id: 3)
Listing.where.not(product_description: nil).where(scrape_id: 3).all.select {|l| l.manuals.confirmed.size > 1}
Category.select(:id, :name)
l = Listing.first
l.categories
Category.last
category = +
c = Category.last
c.listings
Category.where(name: 'Solar Panels')
p = Product.last
p = Product.first
p.categories
exit
Category.first
Category.where.not(source_id: nil)
Category.count
img = Asset.find 2036
img.original_url
url = "https://s3.amazonaws.com/bonanzleimages/afu/images/2296/6010/95/41xtx-qnwbl._sl1500_.jpg"
url.scan('http')
url.scan('http').count > 1
url = "https://s3.amazonaws.com/bonanzleimages/afu/images/2296/6010/95/41xtx-qnwbl._sl1500_.jpg\nhttps://s3.amazonaws.com/bonanzleimages/afu/images/2296/6010/95/41xtx-qnwbl._sl1500_.jpg"
url.scan('http').count > 1
url.scan('http')
url.split('http')
url.split('http').map(&:strip)
url.split('http').map(&:strip).map {|l| "http#{l}"}
Category.first 122
l = Listing.last
s = Scrape.last
exit
s = Scrape.last
s.source
Category.count
Category.last
img = Asset.find 17
img.original_url
img.original_url.strip
Encoding.find('ASCII')
ENCODING_OPTIONS = {
  :invalid           => :replace,  # Replace invalid byte sequences
  :undef             => :replace,  # Replace anything not defined in ASCII
  :replace           => '',        # Use a blank for those replacements
  :universal_newline => true       # Always break lines with \n
}
img_src = Asset.find(17).original_url
img_src.encode(Encoding.find('ASCII'), ENCODING_OPTIONS)
url = 'http%3A%2F%2Fwww.onlineretailcenter.com%2Fprosales_images%2F41KVV2biDaL.jpg&d=bf7b125d4476917258d532ed8adaac52b02682bd&hei=1000&wid=1000&op_sharpen=1'
URI.parse(url)
URI.encode(url)
URI.decode(url)
img = Asset.find 133
img.listing
img.listings
_.url
img.listings
img.listings.first.url
url = "http://c.shld.net/rpx/i/s/pi/mp/38744/prod_8998689225?src=http%3A%2F%2Fwww.onlineretailcenter.com%2Fprosales_images%2F41PFSajH3-L.jpg&d=891a06cbd0fbec321655c229cf92613e883bd94e&hei=1000&wid=1000&op_sharpen=1, http://c.shld.net/rpx/i/s/pi/mp/38744/prod_8998688525?src=http%3A%2F%2Fwww.onlineretailcenter.com%2Fprosales_images%2F41KVV2biDaL.jpg&d=bf7b125d4476917258d532ed8adaac52b02682bd&hei=1000&wid=1000&op_sharpen=1, http://c.shld.net/rpx/i/s/pi/mp/38744/prod_8998688825?src=http%3A%2F%2Fwww.onlineretailcenter.com%2Fprosales_images%2F51zUV0-cPgL.jpg&d=8135ab7194a3ad31cfe7482d4c989a49cd1dcf2c&hei=1000&wid=1000&op_sharpen=1"
url.split('http').map(&:strip).map { |l| "http#{l}" })
url.split('http').map(&:strip).map { |l| "http#{l}" }
url
url.split('http')
url
url.strip.split('http')
url.split('http').map(&:strip).reject(&:blank?).map { |l| "http#{l}" }
url = "http://c.shld.net/rpx/i/s/pi/mp/38744/prod_8998689225?src=http%3A%2F%2Fwww.onlineretailcenter.com%2Fprosales_images%2F41PFSajH3-L.jpg&d=891a06cbd0fbec321655c229cf92613e883bd94e&hei=1000&wid=1000&op_sharpen=1"
url.split('http').map(&:strip).reject(&:blank?).map { |l| "http#{l}" }
url
img = Asset.find 75
img.listings.first
img.listings.first.url
url = 'http://www.dhresource.com/albu_736700276_00-1.0x0/Promotion%20solar%20panel%20with%20frame%2018W,%20with%20SunPower%20cells,%200.9M%20cable,%20fit%20to%20charge%2012V%20battery..jpg'
url.strip
img_src = url
img_src.include?('src=')
img_src.scan('http').size > 1
img_src = img_src.encode(Encoding.find('ASCII'), ENCODING_OPTIONS)
img_src = Asset.find(146).listings.first.url
img = Asset.find 158
img.listings.first
img.listings.first.url
product_imgs = 'http://www.dhresource.com/albu_952970221_00-1.0x0/free%20shipping%20100W%20flexible%20solar%20panel%20with%20Connection%20box+0.9M%20cable,black%20color%20backsheet,%2012V%20battery%20system%20solar%20charger..jpg, http://www.dhresource.com/0x0s/f2-albu-g1-M01-6E-24-rBVaGVSSi-eAPsiUAAGhR4lVz98837.jpg/free%20shipping%20100W%20flexible%20solar%20panel%20with%20Connection%20box+0.9M%20cable,black%20color%20backsheet,%2012V%20battery%20system%20solar%20charger..jpg, http://www.dhresource.com/0x0s/f2-albu-g3-M00-50-B1-rBVaHFZvW6uAS6zgAAL9q1XKSaQ495.jpg/free%20shipping%20100W%20flexible%20solar%20panel%20with%20Connection%20box+0.9M%20cable,black%20color%20backsheet,%2012V%20battery%20system%20solar%20charger..jpg, http://www.dhresource.com/0x0s/f2-albu-g3-M01-C1-59-rBVaHFZvW6uAEUxxAAHtjBPqSZg213.jpg/free%20shipping%20100W%20flexible%20solar%20panel%20with%20Connection%20box+0.9M%20cable,black%20color%20backsheet,%2012V%20battery%20system%20solar%20charger..jpg, http://www.dhresource.com/0x0s/f2-albu-g3-M00-A8-E1-rBVaHFZvW6uAdbqjAAJ1KiSqdII626.jpg/free%20shipping%20100W%20flexible%20solar%20panel%20with%20Connection%20box+0.9M%20cable,black%20color%20backsheet,%2012V%20battery%20system%20solar%20charger..jpg, http://www.dhresource.com/0x0s/f2-albu-g3-M00-A8-E1-rBVaHFZvW6uAdbqjAAJ1KiSqdII626.jpg/free%20shipping%20100W%20flexible%20solar%20panel%20with%20Connection%20box+0.9M%20cable,black%20color%20backsheet,%2012V%20battery%20system%20solar%20charger..jpg, http://www.dhresource.com/0x0s/f2-albu-g3-M00-A8-E1-rBVaHFZvW6uAdbqjAAJ1KiSqdII626.jpg/free%20shipping%20100W%20flexible%20solar%20panel%20with%20Connection%20box+0.9M%20cable,black%20color%20backsheet,%2012V%20battery%20system%20solar%20charger..jpg, http://www.dhresource.com/0x0s/f2-albu-g1-M00-77-50-rBVaGVZvX-aAUvt5AAHZI1zua0E377.jpg/free%20shipping%20100W%20flexible%20solar%20panel%20with%20Connection%20box+0.9M%20cable,black%20color%20backsheet,%2012V%20battery%20system%20solar%20charger..jpg, http://www.dhresource.com/0x0s/f2-albu-g2-M01-0B-F5-rBVaGlZvX-aAAXUBAAIS_TFFbLA038.jpg/free%20shipping%20100W%20flexible%20solar%20panel%20with%20Connection%20box+0.9M%20cable,black%20color%20backsheet,%2012V%20battery%20system%20solar%20charger..jpg'
product_imgs.split(',')
product_imgs.include("%2C")
product_imgs.include?("%2C")
url = "http://www.dhresource.com/albu_952970221_00-1.0x0/free%20shipping%20100W%20flexible%20solar%20panel%20with%20Connection%20box+0.9M%20cable,black%20color%20backsheet,%2012V%20battery%20system%20solar%20charger..jpg"
URI.encode(url)
URI.decode(url)
url
product_imgs.scan(/https?:\/\/[\w.-\/]*?\.(jpe?g|gif|png)/)
product_imgs.scan(/https?.+(jpe?g|gif|png)/)
product_imgs
product_imgs.scan(/http.+\.jpg/)
product_imgs.scan(/http.+\.jpg,/)
product_imgs.split(/http.+\.jpg,/)
product_imgs.split(/,(?=[\s])/)
product_imgs.split(/,(?=[\s])/).size
product_imgs.split(/,(?=[http])/).size
product_imgs.split(/,(?=[\w])/).size
product_imgs.split(/,(?=[\w])/)
product_imgs.split(/,(?=[\s])/)
product_imgs.split(/,(?=[\\s])/)
product_imgs.split(/,(?=[^\w])/)
product_imgs.split(/,(?!\w])/)
product_imgs.split(/,(?!\w)/)
product_imgs.split(/,(?=\s)/)
product_imgs.split(/,(?![.http])/)
product_imgs.split(/,(?=.http)/)
product_imgs.encode(Encoding.find('ASCII'), ENCODING_OPTIONS)
product_imgs.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).split(/,(?=.http)/)
product_imgs.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).split(/,(?=http)/)
url = 'http://c.shld.net/rpx/i/s/pi/mp/38744/prod_9349985925?src=http%3A%2F%2Fwww.onlineretailcenter.com%2Fprosales_images%2F51T1LdhxFfL.jpg&d=2e4fd0314807b3f8a8b13f1ab8a452726d53ae94&hei=1000&wid=1000&op_sharpen=1'
url.split(/(?<!src)http/).map(&:strip).reject(&:blank?).map { |l| "http#{l}" })
url.split(/(?<!src)http/).map(&:strip).reject(&:blank?).map { |l| "http#{l}" }
url.split(/(?<!src=)http/).map(&:strip).reject(&:blank?).map { |l| "http#{l}" }
url.scan(/(?<!src=)http/)
url.scan(/http/)
url.scan(/(?<!src=)http/)
s = Scrape.last
s.products
s.listings.last
s.listings.last.images
s.listings.last.images.first.original_ul
s.listings.last.images.first.original_url
pm = ProductManual.first
pm = ProductManual.last
product_manual.products
pm.products
pm.product_product_manuals
ManualText.last
pm.attachment.url
pm.text
pm.text.length
pm.text.squish.length
s = Scrape.last
s.listings.last
l = _
l.screenshots
l.images
s.listings.first
l = s.listings.first
l.screenshots
l = s.listings.last
l.screenshots
l.id
l = Listing.find 295
l.screenshots
Product.screenshot
ProductScreenshot.last
ProductScreenshot.last.listings.first.url
l = Listing.all.sample
l.name
l.url
l.original_url
img = Asset.find 484
img.original_url
img.listings.first.url
product_imgs = '//www.image-tmart.com/prodimgs/1/13012676/50W-Monocrystalline-Solar-Panel-18V-TUV-RV-Motorhome-Boat-50-Watt_600x600.jpg , //www.image-tmart.com/prodimgs/1/13012676/50W-Monocrystalline-Solar-Panel-18V-TUV-RV-Motorhome-Boat-50-Watt_1_600x600.jpg , //www.image-tmart.com/prodimgs/1/13012676/50W-Monocrystalline-Solar-Panel-18V-TUV-RV-Motorhome-Boat-50-Watt_2_600x600.jpg , //www.image-tmart.com/prodimgs/1/13012676/50W-Monocrystalline-Solar-Panel-18V-TUV-RV-Motorhome-Boat-50-Watt_3_600x600.jpg , //www.image-tmart.com/prodimgs/1/13012676/50W-Monocrystalline-Solar-Panel-18V-TUV-RV-Motorhome-Boat-50-Watt_4_600x600.jpg , //www.image-tmart.com/prodimgs/1/13012676/50W-Monocrystalline-Solar-Panel-18V-TUV-RV-Motorhome-Boat-50-Watt_5_600x600.jpg , //www.image-tmart.com/prodimgs/1/13012676/50W-Monocrystalline-Solar-Panel-18V-TUV-RV-Motorhome-Boat-50-Watt_7_600x600.jpg , //www.image-tmart.com/prodimgs/1/13012676/50W-Monocrystalline-Solar-Panel-18V-TUV-RV-Motorhome-Boat-50-Watt_8_600x600.jpg'
product_imgs.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).split(/,(?=\s?http)/).map(&:strip)
product_imgs.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).split(/,(?=\s?[http]?[\/\/]?www)/).map(&:strip)
product_imgs.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).split(/,(?=\s?(http)?(\/\/)?www)/).map(&:strip)
product_imgs.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).split(/,(?=\s?(http)?(\/\/)www)/).map(&:strip)
product_imgs.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).split(/,(?=\s?(http)?(\/\/)?www)/).map(&:strip)
product_imgs
product_imgs.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).split(/,(?=\s?(http)?\/\/www)/).map(&:strip)
urls = product_imgs.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).split(/,(?=\s?(http)?\/\/www)/).map(&:strip)
urls.map do |img|
  img.starts_with?('http') ? img : "http:#{img}"
end
urls = product_imgs.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).split(/,(?=\s?(http)?\/\/www)/).map(&:strip)
asset = Asset.find 842
asset.listings.first
asset.listings.first.url
url = 'http://i.eb'
URI.valid?(url)
url =~ URI::regexp
ul
url
img = Asset.find_by original_url: 'http://i.ebayimg.com/images/a/(KGrHqYOKp0E5b-qyJUTBO(87Mw,n!~~/s-l1600.jpg'
img = Asset.find_by original_url: 'http://i.ebayimg.com/images/a/(KGrHqYOKp0E5b-qyJUTBO(87Mw,n!~~/s-l1600.jpg '
img = Asset.find 834
img.listings.first.url
urls = 'http://i.ebayimg.com/images/a/(KGrHqQOKjIE5W6Zg9J1BO(87FOK4g~~/s-l1600.jpg , http://i.ebayimg.com/12/!CBbp0vgBGk~$(KGrHqF,!l0Ez+wrKU(qBNHomec1k!~~_12.JPG?set_id=880000500F , http://i.ebayimg.com/images/a/(KGrHqEOKoYE0f7R7wBbBNccs(i6q!~~/s-l500.jpg , http://i.ebayimg.com/images/a/(KGrHqIOKigE0k3DETlUBNccsYqO+w~~/s-l500.jpg , http://i.ebayimg.com/images/a/(KGrHqEOKicE5dzQtO-kBO(87I3uug~~/s-l1600.jpg , http://i.ebayimg.com/images/a/(KGrHqYOKp0E5b-qyJUTBO(87Mw,n!~~/s-l1600.jpg'
urls = urls.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).split(/,(?=\s?(http)?\/\/www)/).map(&:strip)
urls = urls.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).split(/,(?=\s?(http)?\/\/(www)?)/).map(&:strip)
urls = 'http://i.ebayimg.com/images/a/(KGrHqQOKjIE5W6Zg9J1BO(87FOK4g~~/s-l1600.jpg , http://i.ebayimg.com/12/!CBbp0vgBGk~$(KGrHqF,!l0Ez+wrKU(qBNHomec1k!~~_12.JPG?set_id=880000500F , http://i.ebayimg.com/images/a/(KGrHqEOKoYE0f7R7wBbBNccs(i6q!~~/s-l500.jpg , http://i.ebayimg.com/images/a/(KGrHqIOKigE0k3DETlUBNccsYqO+w~~/s-l500.jpg , http://i.ebayimg.com/images/a/(KGrHqEOKicE5dzQtO-kBO(87I3uug~~/s-l1600.jpg , http://i.ebayimg.com/images/a/(KGrHqYOKp0E5b-qyJUTBO(87Mw,n!~~/s-l1600.jpg'
urls = urls.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).split(/,(?=\s?(http)?\/\/www)/).map(&:strip)
urls = 'http://i.ebayimg.com/images/a/(KGrHqQOKjIE5W6Zg9J1BO(87FOK4g~~/s-l1600.jpg , http://i.ebayimg.com/12/!CBbp0vgBGk~$(KGrHqF,!l0Ez+wrKU(qBNHomec1k!~~_12.JPG?set_id=880000500F , http://i.ebayimg.com/images/a/(KGrHqEOKoYE0f7R7wBbBNccs(i6q!~~/s-l500.jpg , http://i.ebayimg.com/images/a/(KGrHqIOKigE0k3DETlUBNccsYqO+w~~/s-l500.jpg , http://i.ebayimg.com/images/a/(KGrHqEOKicE5dzQtO-kBO(87I3uug~~/s-l1600.jpg , http://i.ebayimg.com/images/a/(KGrHqYOKp0E5b-qyJUTBO(87Mw,n!~~/s-l1600.jpg'
urls = urls.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).split(/,(?=\s?(http)?\/\/(www)?)/).map(&:strip)
urls
urls = 'http://i.ebayimg.com/images/a/(KGrHqQOKjIE5W6Zg9J1BO(87FOK4g~~/s-l1600.jpg , http://i.ebayimg.com/12/!CBbp0vgBGk~$(KGrHqF,!l0Ez+wrKU(qBNHomec1k!~~_12.JPG?set_id=880000500F , http://i.ebayimg.com/images/a/(KGrHqEOKoYE0f7R7wBbBNccs(i6q!~~/s-l500.jpg , http://i.ebayimg.com/images/a/(KGrHqIOKigE0k3DETlUBNccsYqO+w~~/s-l500.jpg , http://i.ebayimg.com/images/a/(KGrHqEOKicE5dzQtO-kBO(87I3uug~~/s-l1600.jpg , http://i.ebayimg.com/images/a/(KGrHqYOKp0E5b-qyJUTBO(87Mw,n!~~/s-l1600.jpg'
urls = urls.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).split(/,(?=\s?(http:)?\/\/(www)?)/).map(&:strip)
urls = urls.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).split(/,(?=\s?(http:)?(\/\/)?(www)?)/).map(&:strip)
urls = 'http://i.ebayimg.com/images/a/(KGrHqQOKjIE5W6Zg9J1BO(87FOK4g~~/s-l1600.jpg , http://i.ebayimg.com/12/!CBbp0vgBGk~$(KGrHqF,!l0Ez+wrKU(qBNHomec1k!~~_12.JPG?set_id=880000500F , http://i.ebayimg.com/images/a/(KGrHqEOKoYE0f7R7wBbBNccs(i6q!~~/s-l500.jpg , http://i.ebayimg.com/images/a/(KGrHqIOKigE0k3DETlUBNccsYqO+w~~/s-l500.jpg , http://i.ebayimg.com/images/a/(KGrHqEOKicE5dzQtO-kBO(87I3uug~~/s-l1600.jpg , http://i.ebayimg.com/images/a/(KGrHqYOKp0E5b-qyJUTBO(87Mw,n!~~/s-l1600.jpg'
srcs = urls.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).split(/,(?=\s?(http:)?(\/\/)?(www)?)/).map(&:strip)
srcs = urls.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).split(/,(?=\s?(http:)?(\/\/)*?(www)?)/).map(&:strip)
srcs = urls.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).split(/,(?=\s?(http:)*?(\/\/)*?(www)?)/).map(&:strip)
urls
srcs = urls.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).split(/,(?!\w)(?=\s?(http:)*?(\/\/)*?(www)?)/).map(&:strip)
srcs = urls.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).gsub(' ', '').split(/,(?!\w)(?=\s?(http:)*?(\/\/)*?(www)?)/).map(&:strip)
srcs = urls.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).gsub(' ', '').split(/,(?!\w)(?=(http:)*?(\/\/)*?(www)?)/).map(&:strip)
srcs = urls.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).gsub(' ', '')
srcs = urls.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).gsub(' ', '').split(/,(?!\w)(?=(http:)*?(\/\/)*(www)?)/).map(&:strip)
srcs = urls.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).gsub(' ', '').split(/,(?=(http:)*?(\/\/)*(www)?)/).map(&:strip)
srcs = urls.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).gsub(' ', '').split(/,(?=(http:|\/\/)/).map(&:strip)
srcs = urls.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).gsub(' ', '').split(/,(?=(http:|\/\/))/).map(&:strip)
srcs = urls.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).gsub(' ', '').split(/,(?=(http:|\/\/)*)/).map(&:strip)
srcs = urls.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).gsub(' ', '').split(/,(?=((http:)*|(\/\/)*))/).map(&:strip)
srcs = urls.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).gsub(' ', '').split(/,(?=((http:)*?|(\/\/)*))/).map(&:strip)
srcs = urls.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).gsub(' ', '').split(/,(?=((http:)*?|(\/\/)*?))/).map(&:strip)
srcs = urls.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).gsub(' ', '').split(/,(?=((http:)*?|(\/\/)))/).map(&:strip)
srcs = urls.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).gsub(' ', '').split(/,(?=((http:)*|(\/\/)))/).map(&:strip)
srcs = urls.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).gsub(' ', '').split(/,(?=((http:)|(\/\/)))/).map(&:strip)
srcs = urls.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).gsub(' ', '').split(/,(?=((http:)*?|(\/\/)))/).map(&:strip)
srcs = urls.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).gsub(' ', '').split(/,(?=(http:|\/\/))/).map(&:strip)
url =~ URI::regexp
srcs = urls.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).gsub(' ', '').split(/,(?=(http:|\/\/))/).map(&:strip)
srcs.map do |src|
  src =~ URI::regexp
end
srcs.map do |src|
srcs[1]
uri = URI.parse srcs[1]
uri.path
uri.host
uri.valid?
uri
uri.kind_of?(URI::HTTP)
Asset.find(727).url
Asset.find(727)
urls = [Asset.find(727).url]
urls = [Asset.find(727).original_url]
srcs = urls.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).gsub(' ', '').split(/,(?=(http:|\/\/))/).map(&:strip)
urls = Asset.find(727).original_url
srcs = urls.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).gsub(' ', '').split(/,(?=(http:|\/\/))/).map(&:strip)
urls = urls + ',' + urls
srcs = urls.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).gsub(' ', '').split(/,(?=(http:|\/\/))/).map(&:strip)
urls = 'http://i.ebayimg.com/images/a/(KGrHqQOKjIE5W6Zg9J1BO(87FOK4g~~/s-l1600.jpg , http://i.ebayimg.com/12/!CBbp0vgBGk~$(KGrHqF,!l0Ez+wrKU(qBNHomec1k!~~_12.JPG?set_id=880000500F , http://i.ebayimg.com/images/a/(KGrHqEOKoYE0f7R7wBbBNccs(i6q!~~/s-l500.jpg , http://i.ebayimg.com/images/a/(KGrHqIOKigE0k3DETlUBNccsYqO+w~~/s-l500.jpg , http://i.ebayimg.com/images/a/(KGrHqEOKicE5dzQtO-kBO(87I3uug~~/s-l1600.jpg , http://i.ebayimg.com/images/a/(KGrHqYOKp0E5b-qyJUTBO(87Mw,n!~~/s-l1600.jpg'
srcs = urls.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).gsub(' ', '').split(/,(?=(http:|\/\/))/).map(&:strip)
srcs = urls.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).gsub(' ', '').split(/,(?=(http:|\/\/))/).map {|url| url.gsub(/(http:)?\/\//, '')
srcs = urls.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).gsub(' ', '').split(/,(?=(http:|\/\/))/).map {|url| url.gsub(/(http:)?\/\//, '')}
srcs = urls.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).gsub(' ', '').split(/,(?=(http:|\/\/))/).map {|url| url.gsub(/(http:)?(\/\/)?/, '')}
srcs = urls.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).gsub(' ', '').split(/,(?=(http:|\/\/))/).map {|url| url.gsub(/(http:)?(\/\/)?/, '')}.compact
srcs = urls.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).gsub(' ', '').split(/,(?=(http:|\/\/))/).map {|url| url.gsub(/(http:)?(\/\/)?/, '')}.reject(&:blank?)
srcs = urls..encode(Encoding.find('ASCII'), ENCODING_OPTIONS).gsub(' ', '')
urls.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).gsub(' ', '').split(/,(?=(http:|\/\/))/).map {|url| url.gsub(/(http:)?(\/\/)?/, '')}.reject(&:blank?).map{|url| "http:#{url}"}
urls.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).gsub(' ', '').split(/,(?=(http:|\/\/))/).map {|url| url.gsub(/(http:)?(\/\/)?/, '')}.reject(&:blank?).map{|url| "http://#{url}"}
edit -t
clean_urls(urls)
def clean_urls(url_csv)
  url_csv.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).gsub(' ', '')
  .split(/,(?=(http|\/\/))/).map { |url| url.gsub(/(https?:)?(\/\/)?/, '') }
  .reject(&:blank?).map { |url| "http://#{url}" }
end
clean_urls(urls)
def clean_urls(url_csv)
  url_csv.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).gsub(' ', '')
  .split(/,(?=(http:|\/\/))/).map { |url| url.gsub(/(https?:)?(\/\/)?/, '') }
  .reject(&:blank?).map { |url| "http://#{url}" }
end
clean_urls(urls)
edit -t
clean_urls(urls)
edit -t
clean_urls(urls)
edit -t
clean_urls(urls)
edit -t
clean_urls(urls)
urls2 = 'https://bonanzleimages.s3.amazonaws.com/afu/images/2457/1843/30/-cheap-24-high-efficiency-Sunpower-Solar-Cells-125mm-Monocrystalline-3-5W-30pcs-lot-for-flexible.jpg, https://bonanzleimages.s3.amazonaws.com/afu/images/2457/1855/92/s-l1600__1_.jpg, https://bonanzleimages.s3.amazonaws.com/afu/images/2457/1871/53/s-l1600__2_.jpg, https://bonanzleimages.s3.amazonaws.com/afu/images/2457/1884/36/s-l1600__4_.jpg, https://bonanzleimages.s3.amazonaws.com/afu/images/2457/1893/03/s-l1600.jpg'
clean_urls(urls2)
urls3 = '//www.image-tmart.com/prodimgs/1/13012676/50W-Monocrystalline-Solar-Panel-18V-TUV-RV-Motorhome-Boat-50-Watt_600x600.jpg , //www.image-tmart.com/prodimgs/1/13012676/50W-Monocrystalline-Solar-Panel-18V-TUV-RV-Motorhome-Boat-50-Watt_1_600x600.jpg , //www.image-tmart.com/prodimgs/1/13012676/50W-Monocrystalline-Solar-Panel-18V-TUV-RV-Motorhome-Boat-50-Watt_2_600x600.jpg , //www.image-tmart.com/prodimgs/1/13012676/50W-Monocrystalline-Solar-Panel-18V-TUV-RV-Motorhome-Boat-50-Watt_3_600x600.jpg , //www.image-tmart.com/prodimgs/1/13012676/50W-Monocrystalline-Solar-Panel-18V-TUV-RV-Motorhome-Boat-50-Watt_4_600x600.jpg , //www.image-tmart.com/prodimgs/1/13012676/50W-Monocrystalline-Solar-Panel-18V-TUV-RV-Motorhome-Boat-50-Watt_5_600x600.jpg , //www.image-tmart.com/prodimgs/1/13012676/50W-Monocrystalline-Solar-Panel-18V-TUV-RV-Motorhome-Boat-50-Watt_7_600x600.jpg , //www.image-tmart.com/prodimgs/1/13012676/50W-Monocrystalline-Solar-Panel-18V-TUV-RV-Motorhome-Boat-50-Watt_8_600x600.jpg'
clean_urls(urls3)
Asset.first
exit
y ProductImage.all.map(&:original_url)
Asset.find_by original_url: 'http://i.ebayimg.com/images/a/(KGrHqYOKp0E5b-qyJUTBO(87Mw,n!~~/s-l1600.jpg,'
img = Asset.find 55
img.original_url
img = Asset.find 66
img.listing
img.listings.first
img.listings.first.url
img.url
img.original_url
img.original_url.scan(/(?<!src=)http/)
url_str = 'http://i.ebayimg.com/images/g/XnsAAOSw5dNWhbqz/s-l1600.jpg
http://i.ebayimg.com/images/g/XrkAAOSw5dNWhbq3/s-l1600.jpg
http://i.ebayimg.com/images/g/HvUAAOSwzhVWsS36/s-l1600.jpg
http://i.ebayimg.com/images/g/G3sAAOSwvUlWsS4V/s-l1600.jpg
http://i.ebayimg.com/images/g/IhUAAOSwzhVWsS4b/s-l1600.jpg
edit -t
clean_urls url_str
ENCODING_OPTIONS = {
  :invalid => :replace, # Replace invalid byte sequences
  :undef => :replace, # Replace anything not defined in ASCII
  :replace => '', # Use a blank for those replacements
  :universal_newline => true # Always break lines with \n
}
clean_urls url_str
url_str
edit -t
clean_urls url_str
urls3 = '//www.image-tmart.com/prodimgs/1/13012676/50W-Monocrystalline-Solar-Panel-18V-TUV-RV-Motorhome-Boat-50-Watt_600x600.jpg , //www.image-tmart.com/prodimgs/1/13012676/50W-Monocrystalline-Solar-Panel-18V-TUV-RV-Motorhome-Boat-50-Watt_1_600x600.jpg , //www.image-tmart.com/prodimgs/1/13012676/50W-Monocrystalline-Solar-Panel-18V-TUV-RV-Motorhome-Boat-50-Watt_2_600x600.jpg , //www.image-tmart.com/prodimgs/1/13012676/50W-Monocrystalline-Solar-Panel-18V-TUV-RV-Motorhome-Boat-50-Watt_3_600x600.jpg , //www.image-tmart.com/prodimgs/1/13012676/50W-Monocrystalline-Solar-Panel-18V-TUV-RV-Motorhome-Boat-50-Watt_4_600x600.jpg , //www.image-tmart.com/prodimgs/1/13012676/50W-Monocrystalline-Solar-Panel-18V-TUV-RV-Motorhome-Boat-50-Watt_5_600x600.jpg , //www.image-tmart.com/prodimgs/1/13012676/50W-Monocrystalline-Solar-Panel-18V-TUV-RV-Motorhome-Boat-50-Watt_7_600x600.jpg , //www.image-tmart.com/prodimgs/1/13012676/50W-Monocrystalline-Solar-Panel-18V-TUV-RV-Motorhome-Boat-50-Watt_8_600x600.jpg'
urls2 = 'https://bonanzleimages.s3.amazonaws.com/afu/images/2457/1843/30/-cheap-24-high-efficiency-Sunpower-Solar-Cells-125mm-Monocrystalline-3-5W-30pcs-lot-for-flexible.jpg, https://bonanzleimages.s3.amazonaws.com/afu/images/2457/1855/92/s-l1600__1_.jpg, https://bonanzleimages.s3.amazonaws.com/afu/images/2457/1871/53/s-l1600__2_.jpg, https://bonanzleimages.s3.amazonaws.com/afu/images/2457/1884/36/s-l1600__4_.jpg, https://bonanzleimages.s3.amazonaws.com/afu/images/2457/1893/03/s-l1600.jpg'
clean_urls urls3
edit -t
clean_urls urls3
clean_urls url_str
url_str
url_str.strip
edit -t
clean_urls url_str
url_str
edit -t
clean_urls url_str
clean_urls urls3
clean_urls url_str
url = _
url.scan(/(?<!src=)http/).size
url.first.scan(/(?<!src=)http/).size
url_src = img.first
url_src = url.first
img_src.split(/(?<!src=)http/).map(&:strip)
img_src = url.first
img_src.split(/(?<!src=)http/).map(&:strip)
img_src.split(/(?<!src=)http/).map(&:strip).reject(&:blank?)
img = Asset.find 191
url = 'https://encrypted-tbn1.gstatic.com/shopping?q=tbn:ANd9GcQN79C30nv0H0wng0WDLX_FgB_bGUgI7vbUC1qGOnCcNALka2F'
urls = url.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).gsub(' ', '').split(/,(?=(https?:|\/\/))/)
edit -t
clean_urls url
clean_urls urls3
edit -t
clean_urls urls3
clean_urls url
clean_urls urls2
edit -t
clean_urls urls2
edit -t
clean_urls urls2
clean_urls url
clean_urls urls3
edit -t
clean_urls urls3
clean_urls urls2
clean_urls url
urls4 = 'http://cdn.shopify.com/s/files/1/0011/4102/products/SolarWorld_Mono_Black3_d89402ba-79bb-4705-80f9-a3d09f589917_large.jpeg?v=1454951287, http://cdn.shopify.com/s/files/1/0011/4102/products/SolarWorld_Mono_Black2_76ef1fa8-b8c8-4ba0-ab6e-e95e19d7c2fe_small.jpg?v=1454951287, http://cdn.shopify.com/s/files/1/0011/4102/products/SolarWorld_Mono_Black_b84ae38b-ab3e-41db-86da-240d21a1111c_small.jpg?v=1454951287, http://cdn.shopify.com/s/files/1/0011/4102/products/SolarWorld_Mono_Black4_74054806-3dac-44af-bf81-fdc61d8f45e0_small.png?v=1454951287, http://cdn.shopify.com/s/files/1/0011/4102/products/SolarWorld_Mono_Black3_d89402ba-79bb-4705-80f9-a3d09f589917_large.jpeg?v=1454951287'
clean_urls urls4
urls5 = 
urls5 = 'https://s3.amazonaws.com/bonanzleimages/afu/images/1190/0173/05/_t2ec16f__y0fizfbsmsvbsvubcjmt___60_57.jpg, https://bonanzleimages.s3.amazonaws.com/afu/images/1190/0172/79/__kgrhqn__rufj_hi5yh2bsvubf__zw__60_57.jpg, https://bonanzleimages.s3.amazonaws.com/afu/images/1190/0172/99/_t2ec16r__yyficqrch1mbsvubhe99q__60_57.jpg, https://s3.amazonaws.com/bonanzleimages/afu/images/1190/0174/03/__kgrhqn__q8fjty_u76lbsvubjiqrw__60_12.jpg, https://s3.amazonaws.com/bonanzleimages/afu/images/1190/0175/15/_t2ec16z___4fiz7eielzbsvublrefq__60_57.jpg'
clean_urls urls5
urls6 = 'http://i.ebayimg.com/images/g/9TgAAOSwQYZWy-u9/s-l500.jpg, http://i.ebayimg.com/images/g/9g8AAOSwKtVWy-u9/s-l500.jpg, http://i.ebayimg.com/images/g/v7kAAOSwDuJWy-u9/s-l500.jpg, http://i.ebayimg.com/images/g/t0sAAOSwx-9Wy-u9/s-l500.jpg '
clean_urls urls6
edit -t
clean_urls urls7
_.first.split(/(?<!src=)http/).map(&:strip).reject(&:blank?).map { |l| "http#{l}" }
edit -t
clean_urls urls8
urls8 = url8
clean_urls urls8
_.first.split(/(?<!src=)http/).map(&:strip).reject(&:blank?).map { |l| "http#{l}" }
_.first.split(/(?<!src=)(https?:)?\/\//).map(&:strip).reject(&:blank?).map { |l| "http#{l}" })
clean_urls urls8
_.first.split(/(?<!src=)(https?:)?\/\//).map(&:strip).reject(&:blank?).map { |l| "http#{l}" }
clean_urls urls8
img_src = _.first
img_src.split(/(?<!src=)(http:?)?\/\//).map(&:strip).reject(&:blank?).map { |l| "http://#{l}" }
img_src.scan(/(?<!src=)(https?)?(\/\/)/)
img_src
img_src.scan(/(?<!src=)(https?)?(\/\/)/).size
img_src.scan(/(?<!src=)(\/\/)/).size
img_src.split(/(?<!src=)(\/\/)/).map(&:strip).reject(&:blank?).map { |l| "http://#{l}" }
img_src.split(/(?<!src=)(\/\/)/).map(&:strip).reject(&:blank?).map
img_src.split(/(?<!src=)(\/\/)/).map(&:strip).reject(&:blank?)
img_src.split(/(?<!src=)(\/\/)/).map(&:strip)
img_src.split(/(?<!src=)(\/\/)/)
img_src.split(/(?<!src=)(\/\/)/).gsub(/(https?)?\/\//, '')
img_src.split(/(?<!src=)(\/\/)/).map {|l| l.gsub(/(https?)?\/\//, '')}
img_src.split(/(?<!src=)(\/\/)/).map {|l| l.gsub(/(http)?s?:?\/\//, '')}
img_src.split(/(?<!src=)(\/\/)/).map {|l| l.gsub(/\/\//, '')}
img_src.split(/(?<!src=)(\/\/)/).map {|l| l.gsub(/\/\//, '').gsub(/http:?/,'')}
img_src.split(/(?<!src=)(\/\/)/).map {|l| l.gsub(/\/\//, '').gsub(/http:?/,'')}.reject(&:blank)
img_src.split(/(?<!src=)(\/\/)/).map {|l| l.gsub(/\/\//, '').gsub(/http:?/,'')}.reject(&:blank?)
img_src.split(/(?<!src=)(\/\/)/).map {|l| l.gsub(/\/\//, '').gsub(/http:?/,'').strip}.reject(&:blank?)
edit -t
clean_urls urls8
edit -t
clean_urls urls8
clean_urls urls7
urls7
urls8
edit -t
urls8
clean_urls urls8
clean_urls urls7
clean_urls urls6
clean_urls urls5
clean_urls urls4
clean_urls urls3
urls3
urls3.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).gsub(' ', '').split(/,(?=(https?:|\/\/))/)
edit -t
clean_urls urls3
edit -t
clean_urls urls3
clean_urls urls8
clean_urls urls7
clean_urls urls6
clean_urls urls5
clean_urls urls4
clean_urls urls3
clean_urls urls2
clean_urls urls1
clean_urls url
l = Listing.first
l.images
l = Listing.last
l.images
Listing.count
pm = ProductManual.first
pm.text
pm2 = ProductManual.last
pm2.text
pm.listings.first
l = pm.listings.first
l.products
l = pm2.listings.first
l.products
Listing.all.select {|l| l.products.size > 1 }
listings = _
listings.size
img = Asset.find 85
ids = [1016,
  1016,
  738,
  739,
  737,
  732,
  736,
  738,
  737,
  1017,
  851,
  1017,
  735,
  739,
  734,
  851,
  697,
  733,
  732,
  735,
  736,
  698,
  733,
697]
ids.size
ids.uniq.size
ids.uniq!
Asset.where(id: ids).map(&:original_url)
imgs = Asset.where(id: ids)
imgs.last
imgs.last.listing
imgs.last.listings.first
imgs.last.listings.first.url
imgs.map {|i| i.listings.first.url}
clean_urls
urls = 'http://img.banggood.com/thumb/view/oaupload/banggood/images/C4/2C/074ea918-04a7-47c2-bc9b-37f264f9c9c0.jpg , http://img.banggood.com/thumb/view/oaupload/banggood/images/C4/2C/3282c338-1820-47af-a980-fc76f018827e.jpg , http://img.banggood.com/thumb/view/oaupload/banggood/images/C4/2C/f4aba42b-c210-4e6a-a545-d0aa42db397c.jpg , http://img.banggood.com/thumb/view/oaupload/banggood/images/C4/2C/b87e51c6-cb12-46f4-b11e-4404b774e71c.jpg , http://img.banggood.com/thumb/view/oaupload/banggood/images/C4/2C/472857f7-85ee-4984-a9ed-1b61d6a40b54.jpg , http://img.banggood.com/thumb/view/oaupload/banggood/images/C4/2C/625645ae-f14f-4cf0-a98e-8b8e063d89a9.jpg , http://img.banggood.com/thumb/view/oaupload/banggood/images/C4/2C/67043fe7-d65f-4b29-8233-4f7056c743da.jpg , http://img.banggood.com/thumb/view/oaupload/banggood/images/C4/2C/f0b958d0-fd37-441c-8a4d-569066b27914.jpg'
clean_urls urls
imgs.map {|i| i.listings.first.url}
ursl = 
urls = 'http://img.banggood.com/thumb/view/oaupload/banggood/images/90/8E/af87b099-0e16-41a3-95ce-5245dc01eb5a.jpg , http://img.banggood.com/thumb/view/oaupload/banggood/images/90/8E/43948f69-3650-47e3-b346-e447ceaa92dd.jpg , http://img.banggood.com/thumb/view/oaupload/banggood/images/90/8E/da15f8f0-31d5-4c4c-95f5-78a736945883.jpg , http://img.banggood.com/thumb/view/oaupload/banggood/images/90/8E/78c57805-1aa4-40aa-8122-0f98f13dfdaf.jpg , http://img.banggood.com/thumb/view/oaupload/banggood/images/90/8E/4d0661a3-6c93-4a22-9c18-395c83ca3117.jpg , http://img.banggood.com/thumb/view/oaupload/banggood/images/90/8E/0af97425-2b1b-42a9-9a3a-c0b3a9a06a78.jpg , http://img.banggood.com/thumb/view/oaupload/banggood/images/90/8E/afa24884-2cff-4ce8-aafc-001858337e35.jpg , http://img.banggood.com/thumb/view/oaupload/banggood/images/90/8E/e1d225a7-94c3-4a11-aa48-5dc66c02adca.jpg'
clean_urls urls
imgs.map(&:original_url)
imgs.last 2
imgs.last(2).first.listings.first.url
imgs.last(2)
imgs.map(&:original_url)
imgs.last(3).first.listings.first.url
exit
Listing.first
Listing.select(:id, :scrape_id).find_by(id: 1)
l = _
l.id
l.scrape_id
Listing.select(:id, :scrape_id).find_by(id: 0)
exit
Category.where(name: "Sunpower Logo 01").first
c = _
c.sources
SourceCategory.where(category: c).size
Source.first
s = _
s.categories
s = Source.last
s.categories
s.categories.last
s.categories.count
s = Source.last
s = Source.where(id: 1)
s.categories.last
s.first.categories.last
exit
s = Source.find(-1)
s.categories
s = Source.find 1
s.categories.last
exit
ProductProductManual.confirmed.first
p = Product.find 14
p.manuals
p.manuals.confirmed
p.listings.size
l = p.listings.first
l.manuals
l.manuals.confirmed
l.manuals
l.manuals.confirmed
exit
h = {a: 'hello', b: ''}
h.delete(:'')
h
h[:''] = ''
h
h.delete(:'')
h
proc = Proc.new { |k, v| v.kind_of?(Hash) ? (v.delete_if(&proc); nil) : v.empty? }; 
proc = Proc.new { |k, v| v.kind_of?(Hash) ? (v.delete_if(&proc); nil) : (k.blank? || v.blank?) };
h
h.delete_if(&proc)
h
h[:''] = 'asdf'
h[:c] = ''
h.delete_if(&proc)
h[:b] = { c: 'something', '': '' }
h
h.delete_if(&proc)
exit
Product.count
exit
Product.count
exit
Product.count
exit
Product.count
exit
Product.count
exit
Product
Product.connection
exit
Product.count
''.present
''.present?
h = {a: ''}
h.has_key?(:a)
l = Listing.first
l = Listing.where.not(product_description: nil).first
listing = l
data_hash = listing.data_hash
proc = Proc.new { |k, v| v.kind_of?(Hash) ? (v.delete_if(&proc); nil) : (k.blank? || v.blank?) };
data_hash.delete_if(&proc)
data_hash[:General] ||= {}
product_desc = listing.product_description
if product_desc.present?
  if product_desc.is_a?(Hash)
    data_hash[:'Product Description'] = listing.product_description
  else
    data_hash[:General][:'Product Description'] = listing.product_description
  end
end
data_hash
data_hash.keys
if listing.product_features.present?
  # add to data_hash
  data_hash[:General][:'Product Features'] = listing.product_features
end
if listing.from_mfg.present?
  # add to data_hash
  data_hash[:General][:'From the Manufacturer'] = listing.from_mfg
end
if data_hash.has_key?(:rich_text).present?
  # rename to product description
  data_hash[:General][:'Product Description'] = data_hash[:rich_text] unless data_hash[:rich_text].blank?
  data_hash.delete(:rich_text)
end
data_hash
product_desc.keys
listing2 = Listing.where("product_description like '%From the Manufacturer%'").first
data_hash = listing2.data_hash
l == listing
listing = listing2
listing1 = l
proc = Proc.new { |k, v| v.kind_of?(Hash) ? (v.delete_if(&proc); nil) : (k.blank? || v.blank?) };
proc = Proc.new { |k, v| v.kind_of?(Hash) ? (v.delete_if(&proc); nil) : (k.blank? || v.blank?) }
data_hash = listing.data_hash
data_hash.delete_if(&proc)
data_hash[:General] ||= {}
product_desc = listing.product_description
product_desc.keys.include?('From the Manufacturer')
nil.length
nil.try(:length)
product_desc
product_desc.values.first
product_desc
desc = product_desc
desc.keys
desc.first.keys
desc[:"Product Description"]
listing1.product_description
listing1.url
listing2.url
listing2.product_description
listing1.product_description
listing2.product_description
listing1.product_description
listing = listing1
data_hash = listing.data_hash
exit
listing1 = Listing.find 227899
listing2 = Listing.find 227919
listing = listing1
data_hash = listing.data_hash
proc = Proc.new { |k, v| v.kind_of?(Hash) ? (v.delete_if(&proc); nil) : (k.blank? || v.blank?) }
data_hash.delete_if(&proc)
data_hash[:General] ||= {}
product_desc = listing.product_description
if product_desc.present?
  if product_desc.is_a?(Hash)
    if product_desc[:'Product Description'].is_a?(Hash)
      data_hash[:'Product Description'] = product_desc[:'Product Description']
    elsif product_desc.keys.length > 1
      data_hash[:'Product Description'] = product_desc
    else
      data_hash[:General][:'Product Description'] = product_desc[:'Product Description']
    end
  else
    data_hash[:General][:'Product Description'] = listing.product_description
  end
end
data_hash
listing1
data_hash
listing.update(data_hash: data_hash)
listing = listing2
data_hash = listing.data_hash
proc = Proc.new { |k, v| v.kind_of?(Hash) ? (v.delete_if(&proc); nil) : (k.blank? || v.blank?) }
data_hash.delete_if(&proc)
data_hash[:General] ||= {}
product_desc = listing.product_description
if product_desc.present?
  if product_desc.is_a?(Hash)
    if product_desc[:'Product Description'].is_a?(Hash)
      data_hash[:'Product Description'] = product_desc[:'Product Description']
    elsif product_desc.keys.length > 1
      data_hash[:'Product Description'] = product_desc
    else
      data_hash[:General][:'Product Description'] = product_desc[:'Product Description']
    end
  else
    data_hash[:General][:'Product Description'] = listing.product_description
  end
end
data_hash
listing.attributes
exit
product_ids = ProductProductManual.confirmed.pluck(:product_id).uniq
product_ids.count
ids = ProductListing.where(product_id: product_ids).pluck(:product_id).uniq
ids.count
products = Product.connection.select_all("select id, brand, model from products where id IN (#{ids.join(',')})")
products.first
CSV.open('../product_ids_w_manuals.csv', 'wb') do |csv|
  csv << ['id', 'brand', 'model']
  products.each do |row|
    csv << row.values
  end
end
exit
Product.count
Listing.count
Scrape.count
Scrape.first
exit
@keys = [
  :product_name,
  :product_description,
  :ruvixx_category,
  :product_category,
  :product_url,
  :original_url,
  :brand,
  :model,
  :mpn,
  :alternate_models,
  :product_features,
  :contact_name,
  :seller_name,
  :seller_url,
  :currency_code,
  :min_price,
  :max_price,
  :gtin,
  :asin,
  :upc,
  :sku,
  :product_images,
  :screenshot_filename,
  :manual_filename
]
@keys = [
  :product_name,
  :ruvixx_category,
  :product_category,
  :product_url,
  :original_url,
  :brand,
  :model,
  :mpn,
  :alternate_models,
  :contact_name,
  :seller_name,
  :seller_url,
  :currency_code,
  :min_price,
  :max_price,
  :gtin,
  :asin,
  :upc,
  :sku,
  :product_images,
  :screenshot_filename,
  :manual_filename
]
str = 'Product URL,Merchant Source,Seller Name,Seller URL,Product Category,Listing Category,Product Name,Brand,Model,mpn,gtin,upc,upc14,ean,sku,asin,Product Description,Product Features,Contact Name,Currency Code,Min Price,Max Price,Product Images,Screenshot Filename,Manual Filename,More about this item,Country/Region Of Manufacture,Sold by,Item number,Item Specifics,Source Item Code,Specification,Warranty Information,Internet Number,Product Code'
str.split(',')
first = _
def generate_key_mapping(header_row)
  header_row.each.with_index.each_with_object({}) do |e, a|
    key = e.first.downcase.gsub(' ', '_').to_sym
    key = @keys.include?(key) ? key : e.first.to_sym
    a[key] = e.last
  end
end
generate_key_mapping(first)
key_mapping = generate_key_mapping(first)
str = %q{https://www.ebluejay.com/item/5572140,https://www.google.com/url?url=https://www.ebluejay.com/item/5572140&rct=j&q=&esrc=s&sa=U&ved=0ahUKEwiDlMz7pL3LAhUFHI4KHV6-DhkQ2SkIngIwAQ&sig2=bbszSzR3NBHQ7ZH5V_oOYQ&usg=AFQjCNFaglLdyTL565xmbJCUTpercBxUcw,eBlueJay Online marketplace,https://www.ebluejay.com/,Hardware > Power & Electrical Supplies > Solar Panels,Electrical & Solar,"Solar panel Cell 24% efficiency 125mm Monocrystalline, Sunpower Maxeon 3.5 W Flexible",Sunpower,,SP500,,,,,,,"Brand Name: Sunpower
Material: Monocrystalline Silicon
Size: 125*125mm
Number of Cells: 150pcs
Max. Power: 3.5W/pc
Open-circuit Voltage : 0.699V
Open-circuit Current: 6.40A
Conversion Efficiency: 21.8-24.0%
Working Voltage: 0.61V
Working Current: 6.18A
Color : Deep-blue
Thickness: 145um
Application : Solar Power Generation Electricity
Cetification: CE Rohs
Warranty : 20years at the condition of lamination", , ,USD,550,,https://www.ebluejay.com/img/account/b/a/bahiasolar/1/bd7c992.jpg,screenshot_2, , ,,bahiasolar,5572140, ,,,,,
edit -t
row
str
edit -t
str
str.spit(',')
str.split(',')
edit -t
str.split(',')
row = _
row.length
key_mapping
path = '/Users/jonathan/Downloads/google_agility_scrape/KL_SunPower_1_row.xlsx'
def open_spreadsheet(file_path)
  case File.extname(file_path)
  when '.csv' then
    Roo::CSV.new(file_path)
  when '.xls' then
    Roo::Excel.new(file_path)
  when '.xlsx' then
    Roo::Excelx.new(file_path)
  else
    fail "Unknown file type: #{file_path}"
  end
end
edit -t
f = open_spreadsheet path
first = f.first_row
key_mapping = generate_key_mapping f.row(first)
data_hash_keys = key_mapping.keys - [:product_name, :product_images, :screenshot_filename, :manual_filename]
(first + 1..f.last_row).each do |i|
row = f.row(i)
first
first + 1
row = f.row(2)
data = {}
@keys.each do |key|
  value = key_mapping[key]
  data[key] = row[value] unless value.nil?
end
data
data_hash_keys.each do |key|
  h = row[key_mapping[key]]
  next if h.blank?
  data_hash_category = :General
  data[:data_hash] ||= {}
  data[:data_hash][data_hash_category] ||= {}
  data[:data_hash][data_hash_category][key] = h
end
data[:data_hash]
data_hash_keys = key_mapping.keys -  @keys
row = f.row(2)
@keys.each do |key|
  value = key_mapping[key]
  data[key] = row[value] unless value.nil?
end
data = {}
@keys.each do |key|
  value = key_mapping[key]
  data[key] = row[value] unless value.nil?
end
data_hash_keys.each do |key|
  h = row[key_mapping[key]]
  next if h.blank?
  data_hash_category = :General
  data[:data_hash] ||= {}
  data[:data_hash][data_hash_category] ||= {}
  data[:data_hash][data_hash_category][key] = h
end
data
f
f.close
f
f.close
f.closed?
f.open?
f
f.close
path = '/Users/jonathan/Downloads/google_agility_scrape/KL_SunPower - Google REQUIREMENTS 20160321 complete.xlsx'
f = open_spreadsheet path
row = f.row(3)
data = {}
@keys.each do |key|
  value = key_mapping[key]
  data[key] = row[value] unless value.nil?
end
data_hash_keys.each do |key|
  h = row[key_mapping[key]]
  next if h.blank?
  data_hash_category = :General
  data[:data_hash] ||= {}
  data[:data_hash][data_hash_category] ||= {}
  data[:data_hash][data_hash_category][key] = h
end
data
row = f.row(2)
data = {}
@keys.each do |key|
  value = key_mapping[key]
  data[key] = row[value] unless value.nil?
end
data_hash_keys.each do |key|
  h = row[key_mapping[key]]
  next if h.blank?
  data_hash_category = :General
  data[:data_hash] ||= {}
  data[:data_hash][data_hash_category] ||= {}
  data[:data_hash][data_hash_category][key] = h
end
data
f.close
exit
tmp_folder_path = '/Users/jonathan/Downloads/google_agility_scrape'
data_file_paths = Dir["#{tmp_folder_path}/*.csv", "#{tmp_folder_path}/*.xlsx", "#{tmp_folder_path}/*.xls"]
file_hash = Digest::MD5.hexdigest(Date.today)
file_hash = Digest::MD5.hexdigest(Date.today.to_s)
source = Source.find_by(file_hash: file_hash)
scrape_name = 'google hand'
url = 'fakeurl.com'
source = Source.create name: scrape_name,
upload_url: url,
source_type_id: SourceType::IMPORT,
file_hash: file_hash
tmp_folder_path
data_file_paths
scrape = Scrape.create source: source,
name: scrape_name,
running: true,
created_at: start_time
start_time = DateTime.now
scrape = Scrape.create source: source,
name: scrape_name,
running: true,
created_at: start_time
f = open_spreadsheet data_file_paths.first
edit -t
first = f.first_row
f
f = open_spreadsheet data_file_paths.first
first = f.first_row
key_mapping = generate_key_mapping f.row(first)
@keys = [
  :product_name,
  :ruvixx_category,
  :product_category,
  :product_url,
  :original_url,
  :brand,
  :model,
  :mpn,
  :alternate_models,
  :contact_name,
  :seller_name,
  :seller_url,
  :currency_code,
  :min_price,
  :max_price,
  :gtin,
  :asin,
  :upc,
  :upc14,
  :sku,
  :ean,
  :product_images,
  :screenshot_filename,
  :manual_filename
]
end
key_mapping = generate_key_mapping f.row(first)
row = f.row(3)
data = {}
@keys.each do |key|
  value = key_mapping[key]
  data[key] = row[value] unless value.nil?
end
data_hash_keys.each do |key|
  h = row[key_mapping[key]]
  next if h.blank?
  data_hash_category = :General
  data[:data_hash] ||= {}
  data[:data_hash][data_hash_category] ||= {}
  data[:data_hash][data_hash_category][key] = h
end
data_hash_keys = key_mapping.keys -  @keys
data = {}
@keys.each do |key|
  value = key_mapping[key]
  data[key] = row[value] unless value.nil?
end
data_hash_keys.each do |key|
  h = row[key_mapping[key]]
  next if h.blank?
  data_hash_category = :General
  data[:data_hash] ||= {}
  data[:data_hash][data_hash_category] ||= {}
  data[:data_hash][data_hash_category][key] = h
end
listing = import_listing scrape, data
listing
listing.identifiers
data
key_mapping
data
listing
listing.sellers
img_srcs = clean_urls(row[key_mapping[:product_images]])
ENCODING_OPTIONS = {
  :invalid => :replace, # Replace invalid byte sequences
  :undef => :replace, # Replace anything not defined in ASCII
  :replace => '', # Use a blank for those replacements
  :universal_newline => true # Always break lines with \n
}
img_srcs = clean_urls(row[key_mapping[:product_images]])
save_images(scrape.id, listing, img_srcs)
save_manuals(listing, tmp_folder_path, row[key_mapping[:manual_filename]]) unless listing.nil?
save_screenshots(listing, tmp_folder_path, row[key_mapping[:screenshot_filename]]) unless listing.nil?
listing.images
listing.manuals
listing.screenshots
listings.products
listing.products
p = _.first
p.manuals
p.images
p.identifiers
p.listings
generate_key_mapping
key_mapping
edit -t
listing = import_listing scrape, data
edit -t
listing = import_listing scrape, data
listing
edit -t
listing = import_listing scrape, data
data_hash_keys = key_mapping.keys -  @keys
@keys
(first + 1..f.last_row).each do |i|
  row = f.row(i)
  data = {}
  @keys.each do |key|
    value = key_mapping[key]
    data[key] = row[value] unless value.nil?
  end
  data_hash_keys.each do |key|
    h = row[key_mapping[key]]
    next if h.blank?
    data_hash_category = :General
    data[:data_hash] ||= {}
    data[:data_hash][data_hash_category] ||= {}
    data[:data_hash][data_hash_category][key] = h
  end
edit -t
Listing.count
Product.count
Listing.last
exit
Sidekiq::Queue.new('upload_asset_queue').size
Sidekiq::Queue.new('upload_asset_queue').clear
Sidekiq::Queue.new('upload_asset_queue').size
Sidekiq::Queue.new('product_listing_queue').size
Sidekiq::Queue.new('product_detail_queue').size
Sidekiq::Queue.new('product_manual_search_queue').size
exit
Sidekiq::Queue.new('product_manual_search_queue').size
Sidekiq::Queue.new('product_manual_search_queue').clear
Sidekiq::Queue.new('product_detail_queue').size
Sidekiq::Queue.new('product_detail_queue').clear
exit
Product.count
Asset.count
exit
Source.last
Scrape.last
s = _
s.listings
s.listings.last
l = _
l.data_hash
l.products
l.images
l.screenshots
l.sellers
l.scrapes
l.scrape
l.source
exit
Listing.last
l = _
l.categories
path = '/Users/jonathan/Downloads/Requirements/KL_SunPower - Google REQUIREMENTS 20160321 complete.xlsx'
ENCODING_OPTIONS = {
  :invalid => :replace, # Replace invalid byte sequences
  :undef => :replace, # Replace anything not defined in ASCII
  :replace => '', # Use a blank for those replacements
  :universal_newline => true # Always break lines with \n
}
def initialize
  @keys = [
    :product_name,
    :ruvixx_category,
    :product_category,
    :product_url,
    :original_url,
    :brand,
    :model,
    :mpn,
    :alternate_models,
    :contact_name,
    :seller_name,
    :seller_url,
    :currency_code,
    :min_price,
    :max_price,
    :gtin,
    :asin,
    :upc,
    :upc14,
    :sku,
    :ean,
    :product_images,
    :screenshot_filename,
    :manual_filename
  ]
end
edit -t
start_time = DateTime.now
file_hash = Digest::MD5.hexdigest(path)
path = '/Users/jonathan/Downloads/google_agility_scrape'
tmp_folder_path = path
data_file_paths = Dir["#{tmp_folder_path}/*.csv", "#{tmp_folder_path}/*.xlsx", "#{tmp_folder_path}/*.xls"]
source = Source.last
scrape = Scrape.create source: source,
name: scrape_name,
running: true,
created_at: start_time
scrape_name = 'testing import'
scrape = Scrape.create source: source,
name: scrape_name,
running: true,
created_at: start_time
f = open_spreadsheet data_file_paths.first
first = f.first_row
key_mapping = generate_key_mapping f.row(first)
j    @keys = [
  :product_name,
  :ruvixx_category,
  :product_category,
  :product_url,
  :original_url,
  :brand,
  :model,
  :mpn,
  :alternate_models,
  :contact_name,
  :seller_name,
  :seller_url,
  :currency_code,
  :min_price,
  :max_price,
  :gtin,
  :asin,
  :upc,
  :upc14,
  :sku,
  :ean,
  :product_images,
  :screenshot_filename,
  :manual_filename
]
@keys = [
  :product_name,
  :ruvixx_category,
  :product_category,
  :product_url,
  :original_url,
  :brand,
  :model,
  :mpn,
  :alternate_models,
  :contact_name,
  :seller_name,
  :seller_url,
  :currency_code,
  :min_price,
  :max_price,
  :gtin,
  :asin,
  :upc,
  :upc14,
  :sku,
  :ean,
  :product_images,
  :screenshot_filename,
  :manual_filename
]
@keys
key_mapping = generate_key_mapping f.row(first)
row = f.row(1)
@keys.each do |key|
  value = key_mapping[key]
  data[key] = row[value] unless value.nil?
end
data = {}
@keys.each do |key|
  value = key_mapping[key]
  data[key] = row[value] unless value.nil?
end
data_hash_keys.each do |key|
  h = row[key_mapping[key]]
  next if h.blank?
  data_hash_category = :General
  data[:data_hash] ||= {}
  data[:data_hash][data_hash_category] ||= {}
  data[:data_hash][data_hash_category][key] = h
end
data_hash_keys = key_mapping.keys -  @keys
data_hash_keys.each do |key|
  h = row[key_mapping[key]]
  next if h.blank?
  data_hash_category = :General
  data[:data_hash] ||= {}
  data[:data_hash][data_hash_category] ||= {}
  data[:data_hash][data_hash_category][key] = h
end
cat_name = data[:ruvixx_category].present? ? data[:ruvixx_category] : data[:product_category]
row = f.row(2)
data = {}
@keys.each do |key|
  value = key_mapping[key]
  data[key] = row[value] unless value.nil?
end
data_hash_keys.each do |key|
  h = row[key_mapping[key]]
  next if h.blank?
  data_hash_category = :General
  data[:data_hash] ||= {}
  data[:data_hash][data_hash_category] ||= {}
  data[:data_hash][data_hash_category][key] = h
end
cat_name = data[:ruvixx_category].present? ? data[:ruvixx_category] : data[:product_category]
cat_name.split('>').last.strip
cat_name.split('>').map(&:strip).last
cat_name
cat_name = cat_name.split('>').map(&:strip).last
category = Category.find_or_create_by(name: cat_name)
source_category = SourceCategory.find_or_create_by(category: category, source: scrape.source)
scrape
data
listing = import_listing scrape, data
listing.id
listing.sellers
listing.scrape_listing_sellers
listing.scrape_listing_sellers.first.min_price
listing.scrape_listing_sellers.first.min_price.to_i
listing.scrape_listing_sellers.first.min_price.to_f
listing.screenshots
listing
s
s = Scrape.last
s.listings.first
l = _
l.screenshots
s = Scrape.first
l = s.listings.first
l.screenshots
exit
ENCODING_OPTIONS = {
  :invalid => :replace, # Replace invalid byte sequences
  :undef => :replace, # Replace anything not defined in ASCII
  :replace => '', # Use a blank for those replacements
  :universal_newline => true # Always break lines with \n
}
@keys = [
  :product_name,
  :ruvixx_category,
  :product_category,
  :product_url,
  :original_url,
  :brand,
  :model,
  :mpn,
  :alternate_models,
  :contact_name,
  :seller_name,
  :seller_url,
  :currency_code,
  :min_price,
  :max_price,
  :gtin,
  :asin,
  :upc,
  :upc14,
  :sku,
  :ean,
  :product_images,
  :screenshot_filename,
  :manual_filename
]
edit -t
start_time = DateTime.now
tmp_file = download_data_file(path)
url = '/Users/jonathan/Downloads/Requirements/agility_google_small.zip'
tmp_file = url
url = nil
tmp_file
open
open tmp_file
url = tmp_file
tmp_file = download_data_file(url)
file_hash = Digest::MD5.hexdigest(File.read(tmp_file.path))
source = Source.find_by(file_hash: file_hash)
scrape_name = 'google hand'
source = Source.create name: scrape_name,
upload_url: url,
source_type_id: SourceType::IMPORT,
file_hash: file_hash
tmp_folder_path = extract_data_file tmp_file
data_file_paths = Dir["#{tmp_folder_path}/*.csv", "#{tmp_folder_path}/*.xlsx", "#{tmp_folder_path}/*.xls"]
scrape = Scrape.create source: source,
name: scrape_name,
running: true,
created_at: start_time
f = open_spreadsheet data_file_paths.first
first = f.first_row
key_mapping = generate_key_mapping f.row(first)
data_hash_keys = key_mapping.keys -  @keys
row = f.row(2)
data = {}
@keys.each do |key|
  value = key_mapping[key]
  data[key] = row[value] unless value.nil?
end
data_hash_keys.each do |key|
  h = row[key_mapping[key]]
  next if h.blank?
  data_hash_category = :General
  data[:data_hash] ||= {}
  data[:data_hash][data_hash_category] ||= {}
  data[:data_hash][data_hash_category][key] = h
end
data
data[:min_price].class
data[:min_price].class.to_s
data[:min_price].to_s
f
f.close
f = open_spreadsheet '/Users/jonathan/Downloads/Requirements/KL_SunPower - Google REQUIREMENTS 20160321 complete.xlsx'
first = f.first_row
key_mapping = generate_key_mapping f.row(first)
data_hash_keys = key_mapping.keys -  @keys
row = f.row(2)
Monitize.parse("USD 550")
Monetize.parse("USD 550")
money = 
m = Monetize.parse("USD 550")
m.to_f
550.00 == 550
row
data
data = {}
@keys.each do |key|
  value = fix_numbers(key_mapping[key])
  data[key] = row[value] unless value.blank?
end
def fix_numbers(value)
  # It seems Roo gem converts any numbers to float
  # which may actually be a property of xls files
  # so we need to remove decimals from numbers that don't need to be a float
  return if value.blank?
  (value.is_a?(Float) && value.to_i == value) ? value.to_i : value
end
@keys.each do |key|
  value = fix_numbers(key_mapping[key])
  data[key] = row[value] unless value.blank?
end
data_hash_keys.each do |key|
  value = fix_numbers(row[key_mapping[key]])
  next if value.blank?
  data_hash_category = :General
  data[:data_hash] ||= {}
  data[:data_hash][data_hash_category] ||= {}
  data[:data_hash][data_hash_category][key] = value
end
data
row
550.0.to_i == 550
fix_numbers(550.0)
data
data = {}
@keys[:min_price]
key_mapping[:min_price]
data = {}
@keys.each do |key|
  value = key_mapping[key]
  data[key] = fix_numbers(row[value]) unless value.nil?
end
data_hash_keys.each do |key|
  value = fix_numbers(row[key_mapping[key]])
  next if value.blank?
  data_hash_category = :General
  data[:data_hash] ||= {}
  data[:data_hash][data_hash_category] ||= {}
  data[:data_hash][data_hash_category][key] = value
end
data
Source.last
load 'app/jobs/import_scrape_data_job.rb'
fix_numbers(1.0)
load 'lib/scraping/detail_scrapeable.rb'
load 'lib/scraping/image_scrapeable.rb'
ENCODING_OPTIONS
@keys.each do |key|
  value = key_mapping[key]
  data[key] = fix_numbers(row[value]) unless value.nil?
end
data_hash_keys.each do |key|
  value = fix_numbers(row[key_mapping[key]])
  next if value.blank?
  data_hash_category = :General
  data[:data_hash] ||= {}
  data[:data_hash][data_hash_category] ||= {}
  data[:data_hash][data_hash_category][key] = value
end
start_time = DateTime.now
path = '/Users/jonathan/Downloads/Requirements/small_scrape.zip'
scrape_name = 'test'
tmp_file = download_data_file(path)
file_hash = Digest::MD5.hexdigest(File.read(tmp_file.path))
source = Source.find_by(file_hash: file_hash)
url = path
source = Source.create name: scrape_name,
upload_url: url,
source_type_id: SourceType::IMPORT,
file_hash: file_hash
tmp_folder_path = extract_data_file tmp_file
data_file_paths = Dir["#{tmp_folder_path}/*.csv", "#{tmp_folder_path}/*.xlsx", "#{tmp_folder_path}/*.xls"]
scrape = Scrape.create source: source,
name: scrape_name,
running: true,
created_at: start_time
f = open_spreadsheet data_file_paths.first
first = f.first_row
key_mapping = generate_key_mapping f.row(first)
data_hash_keys = key_mapping.keys - @keys
row = f.row(i)
row = f.row(2)
data = {}
@keys.each do |key|
  value = key_mapping[key]
  data[key] = fix_numbers(row[value]) unless value.nil?
end
data_hash_keys.each do |key|
  value = fix_numbers(row[key_mapping[key]])
  next if value.blank?
  data_hash_category = :General
  data[:data_hash] ||= {}
  data[:data_hash][data_hash_category] ||= {}
  data[:data_hash][data_hash_category][key] = value
end
data
scrape
name = data[:product_name]
brand = data[:brand]
model = data[:model] || data[:mpn]
name
brand
model
if data[:brand].present?
  data[:data_hash][:General][:'Brand'] = data[:brand]
end
if data[:model].present?
  data[:data_hash][:General][:'Model'] = data[:model]
end
if data[:mpn].present?
  data[:data_hash][:General][:'MPN'] = data[:mpn]
end
alternate_models = data[:alternate_models] && data[:alternate_models].split(',').map(&:strip).reject(&:blank)
data[:product_url].blank?
product_url = data[:product_url].gsub(/\?.+$/, '') || data[:original_url].gsub(/\?.+$/, '') || "import_scrape/#{scrape.id}/#{Digest::MD5.hexdigest(data[:data_hash].to_s)}"
price = parse_price data.extract!(:min_price, :max_price, :price, :currency_code)
products = []
unless brand.blank? or model.blank?
  products = create_products(brand, alternate_models.to_a + [model])
end
products
listing = Listing.find_or_create_by(scrape_id: scrape.id, url: product_url) do |l|
  l.name = name
  l.original_url = product_url
  l.data_hash = data[:data_hash]
  l.contact_name = data[:contact_name]
end
listing = import_listing scrape, data
listing.nil? || listing.id.nil?
img_srcs = clean_urls(row[key_mapping[:product_images]])
save_images(scrape.id, listing, img_srcs)
listing.images
img = listing.images.first
img.save_attachment
img
img.relaod
img.reload
row[key_mapping[:screenshot_filename]]
tmp_folder_path
save_manuals(listing, tmp_folder_path, row[key_mapping[:manual_filename]]) 
folder_path = "#{folder_path}/manuals"
folder_path = "/tmp/import/2d998121-c235-46de-b660-91fca612c820/manuals/manuals" 
folder_path = "/tmp/import/2d998121-c235-46de-b660-91fca612c820/manuals" 
tmp_folder_path
filename = row[key_mapping[:screenshot_filename]]
listing
products = listing.products
paths = []
if filename.nil?
  products.each do |product|
    paths |= ["#{folder_path}/#{product.model.strip}",
    "#{folder_path}/#{clean_model(product.model)}"]
  end
else
  paths = ["#{folder_path}/#{filename}"]
end
paths
path = paths.first
if File.directory?(path)
  path += '/*'
else
  path += '.*'
end
Dir[path]
Dir[path].each {|d| puts 'hey'}
folder_path
folder_path = "/tmp/import/2d998121-c235-46de-b660-91fca612c820/"
folder_path = "/tmp/import/2d998121-c235-46de-b660-91fca612c820"
folder_path = "#{folder_path}/screenshots"
filename
filename = row[key_mapping[:screenshot_filename]]
folder_path = "#{folder_path}/screenshots"
tmp_folder_path
folder_path= _
folder_path = "#{folder_path}/screenshots"
products = listing.products
paths = []
if filename.nil?
  products.each do |product|
    paths |= ["#{folder_path}/#{product.model.strip}",
    "#{folder_path}/#{clean_model(product.model)}"]
  end
else
  paths = ["#{folder_path}/#{filename}"]
end
path = paths.first
if File.directory?(path)
  path += '/*'
else
  path += '.*'
end
Dir[path]
file_path = _.first
model = ProductScreenshot
asset = model.create
if model == ProductManual
  products.each { |p| ProductProductManual.find_or_create_by(product: p, manual: asset, status: 1) }
elsif model == ProductScreenshot
  asset.update_attributes url: listing.url
end
ListingAsset.find_or_create_by(listing_id: listing.id, asset_id: asset.id)
file_path
path = _
file = File.open(path)
asset.attachment = file
asset.reload!
asset.reload
asset.original_url
path
asset.original_url = path
asset.save
asset.reload
asset.destroy
asset = model.create original_url: file_path
asset.valid?
asset.update_attributes url: listing.url
ListingAsset.find_or_create_by(listing_id: listing.id, asset_id: asset.id)
file
file.clost
file.close
file = File.open(path)
asset.attachment = file
asset.save
asset.reload
asset.destroy
reload!
asset = model.create
asset.update_attributes url: listing.url
ListingAsset.find_or_create_by(listing_id: listing.id, asset_id: asset.id)
file.close
file = File.open(path)
path
exit
s = Scrape.last
s.listings.count
s.products.count
exit
Product.import
Listing.import
exit
Listing.import
Product.import
exit
Product.import
exit
pm = ProductManual.first
url = pm.attachment.url
pdf = Yomu.new(url)
content = AsciiCleaner::remove_non_ascii(pdf.text.squish.downcase).slice(0, 100000)
require 'ascii.rb'
content = AsciiCleaner::remove_non_ascii(pdf.text.squish.downcase).slice(0, 100000)
content = AsciiCleaner::remove_non_ascii(pdf.text.squish.downcase.squeeze('.')).slice(0, 100000)
pm.confirmed?
relaod
exit
p = Product.first
p.manuals
pm = ProductManual.create original_url: "http://doccloud.lieske-elektronik.de/business/lieske_pdf_kommunikation.pdf"
ProductProductManual.create product: p, manual: pm, status: 1
p.reload
p.manuals
p.manuals.confirmed
p.manuals.first.text
Product.count
p.listings
Listing.import
Product.import
p.id
Listing.import
Listing.delete_index!
Listing.create_index!
Listing.import
reload!
exit
Listing.delete_index!
Listing.create_index!
Listing.import
pm
pm = ProductManual.last
pm.confirmed?
pm.product_product_manuals
p = Product.first
p.manuals
p.manuals.confirmed
pm.text
pm.manual_text
pm.text.length
pm.text.squish.squeeze('.').downcase.slice(0, 100000).length
ProductManual.each do {|pm| pm.manual_text.update text: pm.text.text.squish.squeeze('.').downcase.slice(0, 100000) }
ProductManual.each  {|pm| pm.manual_text.update text: pm.text.text.squish.squeeze('.').downcase.slice(0, 100000) }
ProductManual.all.each  {|pm| pm.manual_text.update text: pm.text.text.squish.squeeze('.').downcase.slice(0, 100000) }
ProductManual.all.each  {|pm| pm.manual_text.update text: pm.text.squish.squeeze('.').downcase.slice(0, 100000) }
pm.reload
pm.text.lenght
pm.text.length
edit -t
ListingUrl.last
url_md5 = Digest::MD5("http://www.amazon.com/Urbanears-04091023-ZINKEN-Headphone-Black/dp/B00OIRBN1G/")
url = "http://www.amazon.com/Urbanears-04091023-ZINKEN-Headphone-Black/dp/B00OIRBN1G/"
url_md5 = Digest::MD5.hexdigest(url)
listing_url = ListingUrl.find_or_create_by(url_md5: url_md5, url: url)
edit -t
Scrape.last
Scrape.first
edit -t
l = Listing.create(attrs)
pm
listing.manuals << pm
l.manuals << pm
l.manuals
l.manuals.confirmed
Listing.index
Listing.import
edit -t
p = Product.create(attrs)
p.manuals
pm
ppm = ProductProductManual.create product: p, manual: pm, status: 1
p.reload
p.manuals
p.manuals.confirmed
l
l.manuals.confirmed
Listing.delete_index!
Product.delete_index!
exit
Product.import
Listing.import
p
Product.last
pm
pm = ProductManual.last
pdf = Yomu.new(pm.original_url)
content = pdf.text.squish.squeeze('.').downcase
pm.manual_text.update text: content
Listing.delete_index!
Product.delete_index!
Listing.import
Product.import
pm.text
pm.text.include? 'pow'
pm.text.include? 'sunpow'
pm.text.include? 'module'
Listing.last
def method1(something)
  puts something
  def method2(something)
    puts something + '2')
def method1(something)
  puts something
  def method2(something)
    puts something + '2'
  end
end
method1('str')
def method1(something)
  puts something
  def method2(something)
    puts something + '2'
  end
  method2(something)
end
method1('str')
method2('str')
lids = Category.find(id: [-1]).flat_map { |l| l.listings.ids }.uniq
lids = Category.where(id: [-1, -2]).flat_map { |l| l.listings.ids }.uniq
Category.where(id: -1).first
c = _
c.listings << Listing.last
exit
ScrapeUrl.first
ScrapeUrl
exit
Scrape.count
Scrape.source
s = Scrape.last
s.source_id
s = Scrape.select(:id, :source_id)
s = s.first
s = Scrape.last
s.sources
s.last_source
s.source
s.sources
detail_url = 'http://www.amazon.com/OPT7-Headlight-Bulbs-Clear-Arc-Beam/dp/B00VNBDWPK/'
detail_uri = URI.parse(url)
url = 'http://www.amazon.com/OPT7-Headlight-Bulbs-Clear-Arc-Beam/dp/B00VNBDWPK/'
detail_uri = URI.parse(url)
seller_url = "http://www.amazon.com/gp/offer-listing/#{detail_uri.path.split('/')[3]}/ref=dp_olp_new?condition=new"
attrs = {"name"=>"Amazon.com", "created_at"=>Fri, 09 Oct 2015 03:39:07 UTC +00:00, "updated_at"=>Mon, 11 Apr 2016 04:58:22 UTC +00:00, "image_url"=>"http://ecx.images-amazon.com/images/I/01dXM-J1oeL.gif"}
attrs = {"name"=>"Amazon.com", "image_url"=>"http://ecx.images-amazon.com/images/I/01dXM-J1oeL.gif"}
exit
attrs = {"name"=>"Amazon.com", "image_url"=>"http://ecx.images-amazon.com/images/I/01dXM-J1oeL.gif"}
Seller.create(attrs)
ProductImage
Asset
Seller.first
exit
attrs = {"name"=>"Amazon.com", "image_url"=>"http://ecx.images-amazon.com/images/I/01dXM-J1oeL.gif"}
Seller.create(attrs)
Asset
Asset.where(type: :image)
ProductImage.first
exit
attrs = {"name"=>"Amazon.com", "image_url"=>"http://ecx.images-amazon.com/images/I/01dXM-J1oeL.gif"}
Seller.create(attrs)
Seller.new(attrs)
s = _
s.valid?
s.save
Seller
exit
attrs = {"name"=>"Amazon.com", "image_url"=>"http://ecx.images-amazon.com/images/I/01dXM-J1oeL.gif"}
Seller.create(attrs)
exit
attrs = {"name"=>"Amazon.com", "image_url"=>"http://ecx.images-amazon.com/images/I/01dXM-J1oeL.gif"}
Seller.create(attrs)
exit
attrs = {"name"=>"Amazon.com", "image_url"=>"http://ecx.images-amazon.com/images/I/01dXM-J1oeL.gif"}
Seller.create(attrs)
s = Seller.first
s.logo
s.logo.url
exit
attrs = {"name"=>"Amazon.com", "image_url"=>"http://ecx.images-amazon.com/images/I/01dXM-J1oeL.gif"}
s = Seller.create(attrs)
s.image
s.reload
s.image
SellerImage.last
exit
attrs = {"name"=>"Amazon.com", "image_url"=>"http://ecx.images-amazon.com/images/I/01dXM-J1oeL.gif"}
s = Seller.create(attrs)
s.image
s.reload
s.image
Seller.destroy_all
Asset.destroy_all
exit
attrs = {"name"=>"Amazon.com", "image_url"=>"http://ecx.images-amazon.com/images/I/01dXM-J1oeL.gif"}
s = Seller.create(attrs)
s.image
s.image.original_url
s.image.attachment.url
exit
ProductImage.first
SellerImage.
SellerImage
si = SellerImage.new
si.destroy
ListingAsset.last
exit
attrs = {"name"=>"Amazon.com", "image_url"=>"http://ecx.images-amazon.com/images/I/01dXM-J1oeL.gif"}
s = Seller.create attrs
s.image
s.reload
s.image
Asset.last
exit
s = Seller.last
s.image
s.assets
s.images
SellerAsset
SellerAsset.first
s.id
s = Seller.first
Seller.destroy_all
Asset.destroy_all
attrs = {"name"=>"Amazon.com", "image_url"=>"http://ecx.images-amazon.com/images/I/01dXM-J1oeL.gif"}
s = Seller.create name: 'Amazon.com'
s.images
Asset.count
Asset.first
s
s = Seller.create('Jonathan')
s = Seller.create(name: 'Jonathan')
reload!
exit
s = Seller.create name: "Amazon.com"
Asset.count
attrs = {"name"=>"Amazon.com", "image_url"=>"http://ecx.images-amazon.com/images/I/01dXM-J1oeL.gif"}
SellerImage.find_or_create_by(original_url: attrs['image_url']
)
img
img = Asset.first
img.attachment.url
s
s.image_url = attrs['image_url']
s.save
s.save_image
s.images
s.reload
s.images
reload!
exit
s = Seller.first
s.save_image
exit
s = Seller.first
s.images
s.save_image
s.images
s.reload
s.images
s.image
s.image.to_sql
s.image
exit
s = Seller.first
s.image
s.image.try(:attachment).try(:url)
[].first
eixt
exit
seller = Seller.last
seller.image
seller.image.attachment.url
exit
docid = '4478400288239372847'
url = '/aclk?sa=L&ai=Cy-G1-VAiV577B83D-QP_h5DAB4n1u7AH2bbRvMcB2ZnKkfsECAkQAiDezc8eKBRgyfb4hsijoBmQARPIAQeqBCZP0C0Gvm2wSvHRmzLw1DqNmyIymc7eit2w4Txxf3_FLHZmPTq0-sAFBaAGJoAH24COFZAHA6gHpr4b2AcB4BKvlI-02qqekz4&sig=AOD64_2euUuAd4ENK949M7l8akeoOryoUA&ctype=5&clui=6&q=&ved=0ahUKEwiJi7SA9rHMAhUT7mMKHQj0BIs4tAEQ2CkIhwgwAQ&adurl=http://rover.ebay.com/rover/1/711-117182-37290-0/2%3Fmtid%3D1588%26kwid%3D1%26crlp%3D53601919689_324272%26itemid%3D151389445266%26targetid%3D170493381849%26rpc%3D0.12%26rpc_upld_id%3D68833%26device%3Dc%26mpre%3Dhttp%253A%252F%252Fwww.ebay.com%252Fulk%252Fitm%252Flike%252F151389445266%253Flpid%253D82%2526chn%253Dps%26adtype%3Dpla%26googleloc%3D9031942%26poi%3D%26campaignid%3D239125209%26adgroupid%3D14978428809%26rlsatarget%3Dpla-170493381849'
url.include?(docid)
url2 = '/aclk?sa=L&ai=Cy-G1-VAiV577B83D-QP_h5DAB4n1u7AH2bbRvMcB2ZnKkfsECAkQAiDezc8eKBRgyfb4hsijoBmQARPIAQeqBCZP0C0Gvm2wSvHRmzLw1DqNmyIymc7eit2w4Txxf3_FLHZmPTq0-sAFBaAGJoAH24COFZAHA6gHpr4b2AcB4BKvlI-02qqekz4&sig=AOD64_2euUuAd4ENK949M7l8akeoOryoUA&ctype=5&clui=6&q=&ved=0ahUKEwiJi7SA9rHMAhUT7mMKHQj0BIs4tAEQ2CkIhwgwAQ&adurl=http://rover.ebay.com/rover/1/711-117182-37290-0/2?mtid=1588&kwid=1&crlp=53601919689_324272&itemid=151389445266&targetid=170493381849&rpc=0.12&rpc_upld_id=68833&device=c&mpre=http%3A%2F%2Fwww.ebay.com%2Fulk%2Fitm%2Flike%2F151389445266%3Flpid%3D82%26chn%3Dps&adtype=pla&googleloc=9031942&poi=&campaignid=239125209&adgroupid=14978428809&rlsatarget=pla-170493381849'
url == url2
url2
url
link = 'https://www.google.com/aclk?sa=L&ai=Cy-G1-VAiV577B83D-QP_h5DAB4n1u7AH2bbRvMcB2ZnKkfsECAkQAiDezc8eKBRgyfb4hsijoBmQARPIAQeqBCZP0C0Gvm2wSvHRmzLw1DqNmyIymc7eit2w4Txxf3_FLHZmPTq0-sAFBaAGJoAH24COFZAHA6gHpr4b2AcB4BKvlI-02qqekz4&sig=AOD64_2euUuAd4ENK949M7l8akeoOryoUA&ctype=5&clui=6&q=&ved=0ahUKEwiJi7SA9rHMAhUT7mMKHQj0BIs4tAEQ2CkIhwgwAQ&adurl=http://rover.ebay.com/rover/1/711-117182-37290-0/2%3Fmtid%3D1588%26kwid%3D1%26crlp%3D53601919689_324272%26itemid%3D151389445266%26targetid%3D170493381849%26rpc%3D0.12%26rpc_upld_id%3D68833%26device%3Dc%26mpre%3Dhttp%253A%252F%252Fwww.ebay.com%252Fulk%252Fitm%252Flike%252F151389445266%253Flpid%253D82%2526chn%253Dps%26adtype%3Dpla%26googleloc%3D9031942%26poi%3D%26campaignid%3D239125209%26adgroupid%3D14978428809%26rlsatarget%3Dpla-170493381849'
result = "http://www.ebay.com/itm/like/151389445266?lpid=82&chn=ps&ul_noapp=true"
href = "/aclk?sa=L&ai=Cy-G1-VAiV577B83D-QP_h5DAB4n1u7AH2bbRvMcB2ZnKkfsECAkQAiDezc8eKBRgyfb4hsijoBmQARPIAQeqBCZP0C0Gvm2wSvHRmzLw1DqNmyIymc7eit2w4Txxf3_FLHZmPTq0-sAFBaAGJoAH24COFZAHA6gHpr4b2AcB4BKvlI-02qqekz4&sig=AOD64_2euUuAd4ENK949M7l8akeoOryoUA&ctype=5&clui=6&q=&ved=0ahUKEwiJi7SA9rHMAhUT7mMKHQj0BIs4tAEQ2CkIhwgwAQ&adurl=http://rover.ebay.com/rover/1/711-117182-37290-0/2%3Fmtid%3D1588%26kwid%3D1…gnid%3D239125209%26adgroupid%3D14978428809%26rlsatarget%3Dpla-170493381849"
CGI.parse(URI.parse(href).query)['adurl'][0]
href = "/aclk?sa=L&ai=Cy-G1-VAiV577B83D-QP_h5DAB4n1u7AH2bbRvMcB2ZnKkfsECAkQAiDezc8eKBRgyfb4hsijoBmQARPIAQeqBCZP0C0Gvm2wSvHRmzLw1DqNmyIymc7eit2w4Txxf3_FLHZmPTq0-sAFBaAGJoAH24COFZAHA6gHpr4b2AcB4BKvlI-02qqekz4&sig=AOD64_2euUuAd4ENK949M7l8akeoOryoUA&ctype=5&clui=6&q=&ved=0ahUKEwiJi7SA9rHMAhUT7mMKHQj0BIs4tAEQ2CkIhwgwAQ&adurl=http://rover.ebay.com/rover/1/711-117182-37290-0/2%3Fmtid%3D1588%26kwid%3D1%26crlp%3D53601919689_324272%26itemid%3D151389445266%26targetid%3D170493381849%26rpc%3D0.12%26rpc_upld_id%3D68833%26device%3Dc%26mpre%3Dhttp%253A%252F%252Fwww.ebay.com%252Fulk%252Fitm%252Flike%252F151389445266%253Flpid%253D82%2526chn%253Dps%26adtype%3Dpla%26googleloc%3D9031942%26poi%3D%26campaignid%3D239125209%26adgroupid%3D14978428809%26rlsatarget%3Dpla-170493381849"
CGI.parse(URI.parse(href).query)['adurl'][0]
exit
href = "/aclk?sa=L&ai=Cy-G1-VAiV577B83D-QP_h5DAB4n1u7AH2bbRvMcB2ZnKkfsECAkQAiDezc8eKBRgyfb4hsijoBmQARPIAQeqBCZP0C0Gvm2wSvHRmzLw1DqNmyIymc7eit2w4Txxf3_FLHZmPTq0-sAFBaAGJoAH24COFZAHA6gHpr4b2AcB4BKvlI-02qqekz4&sig=AOD64_2euUuAd4ENK949M7l8akeoOryoUA&ctype=5&clui=6&q=&ved=0ahUKEwiJi7SA9rHMAhUT7mMKHQj0BIs4tAEQ2CkIhwgwAQ&adurl=http://rover.ebay.com/rover/1/711-117182-37290-0/2%3Fmtid%3D1588%26kwid%3D1%26crlp%3D53601919689_324272%26itemid%3D151389445266%26targetid%3D170493381849%26rpc%3D0.12%26rpc_upld_id%3D68833%26device%3Dc%26mpre%3Dhttp%253A%252F%252Fwww.ebay.com%252Fulk%252Fitm%252Flike%252F151389445266%253Flpid%253D82%2526chn%253Dps%26adtype%3Dpla%26googleloc%3D9031942%26poi%3D%26campaignid%3D239125209%26adgroupid%3D14978428809%26rlsatarget%3Dpla-170493381849"
link = https://www.google.com/aclk?sa=L&ai=Cy-G1-VAiV577B83D-QP_h5DAB4n1u7AH2bbRvMcB2ZnKkfsECAkQAiDezc8eKBRgyfb4hsijoBmQARPIAQeqBCZP0C0Gvm2wSvHRmzLw1DqNmyIymc7eit2w4Txxf3_FLHZmPTq0-sAFBaAGJoAH24COFZAHA6gHpr4b2AcB4BKvlI-02qqekz4&sig=AOD64_2euUuAd4ENK949M7l8akeoOryoUA&ctype=5&clui=6&q=&ved=0ahUKEwiJi7SA9rHMAhUT7mMKHQj0BIs4tAEQ2CkIhwgwAQ&adurl=http://rover.ebay.com/rover/1/711-117182-37290-0/2%3Fmtid%3D1588%26kwid%3D1%26crlp%3D53601919689_324272%26itemid%3D151389445266%26targetid%3D170493381849%26rpc%3D0.12%26rpc_upld_id%3D68833%26device%3Dc%26mpre%3Dhttp%253A%252F%252Fwww.ebay.com%252Fulk%252Fitm%252Flike%252F151389445266%253Flpid%253D82%2526chn%253Dps%26adtype%3Dpla%26googleloc%3D9031942%26poi%3D%26campaignid%3D239125209%26adgroupid%3D14978428809%26rlsatarget%3Dpla-170493381849
link = "https://www.google.com/aclk?sa=L&ai=Cy-G1-VAiV577B83D-QP_h5DAB4n1u7AH2bbRvMcB2ZnKkfsECAkQAiDezc8eKBRgyfb4hsijoBmQARPIAQeqBCZP0C0Gvm2wSvHRmzLw1DqNmyIymc7eit2w4Txxf3_FLHZmPTq0-sAFBaAGJoAH24COFZAHA6gHpr4b2AcB4BKvlI-02qqekz4&sig=AOD64_2euUuAd4ENK949M7l8akeoOryoUA&ctype=5&clui=6&q=&ved=0ahUKEwiJi7SA9rHMAhUT7mMKHQj0BIs4tAEQ2CkIhwgwAQ&adurl=http://rover.ebay.com/rover/1/711-117182-37290-0/2%3Fmtid%3D1588%26kwid%3D1%26crlp%3D53601919689_324272%26itemid%3D151389445266%26targetid%3D170493381849%26rpc%3D0.12%26rpc_upld_id%3D68833%26device%3Dc%26mpre%3Dhttp%253A%252F%252Fwww.ebay.com%252Fulk%252Fitm%252Flike%252F151389445266%253Flpid%253D82%2526chn%253Dps%26adtype%3Dpla%26googleloc%3D9031942%26poi%3D%26campaignid%3D239125209%26adgroupid%3D14978428809%26rlsatarget%3Dpla-170493381849"
href
CGI.parse(URI.parse(href).query)['adurl'][0]
href
link
url = link
CGI.parse(URI.parse(url).query)['adurl'][0]
decoded = 'http://www.ebay.com/ulk/itm/like/151389445266?lpid=82&chn=ps&adtype=pla&googleloc=9031942&poi=&campaignid=239125209&adgroupid=14978428809&rlsatarget=pla-170493381849'
url
url2= "https://www.google.com/aclk?sa=l&ai=CKNVi-VAiV577B83D-QP_h5DAB7XghaIH9bjI1bIBpa7B7_4BCAkQBSDezc8eKBRgyfb4hsijoBmQAROgAd-TgPADyAEHqgQmT9AtBr5tsErx0Zsy8PBekqgiMpnO3ordsOE8cX9_xSx2Zg0MzPjABQWgBiaAB4ns_w-QBwOoB6a-G9gHAeAS-qD5qZiF4vG6AQ&sig=AOD64_2ha-3RZpEDUC5nEikgIusKtEiuDw&adurl=http://www.bonanza.com/listings/Proscan-Pled4242-Uhd-Rk-42-4-K-Ultra-Led-Hdtv-With-Roku-R-Streaming-Stick-R-/295219282%3Fgoog_pla%3D1%26gpid%3D68416460581%26gpkwd%3D%26goog_pla%3D1&ctype=5&clui=12&q=&ved=0ahUKEwiJi7SA9rHMAhUT7mMKHQj0BIs4tAEQpysIVQ&ei=-VAiV8mqBpPcjwOI6JPYCA"
CGI.parse(URI.parse(url2).query)['adurl'][0]
link
url_md5 = Digest::MD5.hexdigest(link)
text = ManualText.first.text
ManualText.first
ManualText.last
text = "ns. • there is an earth hole on the edge of the module frame using this hole an earth conductor and the solar module frame may be recommended to be connected and earthed as the below drawing. • all screws and nuts shall be tightened to a torque of 4~5 nm. • a module with exposed conductive parts is considered to be in compliance with ul 1703 only when is electrically grounded in accordance with the instructions presented below and the requirements of the national electrical code. the installation instructions shall include: 1. details for wiring shall comply the nec article 690. 2. details for the grounding method of the frame of arrays shall comply with the nec article 250. 3. cnl model instruction manuals shall also include a statement that installation shall be in accordance with csa c22.1, safety standard for electrical installations, canadian electrical code, part 1. module frame bolt flat washer star washer cup washer flat washer spring washer nut grounding wire 7 mechanical installation module mounting • the lg electronics’ (lge) limited warranty for solar modules is contingent upon modules being mounted in accordance with the requirements described in this section. • any module without a frame (laminate) shall not be considered to comply with the requirements of ul 1703 unless the module with hardware that has been tested and evaluated with the module under this standard or by a field inspection certifying that the installed module complies with the requirements of ul 1703. site consideration lge solar modules should be mounted in a location that meets the following requirements. operating temperature • maximum operating temperature: +90°c (194°f) • minimum operating temperature: -40°c (-40°f) design strength • lge solar modules are certified to basic loads 75lb/ft2. (mounting by using frame bolts holes) ① : 170mm(6.7 in) ② : 270mm (10.6 in) ① ② ① ② ① ② ① ② excluded operating environments • the solar modules from lg electronics can not be operated in a location where they could come in direct contact with salt water or ammonia. mounting methods general information • select the appropriate orientation to maximize sunlight exposure. • module should not be mounted or stored in a way that the front/top glass faces downward in order to prevent water from entering the junction box, which could cause a safety hazard. • clearance between the solar module frames and structures such as roofs or ground is required to prevent wiring damage and to allow air to circulate behind the solar module. the recommended standoff height is a minimum of 100mm. • when installed on a roof, the solar module must be mounted over a fire-resistant roof covering rated for the application. the fire resistance of the solar module is class c after ansi/ul790. • a slope less than 5in/ft is required to maintain a fire class rating. • the solar module is only etl listed for use when its factory frame is fully intact. • removal or alteration must be done by an authorized and qualified individual • creating additional mounting holes may damage the solar module and reduce the strength of the frame. • we recommend a 6mm gap between module frames to avoid tension from thermal expansion. • the fire rating of this module is valid only when mounted in the manner specified in the mechanical mounting instructions. • the module is considered to be in compliance with ul1703 only when the module in mounted in the manner specified by the mounting instructions below. • the solar module may be mounted by using the following methods: (*torque:8~12nm ) • lg modules (fire performance : type 2) shall be mounted with racking and mounting products certified and listed for system fire class rating in accordance with ul1703 edition 2014 and ul2703 edition 2014. • it is recommended to check with local authorities for fire safety guidelines and requirements for any building or structure on to which the panels will be installed. • when installing modules in heavy snow areas, it is recommended to be taken an appropriate countermeasure to prevent possible damages to the lower side frame by slipping snow. (a snow guard should be installed in accordance with the manufacturer’s instructions.) solar module roof supporting part 8 mounting by using frame bolts holes • secure the solar module to the structure by using the factory mounting holes. • four m8 stainless steel bolts, four nuts, four spring washers, and eight flat washers are recommended per solar module. • the module may be fastened to a support by using both the outer and inner bolt holes of the frame. • each module should be securely fastened at a minimum 4 points on two opposite sides. • specific information on the solar module dimensions and location of mounting holes is provided in ‘product specifications’. bolt flat washer flat washer nut support module frame spring washer • tighten the bolt securely by using the combination shown above. place the spring washer between the flat washer and nut. mounting by using clamps • the module may be fastened to a support by using clamps on both the long edge and the short edge of the modules. • specific information on location of clipping is provided in ‘appendix_mechanical installation’. (refer to 11 page) more than 10mm more than 40mm more than 10mm 9 seitreporp lacinahcemseitreporp lacirtcelesetacifitrec 01 m o d u le s er ie s m o d el n am e ie c 61 21 5 e d .2 , i e c 6 17 30 , s af et y c la ss ii , c e , i s o 90 0 p m ax a t s t c p o w er t o le ra n ce vo c a t s t c is c at s t c v m p p a t s t c im p p a t s t c *e ff ic ie n cy r ed u ct io n m ax . s ys te m v o lt ag e c o n n ec to r l en g th w id th h ei g h t w ei g h t gkmmmmmmv%avav%w l g x x x n 1c (w )- g 4 lg280n1c(w)-g4 yes 280 3% 38.6 9.78 30.7 9.13 <2.0 1000 **mc4 1640 1000 40 17.0 lg285n1c(w)-g4 yes 285 3% 38.9 9.81 31.1 9.17 <2.0 1000 mc4 1640 1000 40 17.0 lg290n1c(w)-g4 yes 290 3% 39.2 9.84 31.4 9.24 <2.0 1000 mc4 1640 1000 40 17.0 lg295n1c(w)-g4 yes 295 3% 39.5 9.87 31.8 9.28 <2.0 1000 mc4 1640 1000 40 17.0 lg300n1c(w)-g4 yes 300 3% 39.8 9.90 32.2 9.34 <2.0 1000 mc4 1640 1000 40 17.0 lg305n1c(w)-g4 yes 305 3% 40.1 9.93 32.5 9.39 <2.0 1000 mc4 1640 1000 40 17.0 lg310n1c(w)-g4 yes 310 3% 40.4 9.96 32.8 9.45 <2.0 1000 mc4 1640 1000 40 17.0 lg315n1c(w)-g4 yes 315 3% 40.6 10.02 33.2 9.50 <2.0 1000 mc4 1640 1000 40 17.0 lg320n1c(w)-g4 yes 320 3% 40.9 10.05 33.6 9.53 <2.0 1000 mc4 1640 1000 40 17.0 lg325n1c(w) g4 y 325 3% 41 2 10 08 34 0 9 57 2 0 1000 mc4 1640 1000 40 17 0lg325n1c(w)-g4 yes 325 3% 41.2 10.08 34.0 9.57 <2.0 1000 mc4 1640 1000 40 17.0 lg330n1c(w)-g4 yes 330 3% 41.5 10.11 34.3 9.63 <2.0 1000 mc4 1640 1000 40 17.0 lg335n1c(w)-g4 yes 335 3% 41.8 10.14 34.6 9.69 <2.0 1000 mc4 1640 1000 40 17.0 lg340n1c(w)-g4 yes 340 3% 42.1 10.17 34.9 9.75 <2.0 1000 mc4 1640 1000 40 17.0 x n 1k -g 4 lg280n1k-g4 yes 280 3% 38.1 9.53 31.0 9.04 <3.0 1000 mc4 1640 1000 40 17.0 lg285n1k-g4 yes 285 3% 38.5 9.57 31.3 9.11 <3.0 1000 mc4 1640 1000 40 17.0 lg290n1k-g4 yes 290 3% 38.9 9.61 31.7 9.16 <3.0 1000 mc4 1640 1000 40 17.0 lg295n1k-g4 yes 295 3% 39.3 9.66 32.1 9.21 <3.0 1000 mc4 1640 1000 40 17.0 lg300n1k-g4 yes 300 3% 39.7 9.70 32.5 9.26 <3.0 1000 mc4 1640 1000 40 17.0 lg305n1k-g4 yes 305 3% 40 1 9 74 32 9 9 28 3.0 1000 mc4 1640 1000 40 17.0 l g x x x lg305n1k g4 yes 305 3% 40.1 9.74 32.9 9.28 < .0 1000 mc4 1640 1000 40 17.0 lg310n1k-g4 yes 310 3% 40.5 9.78 33.3 9.31 <3.0 1000 mc4 1640 1000 40 17.0 lg315n1k-g4 yes 315 3% 40.6 9.82 33.7 9.35 <3.0 1000 mc4 1640 1000 40 17.0 lg320n1k-g4 yes 320 3% 40.9 9.86 34.1 9.39 <3.0 1000 mc4 1640 1000 40 17.0 lg325n1k-g4 yes 325 3% 41.2 9.90 34.5 9.43 <3.0 1000 mc4 1640 1000 40 17.0 lg330n1k-g4 yes 330 3% 41.6 9.94 34.9 9.47 <3.0 1000 mc4 1640 1000 40 17.0 product specifications - electrical and mechanical properties (rated electrical characteristics are within 10 percent) standard test condition(stc) : irradiation 1,000w/m2, cell temp. 25°c, 1.5am - dimensions of modules note : * relative efficiency reduction by respect to irradiance ** mc4 formal name : pv-kst4 / 6ii-ur, pv-kbt4 / 6ii-ur *** a safety locking clip (mc pv-ssh4) may be required per article 690 of nec 2008. note : *holder is for the convenient connection of junction box cable,but that does not warranty if it is broken after installed. lgxxxn1c(w, k)-g4 cross-sectional drawings unit: mm / in. long side frame short side frame 10 disclaimer of liability / disposal disclaimer of liability • by beginning the installation process the installer represents the he/she has read and completely understands this installation manual. he/she further represents if he/she had any questions regarding this installation manual he/she would have contacted lg with any questions or concerns. by installing an lg solar module, i discharge, and covenant not to sue lg, its affiliated companies, successors, or assigns, its administrators, directors, agents, officers, volunteer and employees, other participants in any activity connected to installation, operation, or service of lg solar modules and if applicable from all liabilities claim solar modules, and if applicable, from all liabilities, claims, demands, losses, or damages on my account caused or alleged to be caused in whole or in part by the negligence of the lg its affiliated companies, successors, or assigns, its administrators, directors, agents, officers, volunteer and employees. disposal • please contact us, if you have any queries related to the disposal or recycling of solar modules from lg electronics. transporting and storage • do not loosen the banding, when the module is transported by truck, ship and etc. in case of loose banding, the module will be shaken, which may cause damage. • do not stack on more than one pallet. maximum height is two pallets. severe stacking can give stress to the module and may cause product damage. 11 appendix_mechanical installation fig.1 mounting type fig.2 clamping type ① ② ① ② ① ② ① ② a b ba ① 170mm (6.7 in) ② 270mm (10.6 in) front : 6000pa (125psf) rear : 5400pa (113psf) a : 150mm (5.9 in) b : 400mm (15.7 in) front : 6000pa (125psf) rear : 5400pa (113psf) fig.3 clamping type fig.4 clamping type a b b a a b a b a : 150mm (5.9 in) b : 400mm (15.7 in) front : 6000pa (125psf) rear : 5400pa (113psf) a : 120mm (4.7 in) front : 1800pa (37.5psf)rear : 1800pa (37.5psf) b : 150mm (5.9 in) front : 2850pa (60psf)rear : 2850pa (60psf) fig.5 clamping type fig.6 clamping type a b b a b a① ② ① ② ① ① * 4 point installation is allowed in the following cases: 1. slope roof: if module is installed parallel to the rooftop. 2. flat roof: if installed with an additional stand such as wind shield or deﬂ ector. a : 150mm (5.9 in) b : 400mm (15.7 in) front : 6000pa (125psf) rear : 5400pa (113psf) a : 120mm (4.7 in) *4point(①) front : 1800pa (37.5psf)rear : 1800pa (37.5psf) a : 120mm b : 820 ±100mm (32.3±3.9 in) 6point(①+②) front : 6000pa (125psf)rear : 5400pa (113psf) note) all mechanical installation method(fig1 to fig 6) in this appendix were not tested by intertek.(ul 1703, ulc 1703) it is evaluated by lg internal test. lg electronics u.s.a inc. 1000 sylvan ave, englewood cliffs, nj 07632 contact : lg.solar@lge.com http://www.lgsolarusa.com this document is subject to change without notice. lg, lg logo and life's good are trademarks of lg electronics, inc. worldwide. trademarks and intellectual properties of lg electronics, inc. are protected by international copyright laws. document : ii-g4(n)-ul-v2-en-201501"
url_md5 = Digest::MD5.hexdigest(text)
href
url = URI.parse('http://www.google.com')
href.include?(/http.?:\/\/www\.google/) ? href : "#{url.scheme}://#{url.host}/#{href}"
href.scan(/http.?:\/\/www\.google/) ? href : "#{url.scheme}://#{url.host}/#{href}"
d_url = href.scan(/http.?:\/\/www\.google/) ? href : "#{url.scheme}://#{url.host}/#{href}"
d_url
url.scheme
url.hose
url.host
d_url = href.scan(/http.?:\/\/www\.google/)
d_url = href.scan(/http.?:\/\/www\.google/).nil?
d_url = href.scan(/http.?:\/\/www\.google/).present?
d_url = href.scan(/http.?:\/\/www\.google/).present? ? href : "#{url.scheme}://#{url.host}/#{href}"
href
d_url = href.scan(/http.?:\/\/www\.google/).present? ? href : "#{url.scheme}://#{url.host}#{href}"
url = _
CGI.parse(URI.parse(url).query)['adurl'][0]
url = 'https://www.google.com/aclk?sa=l&ai=CS4vPLl8iV5TDOsiR-gPnorOoBLHjtOUH0YX3h4EC3_jel78BCAkQASDezc8eKBRgyfb4hsijoBmgAeOM4vwDyAEHqgQmT9DNIHnIjAhEbxI_5b6IKZKj6Di4FSb0Dtjvuy2-eYjofnjnoZ_ABQWgBiaAB4byTIgHAZAHAqgHpr4b2AcB4BKrluaK1aWwvtYB&sig=AOD64_1lXUrB-ITnfGmC0IKBHiki7duQfQ&ctype=5&clui=11&q=&ved=0ahUKEwiAvIbHg7LMAhUM7WMKHZvFCQ8Q1CkIuQYwAA&adurl=http://clickserve.dartsearch.net/link/click%3Flid%3D92700007036305828%26ds_s_kwgid%3D58700000387970821%26ds_e_adid%3D68908774873%26ds_e_product_group_id%3D51320962143%26ds_e_product_id%3DN82E16889007089%26ds_e_product_merchant_id%3D8438988%26ds_e_product_country%3DUS%26ds_e_product_language%3Den%26ds_e_product_channel%3Donline%26ds_e_product_store_id%3D%26ds_e_ad_type%3Dpla%26ds_s_inventory_feed_id%3D97700000000000031%26ds_url_v%3D2%26ds_dest_url%3Dhttp://www.newegg.com/Product/Product.aspx%3FItem%3DN82E16889007089%26nm_mc%3DKNC-GoogleAdwords-PC%26cm_mmc%3DKNC-GoogleAdwords-PC-_-pla-_-LED%2BTV-_-N82E16889007089'
CGI.parse(URI.parse(url).query)['adurl'][0]
CGI.parse(URI.parse(url).query)['adurl'][0].gsub(/.+dest_url=/, '')
d_url = href.scan(/http.?:\/\/www\.google/).present? ? href : "#{url.scheme}://#{url.host}#{href}"
url = URI.parse('http://www.google.com')
d_url = href.scan(/http.?:\/\/www\.google/).present? ? href : "#{url.scheme}://#{url.host}#{href}"
CGI.parse(URI.parse(d_url).query)['adurl'][0].gsub(/.+dest_url=/, '')
Category.last
c = Category.last
c.source_id
reload!
SourceCategory.last
SourceCategory.last 2
Category.last
SourceCategory.where(source_id: 1, test_process: true)
Category.last
c = _
SourceCategory.where(category: c)
Listing.last
l = _
l.screenshots
l.screenshots.first.attachment.url
l.reload
l.screenshots.first.attachment.url
s = Scrape.last
s.listings.count
s.listings.where(data_hash: nil).count
s.listings.where.not(data_hash: nil).count
href
href.scan(/^http.?:\/\/www\.google/).present?
href = 'https://www.google.com' + href
href.scan(/^http.?:\/\/www\.google/).present?
l.screenshots.first.attachment.url
l.screenshots.first.url
s.listings.where(data_hash: nil).count
s.listings.where(data_hash: nil).first
exit
args = [55, 314, "https://www.google.com/aclk?sa=l&ai=C0Dmp6J4iV9rbFIS_7Qbo0bn4DfCz9tcF8P77rqwBqOC7jkQICRADIN7Nzx4oFGDJBpABA6ABmJGi_QPIAQeqBCZP0PdDHQCtdUdGBS-ShhL9jS6aP6417f6eQAtyzVaHtOAcLAlPxsAFBaAGJoAH0O7dApAHAagHpr4b2AcB4BLV8trW1MSC7gM&sig=AOD64_1qsML1XzXOK1Z-dC0nSKveHqb-fw&ctype=5&clui=16&q=&ved=0ahUKEwi1jvGpwLLMAhWBNxQKHbEHAIU4FBDYKQjIAjAC&adurl=http://www.freecleansolar.com/ProductDetails.asp%3FProductCode%3DCSUN280-60M-BW", 0]
scrape_id, source_category_id, url, retry_time = args
scrape_id
retry_time
GoogleImageDetailJob
GoogleImageDetailJob.perform(scrape_id, source_category_id, url, retry_time)
GoogleImageDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
s = Scrape.last
scrape_id = 1
GoogleImageDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
Listing.last
l = _
l.screenshots.first.url
l.screenshots.first.attachment.url
exit
args = [1, 314, "https://www.google.com/aclk?sa=l&ai=Cid1ASJ8iV4TdJo3W-QOnkrToA_Cz9tcF8P77rqwBqOC7jkQICRACIN7Nzx4oFGDJvsmGxKPIF5ABCaABmJGi_QPIAQeqBCZP0HgT83EDFwDHFlTvZObesYZ4xPgjiJ0ZXs9YWYm_ewdcTK4e_MAFBaAGJoAH0O7dApAHAagHpr4b2AcB4BKJ5oX8sOiUxPUB&sig=AOD64_1_zEKY-PC2x7vIwo6a5cxZ_vo_zg&ctype=5&clui=10&q=&ved=0ahUKEwiv8ebXwLLMAhUS72MKHZGrBu04UBDYKQjAAjAB&adurl=http://www.freecleansolar.com/ProductDetails.asp%3FProductCode%3DCS6K-270M", 0]
scrape_id, source_category_id, url, retry_time = args
GoogleImageDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
exit
args = [1, 314, "https://www.google.com/aclk?sa=l&ai=Cid1ASJ8iV4TdJo3W-QOnkrToA_Cz9tcF8P77rqwBqOC7jkQICRACIN7Nzx4oFGDJvsmGxKPIF5ABCaABmJGi_QPIAQeqBCZP0HgT83EDFwDHFlTvZObesYZ4xPgjiJ0ZXs9YWYm_ewdcTK4e_MAFBaAGJoAH0O7dApAHAagHpr4b2AcB4BKJ5oX8sOiUxPUB&sig=AOD64_1_zEKY-PC2x7vIwo6a5cxZ_vo_zg&ctype=5&clui=10&q=&ved=0ahUKEwiv8ebXwLLMAhUS72MKHZGrBu04UBDYKQjAAjAB&adurl=http://www.freecleansolar.com/ProductDetails.asp%3FProductCode%3DCS6K-270M", 0]
scrape_id, source_category_id, url, retry_time = args
GoogleImageDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
url = 'http://rover.ebay.com/rover/1/711-117182-37290-0/2?mtid=1588&kwid=1&crlp=53601919689_324272&itemid=172165540359&targetid=154774687449&rpc=0.10&rpc_upld_id=69573&device=c&mpre=http%3A%2F%2Fwww.ebay.com%2Fulk%2Fitm%2Flike%2F172165540359%3Flpid%3D82%26chn%3Dps&adtype=pla&googleloc=9031942&poi=&campaignid=239125209&adgroupid=14978428809&rlsatarget=pla-154774687449'
url = 'https://www.google.com/aclk?sa=L&ai=CZ0adZpwjV8iLKcfW-QPw2LmQD4n1u7AH2bbRvMcB2Z2pysAECAkQCiDWzs8eKBlgyfb4hsijoBnIAQeqBCZP0PBagV0O7TQg5GFoJFREdgceBXIKys6jysncVJlR5F3kN076f8AFBaAGJoAH24COFZAHA6gHpr4b2AcB4BK-x8mskI3XwrYB&sig=AOD64_39woXU4Lh9Os2WIYuiUeTYXt762A&ctype=5&clui=20&q=&ved=0ahUKEwjYtPWJsrTMAhUL_mMKHTktB6YQ2ikIVw&adurl=http://rover.ebay.com/rover/1/711-117182-37290-0/2%3Fmtid%3D1588%26kwid%3D1%26crlp%3D53601919689_324272%26itemid%3D172165540359%26targetid%3D154774687449%26rpc%3D0.10%26rpc_upld_id%3D69573%26device%3Dc%26mpre%3Dhttp%253A%252F%252Fwww.ebay.com%252Fulk%252Fitm%252Flike%252F172165540359%253Flpid%253D82%2526chn%253Dps%26adtype%3Dpla%26googleloc%3D9031942%26poi%3D%26campaignid%3D239125209%26adgroupid%3D14978428809%26rlsatarget%3Dpla-154774687449'
url
CGI.parse(URI.parse(url).query)['adurl'][0].gsub(/.+dest_url=/, '')
url = 'http%3A%2F%2Fwww.ebay.com%2Fulk%2Fitm%2Flike%2F172165540359%3Flpid%3D82%26chn%3Dps&adtype=pla&googleloc=9031942&poi=&campaignid=239125209&adgroupid=14978428809&rlsatarget=pla-154774687449'
CGI.parse(URI.parse(url).query)['adurl'][0].gsub(/.+dest_url=/, '')
adurl = CGI.parse(URI.parse(url).query)['adurl'][0]
adurl = CGI.parse(URI.parse(url))['adurl'][0]
adurl = CGI.parse(URI.parse(url))
adurl = CGI.parse(url)
adurl
URI.parse(url)
URI.decode(url)
url
URI.unescape(url)
URI.unescape(url).gsub(/\?.+$/, '')
CGI.unescape(url).gsub(/\?.+$/, '')
args
scrape_id, source_category_id, url, retry_time = args
CGI.unescape(url).gsub(/\?.+$/, '')
CGI.unescape(url)
CGI.unescape(url).split('http')
'http' + CGI.unescape(url).split('http')
'http' + CGI.unescape(url).split('http').last
url2 = 'https://www.google.com/aclk?sa=l&ai=CWLUS3KAjV6CmL8iQ-gO9gomIBrHjtOUH0YX3h4EC3_jel78BCAkQAiDezc8eKBRgyfb4hsijoBmgAeOM4vwDyAEHqgQmT9DgUEBKNCcJ38si3y3AwoqdpI1tpJrz_g0nB0_ZXfPGYLbTlMXABQWgBiaAB4byTJAHA6gHpr4b2AcB4BKrluaK1aWwvtYB&sig=AOD64_2FmCi1ZyTYqgw9mmhmmhGyTgFp5Q&ctype=5&clui=3&q=&ved=0ahUKEwiKvMGqtrTMAhVKy2MKHbTyDU8Q2CkIvAYwAQ&adurl=http://clickserve.dartsearch.net/link/click%3Flid%3D92700007036305828%26ds_s_kwgid%3D58700000387970821%26ds_e_adid%3D68908774873%26ds_e_product_group_id%3D51320962143%26ds_e_product_id%3DN82E16889007089%26ds_e_product_merchant_id%3D8438988%26ds_e_product_country%3DUS%26ds_e_product_language%3Den%26ds_e_product_channel%3Donline%26ds_e_product_store_id%3D%26ds_e_ad_type%3Dpla%26ds_s_inventory_feed_id%3D97700000000000031%26ds_url_v%3D2%26ds_dest_url%3Dhttp://www.newegg.com/Product/Product.aspx%3FItem%3DN82E16889007089%26nm_mc%3DKNC-GoogleAdwords-PC%26cm_mmc%3DKNC-GoogleAdwords-PC-_-pla-_-LED%2BTV-_-N82E16889007089'
'http' + CGI.unescape(url2).split('http').last
url
'http' + CGI.unescape(url).split('http').last
'http' + URI.unescape(url).split('http').last
exit
args = [1, 314, "https://www.google.com/aclk?sa=l&ai=Cid1ASJ8iV4TdJo3W-QOnkrToA_Cz9tcF8P77rqwBqOC7jkQICRACIN7Nzx4oFGDJvsmGxKPIF5ABCaABmJGi_QPIAQeqBCZP0HgT83EDFwDHFlTvZObesYZ4xPgjiJ0ZXs9YWYm_ewdcTK4e_MAFBaAGJoAH0O7dApAHAagHpr4b2AcB4BKJ5oX8sOiUxPUB&sig=AOD64_1_zEKY-PC2x7vIwo6a5cxZ_vo_zg&ctype=5&clui=10&q=&ved=0ahUKEwiv8ebXwLLMAhUS72MKHZGrBu04UBDYKQjAAjAB&adurl=http://www.freecleansolar.com/ProductDetails.asp%3FProductCode%3DCS6K-270M", 0]
scrape_id, source_category_id, url, retry_time = args
GoogleImageDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l.screenshots.last.url
url3 = "https://www.google.com/aclk?sa=l&ai=CS4vPLl8iV5TDOsiR-gPnorOoBLHjtOUH0YX3h4EC3_jel78BCAkQASDezc8eKBRgyfb4hsijoBmgAeOM4vwDyAEHqgQmT9DNIHnIjAhEbxI_5b6IKZKj6Di4FSb0Dtjvuy2-eYjofnjnoZ_ABQWgBiaAB4byTIgHAZAHAqgHpr4b2AcB4BKrluaK1aWwvtYB&sig=AOD64_1lXUrB-ITnfGmC0IKBHiki7duQfQ&ctype=5&clui=11&q=&ved=0ahUKEwiAvIbHg7LMAhUM7WMKHZvFCQ8Q1CkIuQYwAA&adurl=http://clickserve.dartsearch.net/link/click%3Flid%3D92700007036305828%26ds_s_kwgid%3D58700000387970821%26ds_e_adid%3D68908774873%26ds_e_product_group_id%3D51320962143%26ds_e_product_id%3DN82E16889007089%26ds_e_product_merchant_id%3D8438988%26ds_e_product_country%3DUS%26ds_e_product_language%3Den%26ds_e_product_channel%3Donline%26ds_e_product_store_id%3D%26ds_e_ad_type%3Dpla%26ds_s_inventory_feed_id%3D97700000000000031%26ds_url_v%3D2%26ds_dest_url%3Dhttp://www.newegg.com/Product/Product.aspx%3FItem%3DN82E16889007089%26nm_mc%3DKNC-GoogleAdwords-PC%26cm_mmc%3DKNC-GoogleAdwords-PC-_-pla-_-LED%2BTV-_-N82E16889007089"
'http' + CGI.unescape(url).split('http').last
args = [1, 314, "https://www.google.com/aclk?sa=l&ai=Cid1ASJ8iV4TdJo3W-QOnkrToA_Cz9tcF8P77rqwBqOC7jkQICRACIN7Nzx4oFGDJvsmGxKPIF5ABCaABmJGi_QPIAQeqBCZP0HgT83EDFwDHFlTvZObesYZ4xPgjiJ0ZXs9YWYm_ewdcTK4e_MAFBaAGJoAH0O7dApAHAagHpr4b2AcB4BKJ5oX8sOiUxPUB&sig=AOD64_1_zEKY-PC2x7vIwo6a5cxZ_vo_zg&ctype=5&clui=10&q=&ved=0ahUKEwiv8ebXwLLMAhUS72MKHZGrBu04UBDYKQjAAjAB&adurl=http://www.freecleansolar.com/ProductDetails.asp%3FProductCode%3DCS6K-270M", 0]
scrape_id, source_category_id, url, retry_time = args
'http' + CGI.unescape(url).split('http').last
exit
args = [1, 314, "https://www.google.com/aclk?sa=l&ai=Cid1ASJ8iV4TdJo3W-QOnkrToA_Cz9tcF8P77rqwBqOC7jkQICRACIN7Nzx4oFGDJvsmGxKPIF5ABCaABmJGi_QPIAQeqBCZP0HgT83EDFwDHFlTvZObesYZ4xPgjiJ0ZXs9YWYm_ewdcTK4e_MAFBaAGJoAH0O7dApAHAagHpr4b2AcB4BKJ5oX8sOiUxPUB&sig=AOD64_1_zEKY-PC2x7vIwo6a5cxZ_vo_zg&ctype=5&clui=10&q=&ved=0ahUKEwiv8ebXwLLMAhUS72MKHZGrBu04UBDYKQjAAjAB&adurl=http://www.freecleansolar.com/ProductDetails.asp%3FProductCode%3DCS6K-270M", 0]
scrape_id, source_category_id, url, retry_time = args
GoogleImageDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
Listing.where(url: 'http://www.freecleansolar.com/ProductDetails.asp?ProductCode=CS6K-270M')
Listing.where(url: 'http://www.freecleansolar.com/ProductDetails.asp?ProductCode=CS6K-270M').destroy
Listing.where(url: 'http://www.freecleansolar.com/ProductDetails.asp?ProductCode=CS6K-270M').destroy_all
GoogleImageDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
exit
Product.count
exit
Product.count
edit -t
exit
edit -t
models.count
models.uniq.count
models.any?(&:blank?)
models.any? {|m| m.include?(' ')}
models.first? {|m| m.include?(' ')}
models.first {|m| m.include?(' ')}
models.first {|m| m.scan(/\s/).presnt?}
models.first {|m| m.scan(/\s/).present?}
models.select {|m| m.scan(/\s/).present?}
models.select {|m| m.scan(/\s/).present?}.uniq
models
models.count
manuals
models
edit -t
["105UC9",
  "105UC900V",
  "105UC9T",
  "105UC9V",
  "14CD1RBE-TB",
  "14FK3RB",
  "14FU7",
  "14FU7R",
  "14FU7RB",
  "14SB1RB",
  "14SB1RL",
  "14SR1AB",
  "15EL9500",
  "15EL950N",
  "15FC2",
  "15LA6R",
  "15LC1R",
  "17LX1R",
  "17LZ50",
  "19LD320",
  "19LD330",
  "19LD350",
  "19LD350N",
  "19LD355",
  "19LE3300",
  "19LE330N",
  "19LE3400",
  "19LE5300",
  "19LF10",
  "19LG3000",
  "19LG3010",
  "19LG3100",
  "19LH20",
  "19LH2000",
  "19LH20R",
  "19LH250C",
  "19LN4050",
  "19LS3500",
  "19LS350T",
  "19LS3590",
  "19LU4000",
  "19LU4010",
  "19LU5000",
  "19LU50R",
  "19LU55",
  "19LU7000",
  "19LV2500",
  "19LV250N",
  "19LV250U",
  "19MN43D",
  "20CD1RGS",
  "20LA6R",
  "20LA70",
  "20LB450A",
  "20LB451A",
  "20LB451A-TA",
  "20LB452A",
  "20LB455A-TI",
  "20LB456A",
  "20LB474A-TF",
  "20LC1R",
  "20LC1R-TG",
  "20LF430A",
  "20LF460A",
  "20MT45A",
  "21CA8RGS",
  "21CD1RGE-TB",
  "21CD1RGS",
  "21FC2",
  "21FC2RG",
  "21FD1RB",
  "21FD3AB",
  "21FD3RB",
  "21FE9RGE",
  "21FE9RGS",
  "21FG1RG",
  "21FG5RG",
  "21FJ4A",
  "21FJ8RG",
  "21FK2",
  "21FK2RG",
  "21FS4RG",
  "21FS4RL",
  "21FS4RLX",
  "2 "2 "2 "2 1F "2 "2 "2 1F "2 "2 "2 "2 1", "2 "2 "2 ", "2 "2 "RG "2 "2 "29R "2 "2 "2 1" "2 "2 "2 E" "2 "2 "2 G" "2 "2 "2 L" "2 "2 "2 "
21         "         
"         
          ,
          ",          ",          
"          21           A1           L"           
"         
             "           1S           3"         10           0"          10           0A           0B           0U           1A           B4           B4           B4           B4          LB         22           22           22           22           22          "2           "2         
,
"             "LD           LD             "00           0N             ",
0"         0" ",         0"",         0"",         00"         0"0"         0"   "2         0"22        
,
",          A"          0A          "0A          "1U          93          54         LG          2L         "           "          
"            "            "            "            "            "            "            "            "            "            "            "    50            "            "      2L            "       
,
"       50            "       50            "       50            "     S3            "       502L            "       50            "       50            "       50            "     S3            "       502L            "       50            "       50            "       50            "     S3            "       502L            "       50            "       50            "       50  ,
,
"       50            "       50            "       50            "     S32M"       50            "       50            "       50            "     S32M"       50            "       50    "2"       50 ,
"       50            "       50            "       50            "     S32M"       50            "       50            "       50          Z""       50            "       50            "       50            "     S32M"       50            "       50            "       50          Z""       50            "       50            "       50            "     S
55          55          4M          4M                                                                                                                                                                                                         0"                                                                                                                                                                                                                                                                                                                                                                    K3           33           30",
"42LH7000",
"42LH7020",
"42LH7030",
"42LH70YR",
"42LH90",
"42LH9000",
"42LH90QD",
"42LH90QR",
"42LK430",
"42LK430N",
"42LK435C",
"42LK450",
"42LK450N",
"42LK450U",
"42LK451",
"42LK460-CC",
"42LK465C",
"42LK520",
"42LK530",
"42LK530N",
"42LK530T",
"42LK530Y",
"42LK550",
"42LK551",
"42LM3400",
"42LM340T",
"42LM340Y",
"42LM3410",
"42LM3450",
"42LM345T",
"42LM4600",
"42LM5700",
"42LM5800",
"42LM580T",
"42LM580Y",
"42LM615S",
"42LM615T",
"42LM6200",
"42LM62000",
"42LM620S",
"42LM620T",
"42LM620Y",
"42LM6210",
"42LM621Y",
"42LM6400",
"42LM640S",
"42LM640T",
"42LM6410",
"42LM641Y",
"42LM649S",
"42LM649T",
"42LM6600",
"42LM660S",
"42LM660T",
"42LM6610",
"42LM66100",
"42LM6690",
"42LM66900",
"42LM669S",
"42LM669T",
"42LM6700",
"42LM670S",
"42LM670T",
"42LM6710",
"42LM67100",
"42LM671S",
"42LM671Y",
"42LM7600",
"42LM760S",
"42LM760T",
"42LM7610",
"42LM76100",
"42LM761S",
"42LM761T",
"42LM765S",
"42LM860V",
"42LM860W",
"42LN5100",
"42LN510 -T "42LN510 -11 "42LN510 -12 "42LN510 -20 "42LN510 -20 "42LN510N5 "42LN510 -N5 "42LN510 -N5 "42LN510 -N5 "42LN510 LN "42LN510 -LN "42LN510 -LN "42LN510  " "42LN510 - " "42LN510 - " "42LN510 ,
"42LN510 -T ", "42LN510 -", "42LN510 V" "42LN510 40 "42LN510 -45 "42LN510 -46 "42LN510 -49 "42LN510 54 "42LN510 N5 "42LN510 -N5 "42LN510 -LN "42LN510 -"4 "42LN510 -"4 "42LN510 -"4 "42LN510 - " "42LN510 - " "42LN510 - " "42LN510 - " "42LN510 - " "42LN510 - " "42LN510 -
"            "            "            "            "            "            "            "            "            "            "           
            
            ,
            
              "            "            "           
edit -t
models.count
price = "USD 140"
require 'monetize'
Monetize.parse(price)
min = _
min.exchange_to(:EUR)
min.exchange_to(:EUR).to_f
Monetize.parse(price).to_f
Monetize.parse('$150').to_f
Monetize.parse('$150').currency
Monetize.currency_symbol_regex
Monetize.parse('INR 150')
Monetize.parse('INR 150').to_f
Monetize.parse('INR 150').exchange_to(:USD)
Monetize.parse('INR 150').exchange_to(:USD).to_f
Monetize.parse('599 €').exchange_to(:USD).to_f
Monetize.parse('€ 599').exchange_to(:USD).to_f
Monetize.parse('EUR 599').exchange_to(:USD).to_f
Monetize.parse('THB 5990').exchange_to(:USD).to_f
Monetize.parse('INR 8990').exchange_to(:USD).to_f
Monetize.parse('EUR 349').exchange_to(:INR).to_f
Monetize.parse('2550 CNY').exchange_to(:INR).to_f
Monetize.parse('2550 CNY').exchange_to(:USD).to_f
Monetize.parse('THB 14 900').exchange_to(:USD).to_f
Monetize.parse('THB 14900').exchange_to(:USD).to_f
Monetize.parse('THB14900').exchange_to(:USD).to_f
Monetize.parse('156EUR').exchange_to(:USD).to_f
Monetize.parse('THB 15 900').exchange_to(:USD).to_f
Monetize.parse('THB15 900').exchange_to(:USD).to_f
Monetize.parse('THB 15900').exchange_to(:USD).to_f
Monetize.parse('15 900 THB').exchange_to(:USD).to_f
Monetize.parse('15 900THB').exchange_to(:USD).to_f
exit
exit
DateTime.now
DateTime.now.to_i
DateTime.now.strftime('%Y')
DateTime.now.strftime('%Y%m')
DateTime.now.strftime('%Y%m%d')
DateTime.now.strftime('%Y%m%d%H%M%S')
exit
edit -t
urls.count
urls = %w(
http://manuals4owners.com/docs/848304/user_manual/Sima_SL-200LXI-camera-flashe.pdf
http://yourpdfguides.com/user-manual/LG/50PB560B__5579679.pdf
http://www.blocksafety.com/samsung-un65f7100-user-manual.pdf
http://www.socketmailthings.com/samsung-led-tv-use-manual.pdf
http://yourpdfguides.com/user-manual/SAMSUNG/LN40D630__3869240.pdf
http://yourpdfguides.com/user-manual/LG/50PA4520__4298517.pdf
http://www.fungus888.com/pdf/8kJ-samsung-ln40c530-manual.pdf
http://www.sueaton.com/ebooks/FQB-samsung-un55eh6000-manual.pdf
http://www.sueaton.com/ebooks/Gyf-samsung-un55eh6000-user-manual.pdf
http://www.hbrxdahg.com/library/r2a/lg-bp125-blu-ray-and-dvd-player-manual.pdf
http://yourpdfguides.com/user-manual/SHARP/BD-HP90U__4539527.pdf
http://www.socketmailthings.com/lg-60pk550-tv-manual.pdf
http://www.royalagentportugal.com/lg-lfd750-service-manual.pdf
http://www.royalagentportugal.com/lg-fb-162-service-manual.pdf
http://www.royalagentportugal.com/lg-tv-manual.pdf
http://fixitmanuals.com/pdf/1840432/instructhttp://fil/http://fixitmanuals.com/pdf/1840432/instructhttp://fil/http://fixitmanuals.com/pdsehttp://fixitmanuals.com/pdf/1840432/instructhttp://fil/http:p:http://fixitmanuals.com/pdf/1840432/instructhttp://fil/http6.http://fixitmanuals.com/pdf/1840432/instructhttp://fil/http:gahttp://fixitmanuals.com/pdf/1840432/instructhttp://fil/http://fixf
http://fixitmanuals.com/pdf/1840432/instructhttp://fil/http:_5http://fixitmanuals.com/pdf/1840432/instructhttp://fil/http-yhttp://fixitmanuals.com/pdf/1840432/instructhttp://fil/http:_5-mhttp://fixitmanuals.com/pdf/1840432/instructhttp://fil/http:_5http:T0http://fixitmanuals.com/pdf/1840432/instructhttp://fil/http:_5http://fixitman.phttp://fixitmanuals.com/pdf/1840432/instructhttp://fil/http:_5hp:http://fixitmanuals.com/pdf/1840432/instructhttp://fil/
hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh:/hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhpdhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhALhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh:/hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhpdhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhALhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh:/hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhpdhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhALhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhthhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh:/hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhpdhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhALhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh:/hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhpdhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhALhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh:/hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh.pnicslover13.files.wordpress.com/2016/01/b00d2lnlby.pdf
https://manuals4owners.com/docs/2199341/manual/MSI_Gaming-GE62-Apache-002.pdf
https://manuals4owners.com/docs/2199341/setup_guide/MSI_Gaming-GE62-Apache-002.pdf
http://bloggersdelight-dk.bloggersdelight.netdna-cdn.com/wp-content/blogs.dir/133948/files/2016/02/symphonic-wf803-dvd-vcr-combo-manual.pdf
http://bloggersdelight-dk.bloggersdelight.netdna-cdn.com/wp-content/blogs.dir/133064/files/2016/02/manual-for-cyberhome-320-dvd-300-players.pdf
http://www.pickelswildlife.com/pdf/Cdi/magnavox-dvd-recorder-zc350ms8-manual.pdf
http://yourpdfguides.com/user-manual/AOC/L42H961__3167227.pdf
https://uwydmlsoy.files.wordpress.com/2015/08/venturer-portable-dvd-player-pvs1262-user-manual.pdf
https://manuals4owners.com/docs/424274/owners_manual/Samsung_DVD-P390.pdf
http://bloggersdelight-dk.bloggersdelight.netdna-cdn.com/wp-content/blogs.dir/133539/files/2016/02/samsung-46-lcd-hdtv-manual.pdf
http://lovezkhvezda.ru/samsung/samsung-le40b650-user-manual.pdf
http://yourpdfhttp://yourpdfhttp://yourpdfhttp://yourpdfhttp://yourtphttp://yourpdfhttp://yourpdfhttp://yourpdfhttp://yourpdfhttp://yourtphttp://u/http://yourpdfhttp://yourpdfhttp://yourpdfhttp://yourpdfhttp://yourtphttp://yourpC/http://yourpdfhttp://yourpdfhttp://yourpdfhttp://yourpdfhttp://yourtphttp://yourpdfhttp://yourpdfhttp://yourpdfhttp://yourpdfhttp://yourtp/ohttp://yourpdfhttp://yourpdfhttp://yourpdfhttp://yourpdfhttp://yourtphttp://yourpdfhttp://yourpdfhttp://yourpdfhttp://yourpdfhttp://yourtphttpNIhttp://yourpdfhttp://yourpdfhttp://yourpdfhttp://yourpdfhttp://yourtphttp://youm/http://yourpdfhttp://yourpdfhttp://yourpdfhttp://yourpdfhttp-4http://yourpdfhttp://yourpdfhttp://yourpdfhttp://yourpdfhttp://yourtphttp://yourpdfhttp://yourpdfhthttp://yourpdfhttp://yourpdfhttp://yourpdfhttp://yourpdfhttp://yourtphtt/mhttp://yourpdfhttp://yourpdfhttp://yourpdfhttp://yourpdfhttp://yourtphttp://yourpdfhttp://yourpdfhttp://yourpdfhttp://yourpdfhttu-http://yourpdfhttp://yourpdfhttp://yourpdfhttp://yourpdfhttp://mses-camera-recording-dvr-glasses-camcorder-5-mega-pixels-1280x720yellow-black.pdf
http://www.ts88soft.com/repair-manual/1jk/toshiba-26av502u-manual.pdf
https://pcbzawjzd.files.wordpress.com/2015/08/elid-el1300-user-manual.pdf
http://www.xjqcxx.com/readers/ixx/samsung-pn50a450-owners-manual.pdf
http://yourpdfguides.com/user-manual/ASUS/CD-S520__4198499.pdf
http://www.pickelswildlife.com/pdf/vT0/iomega-35115-storage-owners-manual.pdf
http://yourpdfguides.com/user-manual/ADDONICS/ADU3ESA__2715089.pdf
https://manuals4owners.com/docs/579881/manual/Fujitsu_PA03541-0001.pdf
https://manuals4owners.com/docs/579881/owners_manual/Fujitsu_PA03541-0001.pdf
http://bloggersdelight-dk.bloggersdelight.netdna-cdn.com/wp-content/blogs.dir/130549/files/2016/02/epson-workforce-210-manual-feed.pdf
http://bloggersdelight-dk.bloggersdelight.netdna-cdn.com/wp-content/blogs.dir/134062/files/2016/02/use-manual-for-magellan-roadmate-1412.pdf
http://yourpdfguides.com/user-manual/SAMSUNG/SYNCMASTER+2253LW__2595539.pdf
http://www.ts88soft.com/repair-manual/3eI/kocaso-m1062-user-manual.pdf
http://bloggersdelight-dk.blogghttp://bloggersdelight-dk.blogghttp://bloggersdelight-dk.blogghttp:/sahttp://bloggersdelight-dk.blogghttp://bloggersdelight-dk.blogghttp://blohLhttp://bloggersdelight-dk.blogghttp://bloggersdelight-dk.blogghttp:4.http://bloggersdelight-dk.blogghttp://bloggersdelight-dk.blogighttp://bloggersdelight-dk.blogghttp://bloggersdelight-dk.blogghttp://blogger02http://bloggersdelight-dk.blogghttp://bloggersdelight-dk.blogghttimhttp://bloggersdelight-dk.blogghttp://bloggersdelight-dk.blogghttp://2.http://bloggersdelight-dk.blogghttp://bloggersdelight-dk.blogghttp://bloggeruahttp://bloggersdelight-dk.blogghttp://bloggersdelight-dk.blogghttp://bloggersdelight-dk.blogghttp:/sahttp://bloggersdelight-dk.bloggh70http://bloggersdelight-dk.blogghttp://bloggersdelight-dk.blogghttp://bloggersdelight-dk.blogghttp:/sahttp://bloggersdelight-dk.blogghttp://cohttp://bloggersdelight-dk.blogghttp://bloggersdelight-dk.blogghttp://bloggs/http://bloggersdelight-dk.blogghttp://bloggersdelight-dk.blogghttp://getp://yourpdfguides.com/user-manual/GIGABYTE/GV-N630-2GI__4350918.pdf
http://guidecursor.com/support/355344/instruction_manual/Western-Digital_WDCC05RNN.pdf
http://guidecursor.com/support/355344/user_manual/Western-Digital_WDCC05RNN.pdf
http://yourpdfguides.com/user-manual/AOC/931SWL__3232944.pdf
http://bloggersdelight-dk.bloggersdelight.netdna-cdn.com/wp-content/blogs.dir/133913/files/2016/02/harmontec-dv-105-dvd-player-manual.pdf
http://yourpdfguides.com/user-manual/GIGABYTE/GA-F2A88XM-DS2__5435318.pdf
https://electronicslover11.files.wordpress.com/2016/01/b00wnrriiu.pdf
http://bloggersdelight-dk.bloggersdelight.netdna-cdn.com/wp-content/blogs.dir/133556/files/2016/02/motorola-moto-w755-user-manual.pdf
http://bloggersdelight-dk.bloggersdelight.netdna-cdn.com/wp-content/blogs.dir/128982/files/2016/02/bluetooth-for-windows-81-64-bit-dell.pdf
https://sportgear11.files.wordpress.com/2016/01/b001z0c5mu.pdf
https://electronicslover14.files.wordpress.com/2016/01/b0019mgitk.pdf
http://www.pickelswildlife.com/pdf/a5r/aten-kl9108m-monitors-owners-manual.pdf
http://www.pickelswildlife.com/pdf/8YS/aten-switch-owners-manual.pdf
http://yourpdfguides.com/user-manual/ASROCK/Z87+PRO3__5200813.pdf
http://www.pickelswildlife.com/pdf/EWe/northwestern-bell-581http://www.pickelswildlife.com/pdf/EWe/northwestern-bell-58cohttp://www.pickelswildlife.com/pdf/EWe/northwestern-bell-581http://www.pickelswildlife.com/pdf/EWe/northwestern-bell-58cohttp://www.picktphttp://www.pickelswildlife.com/pdf/EWe/northwestern-bell-581http://www.pr/http://www.pickelswildlife.com/pdf/EWe/northwestern-bell-581http://w//http://www.pickelswildlife.com/pdf/EWe/northwestern-bell-581http://www.pickelswildlife.com/pdf/EWe/northwestern-bell-58cohttp://www.alhttp://www.pickelswildlife.com/pdf/EWe/northwestern-bell-581http://www.pickelswildlife.com/pdf/EWe/northwestern-bell-58cohttp://www.pickelt/http://www.pickelswildlife.com/pdf/EWe/northwestern-bell-581hrshttp://www.pickelswildlife.com/pdf/EWe/northwestern-bell-581http://w2/http://www.pickelswildlife.com/pdf/EWe/northwestern-bell-581http://www.pickelnuhttp://www.pickelswildlife.com/pdf/EWe/northwestern-bell-581http://rdhttp://www.pickelswildlife.com/pdf/EWe/northwestern-bell-581httpanelight.netdna-cdn.com/wp-content/blogs.dir/133472/files/2016/02/rca-amplified-indoor-antenna-ant1250-manual.pdf
https://hometool12.files.wordpress.com/2015/12/b004vfgvb2.pdf
http://yourpdfguides.com/user-manual/SAMSUNG/SYNCMASTER+S24B350HL__4458403.pdf
http://yourpdfguides.com/user-manual/MSI/FM2-A55M-E33__5445901.pdf
https://electronicslover20.files.wordpress.com/2016/01/b00sm88eua.pdf
https://musicgears2.files.wordpress.com/2015/12/b00jefntg4.pdf
https://rhjliiy.files.wordpress.com/2015/07/oregon-scientific-bpw128-user-manual.pdf
http://yourpdfguides.com/user-manual/LG/E2360V-PN__4208126.pdf
https://manuals4owners.com/docs/847882/user_manual/Panasonic_DMC-ZS10K-compact-camera.pdf
https://7oalk18ayyzebf6rd.files.wordpress.com/2015/06/canon-powershot-elph-510-hs-121-mp-cmos-digital-camera-with-full-hd-video-and-ultra-wide-angle-lens-silver.pdf
https://manuals4owners.com/docs/847882/setup_guide/Panasonic_DMC-ZS10K-compact-camera.pdf
https://manuals4owners.com/docs/688048/user_manual/Audio-Technica_ATM410-microhttps://manuals4owners.com/docs/688048/user_manual/Audio-Technica_ATM410-microhttps://manuals4owners.com/docs/amhttps://manuals4owners.com/docs/688048/user_manual/Audio-TecjOhttps://manuals4owners.com/docs/688048/user_manual/Audio-Technica_ATM410-micrENhttps://manuals4owners.com/docs/688048/user_manual/Audio-TechnicaANhttps://manuals4owners.com/docs/688048/user_manual/Audio-Technica_ATunhttps://manuals4owners.com/docs/688048/user_manual/Audio-Techf/https://manuals4owners.com/docs/688048/user_manual/Audio-Technica_ATM410-microhttpsMShttps://manuals4owners.com/docs/688048/user_manual/Audio-Techighttps://manuals4owners.com/docs/688048/user_manual/Audio-Technica_ATM410-microhttps://mauahttps://manuals4owners.com/docs/688048/user_manual/Audio-Technica_ATM410-microhttps://manuals4owners.com/docs/688048/user_manual/Audio-Technica_ATM410-microhttps:/dehttps://manuals4owners.com/docs/688048/user_manual/Audio-Technica_ATM410-microhttps://maonss.com/2016/01/b002kl26ja.pdf
http://yourpdfguides.com/user-manual/PLANTRONICS/DA45__479228.pdf
https://electronicslover20.files.wordpress.com/2016/01/b00rvhzsw0.pdf
http://yourpdfguides.com/user-manual/TRANSCEND/TS8GCF400__3700827.pdf
http://yourpdfguides.com/user-manual/TRANSCEND/TS16GJF530__3700424.pdf
http://www.xzxtrj.com/ebooks/o/oregon-scientific-wr196t-manual.pdf
https://manuals4owners.com/docs/1659551/owners_manual/Veho_VSS-007-360BT.pdf
https://manuals4owners.com/docs/1659551/setup_guide/Veho_VSS-007-360BT.pdf
https://manuals4owners.com/docs/436972/setup_guide/ASUS_VH232H.pdf
http://www.xjqcxx.com/readers/jA6/uniden-dect-60-d1660-user-manual.pdf
https://manuals4owners.com/docs/857677/owners_manual/Toshiba_PA3959U-1ETB.pdf
https://manuals4owners.com/docs/827128/user_manual/Kensington_K33084-headphone.pdf
https://manuals4owners.com/docs/857677/manual/Toshiba_PA3959U-1ETB.pdf
https://manuals4owners.com/docs/827128/owners_manual/Kensington_K33084-headphone.pdf
http://lovezkhvezda.ru/samsung/samsung-monitor-204bw-manual.pdf
https://mhttps://mhttps://mhttps://mhttps://mhttps://mhttps://mhttps://mh-9https://mhttps://mhttps://mhttps://mhttps://mhttps://mhttps://mhttps43https://mhttps://mhttps://mhttps://mhttps://mhttps://mhttps://mhttpsl.https://mhttps://mhttps://mhttps://mhttps://mhttps://mhttps://mhttps:X/https://mhttps://mhttps://mhttps://mhttps://mhttps://mhttps://mht0Ehttps://mhttps://mhttps://mhttps://mhttps://mhttps://mhttps://mhttps://mh-9//https://mhttps://mhttps://mhttps://mhttps://mhttps://mhttps://mhttps://mhowhttps://mhttps://mhttps://mhttps://mhttps://mhttps://mhttps://mhtidhttps://mhttps://mhttps://mhttps://mhttps://mhttps://mhttps://mhttps:.chttps://mhttps://mhttps://mhttps://mhttps://mhttps://mhttps://mhttps://mh-9hprhttps://mhttps://mhttps://mhttps://mhttps://mhttps://mhttps://mhttps://mh-9https:6/https://mhttps://mhttps://mhttps://mhttps://mhttps://mhttps://mhttps:/0https://mhttps://mhttps://mhttps://mhttps://mhttps://mhttps://mhttps://mh-9https://MIhttps://mhttps://mhttps://mhttps://mhttps://mhttps://mhttps://s/933951.pdf
http://yourpdfguides.com/user-manual/MOTOROLA/SD4551__327110.pdf
http://yourpdfguides.com/user-manual/MOTOROLA/DIGITAL+CORDLESS+PHONE-SD4551__302125.pdf
http://www.xjqcxx.com/readers/hxL/samsung-1501mp-monitors-owners-manual.pdf
https://ptuw94jv.files.wordpress.com/2015/06/canon-powershot-a400-32mp-digital-camera-with-22x-optical-zoom-silver.pdf
http://yourpdfguides.com/user-manual/GIGABYTE/GV-N610SL-1GI__4350968.pdf
https://manuals4owners.com/docs/894211/manual/Sennheiser_CX-310-Originals.pdf
http://yourpdfguides.com/user-manual/GIGABYTE/GA-X99-GAMING+G1+WIFI__5732881.pdf
http://yourpdfguides.com/user-manual/MSI/C847IS-P33__5488237.pdf
https://manuals4owners.com/docs/1918432/manual/HP_Pavilion-Slimline-400-224.pdf
https://manuals4owners.com/docs/1918432/setup_guide/HP_Pavilion-Slimline-400-224.pdf
http://yourpdfguides.com/user-manual/PANASONIC/KX-TG2632W__207418.pdf
https://ckhpynpgq.files.wordpress.com/2015/08/x5-hair-laser-user-manual.pdf
https://fbibqqmzt.files.wordpress.com/2015/08/3com-wx1200https://fuahttps://fbibqqmzt.files.wordpress.com/2015/08/3com-wx1200https:44https://fbibqqmzt.files.wordpress.com/2015/08/3com-wx1200https://fuahttps://fbibqqmzt.s:https://fbibqqmzt.files.wordpress.com/2015/08/3com-wx1200https://fuahttps:pdhttps://fbibqqmzt.files.wordpress.com/2015/08/3com-wx1200https://fuahttps://fbibqqmzt.files.wordpress.com/2015/08/3co/Thttps://fbibqqmzt.files.wordpress.com/2015/08/3com-wx1200https://fuahttSHhttps://fbibqqmzt.files.wordpress.com/2015/08/3com-wx1200https://fuahttps://dihttps://fbibqqmzt.files.wordpress.com/2015/08/3com-wx1200https://fuahttps://fbiudhttps://fbibqqmzt.files.wordpress.com/2015/08/3com-wx1200https:30https://fbibqqmzt.files.wordpress.com/2015/08/3com-wx1200https://fuahttps://fberhttps://fbibqqmzt.files.wordpress.com/2015/08/3com-wx1200https://fuahttps://fbibqqm5Mhttps://fbibqqmzt.files.wordpress.com/2015/08/3com-wx1200https://fuay_https://fbibqqmzt.files.wordpress.com/2015/08/3com-wx1200https://fuahttps:R-.pdf
http://yourpdfguides.com/user-manual/ART/M-FIVE__2695350.pdf
https://manuals4owners.com/docs/1908861/setup_guide/Elite-Screens_R100DHD5-projection-screen.pdf
http://yourpdfguides.com/user-manual/TRENDNET/TPE-104S__4260072.pdf
https://manuals4owners.com/docs/776992/user_manual/Sony_VAIO-VPCEB23FX/WI-notebook.pdf
https://manuals4owners.com/docs/776992/manual/Sony_VAIO-VPCEB23FX/WI-notebook.pdf
http://yourpdfguides.com/user-manual/TRANSCEND/TS2GCF100I__3802682.pdf
http://yourpdfguides.com/user-manual/TRENDNET/TPL-302E2K__3182573.pdf
https://manuals4owners.com/docs/591260/owners_manual/Audio-Technica_ATH-M3X.pdf
https://manuals4owners.com/docs/591260/user_manual/Audio-Technica_ATH-M3X.pdf
http://yourpdfguides.com/user-manual/CORSAIR/CMX4GX3M2A1600C8__3927602.pdf
https://manuals4owners.com/docs/1534144/user_manual/Skullcandy_S2FXDM213-mobile-headset.pdf
https://manuals4owners.com/docs/1534144/manual/Skullcandy_S2FXDM213-mobile-headset.pdf
http://www.xjqcxx.com/readers/bXZ/linksys-usb10t-owners-manual.pdf
httphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphe-httphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphe-httphtt/mhttphtt/mhttphttdehttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttomhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphe-httphtt/mhttpht42httphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphe-httphtt/mh84httphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttpheJ2httphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphnuhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphe-httphtt/cahttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphe-httphtG-httphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphe-httashttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphe-httphtt/mhttphtt/mhtRPhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphe-httphtt/mhttphtf
httphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhts-httphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhts-httphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhts-httphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhts-httphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhts-httphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhts-httphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhts-httphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhts-httphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhts-httphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhts-httphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhts-httphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhts-httphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhts-httphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhts-httphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhttphtt/mhts-httphtt/mhttphtt/19slover14.files.wordpress.com/2016/01/b00ipketbs.pdf
http://www.smartweb-seo.com/guides/6PP/lacie-rugged-external-hard-drive-manual.pdf
http://www.smartweb-seo.com/guides/6Kp/lacie-2big-nas-manual.pdf
http://yourpdfguides.com/user-manual/BENQ/DW822A+(PLUS)__1150364.pdf
http://yourpdfguides.com/user-manual/SAMSUNG/SD-816B__480258.pdf
https://electronicslover13.files.wordpress.com/2016/01/b007gwmqne.pdf
http://www.etrader.ro/uploads/157/lead_files/b6ee00a4-1621-4c8e-98b2-727b6c41ce72.pdf
https://electronicslover14.files.wordpress.com/2016/01/b00qpsrx2e.pdf
https://manuals4owners.com/docs/770188/owners_manual/JVC_HA-S160-A-E-headphone.pdf
http://www.nailb888.com/documentation/sennheiser-hd-380-pro-headphones-owners-manual.pdf
https://musicgears3.files.wordpress.com/2015/12/b0047dwtku.pdf
https://electronicslover13.files.wordpress.com/2016/01/b00dh9m3c4.pdf
https://hjwwgjmxf.files.wordpress.com/2015/07/infocus-lp240-user-manual.pdf
http://lovezkhvezda.ru/samsung/samsung-hm7000-instruction-manual.pdf
http://lovezkhvezda.ru/samsung/manual-samsung-hm7000.pdf
http://yourpdfguides.com/user-manual/ASUS/CD-S520%2BA4__4180370.pdf
http://www.sociallab101.http://www.sociallab101.http://www.sociallab101.http://www.sociowhttp://www.sociallab101.http://www.sociallab101.http://www.sociallatthttp://www.sociallab101.http://www.sociallab101.http://www.soci72http://www.sociallab101.http://www.sociallab101.http://www.sociallaberhttp://www.sociallab101.http://www.sociallab101.http://www.sociallab101.http://www.stthttp://www.sociallab101.http://www.sociallab101.http://www.sociallabnthttp://www.sociallab101.http://www.sociallab101.http://www.sociallab101.http://wwt.http://www.sociallab101.http://www.sociallab101.http://www.sociallab101.http://www.soci
hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhrohhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhrohhhhnehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhrohhhhhhhhhh3Shhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhrohhhdehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh8.hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhrohh.whhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhrohhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhrohhhhnehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhrohhhhhhhhhh3Shhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhrohhhdehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh8.hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhrohh.whhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhrohhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhrohhhhnehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhrohhhhhhhhhh3Shhhhui/user-manual/SONY/MDR-AS35W__2743757.pdf
http://yourpdfguides.com/user-manual/LG/CP40NG10__4298752.pdf
https://manuals4owners.com/docs/1183236/owners_manual/JVC_DT-X71HX2.pdf
http://yourpdfguides.com/user-manual/GIGABYTE/GA-Z87X-HD3__5453250.pdf
https://manuals4owners.com/docs/2188022/user_manual/AOC_Q2778VQE.pdf
https://electronicslover20.files.wordpress.com/2016/01/b00s9dxrhi.pdf
http://yourpdfguides.com/user-manual/GIGABYTE/GV-N430-2GI__4023164.pdf
http://yourpdfguides.com/user-manual/TRANSCEND/TS256MCF100I__3802688.pdf
https://manuals4owners.com/docs/130873/owners_manual/Sennheiser_PMX-70.pdf
http://lkchutqav.freeblog.biz/files/2016/03/plantronics-bluetooth-explorer-320-manual.pdf
https://manuals4owners.com/docs/909412/user_manual/Manhattan_460415-webcam.pdf
https://manuals4owners.com/docs/909412/user_guide/Manhattan_460415-webcam.pdf
http://lovezkhvezda.ru/samsung/samsung-hlp5085w-service-manual.pdf
http://yourpdfguides.com/user-manual/COBY/MP-C848__312237.pdf
edit -t
urls.count
URI
cgi = CGI.parse(urls.first)
cgi.path
URI = URI.parse(urls.first)
uri.domain
uri.path
uri = URI.parse(urls.first)
exit
edit -t
uri = URI.parse(urls.first)
uri.path
uri.domain
uri.host
uri = URI.parse(urls.last)
uri.host
domains = Hash.new(0)
domains = []
urls.each do |url|
  uri = URI.parse(url)
  domains << uri.host
end
domains
domains.count
domains.uniq.count
domains.uniq
domains.sort.uniq
edit -t
domains.count
urls.count
wps = domains.select {|url| url.downcase.include?('wordpress') }
wps.count
urls.count
count = 0
urls.each {|url| count += 1 if url.downcase.include?("wordpress") }
count
domains
domains.count
nonwps = domains.reject {|url| url.downcase.include?("wordpress") }
nonwps.count
nonwps
wps.count
count
nonwps.count
count = 0
urls.each {|url| count += 1 if nonwps.any? {|non| url.downcase.include?(non) }
urls.each {|url| count += 1 if nonwps.any? {|non| url.downcase.include?(non) } }
count
urls.count
1626 / 1884.to_f
1884 - 259
yourpdf = 0
urls.each {|url| count += 1 if url.downcase.include?('yourpdf') }
yourpdf
urls.each {|url| count += 1 if url.downcase.include?('yourpdf') }
count
urls.each {|url| yourpdf += 1 if url.downcase.include?('yourpdf') }
yourpdf
nonwps
exit
url = 'http://www.dhgate.com/product/10w-18v-half-flexible-monocrystalline-solar/185474027.html#s1-9-1a;searl|383610021'
s = Scrape.last
source = 1
source = Source.find 1
Category.where(name: 'Solar Panels')
c = _
SourceCategory.where(source: source, category: c)
sc = _
scrape_id, source_category_id, url, retry_time = 1, sc.id, url, 0
scrape_id, source_category_id, url, retry_time = 1, sc.first.id, url, 0
args = _
args
DHGateListingDetailJob
scrape_id, source_category_id, url, retry_time = [1, 406, "http://www.dhgate.com/product/10w-18v-half-flexible-monocrystalline-solar/185474027.html#s1-9-1a;searl|383610021", 0]
exit
DHGateListingDetailJob
exit
DhGateListingDetailJob
require '/Users/jonathan/rvx-rds/app/jobs/dhgate_listing_detail_job.rb'
DhGateListingDetailJob
exit
DhGateListingDetailJob
scrape_id, source_category_id, url, retry_time = [1, 406, "http://www.dhgate.com/product/10w-18v-half-flexible-monocrystalline-solar/185474027.html#s1-9-1a;searl|383610021", 0]
DhGateListingDetailJob.new
DhGateListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
exit
scrape_id, source_category_id, url, retry_time = [1, 406, "http://www.dhgate.com/product/10w-18v-half-flexible-monocrystalline-solar/185474027.html#s1-9-1a;searl|383610021", 0]
s = Scrape.last
s.listings.last
scrape_id, source_category_id, url, retry_time = [1, 406, "http://www.dhgate.com/product/10w-18v-half-flexible-monocrystalline-solar/185474027.html#s1-9-1a;searl|383610021", 0]
DhGateListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
exit
scrape_id, source_category_id, url, retry_time = [1, 406, "http://www.dhgate.com/product/10w-18v-half-flexible-monocrystalline-solar/185474027.html#s1-9-1a;searl|383610021", 0]
DhGateListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l.screenshots
l.screenshots.first
l.screenshots.first.attachment.url
l.destroy
exit
scrape_id, source_category_id, url, retry_time = [1, 406, "http://www.dhgate.com/product/10w-18v-half-flexible-monocrystalline-solar/185474027.html#s1-9-1a;searl|383610021", 0]
DhGateListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
exit
l = Listing.last
scrape_id, source_category_id, url, retry_time = [1, 406, "http://www.dhgate.com/product/10w-18v-half-flexible-monocrystalline-solar/185474027.html#s1-9-1a;searl|383610021", 0]
DhGateListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l.screenshots.last.attachment.url
gst
exit
Listing.import
operator = 'AND'
s = Scrape.last
s.listings.count
s.listings.first
s.listings.sample
operator = 'AND'
terms = { keywords: 'THX' }
per_page = 10
search_response = Listing.search_by(terms, operator, filters, { size: per_page, scroll: '3m' })
filters = {}
search_response = Listing.search_by(terms, operator, filters, { size: per_page, scroll: '3m' })
lids = search_response.results.map { |l| l.id }.compact
total = search_response.results.total
filters[:source_ids] = '1,2,3,4'.split(',').map(&:strip)
search_response = Listing.search_by(terms, operator, filters, { size: per_page, scroll: '3m' })
total = search_response.results.total
filters[:source_ids] = '5,6,7'.split(',').map(&:strip)
search_response = Listing.search_by(terms, operator, filters, { size: per_page, scroll: '3m' })
total = search_response.results.total
l
s.listings.sample
l = _
Scrape.new(source_id: 3)
s = _
s.valid?
s.save
l
l.scrape_listing_sellers
l.update scrape_id: 2
Listing.import
filters[:source_ids] = '3'.split(',').map(&:strip)
search_response = Listing.search_by(terms, operator, filters, { size: per_page, scroll: '3m' })
total = search_response.results.total
page = Nokogiri::HTML(open("http://txt.dhstatics.com/product/specificationInfo.do?itemcode=185474027&supplierid=ff8080813281648001328d07128a29c4"))
page.text
page.text.squish
page.images
image_srcs = page.xpath('//img/@src').map(&:value)
exit
Product.last
Product.first
Product.second
hash = {}
hash << { key: 'value'}
hash.merge { key: 'value'}
hash.merge({ key: 'value'})
hash
hash.merge!({ key: 'value'})
hash
Listing.last
Listing.last.destroy
Listing.last
scrape_id, source_category_id, url, retry_time = [1, 406, "http://www.dhgate.com/product/10w-18v-half-flexible-monocrystalline-solar/185474027.html#s1-9-1a;searl|383610021", 0]
DhGateListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
exit
scrape_id, source_category_id, url, retry_time = [1, 406, "http://www.dhgate.com/product/10w-18v-half-flexible-monocrystalline-solar/185474027.html#s1-9-1a;searl|383610021", 0]
DhGateListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l.data_hash
Listing.last.destroy
exit
scrape_id, source_category_id, url, retry_time = [1, 406, "http://www.dhgate.com/product/10w-18v-half-flexible-monocrystalline-solar/185474027.html#s1-9-1a;searl|383610021", 0]
DhGateListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l.data_hash
l.destroy
exit
info = "Product Name: 10W/18V half-flexible monocrystalline solar panel very thin,light for outdoor Diy,Car,Boat,12V battery and charger"
info.gsub(/\A.+:/, '')
info.match(/\A.+:/, '')
/\A.+:/.match(info)
/\A.+:/.match(info)[0]
/\(<key>\A.+:)/.match(info)
/\(?<key>\A.+:)/.match(info)
/\?<key>\A.+:/.match(info)
/(?<key>\A.+:)/.match(info)
/(?<key>\A.+:)/.match(info)[:key]
item = {text: 'Product Name: 10W/18V half-flexible monocrystalline solar panel very thin,light for outdoor Diy,Car,Boat,12V battery and charger')
item = {text: 'Product Name: 10W/18V half-flexible monocrystalline solar panel very thin,light for outdoor Diy,Car,Boat,12V battery and charger'}
item.text
item = HashWithIndifferentAccess.new
item.text = Product Name: 10W/18V half-flexible monocrystalline solar panel very thin,light for outdoor Diy,Car,Boat,12V battery and charger"
item.text = "Product Name: 10W/18V half-flexible monocrystalline solar panel very thin,light for outdoor Diy,Car,Boat,12V battery and charger"
item[:text] = "Product Name: 10W/18V half-flexible monocrystalline solar panel very thin,light for outdoor Diy,Car,Boat,12V battery and charger"
item
item.text
item
key = /(?<key>\A.+:)/.match(item[:text])[:key].gsub(/:/, '').squish
value = item[:text].gsub(/\A.+:/, '').squish
exit
scrape_id, source_category_id, url, retry_time = [1, 406, "http://www.dhgate.com/product/10w-18v-half-flexible-monocrystalline-solar/185474027.html#s1-9-1a;searl|383610021", 0]
DhGateListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l.data_hash
l.destroy
exit
hash = {}
hash.merge!(nil)
hash.merge!({}{)
hash.merge!({})
exit
scrape_id, source_category_id, url, retry_time = [1, 406, "http://www.dhgate.com/product/10w-18v-half-flexible-monocrystalline-solar/185474027.html#s1-9-1a;searl|383610021", 0]
DhGateListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l.data_hash[:'Item specifics']
l.data_hash.keys
hash
hash = {}
hash
hash[:title] = {}
hash
hash.empty?
hash.values
hash.values.all?(&:blank?)
l
l.destroy
exit
scrape_id, source_category_id, url, retry_time = [1, 406, "http://www.dhgate.com/product/10w-18v-half-flexible-monocrystalline-solar/185474027.html#s1-9-1a;searl|383610021", 0]
DhGateListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l.destroy
exit
scrape_id, source_category_id, url, retry_time = [1, 406, "http://www.dhgate.com/product/10w-18v-half-flexible-monocrystalline-solar/185474027.html#s1-9-1a;searl|383610021", 0]
DhGateListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l = Listing
l = Listing.last
l.destroy
exit
scrape_id, source_category_id, url, retry_time = [1, 406, "http://www.dhgate.com/product/10w-18v-half-flexible-monocrystalline-solar/185474027.html#s1-9-1a;searl|383610021", 0]
DhGateListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l.images
l.reload
l.images
l
exit
l = Listing.last
l.destroy
ProductAttributeValue
ProductAttributeValue.count
l = Listing.last
scrape_id, source_category_id, url, retry_time = [1, 406, "http://www.dhgate.com/product/10w-18v-half-flexible-monocrystalline-solar/185474027.html#s1-9-1a;searl|383610021", 0]
DhGateListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l.data_hash
l.screenshots
l.screenshots.first.attachment.url
url = 'http://www.dhgate.com/product/high-efficiency-sunpower-120w-monocrystalline/231341788.html?recinfo=8,38,3#cppd-3-5|null:38:307860353'
DhGateListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l.screenshots.last
l.screenshots.last.attachment.url
l.id
l.products
url = 'http://www.dhgate.com/store/product/bluedio-t2s-bluetooth-stereo-headphones-wireless/216137579.html#dcp_008newdcp-brandcate-6'
DhGateListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
exit
url = 'http://www.dhgate.com/store/product/bluedio-t2s-bluetooth-stereo-headphones-wireless/216137579.html#dcp_008newdcp-brandcate-6'
scrape_id, source_category_id, url, retry_time = [1, 406, "http://www.dhgate.com/product/10w-18v-half-flexible-monocrystalline-solar/185474027.html#s1-9-1a;searl|383610021", 0]
url = 'http://www.dhgate.com/store/product/bluedio-t2s-bluetooth-stereo-headphones-wireless/216137579.html#dcp_008newdcp-brandcate-6'
DhGateListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
exit
l = Listing.last
scrape_id, source_category_id, url, retry_time = [1, 406, "http://www.dhgate.com/product/10w-18v-half-flexible-monocrystalline-solar/185474027.html#s1-9-1a;searl|383610021", 0]
url = 'http://www.dhgate.com/store/product/bluedio-t2s-bluetooth-stereo-headphones-wireless/216137579.html#dcp_008newdcp-brandcate-6'
DhGateListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l.products
l = Listing.last
l.destroy
exit
scrape_id, source_category_id, url, retry_time = [1, 406, "http://www.dhgate.com/product/10w-18v-half-flexible-monocrystalline-solar/185474027.html#s1-9-1a;searl|383610021", 0]
url = 'http://www.dhgate.com/store/product/bluedio-t2s-bluetooth-stereo-headphones-wireless/216137579.html#dcp_008newdcp-brandcate-6'
DhGateListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l.products
def serialize_models(data, *keys)
  part_numbers = serialize_data(data, *keys)
  return [] if part_numbers.blank?
  (part_numbers || '').split(',').map(&:strip)
end
def serialize_models(data, *keys)
  part_numbers = serialize_data(data, *keys)
  return [] if part_numbers.blank?
  (part_numbers || '').split(',').map(&:strip)
end
@model_key = ['Model']
data = l.data_hash
models = serialize_models data, *@model_key
def serialize_models(data, *keys)
  part_numbers = serialize_data(data, *keys)
  return [] if part_numbers.blank?
  (part_numbers || '').split(',').map(&:strip)
end
def serialize_data(obj, *keys)
  key = keys.map(&:to_sym).find { |k| obj.key?(k) } if obj.respond_to?(:key?)
  if key.present?
    obj[key]
  elsif obj.respond_to?(:each)
    r = nil
    obj.find { |*a| r = serialize_data(a.last, *keys) if a.last }
    r
  end
end
def substring_match(list)
  return if list.empty? || list.any?(&:blank?)
  character_matches = []
  list.min.split('').each_with_index do |char, index|
    break unless list.map { |m| m[index] }.all? { |m| m == char }
    character_matches.push(char)
  end
  character_matches.join.strip if character_matches.any?
end
def add_listing_to_category(listing_id, source_category_id)
  source_category = SourceCategory.find source_category_id
  CategoryListing.find_or_create_by(
    listing_id: listing_id,
    category_id: source_category.category_id,
    source_ca      source_ca      source_ca      source_cade      source_ca      source_ca     ed_      source_ca      source_ca      source_ca  ch    |as      source_ca      source_ca je      source_ca      source_ca      source_ca      source_cade      source_ca tiv      source_ca   Un      source_        source_ca      source_ca at      sourcen       source_ca      source_ca      source_ca      source_cade    _p      s(       m    s)
edit -t
models = serialize_models data, *@model_key
exit
l = Listing.last
l.destroy
Listing.last
url = 'http://www.dhgate.com/store/product/bluedio-t2s-bluetooth-stereo-headphones-wireless/216137579.html#dcp_008newdcp-brandcate-6'
args = [1, 314, "https://www.google.com/aclk?sa=l&ai=Cid1ASJ8iV4TdJo3W-QOnkrToA_Cz9tcF8P77rqwBqOC7jkQICRACIN7Nzx4oFGDJvsmGxKPIF5ABCaABmJGi_QPIAQeqBCZP0HgT83EDFwDHFlTvZObesYZ4xPgjiJ0ZXs9YWYm_ewdcTK4e_MAFBaAGJoAH0O7dApAHAagHpr4b2AcB4BKJ5oX8sOiUxPUB&sig=AOD64_1_zEKY-PC2x7vIwo6a5cxZ_vo_zg&ctype=5&clui=10&q=&ved=0ahUKEwiv8ebXwLLMAhUS72MKHZGrBu04UBDYKQjAAjAB&adurl=http://www.freecleansolar.com/ProductDetails.asp%3FProductCode%3DCS6K-270M", 0]
url
scrape_id, source_category_id, url, retry_time = [1, 314, "http://www.dhgate.com/store/product/bluedio-t2s-bluetooth-stereo-headphones-wireless/216137579.html#dcp_008newdcp-brandcate-6", 0]
scrape_id
DhGateLisitngDetailJob
DhGateListngDetailJob
DhGateListingDetailJob
DhGateListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
exit
scrape_id, source_category_id, url, retry_time = [1, 314, "http://www.dhgate.com/store/product/bluedio-t2s-bluetooth-stereo-headphones-wireless/216137579.html#dcp_008newdcp-brandcate-6", 0]
DhGateListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l.products
url = 'http://www.dhgate.com/store/product/us-stock-wireless-sport-bluetooth-stereo/259452799.html'
DhGateListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
text = "
HBS 700 Wireless Sport Bluetooth Stereo Headset Neckband Earphone Handfree for Cellphone
Customer Label:CT048 CT101--CT104
Feature
Features:
Stereo Bluetooth headset for wireless music plus call functionality
Unique around-the-neck design that's lightweight and comfortable for all-day use
Support connect two cell phones at the same time.
Vibration alert on call
Not Support apt-X technology
Exceptional audio with noise suppression and echo cancellation
Magnetic ear buds snap into place when not in use; music controls on neck strap
High quality guaranteed
Specifications:
Bluetooth Version:2.1+EDR
Type: Stereo
Dimension:167.3*135.6*16.9mm(L*W*T)
Weight: 32g
Talk time: up to 10 hours
Play time: up to 10 hours
Standby time: up to 240 hours
Charging time: less than 2.5 hours
Support volume adjusting : long press the ' + ' & ' - ' key
Support to change the song: shortly press the ' <' & '>' key
Compatible with cellphones such as iPhone, Nokia, HTC, Samsung, LG, Moto, PC, iPad, PSP and so on enabled BluetooCompatible with cellphones such as iPhone, Nokia, HTC, Samsung, LG, Moto, PC, iPad, PSP uCompatible with cellphones such ou place tCompater,CompatibilCompatible with cellphones such as iPhone, Nokia, HTC, Samsung, LGinCompatible with cellphones such as iPhone, Nokia, HTC, Samsung, LG, Moto, PC, ionCompatible with cellphones such as iPhone, NokianaCompatible with cellphs Compatible with cellphones stCompatible with cellphones such as iPhone, Nokia, HTC, SamsunngCompatible with cellphones such as iPhone, Nokia, HTC, Samsung, LG, Moto, PC, ugCompatible with cellphireCompatible witthCompatible with cellphons Compatible  dCompatible with cellphones such asded
1 x Bluet1 x Bluet1 x Bluet1 x Blle1 x Bluet1 x Bluet1 x Blte1 x Bluet1 x Bluet1 x Bluet1
CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCstCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCstCCCCCCCCCCCCCCCCCCCCCC.
edit -t
text.squish
text.strip.gsub(/\s+/, " ")
text
text.strip.gsub(/\s+/, " ")
text
text.squeeze
text.squish
ProductManual.last
pm = _
pm.text
text
text.squeeze(/s+/)
text.squeeze(/s+/.to_s)
/\s+/
/\s+/.to_s
text.squeeze(/\s+/.to_s)
text.squeeze('\n')
text.squeeze(' ')
l = Listing.last
l.destroy
exit
scrape_id, source_category_id, url, retry_time = [1, 314, "http://www.dhgate.com/store/product/bluedio-t2s-bluetooth-stereo-headphones-wireless/216137579.html#dcp_008newdcp-brandcate-6", 0]
Listing.last
Listing.last.destroy
&zwj;
'&zwj;'
str = "&zwj;"
str.encode
str.decode
def after(url)
  io = URI.parse(url).read
  charset = io.scan(/charset="?([^\s"]*)/i).flatten.inject(Hash.new{0}){|a, b|
    a[b]+=1
    a
  }.to_a.sort_by{|a|
    a[1]
  }.reverse.first[0]
  Nokogiri(io, url, charset)
end
after
url
after url
edit -t
after url
edit -t
io = URI.parse(url)
io = URI.parse(url).read
Nokogiri(io)
page = _
page.text
page.text.strip.gsub(/s+/, ' ')
text = page.text.strip.gsub(/s+/, ' ')
text
text.class
text.squish
text.inlclude?('Our strength')
text.include?('Our strength')
text.include?('strength')
text = page.text.strip.gsub(/s+/, ' ')
text
p = Product.last
p.sellers
p = Product.first
p.sellers
p.listings.first
l = _
l.sellers
l.scrape_listing_sellers
seller = l.scrape_listing_sellers.first
seller.min_price
seller.min_price.to_f
seller.min_price.class
seller.min_price.to_i
seller.min_price
Monetize.parce(seller.min_price)
Monetize.parse(seller.min_price)
Monetize.parse(seller.min_price).to_f
Monetize.parse(seller.min_price).to_s
Monetize.parse(seller.min_price).convert(:EUR)
Monetize.parse(seller.min_price).exchange_to(:EUR)
Monetize.parse(seller.min_price).exchange_to(:EUR).to_s
Monetize.parse(seller.min_price).exchange_to(:CNY).to_s
Monetize.parse(seller.min_price).exchange_to(:THB).to_s
Currency.all
l = Listing.last
exit
scrape_id, source_category_id, url, retry_time = [1, 314, "http://www.dhgate.com/store/product/bluedio-t2s-bluetooth-stereo-headphones-wireless/216137579.html#dcp_008newdcp-brandcate-6", 0]
url = "http://www.dhgate.com/product/one-box-lot-of-1-1-4-zig-zag-orange-cigarette/256363270.html#s1-4-1a;srp|513725370"
DhGateListingDetailJob
DhGateListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
exit
scrape_id, source_category_id, url, retry_time = [1, 314, "http://www.dhgate.com/store/product/bluedio-t2s-bluetooth-stereo-headphones-wireless/216137579.html#dcp_008newdcp-brandcate-6", 0]
url = "http://www.dhgate.com/product/one-box-lot-of-1-1-4-zig-zag-orange-cigarette/256363270.html#s1-4-1a;srp|513725370"
DhGateListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l.images.count
l.destroy
Listing.last
exit
scrape_id, source_category_id, url, retry_time = [1, 314, "http://www.dhgate.com/store/product/bluedio-t2s-bluetooth-stereo-headphones-wireless/216137579.html#dcp_008newdcp-brandcate-6", 0]
url = "http://www.dhgate.com/product/one-box-lot-of-1-1-4-zig-zag-orange-cigarette/256363270.html#s1-4-1a;srp|513725370"
DhGateListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l.images
src = 'http://www.dhresource.com/0x0s/f2-albu-g1-M01-C6-25-rBVaGFX-xh-AdqIgAAHS3Ya-Rf8916.jpg/one-box-lot-of-1-1-4-zig-zag-orange-cigarette.jpg'
src = 'http://www.dhresource.com/100x100s/f2-albu-g1-M01-C6-25-rBVaGFX-xh-AdqIgAAHS3Ya-Rf8916.jpg/one-box-lot-of-1-1-4-zig-zag-orange-cigarette.jpg'
src.gsub('100x100', '0x0')
exit
scrape_id, source_category_id, url, retry_time = [1, 314, "http://www.dhgate.com/store/product/bluedio-t2s-bluetooth-stereo-headphones-wireless/216137579.html#dcp_008newdcp-brandcate-6", 0]
url = "http://www.dhgate.com/product/one-box-lot-of-1-1-4-zig-zag-orange-cigarette/256363270.html#s1-4-1a;srp|513725370"
DhGateListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l.images
l.images.count
l.images.pluck(:original_url)
nil == 0
nil == nil
l = Listing.last
l.images
l.images.destroy_all
l.destroy
exit
scrape_id, source_category_id, url, retry_time = [1, 314, "http://www.dhgate.com/store/product/bluedio-t2s-bluetooth-stereo-headphones-wireless/216137579.html#dcp_008newdcp-brandcate-6", 0]
url = "http://www.dhgate.com/product/one-box-lot-of-1-1-4-zig-zag-orange-cigarette/256363270.html#s1-4-1a;srp|513725370"
DhGateListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
exit
scrape_id, source_category_id, url, retry_time = [1, 314, "http://www.dhgate.com/store/product/bluedio-t2s-bluetooth-stereo-headphones-wireless/216137579.html#dcp_008newdcp-brandcate-6", 0]
url = "http://www.dhgate.com/product/one-box-lot-of-1-1-4-zig-zag-orange-cigarette/256363270.html#s1-4-1a;srp|513725370"
DhGateListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l.images
l.images.count
l.images.pluck(:original_url)
exit
scrape_id, source_category_id, url, retry_time = [1, 314, "http://www.dhgate.com/store/product/bluedio-t2s-bluetooth-stereo-headphones-wireless/216137579.html#dcp_008newdcp-brandcate-6", 0]
url = "http://www.dhgate.com/product/one-box-lot-of-1-1-4-zig-zag-orange-cigarette/256363270.html#s1-4-1a;srp|513725370"
DhGateListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l.images.count
l.images.pluck(:original_url
l.images.pluck(:original_url)
scrape_id, source_category_id, url, retry_time = [1, 314, "http://www.dhgate.com/store/product/bluedio-t2s-bluetooth-stereo-headphones-wireless/216137579.html#dcp_008newdcp-brandcate-6", 0]
DhGateListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
url
Listing.where(url: "http://www.dhgate.com/store/product/bluedio-t2s-bluetooth-stereo-headphones-wireless/216137579.html")
Listing.where(url: "https://www.dhgate.com/store/product/bluedio-t2s-bluetooth-stereo-headphones-wireless/216137579.html")
exit
l = Listing.last
l.destroy
scrape_id, source_category_id, url, retry_time = [1, 314, "http://www.dhgate.com/store/product/bluedio-t2s-bluetooth-stereo-headphones-wireless/216137579.html#dcp_008newdcp-brandcate-6", 0]
url = "http://www.dhgate.com/product/one-box-lot-of-1-1-4-zig-zag-orange-cigarette/256363270.html#s1-4-1a;srp|513725370"
DhGateListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l.images.count
l.images.pluck(:original_url)
exit
l = Listing.last
l.images
l.images.map {|img| img.attachment.url}
l.reload
l.images.map {|img| img.attachment.url}
exit
scrape_id, source_category_id, url, retry_time = [1, 314, "http://www.dhgate.com/store/product/bluedio-t2s-bluetooth-stereo-headphones-wireless/216137579.html#dcp_008newdcp-brandcate-6", 0]
url = 'http://www.dhgate.com/product/colorful-new-design-silicone-band-with-connector/375087859.html'
DhGateListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l.products
edit -t
@brand_key = ['Brand', 'Brand Name', 'Chinese Brand']
name = l.name
brand = get_brand_name(l.data_hash, name)
edit -t
brand = get_brand_name(l.data_hash, name)
@brand_key = ['Brand', 'Brand Name']
brand = get_brand_name(l.data_hash, name)
Money
Money.new('THB')
money = 1.to_money(:USD)
money = 1.to_money(:THB)
moeny.currency
money.currency
money.currency.symbol
money = 1.to_money(:CNY)
money.currency.symbol
money = 1.to_money(:JPY)
money.currency.symbol
Currency
ScrapeListingSeller
money = 1.to_money(:JPY)
money.currency.symbol
money = 1.to_money(:jpy)
money.currency.symbol
money = 1.to_money(:jPy)
money.currency.symbol
exit
Currency.all
usd = Currency.first
usd.code = 'USD'
usd.save
exit
usd = Currency.first
usd.update code: "USD"
ScrapeListingSeller.where(currency_id: usd.id).count
edit -t
codes.each do |code|
  1.to_money(code.to_sym)
end
codes.each do |code|
  1.to_money(code.to_sym)
codes.each do |code|
  m = 1.to_money(code.to_sym)
  m.currency.symbol
codes.each do |code|
  m = 1.to_money(code.to_sym)
  symbol = m.currency.symbol
  puts symbol
end
edit -t
codes.each_with_index do |code, idx|
  exit
usd = Currency.last
usd.update: name: 'US Dollar', code: 'USD'
usd.update name: 'US Dollar', code: 'USD'
usd.reload
edit -t
codes.each do |code|
  m = 1.to_money(code.to_sym)
  symbol = m.currency.symbol
  puts symbol
end
edit -t
codes
codes.count
names.count
edit -t
names.count
codes.count
edit -t
names.count
codes.count
edit -t
create_currencies codes, names
edit -t
create_currencies codes, names
Currency.count
Currency.first
Currency.where(symbol: '$')
Currency.where(code: "EUR")
Currency.where(code: "EUR").first.symbol
sym = _
Currency.where(symbol: sym)
Currency.where(code: "THB").first.symbol
sym = _
Currency.where(symbol: sym)
Currency.where(code: "CNY").first.symbol
sym = _
Currency.where(symbol: sym)
File.join(Rails.root, 'db', 'seeds')
File.join(Rails.root, 'db', 'seeds', '*.rb')
Dir[File.join(Rails.root, 'db', 'seeds', '*.rb')]
Dir[File.join(Rails.root, 'db', 'seeds']
Dir[File.join(Rails.root, 'db', 'seeds')]
file_names.map do |file|
  Dir[File.join(Rails.root, 'db', 'seeds', "#{file}.rb")]
end
file_names = %w(sources, categories)
file_names.map do |file|
  Dir[File.join(Rails.root, 'db', 'seeds', "#{file}.rb")]
end
file_names = %w(sources categories)
file_names.map do |file|
  Dir[File.join(Rails.root, 'db', 'seeds', "#{file}.rb")]
end
file_names.map do |file|
  File.join(Rails.root, 'db', 'seeds', "#{file}.rb")
end
file_names.map do |file|
  load File.join(Rails.root, 'db', 'seeds', "#{file}.rb")
end
Categories.count
Categoru.count
Category.count
file_names.map do |file|
  load File.join(Rails.root, 'db', 'seeds', "#{file}.rb")
end
edit -t
names.count
codes.count
edit -t
codes.count
names.count
currencies = {}
codes.each_with_index do |code|
codes.each_with_index do |code, i|
  currencies[code.to_sym] = names[i]
end
currencies
currencies.first
currencies.first.key
currencies.each {|k, v| puts k}
edit -t
1.to_money("THB")
Currency.last
edit -t
curr_map
edit -t
curr_map
1.to_money(:TRY)
1.to_money(:TRY).currency.symbol
curr_map[:TRY]
p _
curr_map[:TRY][:symbol]
p curr_map[:TRY][:symbol]
inspect curr_map[:TRY][:symbol]
curr_map.inspect
curr_map
inspect curr_map[:TRY][:symbol]
p curr_map[:TRY][:symbol]
curr_map[:RUB]
curr_map[:INR]
curr_map[:AZN]
Currency[0]
Currency.first
Currency.find(1..Currency.last.id)
Currency.find((1..Currency.last.id).to_a)
Currency.find((1..Currency.last.id).to_a).map(&:destroy
Currency.find((1..Currency.last.id).to_a).map(&:destroy)
Currency.all
edit -t
Currency.create(attrs)
curr_map[:USD]
curr_map.first
curr_map.each {|k, v| puts k}
curr_map.sort.each {|k, v| puts k}
1.to_money(:NTD)
1.to_money(:NT$)
1.to_money(:TWD)
money = 1.to_money(:TWD)
money.currency.symbol
money.currency.symbols
Currency.all
usd
ScrapeListingSeller.all
ScrapeListingSeller.all.pluck(:currency_id
ScrapeListingSeller.all.pluck(:currency_id)
ScrapeListingSeller.all.update_all(currency_id: 2)
Currency.destroy_all
ScrapeListingSeller.all.pluck(:currency_id)
codes = %w(GBP USD EUR CNY JPY HKD THB AUD BGN INR AFN ALL DZD AOA XCD ARS AMD AWG AZN BSD BHD BDT BBD BYR BZD XOF BMD BTN BOB BAM BWP NOK BRL BND BIF CVE KHR XAF CAD KYD CLP CLF COP KMF CDF NZD CRC HRK CUP CUC ANG CZK DKK DJF DOP EGP SVC ERN ETB FKP FJD XPF GMD GEL GHS GIP GTQ GNF GYD HTG HNL HUF ISK IDR XDR IRR IQD ILS JMD JOD KZT KES KPW KRW KWD KGS LAK LBP LSL ZAR LRD LYD CHF MOP MKD MGA MWK MYR MVR MRO MUR MXN MDL MNT MAD MZN MMK NAD NPR NIO NGN OMR PKR PAB PGK PYG PEN PHP PLN QAR RON RUB RWF SHP WST STD SAR RSD SCR SLL SGD SBD SOS SSP LKR SDG SRD SZL SEK SYP TWD TJS TZS TOP TTD TND TRY TMT UGX UAH AED UYU UZS VUV VEF VND YER ZMW ZWL XAU XAG)
codes.count
edit -t
create_currencies codes
Currency.all
Currency.destroy_all
edit -t
create_currencies codes
Currency.all
Currency.first.destroy
Currency.all
edit -t
create_currencies codes
Currency.count
Currency.first
edit -t
Currecy.delete_all
Currency.delete_all
create_currencies codes
Currency.first
codes.first
edit -t
Currency.destroy_all
Currency.first
codes.first
create_currencies codes
Currency.first
currency = 1.to_money(codes.first).currency
edit -t
Currency.destroy_all
create_currencies codes
codes.count
codes.uniq.count
Currency.all
currencies = {}
Currency.all.each do |curr|
currencies = []
Currency.all.each do |curr|
curr = Currency.first
curr.attributes
curr.attributes.slice(:id)
curr.slice(:id)
curr.slice(:id, code, symbol, name)
curr.slice(:id, :code, :symbol, :name)
Currency.all.each do |curr|
  curr.slice(:id, :code, :symbol, :name)
Currency.all.each do |curr|
  currencies << curr.slice(:id, :code, :symbol, :name)
end
currencies[6]
puts currencies
edit -t
monies = []
currencies.each_with_index do |curr, i|
  curr['id'] = i + 1
  monies << curr
end
puts monies
p monies
edit -t
currencies.map!(&:symbolize_keys)
currencies
p currencies
codes
p codes
edit -t
Currency.destroy_all
edit -t
create_currencies codes
Currency.all.count
currencies = []
Currency.all.each do |curr|
  curr.slice(:id, :code, :symbol, :name).symbolize_keys
Currency.all.each do |curr|
  currencies <<  curr.slice(:id, :code, :symbol, :name).symbolize_keys
end
p currencies
1.to_money(:XDR)
1.to_money(:XDR).symbol
1.to_money(:XDR).currency
1.to_money(:XDR).currency.code
1.to_money(:XDR).currency.symbol
1.to_money(:XDR).currency.iso_code
1.to_money(:XAG).currency.iso_code
1.to_money(:XAG).currency.symbol
3 ** 3
3 ** 3 ** 3
26 * 26 * 26
Currency.all
Currency.destroy_all
currencies
currencies.map {|curr| curr[:id]}
codes.count
codes.uniq.count
Currency.all
exit
Currency.all
Currency.first
exit
Currency.all
Currency.first
exit
Currency.all
Currency.destroy_all
Currency.all
exit
Currency.first
Currency
exit
currency = { :id => 161, :iso_code => 'XAG', :symbol => 'oz t', :name => 'Silver (Troy Ounce)' }
currency.except(:id)
exit
Currency.all
Currency.count
Currency.destroy_all
Currency.all
exit
Currency.all
xit
exit
Currency.all
Currency.count
Currency.first
Currency.last
Currency.all.sample
prices = ['100.00', 50, 150]
prices.map(&:to_f)
prices.map(&:to_i)
prices.map(&:to_f)
prices.min_max
prices.min\max
prices.minmax
prices.map(&:to_f).minmax
prices = ['100.00', 50, 150, nil]
prices.map(&:to_f).minmax
prices.compact.map(&:to_f).minmax
prices.reject(&:blank?).map(&:to_f).minmax
prices = [nil, nil]
prices.reject(&:blank?).map(&:to_f).minmax
min, max = prices.reject(&:blank?).map(&:to_f).minmax
min
max
prices = [nil, 0]
min, max = prices.reject(&:blank?).map(&:to_f).minmax
min, max = [0, 0] if min.nil?
min
max
prices = [nil, nil]
min, max = prices.reject(&:blank?).map(&:to_f).minmax
min, max = [0, 0] if min.nil?
min
nil.to_f
Currency.all
Currency.destroy_all
Currency.all
url = 'https://www.google.co.uk/shopping/product/297554711907358330?q=Microsoft+Xbox+One&prds=hsec:related,paur:ClkAsKraX-Hzz7ntyF-xjxhSGiIAm1LMxFJZkC7aObXKSHCr285y7Q68hemdUSOOlet4m1H9MgQaY2MDHcC7zNHGpx1GOukj1Z33_GxQ-HGNihfzY8AT0AhNmRIZAFPVH712z23Kn4PDsFIMBV2vuyVW5OtaFA&ved=0ahUKEwiRwMTcx83MAhVCxmMKHZxOCkgQ4SsICygA&ei=is4wV5HTH8KMjwOcnanABA'
uri = URI.parse(url)
uri.domain
uri.host
Currency.find_by(iso_code: 'USD')
Currency.find_by(iso_code: 'usd')
Currency.find_by(symbol: '$')
Currency.first
url2 = 'https://www.google.de/?source=pshome-c-0-1&sa=X&ved=0ahUKEwj4-6Tdys3MAhUG9mMKHUgpCTIQ7j8ICg#tbm=shop&q=microsoft+xbox+one&spd=4394803134247564021'
uri2 = URI.parse url
uri2.host
uri2.host.split('.').last
Currency.find_by(iso_code: nil)
exit
url2 = 'https://www.google.de/?source=pshome-c-0-1&sa=X&ved=0ahUKEwj4-6Tdys3MAhUG9mMKHUgpCTIQ7j8ICg#tbm=shop&q=microsoft+xbox+one&spd=4394803134247564021'
iso_code = SellersJob::CURRENCIES[URI.parse(url2).host.split('.').last]
require '/Users/jonathan/rvx-rds/lib/scraping/jobs/sellers_job.rb'
iso_code = SellersJob::CURRENCIES[URI.parse(url2).host.split('.').last]
GoogleProductDetailJob
url = url2
url = 'https://www.google.de/shopping/product/6549358315327648981?q=microsoft+xbox+one&prds=hsec:specs&ved=0ahUKEwii8ougzc3MAhVI-mMKHRd-D9oQ4SsIFSgC&ei=VtQwV6LvFsj0jwOX_L3QDQ'
url = 'https://www.google.de/shopping/product/6549358315327648981/specs
url = 'https://www.google.de/shopping/product/6549358315327648981/specs'
SourceCategory.find_by(source_id: 1).first
SourceCategory.find_by(source_id: 1)
source_category_id = _.id
scrape_id = 1
url = url
retry_time = 0
GoogleProductDetailJob.new.perform(scrape_id, source_category_id, url, 0)
reload!
GoogleProductDetailJob.new.perform(scrape_id, source_category_id, url, 0)
exit
url = 'https://www.google.de/shopping/product/6549358315327648981/specs'
GoogleProductDetailJob.new.perform(1, 1, url, 0)
Listing.last
GoogleProductDetailJob.new.perform(1, 1, url, 0)
url = 'https://www.google.de/shopping/product/6549358315327648981'
GoogleProductDetailJob.new.perform(1, 1, url, 0)
Listing.last
Listing.last.destroy
Listing.last
exit
url = 'https://www.google.de/shopping/product/6549358315327648981'
GoogleProductDetailJob.new.perform(1, 1, url, 0)
Listing.last
l = )
l = Listing.last
l.seller
l.sellers
Listing.last
Listing.last.destroy
exit
url = 'https://www.google.de/shopping/product/6549358315327648981'
GoogleProductDetailJob.new.perform(1, 1, url, 0)
Listing.last
Listing.last.destroy
GoogleProductDetailJob.new.perform(1, 1, url, 0)
l = Listing.last
l.scrape_listing_sellers
l.scrape_listing_sellers.firt
l.scrape_listing_sellers.first
sls l.scrape_listing_sellers.first
sls =  l.scrape_listing_sellers.first
sls.currency
exit
0 == 0.0
nil.to_f
[nil, nil].map(&:to_f)
[nil, nil, 1].reject(:blank?).map(&:to_f)
[nil, nil, 1].reject(:blank?).minmax.map(&:to_f)
[nil, nil, 1].reject(&:blank?).minmax.map(&:to_f)
[nil, nil, ].reject(&:blank?).minmax.map(&:to_f)
0.zero?
0.0.zero?
nil.zero?
Currency.destroy_all
SellersJob::CODE_MAP[URI.parse(url).host.split('.').last]
require 'sellers_job'
require 'sellers_job.rb'
require '/Users/jonathan/rvx-rds/lib/scraping/jobs/sellers_job.rb'
SellersJob::CODE_MAP[URI.parse(url).host.split('.').last]
url = 'www.alibaba.com'
SellersJob::CODE_MAP[URI.parse(url).host.split('.').last]
url
SellersJob::CODE_MAP[URI.parse(url).host.split('.').last]
URI.parse(url).host
url = 'http://www.aliabab.com'
SellersJob::CODE_MAP[URI.parse(url).host.split('.').last]
url = 'http://www.alibaba.com/product-detail/42-outdoor-digital-interactive-advertisement-poster_490842349.html?spm=a2700.7724838.0.0.h3RLFq'
AlibabaProductDetailJob.new.perform(1,1, url, 0)
Listing.last
AlibabaProductDetailJob.new.perform(1,1, url, 0)
Listing.last
l = _
l.sellers
l.scrape_listing_seller
l.scrape_listing_sellers
l.scrape_listing_sellers.min_price
l.scrape_listing_sellers.first.min_price
l.scrape_listing_sellers.first.min_price.to_f
l.url
l.scrape_listing_sellers
l.scrape_listing_sellers.destroy_all
l.destroy
min_price = '1,000'
min_price.to_f
min_price.gsub(/,/, '').to_f
exit
url = 'http://www.alibaba.com/product-detail/42-outdoor-digital-interactive-advertisement-poster_490842349.html?spm=a2700.7724838.0.0.h3RLFq'
AlibabaProductDetailJob.new.perform(1,1, url, 0)
Listing.last
url = 'http://www.alibaba.com/product-detail/42-outdoor-digital-interactive-advertisement-poster_490842349.html?spm=a2700.7724838.0.0.h3RLFq'
AlibabaProductDetailJob.new.perform(1, 1, url, 0)
l = Listing.last
l.scrape_listing_sellers
l.scrape_listing_sellers.first.min_price.to_f
price = 'USD 1,000.50'
Monetize.parse(price)
Monetize.parse(price).to_f
currency = Currency.first
price = '199,90 €'
Monetize.parse(price).to_f
Monetize.parse(price).currency
price = 'EUR 199,90'
Monetize.parse(price).to_f
Monetize.parse(price).currency
Monetize.parse(price).convert_to(:USD)
Monetize.parse(price).exchange_to(:USD)
Monetize.parse(price).exchange_to(:USD).to_f
price = 'EUR199,90'
Monetize.parse(price).exchange_to(:USD).to_f
Monetize.parse(price).to_f
url = 'https://www.google.fr/shopping/product/2138502497898391683'
price = 'EUR199,90'
Monetize.parse(price).to_f
price = '#{nil}199,90'
price = "#{nil}199,90"
price = "#{nil}#{nil}"
Monetize.parse(price).to_f
min = Monetize.parse("#{currency.iso_code}#{price[:min]}") if price[:min].present?
price = { min: nil, max: nil }
Currency = Currency.first
min = Monetize.parse("#{currency.iso_code}#{price[:min]}") if price[:min].present?
min
max = Monetize.parse("#{currency.iso_code}#{price[:max]}") if price[:max].present?
exit
url = "https://www.google.fr/shopping/product/2138502497898391683"
GoogleProductDetailJob.new.perform(1, 1, url, 0)
l = Listing.last
l.sellers
l.scrape_listing_sellers.last.min_price
l.scrape_listing_sellers.last.min_price.to_f
l.scrape_listing_sellers.last.min_price.currency
sls = l.scrape_listing_sellers.last
sls.min_price
sls.min_price.to_s
sls.currency
sls.currency.iso_code
min = Monetize.parse("#{sls.currency.iso_code}2 049,46")
min.to_f
min.to_s
sls.min_price
sls.min_price.to_f
min
sls.min_price = min
sls.save
sls.min_price
sls.min_price.to_f
sls.max_price.to_f
l = Listing.last
l.destroy
exi
exit
l = Listing.last
url = "https://www.google.fr/shopping/product/2138502497898391683"
GoogleProductDetailJob.new.perform(1, 1, url, 0)
l = Listing.last
sls = l.scrape_listing_sellers.last
sls.min_price.to)f
sls.min_price.to_f
min
price_text = '2 049,46 €'
price_text.delete('^0-9.')
min= price_text.delete('^0-9.,'),
min= price_text.delete('^0-9.,')
Monetize.parse(price_text)
Monetize.parse(price_text).to_f
Monetize.parse('2,133,00).to_f
Monetize.parse('2,133,00').to_f
Monetize.parse('2133,00').to_f
Monetize.parse('2.133,00').to_f
'^0-9.,'.to_r
'^0-9.,'.to_regex
Regex.new('^0-9.,')
Regexp.new('^0-9.,')
price_text = '2 049,46 €'
min= price_text.delete('^0-9.,')
l = Listing.last
l.destroy
exit
url = 'https://www.google.fr/shopping/product/2138502497898391683'
GoogleProductDetailJob.new.perform(1, 1, url, 0)
l = Listing.last
sls = l.scrape_listing_sellers.last
sls.min_price
sls.min_price.to_f
sls.currency
sls.price
"#{sls.currency.iso_code} #{sls.min_price}"
Monetize.parse("#{sls.currency.iso_code} #{sls.min_price}")
Monetize.parse("#{sls.currency.iso_code} #{sls.min_price}").to_s
mon = Monetize.parse("#{sls.currency.iso_code} #{sls.min_price}")
mon.currency_as_string
mon.pretty_print
mon.default_formatting_rules
mon.ban
mon.bank
mon.format
mon.format_with_settings
mon.separator
MoneyRails
MoneyRails::ActionViewExtension
MoneyRails::ActionViewExtension.humanized_money_with_symbol(mon)
MoneyRails::ActionViewExtension.send(:humanized_money_with_symbol, mon)
exit
sls = ScrapeListingSeller.last
sls.humanized_money_with_symbol
mon = Monetize.parse("#{sls.currency.iso_code}#{sls.min_price}")
sls.humanized_money_with_symbol mon
url = 'https://www.google.com/shopping/product/2593068974399024491'
GoogleProductDetailJob.new.perform(1, 1, url, 0)
l = Listing.last
l.scrape_listing_sellers
sls = _.first
sls.min_price == sls.max_price
sls.min_price.to_f
sls.currency.iso_code
"#{sls.currency.iso_code} #{sls.min_price}"
"#{sls.currency.iso_code} #{sls.min_price}.round(2)"
"#{sls.currency.iso_code} #{sls.min_price.round(2)}"
"#{sls.currency.iso_code} #{sls.min_price.format(2)}"
"#{sls.currency.iso_code} #{sls.min_price}"
Monetize.parse("#{sls.currency.iso_code} #{sls.min_price}")
Monetize.parse("#{sls.currency.iso_code} #{sls.min_price}").format
Monetize.parse("#{sls.currency.iso_code}#{sls.min_price}").format
sls
ScrapeListingSeller.where(currency_id: nil)
ScrapeListingSeller.pluck(:currency_id)
ScrapeListingSeller.pluck(:currency_id).uniq
Monetize.parse("#{sls.currency.iso_code} #{sls.min_price}")
Monetize.parse('')
Monetize.parse('').to_f
min = Monetize.parse("#{sls.currency.iso_code} #{sls.min_price}")
min.to_f
min.format
BigDecimal.new(min)
BigDecimal.new(min.to_s)
url = 'http://www.alibaba.com/product-detail/bulk-4gb-usb-flash-drives-2015_60343222515.html?spm=a2700.7743248.51.1.lqcKrp'
AlibabaProductDetailJob.new.perform(1, 1, url, 0)
exit
url = 'http://www.alibaba.com/product-detail/bulk-4gb-usb-flash-drives-2015_60343222515.html?spm=a2700.7743248.51.1.lqcKrp'
AlibabaProductDetailJob.new.perform(1, 1, url, 0)
l = Listing.last
l
l.scrape_listing_sellers
sls = _.first
sls.min_price
sls.max_price
min = '20 000,50'
min.to_f
min.gsub(/\s+/, '')
min.gsub(/\s+/, '').to_f
min.gsub(/\s+/, '').gsub(/,/, '')
min.gsub(/\s+/, '').gsub(/,\d\d?/, '')
min.gsub(/\s+/, '').gsub(/,(?=\d\d?)/, '.')
min.gsub(/\s+/, '').gsub(/,(?=\d\d)/, '.')
min.gsub(/\s+/, '').gsub(/,(?=!\d\d?)/, '.')
min.gsub(/\s+/, '').gsub(/,(?=\d\d?)/, '.')
min.gsub(/\s+/, '').gsub(/,(?=\d\d?)/, '.').to_f
BigNumber.new(1.0)
BigDecimal
BigDecimal.new(1.0)
BigDecimal.new('1.0')
num = _
num.to_i
num.to_f
num.to_digits
num.to_d
num.to_d.to_digits
min = Monetize.parse('24 123,40')
min.to_f
min = Monetize.parse('EUR24 123,40')
min.to_f
min.exchange_to(:THB)
min.exchange_to(:THB).to_f
min.exchange_to(:THB).to_digits
min.exchange_to(:THB).format
min = Monetize.parse('EUR24 123,40')
min.delimiter
exit
Currency.count
Currency.count.all
Currency.all
Currency.destroy_all
load '/Users/jonathan/rvx-rds/db/seeds/currencies.rb'
Currency.all
Currency.find_by(iso_code: 'JPY')
Currency.all
Currency.find(5)
Currency.all.map(&:name)
Currency.all
text = '( US $59.90 - 60.42 / Piece )'
text.gsub(/[()]/, '')
text.gsub(/[()\d]/, '')
text.gsub(/[()^\d]/, '')
text.gsub(/[()]/, '')
text.gsub(/[()]/, '').strip
text.gsub(/[()]/, '').strip.split(' ')
text = '59.90  - 60.42 '
text.split('-')
text.split('-').map(&:strip
text.split('-').map(&:strip)
name = "Bob's Technology Co.,Ltd Add To Favorites ( 1047 )"
name.gsub(/Add.+$/, '')
name.gsub(/Add\s To Favorites.+$/i, '')
name.gsub(/Add\sTo\sFavorites.+$/i, '')
name = "Bob's Technology Co.,Ltd add to favorites ( 1047 )"
name.gsub(/Add\sTo\sFavorites.+$/i, '')
exit
url = 'http://www.dhgate.com/store/product/s7-sm-g930-s7-edge-shown-64bit-mtk6592-octa/374533375.html'
DhGateListingDetailJob
DhGateListingDetailJob.new.perform(1, 1, url, 0)
l = Listing.last
l.scrape_listing_sellers
sls = _.first
sls.min_price
sls.min_price.to_f
sls.max_price.to_f
sls.seller
sls
sls.destroy
l
l.url
l.destroy
url = 'http://www.dhgate.com/store/product/s7-sm-g930-s7-edge-shown-64bit-mtk6592-octa/374533375.html'
exit
url = 'http://www.dhgate.com/store/product/s7-sm-g930-s7-edge-shown-64bit-mtk6592-octa/374533375.html'
DhGateListingDetailJob.new.peform(1, 1, url, 0)
DhGateListingDetailJob.new.perform(1, 1, url, 0)
l = Listing.last
l.url
DhGateListingDetailJob.new.perform(1, 1, url, 0)
l = Listing.last
l.url
l.images.count
sls = l.scrape_listing_sellers.first
sls.seller_url
sls.seller
sls
sls.destroy
l.destroy
exit
url = 'http://www.dhgate.com/store/product/s7-sm-g930-s7-edge-shown-64bit-mtk6592-octa/374533375.html'
DhGateListingDetailJob.new.perform(1, 1, url, 0)
l = Listing.last
l.url
sls = l.scrape_listing_sellers.first
l.destroy
exit
url = 'http://www.dhgate.com/store/product/s7-sm-g930-s7-edge-shown-64bit-mtk6592-octa/374533375.html'
DhGateListingDetailJob.new.perform(1, 1, url, 0)
l = Listing.last
sls = l.scrape_listing_sellers.first
sls.seller
sls.seller.name
sls.seller.name.strip
l.created_at
name = sls.seller.name.strip
Seller.find_by(name: name)
s = _
s.name = name
s
s.save
l
l.images.size
url = 'http://www.dhgate.com/product/2016-big-touch-screen-6-0-quot-android-4/376133806.html#lp_dailyDeals-www-376133806'
DhGateListingDetailJob.new.perform(1, 1, url, 0)
l = Listing.last
l.url
url = ListingUrl.last
DhGateListingDetailJob.new.perform(1, 1, url, 0)
url = ListingUrl.last.url
DhGateListingDetailJob.new.perform(1, 1, url, 0)
exit
url = 'http://www.dhgate.com/product/2016-big-touch-screen-6-0-quot-android-4/376133806.html#lp_dailyDeals-www-376133806'
DhGateListingDetailJob.new.perform(1, 1, url, 0)
l = Listing.last
l.url
l
sls = l.scrape_listing_sellers.first
sls.seller.name
sls.seller_url
url = 'http://www.dhgate.com/product/20-pieces-9-5-quot-mini-tft-lcd-color-analog/380703761.html'
Listing.find_by(url: url)
DhGateListingDetailJob.new.perform(1, 1, url, 0)
url = 'http://www.dhgate.com/product/20-pieces-9-5-quot-mini-tft-lcd-color-analog/380703761.html'
DhGateListingDetailJob.new.perform(1, 1, url, 0)
exit
url = 'http://www.dhgate.com/product/20-pieces-9-5-quot-mini-tft-lcd-color-analog/380703761.html'
DhGateListingDetailJob.new.perform(1, 1, url, 0)
Listing.last
Listing.last.destroy
exit
url = 'http://www.dhgate.com/product/20-pieces-9-5-quot-mini-tft-lcd-color-analog/380703761.html'
DhGateListingDetailJob.new.perform(1, 1, url, 0)
l = Listing.last
l.images.count
sls = l.scrape_listing_sellers.first
sls.seller.name
sls.min_price
sls.min_price.to_f
sls.max_price
sls.max_price.to_F
sls.max_price.to_f
Monetize.parse(
exit
url = 'http://www.dhgate.com/product/high-birght-5m-5050-led-strips-light-warm/245601440.html#se1-0-1b;price|2283034767'
DhGateListingDetailJob.new.perform(1, 1, url, 0)
l = Listing.last
l.images.count
sls = l.scrape_listing_sellers.first
sls.min_price
sls.seller
sls.seller_url
exit
url = 'http://www.dhgate.com/product/4ch-8ch-16-channel-h-264-network-full-d1/249067957.html#s1-0-1b;searl|208711808'
url.gsub(/(?<=html)/, '')
url.gsub(/(?<=foo)/, '')
url
url.gsub(/(?<=html)/, '')
url.gsub(/(?<=html).+$/, '')
url = 'http://www.dhgate.com/wholesale/Digital+Video+Recorder/s103013001-3.html?leftpars=c2hpcGNvbXBhbmllcz1zNG8tc2o5LWRobC1zZDQtc2JpLXVwcy10bnQtc2FvZGhnYXRl'
uri = URI.parse(url)
scheme, host, path = uri.scheme, uri.host, uri.path
"#{scheme}://#{host}#{path}"
SourceCategory.last
Category.find_by name: 'Digital Video Recorder'
Category.find_by name: 'Digital Video Recorders'
c = _
Source.last
atters =   {
  id: 5,
  name: 'dhgate',
  pretty_name: 'DH Gate',
  domain: 'http://www.dhgate.com',
  job_name: 'DhGateProductListingJob',
  stack_id: '12345',
  source_type_id: 1
}
Source.create(attrs)
Source.create atters
s = _
c
SourceCategory.create(source_id: s.id, category_id: c.id, url: 'http://www.dhgate.com/wholesale/Digital+Video+Recorder/s103013001.html')
SourceCategory.first
s.categories
Scrape.first
Scrape.first.update running: false
l = Listing.find 64
l
exit
s = Scrape.last
s.destroy
exit
s = Scrape.last
s.products
s.destroy
s.listing
s.listings
s.listings.destroy_all
s.destroy
s = Scrape.last
s.listings.count
s.products.count
s.listings.last
l = _
s.listings.where(data_hash: "'%dvr%'").to_sql
s.listings.where(data_hash: "'%dvr%'")
s.listings.where("data_hash like '%dvr%'")
.count
s.listings.where("data_hash like '%dvr%'").count
s.products.count
s.listings.last
l = _
l.identifiers
s = Scrape.last
s.products.count
s.listings.count
l = s.listings.last
l
l.images.count
s.reload
s.products.count
s.listings.count
SourceCategory.where(source_id: 5)
s.listings.count
l = s.listing.last
l = s.listings.last
l.categories
s.reload
UpdateScrapeCounterJob.new.perform(s.id)
s.reload
scrape_id = s.id
edit -t
s.reload
s.listings.count
l = s.listings.first
s.images.map {|img| img.attachment.url }
l.images.map {|img| img.attachment.url }
s.
s
s.duration
Time.now - s.created_at
DateTime.now - s.created_at
DateTime.now
s.created_at
DateTime.now - s.created_at
s.created_at.class
DateTime.new(s.created_at)
DateTime.parse(s.created_at)
DateTime.parse(s.created_at.to_s)
DateTime.now - _
DateTime.now.to_i - s.created_at.to_i
3706 / 60
s.created_at
s.running = false
s.save
s.listings.first
l = _
l.original_url
l.url
l.products
s.products
s.listings.where("data_hash like '%Model%'")
s.listings.where("data_hash like '%Model%'").size
s.listings.where("data_hash like '%Model%'").first
s.listings.where("data_hash like '%Brand%'").first
s.listings.where("data_hash like '%:Brand%'").first
s.listings.where("data_hash like '%:Model%'").first
l = _
l.scrape_listing_sellers
s.sellers
s.listings.map {|l| l.sellers.blank?}
listings = s.listings.select {|l| l.sellers.blank?}
listings = s.listings.map {|l| l.sellers.flat_map(&:name)}
listings.count
s.scrape_listing_sellers
s.scrape_listing_sellers.pluck(:seller_url)
s.scrape_listing_sellers.select{|sls| sls.seller_url.nil?}
ids = _.map(&:id)
sls = ScrapeListingSeller.find(ids.first)
sls.listing
sls.listing.url
exit
s = Scrape.last
s.listings.count
l = s.listings.last
l.listings_matching_url
l.listings_matching_url.count
s.listings.count
s = Scrape.last(2).first
l = s.listings.first
l.images
l_ids = s.listings.ids
img_ids = ListingAsset.where(listing_id: ids).pluck(:asset_id)
img_ids = ListingAsset.where(listing_id: lids).pluck(:asset_id)
img_ids = ListingAsset.where(listing_id: l_ids).pluck(:asset_id)
Asset.where(id: img_ids).count
Asset.find(img_ids).last
img = _
img.attachment
Asset.find(img_ids).last(10)
Asset.find(img_ids).map {|img| img.id unless img
Asset.find(img_ids).last(10).attachment
Asset.find(img_ids).last(10).first.attachment
Asset.find(img_ids).last(10).first.attachment.file_size
Asset.find(img_ids).last(10).first.attachment.attachment_file_size
Asset.find(img_ids).last(10).first.attachment
Asset.find(img_ids).last(10).first.attachment.methods
Asset.find(img_ids).last(10).first.attachment.updated_at
missing_ids = Asset.find(img_ids).map {|img| img.id if img.attachment.updated_at.nil? }
missing_ids
missing_ids = Asset.find(img_ids).select {|img| img.attachment.updated_at.nil? }
imgs = _
imgs.ids
imgs.map(&:id)
missing = imgs.map(&:id)
missing.count
s = Scrape.last
s.products.count
s = Scrape.last
s.update running: false
s
Scrape.first
s
s.listings.count
s1 = Scrape.last(2).first
s1.listings.count
url1 = 'http://www.dhgate.com/product/smart-ring-computer-parts-components-components/378087870.html#s2-2-1b;searl|2134589161'
url2 = 'http://www.dhgate.com/product/smart-ring-computer-parts-components-components/377793654.html#s2-1-1b;searl|2134589161'
c
c = Category.find_by(name: 'TV Tuner Card')
edit -t
links = dhgate_links.split('\,')
links
new_links = []
links.each do |link|
  next if link.blank?
  new_links << link
end
new_links
links.first
dhgate_links
dhgate_links.first
links = dhgate_links.first.split('\,')
exit
c = Category.find_by(name: 'Televisions')
p_ids = c.products.where.not(id: ProductProductManual.confirmed.pluck(:product_id)).pluck(:id)
p_ids.count
p_ids.sample(300)
ids = p_ids.sample(300)
products = Product.where(id: ids).select(:id, :brand, :model)
results = {}
edit -t
products
edit -t
c = Category.find_by(name: 'Televisions')
c.products.size
p_ids = c.products.where.not(id: ProductProductManual.confirmed.pluck(:product_id)).pluck(:id)
p_ids.count
c.products.count
ids
products = Product.where(id: p_ids).select(:id, :brand, :model)
products.count
products.size
products.count
edit -t
exit
CategoriesManager
require './lib/categories_manager.rb'
CategoriesManager
path = '/Users/jonathan/Documents/DH_GATE_CATEGORIES.xls'
cm = CategoriesManager.new
File.extname(path)
xlsx = Roo::Excel.new(path)
xlsx.open?
xlsx.closed?
xlsx.close
xlsx.open
xlsx = Roo::Excel.new(path)
sheet = xlsx.sheet('dhgate')
edit -t
xlsx.close
exit
path = '/Users/jonathan/Documents/DH_GATE_CATEGORIES.xls'
xlsx = Roo::Excel.new(path)
xlsx.sheets
xlsx.sheets.class
xlsx.sheet('dghate')
sheet = xlsx.sheet('dhgate')
sheet.columns
sheet.column(0)
sheet.column(1)
sheet.first
sheet.first.map(&:underscore)
sheet.first.map(&:capitalize)
sheet.first.map(&:tabelize)
sheet.first.map {|col| col.downcase.gsub(/\s+/, '_')
sheet.first.map {|col| col.downcase.gsub(/\s+/, '_') }
sheet.first.map {|col| col.downcase.gsub(/\s*/, '_') }
sheet.first.map {|col| col.downcase.gsub(/\s+/, '_') }
xlsx.first_row
xlsx.row(1)
xlsx.first
columns = _
hash = {1: 'a', 2: 'b', 3: 'c' }
hash = {a: 'a', b: 'b', c: 'c' }
hash.each {|a| puts a}
hash.first
hash.each {|a| p a}
xlsx.row(2)
row = _
row[0]
p hash
hash.inspect
puts "#{hash.inspect}"
xlsx.close
path
cm
require './lib/categories_manager.rb'
cm = CategoriesManager.new
exit
path = '/Users/jonathan/Documents/DH_GATE_CATEGORIES.xls'
require './lib/categories_manager.rb'
cm = CategoriesManager.new
cm.generate_categories(path)
exit
path = '/Users/jonathan/Documents/DH_GATE_CATEGORIES.xls'
require './lib/categories_manager.rb'
cm = CategoriesManager.new
cm.generate_categories(path)
exit
path = '/Users/jonathan/Documents/DH_GATE_CATEGORIES.xls'
require './lib/categories_manager.rb'
cm = CategoriesManager.new
cm.generate_categories(path)
exit
path = '/Users/jonathan/Documents/DH_GATE_CATEGORIES.xls'
require './lib/categories_manager.rb'
cm = CategoriesManager.new
cm.generate_categories(path)
reload!
cm = CategoriesManager.new
cm.generate_categories(path)
exit
path = '/Users/jonathan/Documents/DH_GATE_CATEGORIES.xls'
require './lib/categories_manager.rb'
cm = CategoriesManager.new
cm.generate_categories(path)
exit
path = '/Users/jonathan/Documents/DH_GATE_CATEGORIES.xls'
require './lib/categories_manager.rb'
cm = CategoriesManager.new
cm.generate_categories(path)
hash = {dhgate: [{:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Tablet PC", :url=>"http://www.dhgate.com/wholesale/tablet-pc/c104012.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Laptops", :url=>"http://www.dhgate.com/wholesale/laptops/c104003.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Monitors", :url=>"http://www.dhgate.com/wholesale/monitors/c104008.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Software", :url=>"http://www.dhgate.com/wholesale/software/c104002.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Scanners", :url=>"http://www.dhgate.com/wholesale/scanners/c104011.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Drives & Storages > Hard Drives", :url=>"http://www.dhgate.com/wholesale/hard-drives/c104001004.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Drives & Storages > Internal Solid State Disks (SSD)", :url=>"http://www.dhgate.com/wholesale/internal-solid-state-disks-ssd-/c104001013.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Drives & Storages > Optical Drives", :url=>"http://www.dhgate.com/wholesale/optical-drives/c104001005.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Drives & Storages > USB Flash Drives", :url=>"http://www.dhgate.com/wholesale/usb-flash-drives/c104001003.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Drives & Storages > External Hard Drives", :url=>"http://www.dhgate.com/wholesale/external-hard-drives/c104001006.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Drives & Storages > Other Drives & Storages", :url=>"http://www.dhgate.com/wholesale/other-drives-storages/c104001008.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Computer Components > Motherboards", :url=>"http://www.dhgate.com/wholesale/motherboards/c104007002.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Computer Components > Graphics Cards", :url=>"http://www.dhgate.com/wholesale/graphics-cards/c104007007.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Computer Components > Sound Cards", :url=>"http://www.dhgate.com/wholesale/sound-cards/c104007005.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Computer Components > Laptop Screens & LCD Panels", :url=>"http://www.dhgate.com/wholesale/laptop-screens-lcd-panels/c104007009.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Computer Components > Computer Interface Cards, Controllers", :url=>"http://www.dhgate.com/wholesale/computer-interface-cards-controllers/c104007012.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Computer Components > RAMs", :url=>"http://www.dhgate.com/wholesale/rams/c104007004.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Computer Components > CPUs", :url=>"http://www.dhgate.com/wholesale/cpus/c104007003.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Keyboards, Mice & Inputs > Mice", :url=>"http://www.dhgate.com/wholesale/mice/c104006002.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Keyboards, Mice & Inputs > Keyboards", :url=>"http://www.dhgate.com/wholesale/keyboards/c104006001.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Keyboards, Mice & Inputs > Graphics Tablets & Pens", :url=>"http://www.dhgate.com/wholesale/graphics-tablets-pens/c104006004.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Keyboards, Mice & Inputs > PC Remote Controls", :url=>"http://www.dhgate.com/wholesale/pc-remote-controls/c104006005.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Computer Accessories > Mini PCs", :url=>"http://www.dhgate.com/wholesale/mini-pcs/c104005022.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Computer Accessories > Computer Cables & Connectors", :url=>"http://www.dhgate.com/wholesale/computer-cables-connectors/c104005008.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Computer Accessories > USB Hubs", :url=>"http://www.dhgate.com/wholesale/usb-hubs/c104005007.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Computer Accessories > Memory Card Readers", :url=>"http://www.dhgate.com/wholesale/memory-card-readers/c104005013.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Computer Accessories > Webcams", :url=>"http://www.dhgate.com/wholesale/webcams/c104005010.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Computer Accessories > Computer Speakers", :url=>"http://www.dhgate.com/wholesale/computer-speakers/c104005011.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Computer Accessories > Docking Stations", :url=>"http://www.dhgate.com/wholesale/docking-stations/c104005018.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Printers & Supplies > Printers", :url=>"http://www.dhgate.com/wholesale/printers/c104010001.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Printers & Supplies > 3D Printer", :url=>"http://www.dhgate.com/wholesale/3d-printer/c104010004.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Networking & Communications > Modems", :url=>"http://www.dhgate.com/wholesale/modems/c104004003.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Networking & Communications > 3G Modems", :url=>"http://www.dhgate.com/wholesale/3g-modems/c104004012.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Networking & Communications > Routers", :url=>"http://www.dhgate.com/wholesale/routers/c104004007.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Networking & Communications > KVM Switches", :url=>"http://www.dhgate.com/wholesale/kvm-switches/c104004001.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Networking & Communications > Network Servers", :url=>"http://www.dhgate.com/wholesale/network-servers/c104004022.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Networking & Communications > Network Switches", :url=>"http://www.dhgate.com/wholesale/network-switches/c104004015.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Networking & Communications > Network Adapters", :url=>"http://www.dhgate.com/wholesale/network-adapters/c104004024.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Networking & Communications > Powerline Adapters", :url=>"http://www.dhgate.com/wholesale/powerline-adapters/c104004018.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Networking & Communications > Telecom Systems > VOIP, Internet Telephony", :url=>"http://www.dhgate.com/wholesale/voip-internet-telephony/c104004005004.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Networking & Communications > Telecom Systems > Other Telecom Systems", :url=>"http://www.dhgate.com/wholesale/other-telecom-systems/c104004005005.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Wholesale Tablet PC Accessories > Tablet PC Screens", :url=>"http://www.dhgate.com/wholesale/tablet-pc-screens/c104014011.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Wholesale Tablet PC Accessories > Tablet PC Batteries", :url=>"http://www.dhgate.com/wholesale/tablet-pc-batteries/c104014010.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Wholesale Tablet PC Accessories > Tablet PC Chargers", :url=>"http://www.dhgate.com/wholesale/tablet-pc-chargers/c104014003.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Wholesale Tablet PC Accessories > Tablet PC Motherboards", :url=>"http://www.dhgate.com/wholesale/tablet-pc-motherboards/c104014013.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Wholesale Tablet PC Accessories > Memory Cards", :url=>"http://www.dhgate.com/wholesale/memory-cards/c104014007.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Computers & Networking > Wholesale Tablet PC Accessories > Tablet PC Cables & Connections", :url=>"http://www.dhgate.com/wholesale/tablet-pc-cables-connections/c104014009.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Electronics > MP3 Players", :url=>"http://www.dhgate.com/wholesale/mp3-players/c103001.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Electronics > MP4 Players", :url=>"http://www.dhgate.com/wholesale/mp4-players/c103002.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Electronics > Headphones & Earphones", :url=>"http://www.dhgate.com/wholesale/headphones-earphones/c103029.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Electronics > Gadgets > Digital Voice Recorder", :url=>"http://www.dhgate.com/wholesale/digital-voice-recorder/c103013001.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Electronics > Television & Parts > Television", :url=>"http://www.dhgate.com/wholesale/television/c103004001.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Electronics > Television & Parts > Antennas", :url=>"http://www.dhgate.com/wholesale/antennas/c103004005.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Electronics > Solar & Wind Electronics > Solar Panels & Accessories", :url=>"http://www.dhgate.com/wholesale/solar-panels-accessories/c103027002.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Electronics > Batteries & Charger > Solar Panels", :url=>"http://www.dhgate.com/wholesale/solar-panels/c103006009.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Electronics > A/V Accessories & Cables > Video Cables & Connectors", :url=>"http://www.dhgate.com/wholesale/video-cables-connectors/c103008017.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Electronics > A/V Accessories & Cables > Audio Cables & Connectors", :url=>"http://www.dhgate.com/wholesale/audio-cables-connectors/c103008001.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Electronics > A/V Accessories & Cables > Receivers", :url=>"http://www.dhgate.com/wholesale/receivers/c103008011.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Electronics > A/V Accessories & Cables > Processors", :url=>"http://www.dhgate.com/wholesale/processors/c103008010.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Electronics > HiFi Audio > HiFi Speaker", :url=>"http://www.dhgate.com/wholesale/hifi-speaker/c103025004.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Electronics > HiFi Audio > HiFi Cable", :url=>"http://www.dhgate.com/wholesale/hifi-cable/c103025001.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Electronics > HiFi Audio > Amplifiers", :url=>"http://www.dhgate.com/wholesale/amplifiers/c103025002.html"}, {:bulk_category=>"Consumer Electronics", :source_category=>"Electronics > Home Audio & Video > Projector & Accessories > Projectors", :url=>"http://www.dhgate.com/wholesale/projectors/c103010002001.html"}]}
hash
hash.inspect
exit
path = '/Users/jonathan/Documents/DH_GATE_CATEGORIES.xls'
require './lib/categories_manager.rb'
cm = CategoriesManager.new
cm.generate_categories(path)
exit
path = '/Users/jonathan/Documents/DH_GATE_CATEGORIES.xls'
require './lib/categories_manager.rb'
cm = CategoriesManager.new
cm.generate_categories(path)
exit
path = '/Users/jonathan/Documents/DH_GATE_CATEGORIES.xls'
require './lib/categories_manager.rb'
cm = CategoriesManager.new
cm.generate_categories(path)
exit
path = '/Users/jonathan/Documents/DH_GATE_CATEGORIES.xls'
require './lib/categories_manager.rb'
cm = CategoriesManager.new
cm.generate_categories(path)
exit
Category.find_by(name: 'Over 65 inch')
c = _
SourceCategory.where(category: c)
cm.generate_categories(path)
path = '/Users/jonathan/Documents/DH_GATE_CATEGORIES.xls'
exit
path = '/Users/jonathan/Documents/DH_GATE_CATEGORIES.xls'
require './lib/categories_manager.rb'
cm = CategoriesManager.new
cm.generate_categories(path)
xlsx = Roo::Excel.new(path)
xlsx.first_row
xlsx.row(2)
xlsx.row(2)[2]
xlsx.row(2)[2].split('\,')
xlsx.close
exit
path = '/Users/jonathan/Documents/DH_GATE_CATEGORIES.xls'
require './lib/categories_manager.rb'
cm = CategoriesManager.new
cm.generate_categories(path)
xlsx = Roo::Excel.new(path)
row = xlsx.row(2)
row[2
]
p row[2]
puts row[2]
row[2].split('\,')
edit -t
source_categories[:dhgate]
source_categories
source_categories.first[:dhgate]
dhgate = _
dhgate
cat = dhgate.first
c = dhgate.first
c[:url].split('\,')
edit -t
gsus_links.first
exit
path = '/Users/jonathan/Documents/DH_GATE_CATEGORIES.xls'
require './lib/categories_manager.rb'
cm = CategoriesManager.new
cm.generate_categories(path)
edit -t
source_categories.each_pair do |source, categories|
  p source
  break
end
source_categories.each_pair do |source, categories|
  p source.to_s
  break
end
exit
Category.first
Category.last
c = Category.where(name: 'Televisions')
SourceCategory.last
Source.all.pluck(:name)
Category.first
Category.all[5]
Boolean('true')
Number('true')
Number.new('true')
NUMBER('1')
Number('1')
Integer('1')
Bool('1')
false
false.class
FalseClass
SourceCategory.last
reload!
SourceCategory.last
edit -t
source_categories.first
source, categories = source_categories.first
qqq
qq
source
source_categories.first
asdf
exit
edit -t
source, categories = source_categories.first
source
categories
cat = categories.first
c = categories.first
idx = 0
source = Source.find_by(name: source.to_s)
cats = c[:source_category].split('>').map(&:strip)
urls = c[:url].split('\,').map(&:strip)
parent = nil
urls.first
cats
edit -t
Category.find_by(name: 'Table PC')
Category.find_by(name: 'Tablet PC')
Category.find_by(name: 'Tablet PC').products.count
Category.find_by(name: 'Tablet PC').listings.count
Category.count
Category.last
cats.first
cat_name = _
category = Category.find_or_create_by name: cat_name
edit -t
SourceCategory.last
Category.last 2
computers = Category.last(2).first
SourceCategory.where(category: computers)
computers = Category.last(2).first
computers = Category.last(2)
computers = Category.last
SourceCategory.where(category: computers)
edit -t
computers = Category.last
SourceCategory.where(category: computers)
edit -t
SourceCategory.where(category: computers)
computers = Category.last(2).first
SourceCategory.where(category: computers)
SourceCategory.where(category: computers).destroy_all
computers = Category.last(2).last
SourceCategory.where(category: computers).destroy_all
c
edit -t
SourceCategory.last 2
SourceCategory.last 4
Category.last
Category.last 2
SourceCategory.where(category_id: 113)
SourceCategory.where(category_id: 114)
SourceCategory.where(category_id: 113)
Category.last
Category.last.destroy
Category.last
SourceCategory.where(category: Category.last).destroy_all
Category.last
Category.last.destroy
Category.last
SourceCategory.where(category: Category.last).destroy_all
Category.last.destroy
Category.last
c
cats
source
edit -t
SourceCategory.last
SourceCategory.last(2)
SourceCategory.last(3)
SourceCategory.last(3).destroy_all
SourceCategory.last(3).each(&:destroy)
Category.last
Category.last.destroy
Category.last
Category.last.destroy
edit -t
SourceCategory.last 3
SourceCategory.last(3).each(&:destroy)
edit -t
SourceCategory.last 3
SourceCategory.find 433
SourceCategory.find(433).category
SourceCategory.find(433)
SourceCategory.where(source_id: 5)
SourceCategory.where(source_id: 5).destroy_all
edit -t
SourceCategory.where(source_id: 5)
c = { :bulk_category => "Consumer Electronics", :active => "true", :source_category => "Computers & Networking > Drives & Storages > Hard Drives", :url => "http://www.dhgate.com/wholesale/hard-drives/c104001004.html" }
edit -t
SourceCategory.where(source_id: 5)
SourceCategory.where(source_id: 5).size
SourceCategory.where(source_id: 5).last(2)
SourceCategory.where(source_id: 5).last(2).map {|sc| sc.category.name }
exit
SourceCategory.last
sc = _
sc.name
sc.category_name
sc
Category.last
Category.first
listing = Listing.first
listing.categories
listing = Listing.last
listing.categories
listing.category_listings.last
cl = _
listing.category_listings.first
listing.category_listings
listing.category_listings.map(&:category)
listing.category_listings
cl =listing.category_listing.last
cl = listing.category_listings.last
cl = listing.category_listings[0]
cl.category
exit
c = Listing.last.category
c = Listing.last.categories.last
c.listings.count
Listing.count
SourceCategory.where(category: c)
c
sc
sc = SourceCategory.where(category: c).last
sc = SourceCategory.where(category: c).first
sc
sc.category.listings
sc
sc.parent
exit
sc = SourceCategory.last
sc.parent
sc.parent.category_name
CategoryListing.last
cl = _
cl = CategoryListing.where.not(source_category_id: nil).last
parent = cl.source_category.parent
cl.source_category
cl.last
cl
exit
cl = CategoryListing.where.not(source_category_id: nil).last
cl.source_category
SourceCategory.find(418)
cl
cl.category
CategoryListing.where(source_category_id: 418).count
CategoryListing.where(source_category_id: 418).destroy_all
cl = CategoryListing.where.not(source_category_id: nil).last
cl.source_category
cl.source_category.category_name
[1,2,3,4,5] - [1,4,6]
path = '/Users/jonathan/Documents/DH_GATE_CATEGORIES.xls'
exit
path = '/Users/jonathan/Documents/DH_GATE_CATEGORIES.xls'
require './lib/categories_manager.rb'
cm = CategoriesManager.new
cm.generate_categories(path)
exit
sc = SourceCategory.last
Category.all
exit
load File.join(Rails.root, 'db', 'seeds', 'categories.rb')
source_categories
path = File.join(Rails.root, 'db', 'seeds', 'categories.rb')
data = File.open(path, 'rb') {|io| io.read }
data
JSON.parse(data)
data = File.open(path, 'rb') {|io| io.read }
JSON.parse(data)
path = File.join(Rails.root, 'db', 'seeds', 'categories.rb')
load File.join(Rails.root, 'db', 'seeds', 'categories.rb')
@source_categories
exit
load File.join(Rails.root, 'db', 'seeds', 'categories.rb')
exit
sc = SourceCategory.last
sc.parent
sc = SourceCategory.last.category_name
sc = SourceCategory.last.parentcategory_name
sc = SourceCategory.last.parent.category_name
parent = sc.parent
sc = SourceCategory.last
parent = sc.parent
while parent !nil?
str = sc.category_name
while parent !nil?
  str = parent.category_name + ' > ' + str
  parent = parent.parent
end
parent = sc.parent
while parent !nil?
  str = parent.category_name + ' > ' + str
  parent = parent.parent
end
while !parent.nil?
  str = parent.category_name + ' > ' + str
  parent = parent.parent
end
str
parent = sc.parent
urls = []
url.push sc.url
sc
urls.shiftsc.url
urls.shift sc.url
urls.unshift sc.url
parent = sc.parent
while !parent.nil?
  urls.unshift parent.url
  parent = parent.parent
end
urls
parent = sc.parent
while !parent.nil?
  puts parent.active
end
while !parent.nil?
  puts parent.active
  parent = parent.parent
end
exit
sc = SourceCategory.last
SourceCategory.where.not(url: nil)
SourceCategory.where.not(url: nil).count
sc = SourceCategory.last
sc.category
sc.parent
SourceCategory.active
Category.count
SourceCategory.count
SourceCategory.where.not(url: nil).count
SourceCategory.where(test_process: true).count
SourceCategory.where(test_process: true, test_only: true).count
SourceCategory.where(active: [nil, false]).count
SourceCategory.where(active: [nil, false]).last 3
SourceCategory.where(active: [nil, false]).last 6
exit
Category.find_by name: 'Solar Panels'
c = _
SourceCategory.where(category: c, source_id: 5)
exit
SeedDump.dump(Source, file: 'db/seeds/dump.rb', append: true)
SeedDump.dump(Category, exclude: [:url, :parent_id], file: 'db/seeds/dump.rb', append: true)
SeedDump.dump(Category, exclude: [:url, :parent_id, :created_at, :updated_at], file: 'db/seeds/dump.rb', append: true)
SeedDump.dump(SourceCategory, exclude: [:created_at, :updated_at], file: 'db/seeds/dump.rb', append: true)
SeedDump.dump(SourceCategory, exclude: [:name, :created_at, :updated_at], file: 'db/seeds/dump.rb', append: true)
SeedDump.dump(Category, exclude: [:url, :source_id, :parent_id, :created_at, :updated_at], file: 'db/seeds/dump.rb', append: true)
SeedDump.dump(SourceCategory, exclude: [:name, :created_at, :updated_at], file: 'db/seeds/dump.rb', append: true)
SeedDump.dump(SourceCategory.where.not(category: nil), exclude: [:name, :created_at, :updated_at], file: 'db/seeds/dump.rb', append: true)
SeedDump.dump(SourceCategory.where.not(category: nil, url: nil), exclude: [:name, :created_at, :updated_at], file: 'db/seeds/dump.rb', append: true)
SourceCategory.fidn(2)
SourceCategory.find(2)
sc = _
exit
edit -t
alius_links.first
edit -t
categories
edit -t
results
categories[0]
alius_links[0]
alius_links[0].split('\,').reject(&:blank?)
alius_links
results
edit -t
alius_links.count
categories.count
alius_links[104].split('\,').reject(&:blank?)
alius_links[103].split('\,').reject(&:blank?)
edit -t
results
results.each do |k, v|
  puts v[url1
puts v['url1']
results.each do |k, v|
  puts v['url1']
end
edit -t
results
edit -t
'alius_links'.constantize
Object.call('alius_links')
Object.const_get('alius_links')
eval('alius_links')
edit -t
jjsources = %w(alius_links gsus_links amazonus_links gsuk_links)
sources = %w(alius_links gsus_links amazonus_links gsuk_links)
sources.each {|source| eval(source) }
sources.each {|source| eval(source.to_s) }
sources.first
eval(sources.first)
eval(sources[1])
eval(sources[2])
eval(sources[3])
eval(sources[4])
eval(sources[3])
eval(sources[3]).send(:[], 1)
eval(sources[3])[3]
edit -t
gsuk_links.count
gsus_links.count
amazonus_links.count
alius_links.count
edit -t
exit
edit -t
path = "/Users/jonathan/Documents/rds_categories.xls"
xlsx = open_spreadsheet(path)
sources = {}
source_names = %w(gsus alius amazonus gsuk dhgate ebay)
sheet = xlsx.sheet(source_name)
sheet = xlsx.sheet('gsus')
columns = sheet.first.map { |col| col.downcase.gsub(/\s+/, '_') }
categories = []
cur_row = xlsx.row(2).map { |row| row.strip.gsub(/\s+/, ' ') }
xlsx.row(2)
cur_row = xlsx.row(2).map { |row| row.to_s.strip.gsub(/\s+/, ' ') }
xlsx.close
exit
Category.count
SourceCategory.count
SourceCategory.where(source_id: 5).active.count
SourceCategory.where(source_id: 5).active.first
SourceCategory.where(source_id: 5).active.pluck(:url)
SourceCategory.where(source_id: 5).active.pluck(:url).count
SourceCategory.where(source_id: 5).active.pluck(:url).uniq.count
SourceCategory.where(source_id: 5).active.pluck(:category_id)
SourceCategory.where(source_id: 5).active.pluck(:category_id).count
SourceCategory.where(source_id: 5).active.pluck(:category_id).uniq..count
SourceCategory.where(source_id: 5).active.pluck(:category_id).uniq.count
rake db:reset
exit
Category.count
SourceCategory.count
Category.count
sc = SourceCategory.last
sc.category
sc = SourceCategory.where(source_id: [1,2,3,4,5]).last
sc.parent
sc.parent.parent
edit -t
edi -t
edit -t
while !parent.nil?
  str = parent.category_name + ' > ' + str
  parent = parent.parent
end
parent = sc.parent 
while !parent.nil?
  str = parent.category_name + ' > ' + str
  parent = parent.parent
end
cd.parent
sc.parent
sc.parent.category_name
sc.category
sc.reload
SourceCategory.last
sc.reload
Curency.last
Currency.last
Monetize.parse('Rs. 1,649')
Monetize.parse('INR 1,649')
Monetize.parse('INR 1,649').exchange_to(:usd)
Monetize.parse('INR 1,649').exchange_to(:usd).to_f
Monetize.parse('INR 1,649')
Currency.find_by(iso_code: 'INR')
url = "http://www.flipkart.com/laptops/pr?sid=6bo,b5g&p[]=facets.availability%5B%5D=Exclude+Out+of+Stock&p[]=sort=popularity&start=21"
Nokogiri::HTML(open(url))
page = _
page.xpath("//div/@class='gd-col'")
page.xpath("//div/@class=gd-col")
page.xpath("//div/@class=gd-col").first
page
page.xpath("//@id='page-6'").first
page.xpath("//@id='page-6'")
page.xpath("//@id")
Category.count
Category.all.pluck(:name)
'Rs. 94,790'.gsub(/\A.+(?=\d)/, '')
'Rs. 94,790'.gsub(/\A.+(?=>\d)/, '')
'Rs. 94,790'.gsub(/\A.+(?=\d)/, '')
'Rs. 94,790'.gsub(/\A.+(?=\d+)/, '')
'Rs. 94,790'.gsub(/\A.+\s(?=\d+)/, '')
'Rs. 94,790'.gsub(/Rs\./, '')
'Rs. 94,790'.gsub(/Rs\.\s/, '')
'Rs. 94,790'.delete('^0-9.,')
'Rs. 94,790'.delete('^0-9,')
'Rs. 94,790'.delete('^0-9')
'Rs. 94,790'.delete('Rs.')
'Rs. 94,790'.delete('rs.')
'Rs. 94,790'.gsub(/\A.+\s(?=\d)/, '')
'Rs. 94,790'.gsub(/\A.+(?=\d+)/, '')
'Rs. 94,790'.gsub(/\A.+(?=\d*)/, '')
'Rs. 94,790'.gsub(/\A.+(?=\d\d)/, '')
'Rs. 94,790'.gsub(/\A.+\s?(?=\d)/, '')
'Rs. 94,790'.gsub(\D+(?=\d+)/, '')
'Rs. 94,790'.gsub(/\D+(?=\d+)/, '')
'Rs. 94,790'.gsub(/\A\D+(?=\d+)/, '')
exit
Categories.count
Category.count
path = File.join(Rails.root, 'db', 'seeds', 'rds_categories.xls')
def open_spreadsheet(file_path)
  case File.extname(file_path)
  when '.csv' then
    Roo::CSV.new(file_path)
  when '.xls' then
    Roo::Excel.new(file_path)
  when '.xlsx' then
    Roo::Excelx.new(file_path)
  else
    fail "Unknown file type: #{file_path}"
  end
end
xlsx = open_spreadsheet(file)
xlsx = open_spreadsheet(path)
sources = {}
source_names = %w(gsus alius amazonus gsuk dhgate ebay)
sheet = xlsx.sheet('dhgate')
columns = sheet.first.map { |col| col.downcase.gsub(/\s+/, '_') }
categories = []
edit -t
categories
categories.count
categories
exit
file = File.join(Rails.root, 'db', 'seeds', 'rds_categories.xls')
def open_spreadsheet(file_path)
  case File.extname(file_path)
  when '.csv' then
    Roo::CSV.new(file_path)
  when '.xls' then
    Roo::Excel.new(file_path)
  when '.xlsx' then
    Roo::Excelx.new(file_path)
  else
    fail "Unknown file type: #{file_path}"
  end
end
xlsx = open_spreadsheet(file)
sources = {}
sheet =     sheet = xlsx.sheet('dhgate')
columns = sheet.first.map { |col| col.downcase.gsub(/\s+/, '_') }
categories = []
(xlsx.first_row + 1..xlsx.last_row).each do |i|
  cur_row = xlsx.row(i).map { |row| row.nil?  ? row : row.strip.gsub(/\s+/, ' ') }
  info = {}
  columns.each_with_index do |col, idx|
    info[col.to_sym] = cur_row[idx]
  end
  categories << info
end
categories.select {|cat| cat.values.blank?}
categories.count
categories.each {|cat| p cat}
categories.each {|cat| puts cat.values.blank?}
categories[43]
categories[43].values
categories[43].values.blank?
categories[43].values.compact.blank?
File.open('db/seeds/categories.rb', 'w') do |file|
  file.puts "@source_categories = {"
  sources.each_pair do |source, categories|
    file.print "  #{source.to_sym}: " #4
    file.puts "["
    categories.each_with_index do |cat, idx|
      next if categories.values.compact.blank?
      file.puts "    #{cat.inspect}#{idx + 1 == categories.length ? '' : ','}"
    end
    file.puts "    ],"
  end
  file.puts "}\n\n"
end
source_name = 'dhgate'
sources[source_name.to_sym] = categories
edit -t
sources
sources.length
edit -t
source_names = %w(gsus alius amazonus gsuk dhgate ebay)
edit -t
nil.to_s
nil.downcase
String(nil)
String('True')
String(TRUE)
String(true)
String(0)
"true".falsy
0.try(:downcase)
1.try(:downcase)
'true' == true
'true'.to_sym == true
'true'.to_sym
Object.send('true'.to_sym)
ActiveRecord::ConnectionAdapters::Column.value_to_boolean('true')
sc = SourceCategory.last
sc.active = 'true'
sc.save
sc
sc.active = 'FALSE'
sc.save
sc
sc.active = 0
sc.save
sc.reload
sc.active = 1
sc.save
sc.reload
sc.active = nil
sc.save
sc.reload
{:a => 'a', :A => 'A'}
h = _
h[:a]
h[:A]
exit
SourceCategory.count
Category.count
Category.pluck(:name)
Category.pluck(:name).count
Category.pluck(:name).uniq.count
Category.pluck(:name).sort
SourceCategory.where(source_id: 1).count
SourceCategory.where(source_id: 2).count
Source.find 2
SourceCategory.where(source_id: 2)
SourceCategory.where(source_id: 3).count
SourceCategory.where(source_id: 1).count
SourceCategory.where(source_id: 3).count
SourceCategory.where(source_id: 4).count
SourceCategory.where(source_id: 5).count
SourceCategory.where(source_id: 4).count
SourceCategory.where(source_id: 3).count
SourceCategory.where(source_id: 1).count
SourceCategory.where(source_id: 1, active: true).count
SourceCategory.count
SourceCategory.active.count
exit
SourceCategory.count
SourceCategory.active.count
exit
sc = SourceCategory.where(source_id: 5).last(15)
sc = SourceCategory.find(125)
sc.category
sc.parent
SourceCategory.where(parent_id: 121)
SourceCategory.last
s = Source.find_by(name: 'ebay')
s.source_categories.last
sc = _
sc.parent
sc.category
url = 'http://www.cleansolar.com/wp-content/uploads/2014/06/intelfaire.jpg'
ProductImage.where(original_url: url)
Oj
endpoint = 'http://ondemand-p02-api.ditto.us.com/v1/find?'
params = {}
params[:client_id] = '45efef10'
params[:url] = 'http://www.cleansolar.com/wp-content/uploads/2014/06/intelfaire.jpg'
params[:uid] = 'T001'
response = RestClient.get endpoint, params: params, :content_type => :json, :accept => :json
require 'rest-client'
RestClient
response = RestClient.get endpoint, params: params, :content_type => :json, :accept => :json
endpoint = 'http://ondemand-p02-api.ditto.us.com/v1/find'
response = RestClient.get endpoint, params: params, :content_type => :json, :accept => :json
Oj.load(response)
params[:url] = 'http://www.hookedonsolar.com/images/sunpower_roof_man.jpg'
response = RestClient.get endpoint, params: params, :content_type => :json, :accept => :json
Oj.load(response)
response = _
response['data']['matches']
response['data']['matches'].any? {|hit| hit['brand'] == 'Sunpower' }
1000 / 60
params[:url] = 'http://restartsolar.com/wp-content/uploads/2015/11/SUNPOWER-LEVIS-STADIUM-SOLAR-PANELS-BIG-SCREEN.jpg'
response = RestClient.get endpoint, params: params, :content_type => :json, :accept => :json
response = Oj.load(response)
response['data']['matches'].any? {|hit| hit['brand'] == 'Sunpower' }
params[:url] = 'http://www.ligraf.co.il/gallery/contentitem-92.jpg'
response = RestClient.get endpoint, params: params, :content_type => :json, :accept => :json
ENV
path = File.join(Rails.root, 'db', 'seeds', 'rds_categories.xls')
def open_spreadsheet(file_path)
  case File.extname(file_path)
  when '.csv' then
    Roo::CSV.new(file_path)
  when '.xls' then
    Roo::Excel.new(file_path)
  when '.xlsx' then
    Roo::Excelx.new(file_path)
  else
    fail "Unknown file type: #{file_path}"
  end
end
f = open_spreadsheet(path)
f.sheet('gsimg')
c = Category.where(name: 'Solar Panels')
SourceCategory.where(source_id: 1, category: c)
sc = _.first
SourceCategory.where(parent: sc)
Cateogory.find_by(name: '"LED"')
Category.find_by(name: '"LED"')
c = _
SourceCategory.where(category: c)
c = Category.find_by(name: '"LED"')
Category.count
SourceCategory.count
SourceCategory.activecount
SourceCategory.active.count
SourceCategory.active.where(source_id: 1)
SourceCategory.active.where(source_id: 1).count
SourceCategory.active.where(source_id: 2).count
SourceCategory.active.where(source_id: 3).count
SourceCategory.active.where(source_id: 4).count
SourceCategory.active.where(source_id: 5).count
exit
file = 'phantomjs-2.1.1-linux-x86_64.tar.bz2'
file.gsub(/\.tar$/, '')
file.gsub(/\.tar.+$/, '')
exit
scrape_id = 74
exit
s = Scrape.where(source_id: 1).last
s.listings.first
l = _
img = l.images
s.listings.count
l.categories
Scrape.where(source_id: 3)
scrapes = _
listings = Listing.where(scrape: scrapes)
scrape = Scrape.where(source_id: [1,2,3,4]).last
reload!
scrape = Scrape.where(source_id: [1,2,3,4]).last
listing = Listing.where(scrape: scrape).last
category = Category.find_by(name: 'Sunpower')
category = Category.find_by(name: 'Solar Panels')
listings = category.listings.where(scrape: scrape)
listings = category.listings.last
listing.images
ProductImage.last
Time.now.miutes
Time.now.minutes
Time.now
DateTime.now
time = _
time.hour
time.minutes
time.min
Time.now.to_i
Time.now.to_i - _
60 * 60 * 60
(60 * 60 * 60) / 1000
(60 * 60) / 1000
(60 * 60).to_f / 1000
Time.now_to_f
Time.now.to_f
Time.now.to_f - _
Time.now.to_f
Time.now.to_i
image = ProductImage.last
endpoint = 'http://ondemand-p02-api.ditto.us.com/v1/find'
params = {}
params[:client_id] = '45efef10'
params[:uid] = image.id
params[:url] = image.attachment.url
response = RestClient.get endpoint, params: params, :content_type => :json, :accept => :json
require 'rest-client'
response = RestClient.get endpoint, params: params, :content_type => :json, :accept => :json
exit
image = ProductImage.last
iamge.attachment.url
image.attachment.url
exit
endpoint = 'http://ondemand-p02-api.ditto.us.com/v1/find'
params = {}
params[:client_id] = '45efef10'
params[:uid] = image.id
params[:url] = image.attachment.url.gsub(/-dev/, '')
params[:url] = 'https://rvx-rds-dev.s3.amazonaws.com/images/uploads/003/268/959/original/51-95QN6_2BNL._SL1500_.jpg?1463401182'
require 'rest-client'
response = RestClient.get endpoint, params: params, :content_type => :json, :accept => :json
Asset.find(14
)
ProductImage.create
image = _
image.attachment.url
image.attachment
image.attachment['url']
image.attachment.attributes
image.attachment
image.attachment.instance_variable_get(:url)
image.attachment
image.attachment.url
image.attachment.url.gsub(/-dev/, '')
exit
image = ProductImage.last
reload!
image = ProductImage.last
exit
reload!
image = ProductImage.last
exit
image = ProductImage.last
reload!
image = ProductImage.last
exit
image = ProductImage.last
exit
image = ProductImage.last
ProductImage.connection
exit
ProductImage.connection
image = ProductImage.last
exit
image = ProductImage.last
exit
image = ProductImage.last
img.attachment.url
image.attachment.url
params[:url] = image.attachment.url.gsub(/-dev/, '')
params = {}
params[:url] = image.attachment.url.gsub(/-dev/, '')
params[:url] = image.attachment.url.gsub(/-dev/, '').gsub(/\/development/, '')
exit
ProductImage.find(3132218)
image = _
image.attachment.url
endpoint = 'http://ondemand-p02-api.ditto.us.com/v1/find'
params = {}
params[:client_id] = '45efef10'
params[:uid] = image.id
#running on beta need to fix s3 url
params[:url] = image.attachment.url.gsub(/-dev/, '').gsub(/\/development/, '')
response = RestClient.get endpoint, params: params, :content_type => :json, :accept => :json
require 'rest-client'
response = RestClient.get endpoint, params: params, :content_type => :json, :accept => :json
response = Oj.load(response)
brands = response['data']['matches'].map { |hit| hit['brand'] }.join(',')
exit
results = []
ImageLogoMatch.find_each do |match|
  result = {}
  img = Image.find(match.image_id)
  result[:image_id] = img.id
  result[:image_url] = img.attachment.url
  result[:brands] = match.brands
  results
end
results = []
ImageLogoMatch.find_each do |match|
  result = {}
  img = ProductImage.find(match.image_id)
  result[:image_id] = img.id
  result[:image_url] = img.attachment.url
  result[:brands] = match.brands
  results
end
results.count
results = []
ImageLogoMatch.find_each do |match|
  result = {}
  img = ProductImage.find(match.image_id)
  result[:image_id] = img.id
  result[:image_url] = img.attachment.url
  result[:brands] = match.brands
  results << result
end
results.count
results.first
ImageLogoMatch.count
Asset.find(ImageLogoMatch.last.image_id)
iamge = _
image = _
image.listings
exit
image = Asset.find(ImageLogoMatch.last.image_id)
image.listings.count
l = image.listings.last
l.scrape
Scrape.where(source_id: 2).pluck(:id)
ImageLogoMatch.where("brands like '%Sunpower%'")
ImageLogoMatch.where("brands like '%Sunpower%'").count
urls = ProductImage.where(id: ImageLogoMatch.where("brands like '%Sunpower%'").pluck(:image_id)).map {|img| img.attachment.url}
urls.uniq
urls = ProductImage.where(id: ImageLogoMatch.where("brands like '%Sunpower%'").pluck(:image_id)).map {|img| img.attachment.url}
urls = ProductImage.where(id: ImageLogoMatch.where("brands like '%Sunpower%'").pluck(:image_id)).last
img = _
img.listings
img.listings.count
l = img.listings.first
l.url
urls = ProductImage.where(id: ImageLogoMatch.where("brands like '%Sunpower%'").pluck(:image_id)).map {|img| img.attachment.url}
ImageLogoMatch.count
brands = ImageLogoMatch.pluck(:brands).flat_map {|m| m.brands.split(',')}
brands = ImageLogoMatch.pluck(:brands).flat_map {|m| m.split(',')}
brands.count
brands.uniq.count
brands = brands.uniq.sort
brands
urls = ProductImage.where(id: ImageLogoMatch.where("brands like '%Dolby%'").pluck(:image_id)).map {|img| img.attachment.url}
urls = ProductImage.where(id: ImageLogoMatch.where("brands like '%Subaru%'").pluck(:image_id)).map {|img| img.attachment.url}
exit
ImageLogoMatch
ImageLogoMatch.connection
ImageLogoMatch.count
ProductImage.last
image = _
endpoint = 'http://ondemand-p02-api.ditto.us.com/v1/find'
params = {}
params[:client_id] = '45efef10'
params[:uid] = image.id
#running on beta need to fix s3 url
params[:url] = image.attachment.url.gsub(/-dev/, '').gsub(/\/development/, '')
response = RestClient.get endpoint, params: params, :content_type => :json, :accept => :json
require 'rest-client'
response = RestClient.get endpoint, params: params, :content_type => :json, :accept => :json
response['data']['matches'].any? { |m| m['match_quality'] == 'High' }
[].present?
response = Oj.load(response)
response['data']['matches'].any? { |m| m['match_quality'] == 'High' }
[a, b, c].to_s
['a' 'b', 'c'].to_s
'a' 'b'
ImageLogoMatch.count
ImageLogoMatch.last.id
endpoint = 'http://ondemand-p02-api.ditto.us.com/v1/find'
params = {}
params[:client_id] = '45efef10'
params[:url] = 'https://www.instagram.com/p/BFmhl4_JZEq/media/?size=l'
params[:uid] = 'T0004
'
params[:uid] = 'T0004'
response = RestClient.get endpoint, params: params, :content_type => :json, :accept => :json
response = Oj.load(response)
matches = response['data']['matches']
if matches.any? { |m| m['match_quality'] == 'High' }
  brands = matches.map { |hit| hit.try(:[], 'brand') }.compact.join(',')
  puts brands
end
ImageLogoMatch.last
exit
url = 'http://www.amazon.com/s/ref=sr_nr_p_n_feature_browse-b_0?fst=as%3Aoff&rh=n%3A228013%2Ck%3Alighting&keywords=lighting&ie=UTF8&qid=1463695122'
led = 'http://www.amazon.com/gp/search/ref=sr_nr_p_n_feature_browse-b_0?fst=as%3Aoff&rh=n%3A228013%2Ck%3Alighting%2Cp_n_feature_browse-bin%3A5676449011&keywords=lighting&ie=UTF8&qid=1463695134&rnid=389544011'
url - led
url.split
url.split('')
url.split('') - led.split('')
led
url
exit
url = 'http://www.flipkart.com/vu-102cm-40-full-hd-led-tv/p/itme8fmzymk5dasj?pid=TVSE8FMZ9AQMEGC6&al=wjJsaXS8lGW6%2BGoc7obcw8ldugMWZuE7wkNiXfq8GiQv192uhVq2Su%2BPQZGb9jDPL%2FuObVhLhrU%3D&ref=L%3A1695238916065218375&srno=b_1'
hash = {}
hash[:General] = {"Feature Bullets": %w(asdf agaswq agxcaasd)}
hash
keyfeats = { General: { "Key Features": %w(aaaaaa ddddddd bbbbbbbb)}
}
hash
hash.merge(keyfeats)
hash
hash.merge!(keyfeats)
hash
exit
60 * 60
_ / 6.to_f
600 * 24
Category.find_by(name: 'Lighting')
c = Category.find_by(name: 'Lighting')
SourceCategory.where(category: c)
exit
Scrape.last
Scrape.create(source_id: 5)
scrape.valid?
s = Scrape.last
scrape_id, source_category_id, url, retry_time = 1, 1, url, 0
url
url = 'http://www.flipkart.com/apple-macbook-pro-mf839hn-a-core-i5-8-gb-ddr3-mac-os-ultrabook/p/itme6h4eevcshyvs?pid=COME6H4EHXFNHRYJ&al=LlvZzNhz5rN802oVYIt%2Bn8ldugMWZuE75aUsiwTbcENAuJZvEZBXJbgOU72pm6i7%2Bk%2BasFu3GRY%3D&ref=L%3A-5744635892988818957&srno=p_1&otracker=from-search'
FlipkartListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
exit
url = 'http://www.flipkart.com/apple-macbook-pro-mf839hn-a-core-i5-8-gb-ddr3-mac-os-ultrabook/p/itme6h4eevcshyvs?pid=COME6H4EHXFNHRYJ&al=LlvZzNhz5rN802oVYIt%2Bn8ldugMWZuE75aUsiwTbcENAuJZvEZBXJbgOU72pm6i7%2Bk%2BasFu3GRY%3D&ref=L%3A-5744635892988818957&srno=p_1&otracker=from-search'
scrape_id, source_category_id, url, retry_time = 1, 1, url, 0
FlipkartListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
exit
url = 'http://www.flipkart.com/apple-macbook-pro-mf839hn-a-core-i5-8-gb-ddr3-mac-os-ultrabook/p/itme6h4eevcshyvs?pid=COME6H4EHXFNHRYJ&al=LlvZzNhz5rN802oVYIt%2Bn8ldugMWZuE75aUsiwTbcENAuJZvEZBXJbgOU72pm6i7%2Bk%2BasFu3GRY%3D&ref=L%3A-5744635892988818957&srno=p_1&otracker=from-search'
scrape_id, source_category_id, url, retry_time = 1, 1, url, 0
FlipkartListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
eit
exit
url = 'http://www.flipkart.com/apple-macbook-pro-mf839hn-a-core-i5-8-gb-ddr3-mac-os-ultrabook/p/itme6h4eevcshyvs?pid=COME6H4EHXFNHRYJ&al=LlvZzNhz5rN802oVYIt%2Bn8ldugMWZuE75aUsiwTbcENAuJZvEZBXJbgOU72pm6i7%2Bk%2BasFu3GRY%3D&ref=L%3A-5744635892988818957&srno=p_1&otracker=from-search'
scrape_id, source_category_id, url, retry_time = 1, 1, url, 0
FlipkartListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
exit
url = 'http://www.flipkart.com/apple-macbook-pro-mf839hn-a-core-i5-8-gb-ddr3-mac-os-ultrabook/p/itme6h4eevcshyvs?pid=COME6H4EHXFNHRYJ&al=LlvZzNhz5rN802oVYIt%2Bn8ldugMWZuE75aUsiwTbcENAuJZvEZBXJbgOU72pm6i7%2Bk%2BasFu3GRY%3D&ref=L%3A-5744635892988818957&srno=p_1&otracker=from-search'
scrape_id, source_category_id, url, retry_time = 1, 1, url, 0
FlipkartListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
exit
l = Listing.last
l.data_hash
l.data_hash[0]
l.data_hash.second
l.data_hash.last
l.data_hash.first
l.data_hash["Specifications of Apple MacBook Pro MF839HN/A MF839HN/A Core i5 - (8 GB DDR3/Mac OS) Ultrabook (13.3 inch, SIlver)"]
l.data_hash[:"Specifications of Apple MacBook Pro MF839HN/A MF839HN/A Core i5 - (8 GB DDR3/Mac OS) Ultrabook (13.3 inch, SIlver)"]
l.data_hash[:"Specifications of Apple MacBook Pro MF839HN/A MF839HN/A Core i5 - (8 GB DDR3/Mac OS) Ultrabook (13.3 inch, SIlver)"][:SOFTWARE]
l.data_hash
hash = l.data_hash
hash.stringify_keys
hash.as_json
hash.as_json.to_json
p hash.as_json.to_json
p hash.to_json
hash
hash.to_json
exit
l = Listing.last
l.destroy
Seller.count
url = 'http://www.flipkart.com/apple-macbook-pro-mf839hn-a-core-i5-8-gb-ddr3-mac-os-ultrabook/p/itme6h4eevcshyvs?pid=COME6H4EHXFNHRYJ&al=LlvZzNhz5rN802oVYIt%2Bn8ldugMWZuE75aUsiwTbcENAuJZvEZBXJbgOU72pm6i7%2Bk%2BasFu3GRY%3D&ref=L%3A-5744635892988818957&srno=p_1&otracker=from-search'
scrape_id, source_category_id, url, retry_time = 1, 1, url, 0
FlipkartListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
exit
url = 'http://www.flipkart.com/apple-macbook-pro-mf839hn-a-core-i5-8-gb-ddr3-mac-os-ultrabook/p/itme6h4eevcshyvs?pid=COME6H4EHXFNHRYJ&al=LlvZzNhz5rN802oVYIt%2Bn8ldugMWZuE75aUsiwTbcENAuJZvEZBXJbgOU72pm6i7%2Bk%2BasFu3GRY%3D&ref=L%3A-5744635892988818957&srno=p_1&otracker=from-search'
scrape_id, source_category_id, url, retry_time = 1, 1, url, 0
l = Listing.last
l.screenshots
l.destroy
FlipkartListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l.images
l.screenshots
l.screenshots.first
l.screenshots.first.attachment.url
l.sellers
l.scrape_listing_sellers
sls = _.first
sls.currency
sls.min_price
sls.min_price.to_f
Monetize.parse("#{sls.currency.iso_code} #{sls.min_price.to_f}
Monetize.parse("#{sls.currency.iso_code} #{sls.min_price.to_f}")
Monetize.parse("#{sls.currency.iso_code} #{sls.min_price.to_f}").exchange_to(:USD).to_f
l
l.destroy
exit
url = 'http://www.flipkart.com/apple-macbook-pro-mf839hn-a-core-i5-8-gb-ddr3-mac-os-ultrabook/p/itme6h4eevcshyvs?pid=COME6H4EHXFNHRYJ&al=LlvZzNhz5rN802oVYIt%2Bn8ldugMWZuE75aUsiwTbcENAuJZvEZBXJbgOU72pm6i7%2Bk%2BasFu3GRY%3D&ref=L%3A-5744635892988818957&srno=p_1&otracker=from-search'
scrape_id, source_category_id, url, retry_time = 1, 1, url, 0
FlipkartListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l.images
l.reload
l.images
l = Listing.last
url = 'http://www.flipkart.com/safer-v1-0-safety-security-smart-tracker/p/itmeg5as9a2ydg9z
url = 'http://www.flipkart.com/safer-v1-0-safety-security-smart-tracker/p/itmeg5as9a2ydg9z'
FlipkartListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
exit
url = 'http://www.flipkart.com/safer-v1-0-safety-security-smart-tracker/p/itmeg5as9a2ydg9z'
FlipkartListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
scrape_id, source_category_id, url, retry_time = 1, 1, url, 0
FlipkartListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.lsat
l = Listing.last
l.sellers
l.images
exit
Listing.destroy_all
url = 'http://www.flipkart.com/safer-v1-0-safety-security-smart-tracker/p/itmeg5as9a2ydg9z'
scrape_id, source_category_id, url, retry_time = 1, 1, url, 0
FlipkartListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l.images
l.images.size
l.images
l.screenshots
l.manuals
l.selelrs
l.sellers
url = 'http://www.flipkart.com/apple-macbook-pro-mf839hn-a-core-i5-8-gb-ddr3-mac-os-ultrabook/p/itme6h4eevcshyvs?pid=COME6H4EHXFNHRYJ&al=LlvZzNhz5rN802oVYIt%2Bn8ldugMWZuE75aUsiwTbcENAuJZvEZBXJbgOU72pm6i7%2Bk%2BasFu3GRY%3D&ref=L%3A-5744635892988818957&srno=p_1&otracker=from-search'
FlipkartListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l.images
l.screenshots
l.images
l.sellers
exit
url = 'http://www.flipkart.com/lumo-body-tech-sensor-fitness-smart-tracker/p/itmeg37syfz8wuzx?pid=SYMEG37SZV8CN3JZ&otracker=reco_pp_personalhistoryFooter_wearable_na_1'
scrape_id, source_category_id, url, retry_time = 1, 1, url, 0
FlipkartListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l.destroy
exit
url = 'http://www.flipkart.com/lumo-body-tech-sensor-fitness-smart-tracker/p/itmeg37syfz8wuzx?pid=SYMEG37SZV8CN3JZ&otracker=reco_pp_personalhistoryFooter_wearable_na_1'
scrape_id, source_category_id, url, retry_time = 1, 1, url, 0
FlipkartListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l
l.destroy
Listing.count
Listing.destroy_all
Listing.count
exit
url = 'http://www.flipkart.com/lumo-body-tech-sensor-fitness-smart-tracker/p/itmeg37syfz8wuzx?pid=SYMEG37SZV8CN3JZ&otracker=reco_pp_personalhistoryFooter_wearable_na_1'
scrape_id, source_category_id, url, retry_time = 1, 1, url, 0
FlipkartListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
exit
url = 'http://www.flipkart.com/hp-15-af114au-p3c92pa-acj-apu-quad-core-a8-4-gb-ddr3-1-tb-hdd-windows-10-notebook/p/itmeae7xhwzk9auy?pid=COMEAE7XDPMSAMRW&al=fDZKUXZLJhUlXctdi4nB1MldugMWZuE75aUsiwTbcEN7yrs5t0ztNSrlGlv2diDGVwslAzv5HNU%3D&ref=L%3A385670502429311647&srno=b_1
'
url = 'http://www.flipkart.com/hp-15-af114au-p3c92pa-acj-apu-quad-core-a8-4-gb-ddr3-1-tb-hdd-windows-10-notebook/p/itmeae7xhwzk9auy?pid=COMEAE7XDPMSAMRW&al=fDZKUXZLJhUlXctdi4nB1MldugMWZuE75aUsiwTbcEN7yrs5t0ztNSrlGlv2diDGVwslAzv5HNU%3D&ref=L%3A385670502429311647&srno=b_1'
scrape_id, source_category_id, url, retry_time = 1, 1, url, 0
FlipkartListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
arr = [1,2,3,4]
arr.delete_if {|b| b < 3}
arr
exit
url = 'http://www.flipkart.com/hp-15-af114au-p3c92pa-acj-apu-quad-core-a8-4-gb-ddr3-1-tb-hdd-windows-10-notebook/p/itmeae7xhwzk9auy?pid=COMEAE7XDPMSAMRW&al=fDZKUXZLJhUlXctdi4nB1MldugMWZuE75aUsiwTbcEN7yrs5t0ztNSrlGlv2diDGVwslAzv5HNU%3D&ref=L%3A385670502429311647&srno=b_1'
scrape_id, source_category_id, url, retry_time = 1, 1, url, 0
FlipkartListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
exit
url = 'http://www.flipkart.com/hp-15-af114au-p3c92pa-acj-apu-quad-core-a8-4-gb-ddr3-1-tb-hdd-windows-10-notebook/p/itmeae7xhwzk9auy?pid=COMEAE7XDPMSAMRW&al=fDZKUXZLJhUlXctdi4nB1MldugMWZuE75aUsiwTbcEN7yrs5t0ztNSrlGlv2diDGVwslAzv5HNU%3D&ref=L%3A385670502429311647&srno=b_1'
scrape_id, source_category_id, url, retry_time = 1, 1, url, 0
FlipkartListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
exit
Listing.destroy_all
url = 'http://www.flipkart.com/hp-15-af114au-p3c92pa-acj-apu-quad-core-a8-4-gb-ddr3-1-tb-hdd-windows-10-notebook/p/itmeae7xhwzk9auy?pid=COMEAE7XDPMSAMRW&al=fDZKUXZLJhUlXctdi4nB1MldugMWZuE75aUsiwTbcEN7yrs5t0ztNSrlGlv2diDGVwslAzv5HNU%3D&ref=L%3A385670502429311647&srno=b_1'
scrape_id, source_category_id, url, retry_time = 1, 1, url, 0
FlipkartListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l.destroy
exit
url = 'http://www.flipkart.com/hp-15-af114au-p3c92pa-acj-apu-quad-core-a8-4-gb-ddr3-1-tb-hdd-windows-10-notebook/p/itmeae7xhwzk9auy?pid=COMEAE7XDPMSAMRW&al=fDZKUXZLJhUlXctdi4nB1MldugMWZuE75aUsiwTbcEN7yrs5t0ztNSrlGlv2diDGVwslAzv5HNU%3D&ref=L%3A385670502429311647&srno=b_1'
scrape_id, source_category_id, url, retry_time = 1, 1, url, 0
FlipkartListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l.destroy
exit
url = 'http://www.flipkart.com/hp-15-af114au-p3c92pa-acj-apu-quad-core-a8-4-gb-ddr3-1-tb-hdd-windows-10-notebook/p/itmeae7xhwzk9auy?pid=COMEAE7XDPMSAMRW&al=fDZKUXZLJhUlXctdi4nB1MldugMWZuE75aUsiwTbcEN7yrs5t0ztNSrlGlv2diDGVwslAzv5HNU%3D&ref=L%3A385670502429311647&srno=b_1'
scrape_id, source_category_id, url, retry_time = 1, 1, url, 0
FlipkartListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l
l.images.count
l.images
l.images.pluck(:original_url)
l.url
l.original_url
l.url
l.url.gsub(/\?.+$/, '')
l
exit
url = 'http://www.flipkart.com/hp-15-af114au-p3c92pa-acj-apu-quad-core-a8-4-gb-ddr3-1-tb-hdd-windows-10-notebook/p/itmeae7xhwzk9auy'
Listing.destroy_all
scrape_id, source_category_id, url, retry_time = 1, 1, url, 0
FlipkartListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l.images.count
url = 'http://www.flipkart.com/lumo-body-tech-sensor-fitness-smart-tracker/p/itmeg37syfz8wuzx?pid=SYMEG37SZV8CN3JZ&otracker=reco_pp_personalhistoryFooter_wearable_na_1'
FlipkartListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
exit
url = 'http://www.flipkart.com/lumo-body-tech-sensor-fitness-smart-tracker/p/itmeg37syfz8wuzx?pid=SYMEG37SZV8CN3JZ&otracker=reco_pp_personalhistoryFooter_wearable_na_1'
scrape_id, source_category_id, url, retry_time = 1, 1, url, 0
FlipkartListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l.images
l.images.count
l.url
l.original_url
url= 'http://www.flipkart.com/asus-zenwatch-2-silver-case-rubber-strap-sliver-rubber-taupe-smartwatch/p/itmeef7ffqgeaqwp?pid=SMWEEF7FEERVGPYK&al=fDZKUXZLJhWq6p96f%2FV7UsldugMWZuE7eGHgUTGjVrpuemy%2BA6DOF5wKIAtFmiWAFjJaJ%2B3cvIk%3D&ref=L%3A1032500460954403983&srno=b_17&findingMethod=Menu&otracker=hp_header_nmenu_sub_Electronics_0_Smart%20Watch'
FlipkartListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l.images.count
exit
Listing.destroy_all
url = 'http://www.flipkart.com/asus-zenwatch-2-silver-case-rubber-strap-sliver-rubber-taupe-smartwatch/p/itmeef7ffqgeaqwp?pid=SMWEEF7FEERVGPYK&al=fDZKUXZLJhWq6p96f%2FV7UsldugMWZuE7eGHgUTGjVrpuemy%2BA6DOF5wKIAtFmiWAFjJaJ%2B3cvIk%3D&ref=L%3A1032500460954403983&srno=b_17&findingMethod=Menu&otracker=hp_header_nmenu_sub_Electronics_0_Smart%20Watch'
scrape_id, source_category_id, url, retry_time = 1, 1, url, 0
FlipkartListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
exit
url = 'http://www.flipkart.com/asus-zenwatch-2-silver-case-rubber-strap-sliver-rubber-taupe-smartwatch/p/itmeef7ffqgeaqwp?pid=SMWEEF7FEERVGPYK&al=fDZKUXZLJhWq6p96f%2FV7UsldugMWZuE7eGHgUTGjVrpuemy%2BA6DOF5wKIAtFmiWAFjJaJ%2B3cvIk%3D&ref=L%3A1032500460954403983&srno=b_17&findingMethod=Menu&otracker=hp_header_nmenu_sub_Electronics_0_Smart%20Watch'
scrape_id, source_category_id, url, retry_time = 1, 1, url, 0
FlipkartListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
exit
url = 'http://www.flipkart.com/asus-zenwatch-2-silver-case-rubber-strap-sliver-rubber-taupe-smartwatch/p/itmeef7ffqgeaqwp?pid=SMWEEF7FEERVGPYK&al=fDZKUXZLJhWq6p96f%2FV7UsldugMWZuE7eGHgUTGjVrpuemy%2BA6DOF5wKIAtFmiWAFjJaJ%2B3cvIk%3D&ref=L%3A1032500460954403983&srno=b_17&findingMethod=Menu&otracker=hp_header_nmenu_sub_Electronics_0_Smart%20Watch'
scrape_id, source_category_id, url, retry_time = 1, 1, url, 0
FlipkartListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l.data_hash
l = Listing.last
l.data_hash
url = 'http://www.flipkart.com/lenovo-g50-g-series-y700-15isk-80e3020bih-apu-quad-core-a8-6th-gen-4-gb-ddr3-1-tb-hdd-windows-10-notebook/p/itmefggpvhxm5wzt?pid=COMEFGGPGSQRMSY3&al=fDZKUXZLJhWkeVZKaZcXasldugMWZuE75aUsiwTbcEPFx40k%2Fvi8TCf5fcQVGrdBfmYsaOLhdKU%3D&ref=L%3A6023201045343390157&srno=b_3'
FlipkartListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l.data_hash
l
l.data_hash
l
l = Listing.last(2).first
l
xtqqexit
exit
l = Listing.last
l.data_hash
Listing.count
Listing.first
l.last
l
all = '#ires .psli, #ires .psgi, #J-items-content .m-item, [data-role=item], .s-result-item, .rc .r a'
dh_gate = '#proList > .listitem'
flipkart = '#products .pu-details > .pu-title > a'
[all, dh_gate, flipkart].join(', ')
exit
xit
exit
SourceCategory.where(test_process: true, source_id: 3)
exit
source_name, categories, test_only =   'amazonus',
[
  # { name: 'Solar Panels', url: 'http://www.amazon.com/s/ref=lp_2236628011_nr_p_n_feature_keywords_2?fst=as%3Aoff&rh=n%3A2972638011%2Cn%3A%213238155011%2Cn%3A552808%2Cn%3A3236381%2Cn%3A2236628011%2Cp_n_feature_keywords_browse-bin%3A2825448011&bbn=2236628011&ie=UTF8&qid=1443118329&rnid=2825423011' },
  # { name: 'Solar Panels', url: 'http://www.amazon.com/b/ref=dp_bc_4?ie=UTF8&node=2236628011' },
edit -t
source_name
edit -t
source_name
categories
source = Source.find_by(name: source_name)
source_cats = []
cat = categories.first
c = categories.first
c[:url].split(
c[:url].split('\,')
exit
SourceCategory.where(test_process: true, source_id: 3)
cats = _
cats.count
cats.pluck(:url)
cats.pluck(:url).uniq.count
SourceCategory.where(test_process: true, source_id: 1)
exit
SourceCategory.where(test_process: true, test_only: false)
SourceCategory.where(test_process: true, test_only: false).count
SourceCategory.where(test_process: true, test_only: false, source_id: 2).count
SourceCategory.where(test_process: true, test_only: false, source_id: 2)
SourceCategory.where(test_process: true, test_only: false, source_id: 1)
SourceCategory.where(test_process: true, test_only: false, source_id: 3)
SourceCategory.where(test_process: true, test_only: false, source_id: 4)
SourceCategory.where(test_process: true, test_only: false, source_id: 5)
exit
SourceCategory.where(test_process: true source_id: 2)
SourceCategory.where(test_process: true, source_id: 2)
exit
SourceCategory.where(test_process: true, source_id: 2)
exit
SourceCategory.where(test_process: true, source_id: 2)
SourceCategory.where(test_process: true, source_id: 1)
SourceCategory.where(test_process: true, source_id: 3)
SourceCategory.where(test_process: true, source_id: 4)
exit
edit -t
link
edit -t
selector
url = 'http://www.flipkart.com/mobiles/pr?sid=tyy,4io&start=41'
uri = URI.parse(url)
scheme, host, path = uri.scheme, uri.host, uri.path
uri.params
"#{scheme}://#{host}#{path}"
uri.class
uri.query
href = '/mobiles/pr?sid=tyy,4io&start=41'
exit
url = 'http://www.flipkart.com/laptops/pr?p%5B%5D=sort%3Dpopularity&sid=6bo%2Cb5g&pincode=560036&filterNone=true'
Scrape.last
Listing.last
listing_id = _.id
Listing.connection.select_all("select id from listings where id = #{listing_id}").first.try(:[], 'url')
Listing.connection.select_all("select id, url from listings where id = #{listing_id}").first.try(:[], 'url')
Listing.connection.select_all("select url from listings where id = #{listing_id}").first.try(:[], 'url')
Listing.connection.select_all("select url from listings where id = #{listing_id}").first
Listing.connection.select_all("select url from listings where id = #{listing_id}").first.try(:[], 'url')
Listing.select(:url).find(listing_id)
Listing.select(:url).find(listing_id).try(:url)
exit
url = 'http://www.flipkart.com/laptops/pr?p%5B%5D=sort%3Dpopularity&sid=6bo%2Cb5g&pincode=560036&filterNone=true'
scrape_id, source_category_id, category_url, retry_time = 1, 1, url, 0
FlipkartProductListingJob
FlipkartProductListingJob.new.perform(scrape_id, source_category_id, category_url, retry_time)
exit
url = 'http://www.flipkart.com/laptops/pr?p%5B%5D=sort%3Dpopularity&sid=6bo%2Cb5g&pincode=560036&filterNone=true'
scrape_id, source_category_id, category_url, retry_time = 1, 1, url, 0
FlipkartProductListingJob.new.perform(scrape_id, source_category_id, category_url, retry_time)
exit
url = 'http://www.flipkart.com/laptops/pr?p%5B%5D=sort%3Dpopularity&sid=6bo%2Cb5g&pincode=560036&filterNone=true'
scrape_id, source_category_id, category_url, retry_time = 1, 1, url, 0
FlipkartProductListingJob.new.perform(scrape_id, source_category_id, category_url, retry_time)
exit
url = 'http://www.flipkart.com/laptops/pr?p%5B%5D=sort%3Dpopularity&sid=6bo%2Cb5g&pincode=560036&filterNone=true'
scrape_id, source_category_id, category_url, retry_time = 1, 1, url, 0
FlipkartProductListingJob.new.perform(scrape_id, source_category_id, category_url, retry_time)
exit
url = 'http://www.flipkart.com/laptops/pr?p%5B%5D=sort%3Dpopularity&sid=6bo%2Cb5g&pincode=560036&filterNone=true'
scrape_id, source_category_id, category_url, retry_time = 1, 1, url, 0
FlipkartProductListingJob.new.perform(scrape_id, source_category_id, category_url, retry_time)
exit
SourceCategory.count
SourceCategory.where(source_id: 7)
SourceCategory.where(source_id: 7).count
SourceCategory.where(source_id: 7, test_process: true)
Source.last
exit
l = Listing.last
l.manuals
l = Listing.find 1377038
exit
l = Listing.find 1377038
exit
l = Listing.find 1377038
exit
l = Listing.find 1377038
l.manuals
l.mauals.to_sql
l.manuals.to_sql
Asset.joins("`listing_assets` ON `assets`.`id` = `listing_assets`.`asset_id` WHERE `assets`.`type` IN ('ProductManual') AND `listing_assets`.`listing_id` = ?", id)
id = l.id
Asset.joins("`listing_assets` ON `assets`.`id` = `listing_assets`.`asset_id` WHERE `assets`.`type` IN ('ProductManual') AND `listing_assets`.`listing_id` = ?", id)
Asset.joins("`listing_assets` ON `assets`.`id` = `listing_assets`.`asset_id` WHERE `assets`.`type` IN ('ProductManual') AND `listing_assets`.`listing_id` = ?", id).first
Listing.where(id: 3).or(id: 235)
exit
Listing.where(id: 3).or(id: 235)
Listing.find_by(id: 235)
Listing.where(id: 3).or(id: 235).count
Listing.where(id: 3).or(id: 235)
l = Listing.first
l.manuals.to_sql
l = Listing.first
exit
l = Listing.last
l = Listing.first
l.product_manual
l.product_manuals
l.listing_manuals
l = Listing.find 1377038
l.product_manuals
l.listing_manuals
l.listing_manuals.or.product_manuals
l.listing_manuals.or.(l.product_manuals)
l.listing_manuals.or.where(l.product_manuals)
ProductManual.where(l.listing_manuals).to_sql
ProductManual.where(id: l.listing_manuals)
ProductManual.where(id: l.listing_manuals).or(id: l.product_manuals)
ProductManual.where(id: l.listing_manuals).or(id: l.product_manuals).to_sql
exit
edit -t
sp_images = sp_matches.pluck(:image_id).uniq
img = sp_images.first
img = ProductImage.find(img)
img.listings.count
img.listings.flat_map {|l| l.sellers.pluck(:name) }
img.listings.includes(:sellers).flat_map {|l| l.sellers.pluck(:name) }
img.listings.includes(:scrape_listing_sellers, :sellers).flat_map {|l| l.sellers.pluck(:name) }
img.listings.includes({scrape_listing_sellers: :sellers}, {sellers: :scrape_listing_sellers}).flat_map {|l| l.sellers.pluck(:name) }
img.listings.includes({scrape_listing_sellers: :seller}, {sellers: :scrape_listing_sellers}).flat_map {|l| l.sellers.pluck(:name) }
img.listings.includes({scrape_listing_sellers: :seller}, :sellers).flat_map {|l| l.sellers.pluck(:name) }
img.listings.includes({scrape_listing_sellers: :seller}, :sellers).flat_map {|l| l.sellers.pluck(:name) }.uniq
img.listings.pluck(:url).uniq
img.listings.includes({scrape_listing_sellers: :seller}, :sellers).flat_map {|l| l.sellers.pluck(:name) }.uniq
img
img.id.to_sym
img.id.to_s.to_sym
edit -t
img
img.atachment.url
img.attachment.url
img.attachment.url.gsub(/-dev/)
img.attachment.url.gsub(/-dev/, '')
img.attachment.url.gsub(/-dev/, '').gsub(/\/development/, '')
results = {}
matches = ImageLogoMatch.where("id > 1483")
exit
edit -t
exit
Source.uniq.pluck(:name)
exit
Source.uniq.count
Source.uniq.pluck(:name)
exit
source = Source.find(1)
source.source_categories.where(active: false)
Source.find(2).source_categories.where(active: false)
Source.find(3).source_categories.where(active: false)
Source.find(4).source_categories.where(active: false)
Source.find(5).source_categories.where(active: false)
Source.find(6).source_categories.where(active: false)
Source.find(7).source_categories.where(active: false)
Source.find(7).source_categories.where(active: [nil, false])
Source.find(3).source_categories.where(active: [nil, false])
Source.find(1).source_categories.where(active: [nil, false])
Source.find(2).source_categories.where(active: [nil, false])
Source.find(3).source_categories.where(active: [nil, false])
Source.find(\4).source_categories.where(active: [nil, false])
Source.find(4).source_categories.where(active: [nil, false])
Source.find(5).source_categories.where(active: [nil, false])
Source.find(6).source_categories.where(active: [nil, false])
SourceCategory.where(active: [false, nil])
Source.find(5).source_categories.unscoped.where(active: [nil, false])
Source.find(5).categories
Source.find(5).categories.count
Source.find(4).categories.count
Source.find(3).categories.count
Source.find(2).categories.count
Source.find(1).categories.count
Source.find(7).categories.count
Source.find(7).source_categories
exit
sc = Source.find(7).source_categories.first
sc.update(url: 'http://www.flipkart.com/computers/video-players/pr?p%5B%5D=sort%3Dpopularity&sid=6bo%2Cxdz&pincode=560036&filterNone=true')
exit
sc = SourceCategory.where(source_id: 7)
sc = sc.first
sc.update test_process: true
:w
sc
sc.active = true
sc.save
sc
sc.update(active: false, test_process: false)
sc = SourceCategory.find(1674)
:w
s = Scrape.last
s.running = false
s.save
sc
path, query = sc.url.split('?')
source.name =~ /gs/).nil?
(source.name =~ /gs/).nil? 
source = sc.source
(source.name =~ /gs/).nil? 
url
url = sc.url
source = Source.find(source_id = 7)
return if Scrape.where(running: true, source_id: source_id).any?
Scrape.where(running: true).update(running: false)
Scrape.where(running: true).update_all(running: false)
scrape = Scrape.create source: source, running: true
ScrapeSource.create(scrape: scrape, source: source)
sc
source_category_ids = [1674]
source_categories = source_category_ids.present? ?
source.source_categories.where(id: source_category_ids) : source.source_categories.active
source.source_categories.where(id: source_category_ids)
sc
source
scrape
scrape.destroy
Scrape.where(source_id: 7).destroy_all
exit
$detail_job_count
exit
source = Source.find(7)
source.update(job_name: 'FlipkartProductListingJob')
scrape = Scrape.last
scrape.destroy
s = Scrape.last
s.destroy
s = Scrape.last
s.destroy
s.listings
s.listings.destroy_all
s.destroy
exit
SourceCategory.find(1674)
sc = _
sc.url
edit -t
urls.count
urls.uniq.count
clean_urls = urls.map {|url| url.gsub(/\?.+$/, '')
clean_urls = urls.map {|url| url.gsub(/\?.+$/, '') }
clean_urls.uniq.count
old_url = 'http://www.flipkart.com/tools-hardware/tools/alternate-energy/solar-panels/pr?sid=amz,qrp,wa4,i3o&start=61'
SourceCategory.where(source_id: 7, active: true)
SourceCategory.where(source_id: 7, active: true).first
reload!
SourceCategory.where(source_id: 7, active: true).first
SourceCategory.where(source_id: 7, active: true)
sc = SourceCategory.where(source_id: 7, test_process: true)
sc = sc.first
sc.url
SourceCategory.count
SourceCategory.connection
SourceCategory.count
reload!
SourceCategory.count
SourceCategory.activecount
SourceCategory.active.count
SourceCategory.where(source_id: 7).active.count
SourceCategory.where(source_id: 6).active.count
SourceCategory.where(source_id: 5).active.count
SourceCategory.where(source_id: 4).active.count
SourceCategory.where(source_id: 3).active.count
SourceCategory.where(source_id: 2).active.count
SourceCategory.where(source_id: 1).active.count
@no_scroll
exit
empty_v.present?
@empty_v.present?
TrueClass(empty_v)
TrueClass(@empty_v)
TrCla
TrueClass
TrueClass.new(true)
TrueClass.class
Class
Class.class
"@#{some_var}".intern
s = Scrape.last
s.listings.count
s.products.count
s.listings.first
l = _
l.url
s.products.count
s.listings.count
s.products.count
l = s.listings.last
l.products
l.data_hash
s.listings.count
s.products.count
l = Listing.last
l.products
l.images
l.images.count
l.url
l.scrape_listing_sellers
sls = _.first
sls.currency
ProductImage.count
l = s.listings.first
l.images
l.images.count
l.url
l.sellers
l.sellers.pluck(:nameO
l.sellers.pluck(:name)
l.sellers.last
ScrapeListingSeller.where(seller_id: 45, listing_id: l.id)
ScrapeListingSeller.where(seller_id: 45, listing_id: l.id).min_price
ScrapeListingSeller.where(seller_id: 45, listing_id: l.id).first.min_price
ScrapeListingSeller.where(seller_id: 45, listing_id: l.id).first.min_price.to_f
l = s.listings.last
l.sellers
s.listings.count
url = 'http://www.flipkart.com/minda-spv36-12v-40w-solar-panel/p/itmecxjsrtrrt6pp'
l = Listing.where(url: url).first
l.sellers
exit
s = Scrape.last
s.products.map {|p| p.manuals.count }
counts = _
counts.sum
p = s.products.first
p.manuals
p.manuals.confirmed
s.products.includes(:product_product_manuals).map {|p| p.product_product_manuals.confirmed.size }.sum
exit
url = 'http://www.flipkart.com/hp-15-ac178tx-t0z57pa-acj-intel-core-i5-6th-gen-8-gb-ddr3-1-tb-hdd-windows-10-2-graphics-notebook/p/itmeeffdgezkkzca'
Scrape.count
Scrape.last
source = Source.find 7
SourceCategory.where(source: source)
SourceCategory.where(source: source).active
SourceCategory.where(source: source).active.first.id
scrape_id, source_category_id, url = 4, 2485, url, 0
Scrape.find 4
Scrape.last
scrape_id, source_category_id, url = 10, 2485, url, 0
Listing.where(url: url)
scrape_id, source_category_id, url
[scrape_id, source_category_id, url]
scrape_id, source_category_id, url, retry_time = 10, 2485, url, 0
FlipkartListingDetailJob
FlipkartListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
url
price = 'Rs. 12355.50'
price.gsub(/\A.+(?=\d+)/, '')
price.gsub(/\A.+(?=\d)/, '')
price.gsub(/\^(.+?)\d/, '')
price.gsub(/^(.+?)\d/, '')
price.gsub!(/^(.+?)\./, '')
price.gsub!(/^(.+?)\s/, '')
price = 'Rs. 12355.50'
price.gsub(/^(.+?)\s/, '')
price.gsub(/^(.+?)\s?/, '')
price.gsub(/^(.+)\s?/, '')
price.delete('^0-9.,')
price.gsub(/Rs\.?\s?/, '')
l = Listing.last
l.url
url
l.destroy
exit
url = 'http://www.flipkart.com/hp-15-ac178tx-t0z57pa-acj-intel-core-i5-6th-gen-8-gb-ddr3-1-tb-hdd-windows-10-2-graphics-notebook/p/itmeeffdgezkkzca'
scrape_id, source_category_id, url, retry_time = 10, 2485, url, 0
FlipkartListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l.sellers
l.scrape_listing_sellers
sls = l.scrape_listing_sellers.first
sls.min_price.to_f
sls.currency
url = "http://www.flipkart.com/greenmax-sunstar-1215-solar-panel/p/itmecnh2m6ngjstq?pid=SLPECNH2ZZ6MVKD6&al=Fw68AfYO0q5N8TUbgQURTsldugMWZuE7eGHgUTGjVrod49SIHznd7F8OxVaLr00BchJiOSVdoGQ%3D&ref=L%3A2595284963183446767&srno=b_57"
SourceCategory.where(source: source).active.last.id
SourceCategory.where(source_id: 7).active.last.id
SourceCategory.where(source_id: 7).active.last
SourceCategory.where(source_id: 7).active.last.id
source_category_id = _
url = "http://www.flipkart.com/greenmax-sunstar-1215-solar-panel/p/itmecnh2m6ngjstq?pid=SLPECNH2ZZ6MVKD6&al=Fw68AfYO0q5N8TUbgQURTsldugMWZuE7eGHgUTGjVrod49SIHznd7F8OxVaLr00BchJiOSVdoGQ%3D&ref=L%3A2595284963183446767&srno=b_57"
FlipkartListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
Listing.where(url: url)
Listing.where(original_url: url)
url
Listing.where(url: url.gsub(/\?.+$/, ''))
l = _.first
l
l.destroy
FlipkartListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l.sellers
seller = []
sellers << nil
seller << nil
seller
seller.compact
l
url
l.destroy
exit
url = "http://www.flipkart.com/greenmax-sunstar-1215-solar-panel/p/itmecnh2m6ngjstq?pid=SLPECNH2ZZ6MVKD6&al=Fw68AfYO0q5N8TUbgQURTsldugMWZuE7eGHgUTGjVrod49SIHznd7F8OxVaLr00BchJiOSVdoGQ%3D&ref=L%3A2595284963183446767&srno=b_57"
scrape_id, source_category_id, url, retry_time = 10, 2485, url, 0
SourceCategory.find(source_category_id)
source_category_id = SourceCategory.where(source_id: 7).active.last.id
SourceCategory.find(source_category_id)
FlipkartListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l.screenshots.first.url
l.screenshots.first.attachment.url
l.sellers
s.scrape_listing_sellers
l.scrape_listing_sellers
l.scrape_listing_sellers.first
sls = _
sls.seller_url
href = "http://www.flipkart.com/seller/greenmaxsystems/d74c55a8d2014faa"
s = Scrape.last
s.listings.flat_map {|l| l.scrape_listing_sellers.pluck(:seller_url) }
exit
source_category_id = SourceCategory.where(source_id: 7).active.last.id
url = "http://www.flipkart.com/greenmax-sunstar-1215-solar-panel/p/itmecnh2m6ngjstq?pid=SLPECNH2ZZ6MVKD6&al=Fw68AfYO0q5N8TUbgQURTsldugMWZuE7eGHgUTGjVrod49SIHznd7F8OxVaLr00BchJiOSVdoGQ%3D&ref=L%3A2595284963183446767&srno=b_57"
Listing.last.destroy
FlipkartListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
scrape_id, source_category_id, url, retry_time = 10, 2485, url, 0
source_category_id = SourceCategory.where(source_id: 7).active.last.id
FlipkartListingDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
url
l.screenshots.first.attachment.url
exit
Listing.last
l = _
l.url
l.id
exit
l = Listing.last
l.data_hash
data = _
data[:GENERAL]
data[:"Specifications of Greenmax Sunstar 1215 Solar Panel"]
data[:"Specifications of Greenmax Sunstar 1215 Solar Panel"][:GENERAL]
data[:"Specifications of Greenmax Sunstar 1215 Solar Panel"][:GENERAL][:Brand]
data
data[:General]
Listing.last 3
l = _.first
data = l.data_hash
Scrape.where(running: true)
Scrape.where(running: true).update_all(running: false)
l2 = Listing.last
l
l2
Scrape.where(running: true)
Scrape.where(running: true).update_all(running: false)
l2 = Listing.last
l
l.data_hash
l
exit
l = Listing.find(101)
data_hash = l.data_hash
l.data_hash.to_json
l.data_hash.as_json
l = Listing.find(401)
l = Listing.find(98)
l.data_hash.as_json
l.id
l
data = l.data_hash
data[:General]
data[:"Specifications of HP 15-AC178TX T0Z57PA#ACJ Intel Core i5 (6th Gen) - (8 GB DDR3/1 TB HDD/Windows 10/2 GB Graphics) Notebook (15.6 inch, SIlver)"]
data
data[:"Specifications of HP 15-AC178TX T0Z57PA#ACJ Intel Core i5 (6th Gen) - (8 GB DDR3/1 TB HDD/Windows 10/2 GB Graphics) Notebook (15.6 inch, SIlver)"]
data[:"Specifications of HP 15-AC178TX T0Z57PA#ACJ Intel Core i5 (6th Gen) - (8 GB DDR3/1 TB HDD/Windows 10/2 GB Graphics) Notebook (15.6 inch, SIlver)"][:SOFTWARE]
data[:"Specifications of HP 15-AC178TX T0Z57PA#ACJ Intel Core i5 (6th Gen) - (8 GB DDR3/1 TB HDD/Windows 10/2 GB Graphics) Notebook (15.6 inch, SIlver)"][:INPUT]
data.as_json
l2
l2 = Listing.find(401)
l2.data_hash
l2.data_hash.as_json
Listing.all.sample.id
exit
sls = ScrapeListingSeller.first
sls.currency
sls.min_price
Monetize.parse("#{sls.currency.iso_code} sls.min_price")
Monetize.parse("#{sls.currency.iso_code} sls.min_price").to_f
Monetize.parse("#{sls.currency.iso_code} sls.min_price.to_f").to_f
sls.min_price
sls.min_price.to_f
Monetize.parse("#{sls.currency.iso_code} #{sls.min_price}").to_f
Monetize.parse("#{sls.currency.iso_code} #{sls.min_price}").exchange_to(:USD)
ScrapeListingSeller.where(currency_id: nil)
ScrapeListingSeller.where(min_price: nil)
ScrapeListingSeller.where(max_price: nil)
:w
sls
exchanged_min = Monetize.parse("#{sls.currency.iso_code} #{sls.min_price}").exchange_to(:USD)
ScrapeListingSeller.select(:id).first
sls = _
sls.id
sls.class
sls.attributes
l = Listing.first
edit -t
exit
p = Product.find(12838)
exit
ProductManualJob.new.perform(1,1)
exit
ProductManualJob.new.perform(1,1)
exit
ProductManualJob.new.perform(1,1)
exit
ProductManualJob.new.perform(1,1)
exit
ProductManualJob.new.perform(1,1)
exit
ProductManualJob.new.perform(1,1)
exit
ProductManualJob.new.perform(1,1)
exit
ProductManualJob.new.perform(1,1)
exit
ProductManualJob.new.perform(1,1)
exit
ProductManualJob.new.perform(1,1)
reload!
ProductManualJob.new.perform(1,1)
exit
ProductManualJob.new.perform(1,1)
exit
ProductManualJob.new.perform(1,1)
exit
ProductManualJob.new.perform(1,1)
exit
ProductManualJob.new.perform(1,1)
exit
save_path = Rails.root.join('filename.pdf')
save_path = Rails.root.join('tmp', 'filename.pdf')
exit
ProductManualJob.new.perform(1,1)
exit
ProductManualJob.new.perform(1,1)
exit
WickedPdf.new.pdf_from_url(
url = Listing.last.url
pdf = WickedPdf.new.pdf_from_url(url)
edit -t
ProductManualJob.new.perform(1,1)
exit
ProductManualJob.new.perform(1,1)
exit
ProductManualJob.new.perform(1,1)
exit
ProductManualJob.new.perform(1,1)
exit
ProductManualJob.new.perform(1,1)
exit
ProductManualJob.new.perform(1,1)
exit
ProductManualJob.new.perform(1,1)
exit
ProductManualJob.new.perform(1,1)
exit
ProductManualJob.new.perform(1,1)
exit
ProductManualJob.new.perform(1,1)
url = 'http://www.manualslib.com/manual/783200/Emerson-Le391em4.html'
page = Nokogiri::HTML(open(url))
page.css('.manual_here')
pdf = _
pdf.text
pdf
pdf.html
pdf.class
pdf.first
pdf = pdf.first
pdf.html
pdf.inner_html
pdf.html
pdf.to_html
page.css('.pnext')
page.css('.pnext')[:href]
page.css('.pnext')['href']
page.css('.pnext').first
link = page.css('.pnext').first
link[:href]
page.url
link = page.css('.pnext').first
link['href']
link = page.css('.pnext').first[:href]
url = "http://www.manualslib.com#{link}"
pdf
pdf.to_html
pdf = WickedPdf.new.pdf_from_string(pdf, encoding: 'UTF-8')
save_path = Rails.root.join('tmp', 'page1.pdf')
File.open(save_path, 'wb') do |file|
  file << pdf
end
exit
ProductManualJob.new.perform(1,1)
exit
ProductManualJob.new.perform(1,1)
url = 'http://www.manualslib.com/manual/783200/Emerson-Le391em4.html'
page = Nokogiri::HTML(open(url))
link = page.css('.pnext').first[:href]
url = 'http://www.manualslib.com/manual/783200/Emerson-Le391em4.html?page=38#manual'
page = Nokogiri::HTML(open(url))
link = page.css('.pnext').first[:href]
link = page.css('.pnext')
link = page.css('.pnext').try(:first)
exit
ProductManualJob.new.perform(1,1)
exit
ProductManualJob.new.perform(1,1)
edit -t
pop_proxy
ENV['PROXY_USERNAME']
ENV['PROXY_PASSWORD']
page
url = 'http://www.manualslib.com/manual/783200/Emerson-Le391em4.html'
page = Nokogiri::HTML(open(url))
exit
ProductManualJob.new.perform(1,1)
exit
ProductManualJob.new.perform(1,1)
exit
ProductManualJob.new.perform(1,1)
exit
ProductManualJob.new.perform(1,1)
exit
ProductManualJob.new.perform(1,1)
exit
ProductManualJob.new.perform(1,1)
ENV['PROXY_USERNAME']
exit
ProductManualJob.new.perform(1,1)
URI.parse(url)
url = 'http://www.manualslib.com/manual/783200/Emerson-Le391em4.html'
URI.parse(url)
page = Nokogiri::HTML(open(
    url,
    proxy_http_basic_authentication: [pop_proxy, ENV['PROXY_USERNAME'], ENV['PROXY_PASSWORD']]
  )
)
edit -t
page = Nokogiri::HTML(open(
    url,
    proxy_http_basic_authentication: [pop_proxy, ENV['PROXY_USERNAME'], ENV['PROXY_PASSWORD']]
  )
)
# proxy_http_basic_authentication: [pop_proxy, ENV['PROXY_USERNAME'], ENV['PROXY_PASSWORD']]
)
page = Nokogiri::HTML(open(
    url
    # proxy_http_basic_authentication: [pop_proxy, ENV['PROXY_USERNAME'], ENV['PROXY_PASSWORD']]
  )
)
pop_proxy.class
pop_proxy
"#http://{pop_proxy}"
"http://#{pop_proxy}"
exit
ProductManualJob.new.perform(1,1)
exit
ProductManualJob.new.perform(1,1)
exit
ProductManualJob.new.perform(1,1)
pdf_url = 'http://pdfstream.manualsonline.com/8/8647c64e-1b68-417b-b977-baa4f8a3183a.pdf'
exit
ProductManualJob.new.perform(1,1)
ManualslibJob.new.perform(1,1)
exit
ManualslibJob.new.perform(1,1)
url = 'http://www.manualslib.com/manual/687728/Dell-P2210h.html#manual'
open(url,
  proxy_http_basic_authentication: ["http://#{pop_proxy}", ENV['PROXY_USERNAME'], ENV['PROXY_PASSWORD']]
) do |html|
  page = Nokogiri::HTML(html)
  page.class
end
edit -t
open(url,
  proxy_http_basic_authentication: ["http://#{pop_proxy}", ENV['PROXY_USERNAME'], ENV['PROXY_PASSWORD']]
) do |html|
  page = Nokogiri::HTML(html)
  page.class
end
pdf = page.css('.manual_here').try(:first)
exit
ManualslibJob.new.perform(1,1)
exit
ScrapeListingSeller.first
ScrapeListingSeller.first.exchanged_min
ScrapeListingSeller.first.exchanged_min.to_f
exit
ManualslibJob.new.perform(1,1)
exit
edit -t
scrape_name = 'Hisense import'
file_path = '/Users/jonathan/Downloads/hisense1'
file_hash = Digest::MD5.hexdigest(DateTime.now.to_s)
source = Source.find_by(file_hash: file_hash)
if source.nil?
  source = Source.create name: scrape_name,
  # upload_url: url,
  upload_url: "import/#{DateTime.now.strftime('%Y%m%d%H%M%S')}",
  source_type_id: SourceType::IMPORT,
  file_hash: file_hash
end
tmp_folder_path = file_path
data_file_paths = Dir["#{tmp_folder_path}/*.csv", "#{tmp_folder_path}/*.xlsx", "#{tmp_folder_path}/*.xls"]
scrape = Scrape.create(
  source: source,
  name: scrape_name,
  running: true,
  created_at: start_time
)
start_time = DateTime.now
scrape = Scrape.create(
  source: source,
  name: scrape_name,
  running: true,
  created_at: start_time
)
f = open_spreadsheet data_file_paths.first
first = f.first_row
key_mapping = generate_key_mapping f.row(first)
data_hash_keys = key_mapping.keys - @keys
row = f.row(first + 1)
data = {}
@keys.each do |key|
  value = key_mapping[key]
  data[key] = fix_values(row[value]) unless value.nil?
end
edit -t
@keys.each do |key|
  value = key_mapping[key]
  data[key] = fix_values(row[value]) unless value.nil?
end
data_hash_keys.each do |key|
  value = fix_values(row[key_mapping[key]])
  next if value.blank?
  data_hash_category = :General
  data[:data_hash] ||= {}
  data[:data_hash][data_hash_category] ||= {}
  data[:data_hash][data_hash_category][key] = value
end
scrape
data
name = data[:product_name]
brand = data[:brand]
model = data[:model] || data[:mpn]
data ||= {}
data[:data_hash] ||= {}
data[:data_hash][:General] ||= {}
data
if data[:brand].present?
  data[:data_hash][:General][:'Brand'] = data[:brand]
end
data
if data[:model].present?
  data[:data_hash][:General][:'Model'] = data[:model]
end
if data[:mpn].present?
  data[:data_hash][:General][:'MPN'] = data[:mpn]
end
data
alternate_models = data[:alternate_models] && data[:alternate_models].split(',').map(&:strip).reject(&:blank)
product_url = if data[:product_url].present?
  data[:product_url].gsub(/\?.+$/, '')
elsif data[:original_url].present?
  data[:original_url].gsub(/\?.+$/, '')
else
  "import_scrape/#{scrape.id}/#{Digest::MD5.hexdigest(data[:data_hash].to_s + DateTime.now.to_s)}"
end
data
row = f.row(first + 2)
data = {}
@keys.each do |key|
  value = key_mapping[key]
  data[key] = fix_values(row[value]) unless value.nil?
end
data_hash_keys.each do |key|
  value = fix_values(row[key_mapping[key]])
  next if value.blank?
  data_hash_category = :General
  data[:data_hash] ||= {}
  data[:data_hash][data_hash_category] ||= {}
  data[:data_hash][data_hash_category][key] = value
end
name = data[:product_name]
brand = data[:brand]
model = data[:model] || data[:mpn]
if data[:brand].present?
  data[:data_hash][:General][:'Brand'] = data[:brand]
end
if data[:model].present?
  data[:data_hash][:General][:'Model'] = data[:model]
end
if data[:mpn].present?
  data[:data_hash][:General][:'MPN'] = data[:mpn]
end
alternate_models = data[:alternate_models] && data[:alternate_models].split(',').map(&:strip).reject(&:blank)
data[:product_url]
data[:product_url].split(' ')
data[:product_url]
data[:product_url].gsub(/\s+/, '')
data[:product_url].strip.gsub(/\s+$/, '')
data[:product_url].strip.gsub(/\s+.+$/, '')
data[:product_url].strip.gsub(/\s.+$/, '')
exit
ImportScrapeDataJob.new.perform('Hisense Test', '/Users/jonathan/Downloads/hisense1')
s = Scrape.last
l = s.listings.first
l.products.count
s.listings.map {|l| [l.id, l.screenshots.count] }
l = Listing.find 405
l.products.count
l.screenshots.count
l.screenshots
l.screenshots.last.attachment.url
s.listings.count
s.listings.first.products.first.categories.pluck(:name)
Source.last
s.listings.first.categories.pluck(:name)
s.listings.count
s.products.count
exit
s = Scrape.last
l = s.listings.second
s.manuals
l.manual
l.manuals
exit
ListingUrl
ListingUrl.connection
ListingUrl
l = ListingUrl.create(url: 'www.jonathan.test.com')
l = ListingUrl.create(url: 'www.jonathan.test.com.1345', url_md5: 'md5testasdfa')
ListingUrl.last(2).map(&:destroy)
exit
Product.import
Listing.import
exit
Product.where(brand: 'Sony')
Product.where(brand: 'Apple')
p = _
p.listings
p = p.first
p.listings
l = Listing.first
l.products
operator = 'AND'
terms = { keywords: nil.to_s }
@listings = []
filters = {}
filters[:brand] = params[:brand].split(',').map(&:strip) if params[:brand].present?
filters[:brand] = ['asus']
filters
per_page = 10
search_response = Listing.search_by(terms, operator, filters, { size: per_page, scroll: '3m' })
lids = search_response.results.map { |l| l.id }.compact
Listing          total = search_response.results.total
total = search_response.results.total
Product.where(brand: 'asus').size
Product.where(brand: 'Asus').size
Listing.find 5
Product.find 5
total = search_response.results.total
lids = search_response.results.map { |l| l.id }.compact
Listing.import
search_response = Listing.search_by(terms, operator, filters, { size: per_page, scroll: '3m' })
lids = search_response.results.map { |l| l.id }.compact
operator = 'AND'
terms = { keywords: nil.to_s }
@listings = []
filters = {}
filters[:brand] = ['asus']
per_page = 10
search_response = Listing.search_by(terms, operator, filters, { size: per_page, scroll: '3m' })
lids = search_response.results.map { |l| l.id }.compact
total = search_response.results.total
scroll_id = search_response.response._scroll_id
@listings += Listing.includes(
  :listing_url,
  :products,
  :categories,
  :identifiers,
  :images,
  :scrape_listing_sellers,
  { sellers: :images },
  :scrape,
  :source,
  :screenshots,
:manuals).where(id: lids)
search_response
Listing.first
Listing.find 5
Listing.delete_index!
Product.delete_index!
Listing.import
Product.import
search_response = Listing.search_by(terms, operator, filters, { size: per_page, scroll: '3m' })
lids = search_response.results.map { |l| l.id }.compact
exit
Listing.mappings
Listings.all.flat_map {|l| l.products.pluck(:brand) }
Listing.all.flat_map {|l| l.products.pluck(:brand) }
brands = _
brands.sort
exit
edit -t
exit
url = 'http://www.amazon.com/Philips-Morning-Wake-Up-Simulation-HF3520/dp/B0093162RM'
AmazonProductDetailJob.new.perform(1,1, url, 0)
exit
url = 'http://www.amazon.com/Philips-Morning-Wake-Up-Simulation-HF3520/dp/B0093162RM'
AmazonProductDetailJob.new.perform(1,1, url, 0)
l = Listing.last
l.identifiers
l.reload
l.sellers
l.sellers.count
l.scrape_listing_sellers
l.scrape_listing_sellers.count
l = Listing.last
l.identifiers
l.map(&:id_type)
l.identifiers.map(&:id_type)
Digest::MD5.hexdigest(Time.now)
Digest::MD5.hexdigest(DateTime.now)
Digest::MD5.hexdigest(DateTime.now.to_s)
Digest::MD5.hexdigest(Time.now.to_s)
CombinePdf
exit
CombinePdf
exit
CombinePDF
full_pdf = CombinePDF
full_pdf = CombinePDF.new
full_pdf << CombinePDF.load('/Users/jonathan/rvx-rds/tmp/full.pdf')
full_pdf.text
full_pdf.to_s
full_pdf
full_pdf.pages
full_pdf.pages.first
full_pdf.pages.first.to_s
text = Youm.read :text, full_pdf
text = Yomu.read :text, full_pdf
text = Yomu.read  full_pdf
pdf = Yomu.new('/Users/jonathan/rvx-rds/tmp/full.pdf')
pdf.text
pdf.close
pdf.text
pdf.text.length
content = pdf.text.strip.gsub(/\s+/, ' ').squeeze('.').downcase.slice(0, 100000)
exit
l = Listing.last
p = l.products.first
ManualslibJob.new.perform(p.id, l.id)
p.id
Product.find 152
exit
l = Listing.last
p = l.products.first
ManualslibJob.new.perform(p.id, l.id)
exit
l = Listing.last
p = l.products.first
ManualslibJob.new.perform(p.id, l.id)
exit
l = Listing.last
p = l.products.first
ManualslibJob.new.perform(p.id, l.id)
p.manuals
p.manuals.first
p.manuals.first.attachment
p.manuals.first.attachment.url
exit
l = Listing.last
p = l.products.first
url = 'http://www.manualslib.com/manual/416337/Philips-Hf3520-60.html'
ParsePdfJob.new.perform(p.id, l.id, url)
p.manuals
p.manuals.first
p.manuals.first.attachment.url
path = '/Users/jonathan/rvx-rds/tmp/152-ee1bb453f1d4ba0152a51e7df8f61fca.pdf'
exit
l = Listing.last
url = 'http://www.manualslib.com/manual/416337/Philips-Hf3520-60.html'
p = l.products.first
ParsePdfJob.new.perform(p.id, l.id, url)
exit
exi
exit
url = 'http://www.manualslib.com/manual/416337/Philips-Hf3520-60.html'
l = Listing.last
p = l.products.first
ParsePdfJob.new.perform(p.id, l.id, url)
exit
l = Listing.last
p = l.products.first
url = 'http://www.manualslib.com/manual/416337/Philips-Hf3520-60.html'
ParsePdfJob.new.perform(p.id, l.id, url)
exit
l = Listing.last
p = l.products.first
url = 'http://www.manualslib.com/manual/416337/Philips-Hf3520-60.html'
ParsePdfJob.new.perform(p.id, l.id, url)
exit
l = Listing.last
p = l.products.first
url = 'http://www.manualslib.com/manual/416337/Philips-Hf3520-60.html'
ParsePdfJob.new.perform(p.id, l.id, url)
p.manuals
p.product_product_manuals
ppm p.product_product_manuals.first
ppm = p.product_product_manuals.first
ppm.review_pdf_manual
ppm.reload
ppm.confirmed?
ppm.original_url
ppm.original_url.try(:include?, 'anything')
ppm
ppm.id
manual = ppm.manual
manual.original_url
url = manual.original_url
host = URI.parse(url).host
(host =~ Resolv::IPv4::Regex) || (host =~ Resolv::IPv6::Regex) ? true : false
manual.text
manual.text.nil?
product = ppm.product
pdf_content = manual.text.downcase
def check_brand_model(product, pdf_content)
  pdf_content =~ (/#{Regexp.escape(product.brand.downcase)}.+#{Regexp.escape(product.model.downcase)}/) ? true : false
end
def user_manual?(pdf_content)
  pdf_content =~ (/user\s(manual|guide)/)
end
def has_specs?(pdf_content)
  pdf_content =~ (/(product|technical)\s(specifications|specs)/)
end
def is_ip?(url)
  host = URI.parse(url).host
  (host =~ Resolv::IPv4::Regex) || (host =~ Resolv::IPv6::Regex) ? true : false
end
def is_catalog?(url)
  url = url.downcase
  url.include?('catalog') || %w(price list).all? { |word| url.include?(word) }
end
check_brand_model(product, pdf_content)
pdf_content
model
p.model
pdf_content.include?(_)
pdf_content.include?('hf3520')
pdf_content.include?('philips')
p
p.model
p.model.gsub(/[^a-z0-9\s]/i/, ' ')
p.model.gsub(/[^a-z0-9\s]/i, ' ')
p.model.split(/[^a-z0-9\s]/i)
p.model.split(/[^a-z0-9\s]/i).max
p.model.split(/[^a-z0-9\s]/i).sort
p.model.split(/[^a-z0-9\s]/i).max
product = p
model_substrs = product.model.downcase.split(/[^a-z0-9\s]/i)
"(#{model_substrs.join('|')})"
sub_regex = "(#{model_substrs.join('|')})"
pdf_content =~ (/#{Regexp.escape(product.brand.downcase)}.+#{Regexp.escape(sub_regex)}/) ? true : false
Regexp.escape(sub_regex)
pdf_content
pdf_content =~ Regexp.escape(sub_regex)
pdf_content =~ /Regexp.escape(sub_regex)/
sub_regex = "(#{model_substrs.join('|')})"
Regexp.escape(sub_regex)
pdf_content =~ (/#{Regexp.escape(product.brand.downcase)}.+#{sub_regex}/) ? true : false
sub_regex = "(#{model_substrs.join('|')})"
sub_regex = "60"
pdf_content =~ (/#{Regexp.escape(product.brand.downcase)}.+#{sub_regex}/) ? true : false
sub_regex = "hf3520"
pdf_content =~ (/#{Regexp.escape(product.brand.downcase)}.+#{sub_regex}/) ? true : false
product.model.downcase.split(/[^a-z0-9\s]/i).max_by(&:length)
sub_model = product.model.split(/[^a-z0-9\s]/i).max_by(&:length)
pdf_content =~ (/#{Regexp.escape(product.brand.downcase)}.+#{sub_model}/) ? true : false
sub_model = product.model.split(/[^a-z0-9\s]/i).max_by(&:length)
pdf_content =~ (/#{Regexp.escape(product.brand.downcase)}.+#{Regex.escape(sub_model)}/) ? true : false
pdf_content =~ (/#{Regexp.escape(product.brand.downcase)}.+#{sub_model}/) ? true : false
pdf_content =~ (/#{Regexp.escape(product.brand.downcase)}.+#{Regexp.escape(sub_model)}/) ? true : false
sub_model = product.model.split(/[^a-z0-9\s]/i).max_by(&:length).downcase
pdf_content.include?(sub_model)
pdf_content.include?(p.brand)
p.brand
pdf_content
pdf_content.include?(p.brand.downcase)
sub_model = product.model.split(/[^a-z0-9\s]/i).max_by(&:length).downcase
pdf_content =~ (/#{Regexp.escape(product.brand.downcase)}.+#{Regexp.escape(sub_model)}/) ? true : false
p.model
Product.count
sum_model
sub_model = product.model.split(/[^a-z0-9\s]/i).max_by(&:length).downcase
sub_model.length.to_f / product.model.length
subs subs = product.model.split(/[^a-z0-9\s]/i)
subs = product.model.split(/[^a-z0-9\s]/i)
sub = subs.delete(subs.sort_by(&:length).last)
sub.length / (subs.map(&:length).inject(0.0) { |sum, el| sum + el } / subs.size)
(subs.map(&:length).inject(0.0) { |sum, el| sum + el } / subs.size) / sub.length
subs = []
(subs.map(&:length).inject(0.0) { |sum, el| sum + el } / subs.size) / sub.length
sub.length / (subs.map(&:length).inject(0.0) { |sum, el| sum + el } / subs.size)
1 - .333333333
1 - 0.333333333
sub.length / (subs.map(&:length).inject(0.0) { |sum, el| sum + el } / subs.size)
subs = product.model.split(/[^a-z0-9\s]/i)
sub = subs.delete(subs.sort_by(&:length).last)
sub.length / (subs.map(&:length).inject(0.0) { |sum, el| sum + el } / subs.size)
exit
FuzzyMatch
p = Product.last
FuzzyMatch.new(p.manuals.first.text).find(p.model)
FuzzyMatch.new([p.manuals.first.text]).find(p.model)
FuzzyMatch.new(['seamus', 'andy', 'ben']).find('Shamus')
FuzzyMatch.new([p.manuals.first.text]).find('Shamus')
FuzzyMatch.new([p.manuals.first.text]).find(p.model)
text = p.manuals.first.text
FuzzyMatch.new([text, 'jonahtn']).find(p.model)
FuzzyMatch.new([text, 'jon']).find('jon')
exit
url = 'http://www.manualslib.com/manual/214746/Belkin-F4p338.html'
Product.first
Listing.first
l = Listing.first
p = Product.first
ParsePdfJob.new.perform(p.id, l.id, url)
p.manuals
p.manuals.confirmed?
p.manuals.confirmed
p.manuals.last
p.manuals.last.confirmed?
p.manuals.last.attachement.url
p.manuals.last.attachment.url
15000000 * 0.01
exit
nil.blank?
exit
Listing.last
l = _
l.screenshots.first.attachment.url
exit
"https://www.google.com/shopping/product/14764708791838261200/specs".gsub(/\/specs$/, '')
exit
path = File.join(Rails.root, 'db', 'seeds', 'rds_categories.xls')
file = path
def open_spreadsheet(file_path)
  case File.extname(file_path)
  when '.csv' then
    Roo::CSV.new(file_path)
  when '.xls' then
    Roo::Excel.new(file_path)
  when '.xlsx' then
    Roo::Excelx.new(file_path)
  else
    fail "Unknown file type: #{file_path}"
  end
end
xlsx = open_spreadsheet(file)
sources = {}
source_name = 'dhgate
'
source_name = 'dhgate'
sheet = xlsx.sheet(source_name)
columns = sheet.first.map { |col| col.downcase.gsub(/\s+/, '_') }
categories = []
xlsx.first_row + 1
i = _
cur_row = xlsx.row(i).map { |row| row.nil?  ? row : row.strip.gsub(/\s+/, ' ') }
xlsx.close
xlsx = open_spreadsheet(file)
cur_row = xlsx.row(i).map { |row| row.nil?  ? row : row.strip.gsub(/\s+/, ' ') }
exit
data_url = 'http://txt.dhstatics.com/product/specificationInfo.do?itemcode=373550063&supplierid=ff8080814b25d806014bcddb2ba41f55'
html = open(data_url)
Nokogiri::HTML(html)
html.close
html
data = {}
open(data_url) do |html|
  desc = Nokogiri::HTML(html)
  data[:'Detailed Description'] = desc.text.strip.gsub(/\s+/, ' ')
end
data
open(data_url) do |html|
  desc = Nokogiri::HTML(html)
  data[:'Detailed Description'] = desc.text.strip.gsub(/\s+/, ' ')
end
data[:'Detailed Description']
exit
@ignore_price = %w(box unit)
regex = Regexp.new @ignore_price.join('|'), Regexp::IGNORECASE
node = Capybara::Node::Simple.new("<html><body class='jonathan'>Hello</body></html>")
node.first('.jonathan')
node.first('.jonathan').text
edit -t
html
node = Capybara::Node::Simple.new(html)
node.all('li')
node.all('#feature-bullets li')
edit -t
html
edit -t
page = Capybara::Node::Simple.new(html)
node.all('#feature-bullets li')
node.all('#feature-bullets li').count
node.all('#feature-bullets li').map(&:text)
node.all('#feature-bullets li').map {|bullet| bullet['id']}
page.all('#feature-bullets li')
page.all('#feature-bullets li').map { |node| node['outerHTML'] }
page.all('#feature-bullets li').map { |node| node['innerHTML'] }
page.all('#feature-bullets li').map { |node| node.html }
page.all('#feature-bullets li').map { |bullet| bullet.text }
page.all('#feature-bullets li').map { |bullet| bullet.class }
page.all('#feature-bullets li').map { |bullet| bullet['class'] }
page.all('#feature-bullets li').map { |bullet| bullet['id'] }
page.all('#feature-bullets li').map { |bullet| bullet.text }
edit -t
page
page.to_s
edit -t
bullets.to_s
bullets.as_json
p bullets
pp bullets
edit -t
page.all("#feature bullets li")
page.all("#feature-bullets li")
edit -t
imgs
imgs.map(&:src)
imgs.first
imgs.map(&:[], :src)
imgs.first[:src]
edit -t
node
node.text
node['outerHTML']
def argument_chk
def argument_chk(*args)
  puts args
end
argument_chk 1, 1, 'somurl', 0
edit -t
node = _
node[:html]
p node
node.path
node.first
node['innerHTML']
node[:title]
node[:class]
node[:id]
node['id']
node.text
edit -t
node['id']
node.first('div')
node.first('div')['id']
node = node.first('div')
node['outerHTML']
node.innerHTML
node.innerHtml
edit -t
manufacture_node
node = _
exit
AmazonProductDetailJob
AmazonProductDetailJob.new.perform(1,1,'http://www.amazon.com/Autel-Robotics-X-Star-Premium-1-2-Mile/dp/B01B1H8322?ie=UTF8&fpl=fresh&redirect=true&ref_=s9_simh_gw_g421_i1_r', 0)
l = Listing.last
l.destroy
AmazonProductDetailJob.new.perform(1,1,'http://www.amazon.com/Autel-Robotics-X-Star-Premium-1-2-Mile/dp/B01B1H8322?ie=UTF8&fpl=fresh&redirect=true&ref_=s9_simh_gw_g421_i1_r', 0)
l = Listing.last
l.url.gsub(/\/(ref=).+$/, '')
l.url.gsub(/\/(ref=).+$/, '').gsub(/\?.+/, '')
exit
l = Listing.last
l.destroy
AmazonProductDetailJob.new.perform(1,1,'http://www.amazon.com/Autel-Robotics-X-Star-Premium-1-2-Mile/dp/B01B1H8322?ie=UTF8&fpl=fresh&redirect=true&ref_=s9_simh_gw_g421_i1_r', 0)
l = Listing.last
l.destroy
exit
'http://www.airyear.com/images/D/218501.jpg,                                                                                                                                                                                                                                                                                                                                                                                                                                http://www.airyear.com/images/D/218501_1.jpg,                                                                                                                                                                                                                                                                                                                                                                                                                                        http://www.airyear.com/images/D/218501_2.jpg'
exit
file = '/Users/jonathan/Downloads/KL_May 2016 SunPower_20160526_audit Final.xlsx'
edit -t
@keys = [
  :product_name,
  :ruvixx_category,
  :product_category,
  :product_url,
  :original_url,
  :brand,
  :model,
  :mpn,
  :alternate_models,
  :contact_name,
  :seller_name,
  :seller_url,
  :currency_code,
  :min_price,
  :max_price,
  :price,
  :gtin,
  :asin,
  :upc,
  :upc14,
  :sku,
  :ean,
  :product_images,
  :screenshot_filename,
  :manual_filename
]
file
path = file
f = open_spreadsheet path
first = f.first_row
key_mapping = generate_key_mapping f.row(first)
data_hash_keys = key_mapping.keys - @keys
row = f.row(2)
img_srcs = clean_urls(row[key_mapping[:product_images]])
ENCODING_OPTIONS = {
  :invalid => :replace, # Replace invalid byte sequences
  :undef => :replace, # Replace anything not defined in ASCII
  :replace => '', # Use a blank for those replacements
  :universal_newline => true # Always break lines with \n
}
img_srcs = clean_urls(row[key_mapping[:product_images]])
row = f.row(36)
def clean_urls(url_csv)
  urls = url_csv.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).gsub(/\s+/, '').split(/,(?=(https?:|\/\/))/)
  urls.map do |url|
    if url.starts_with?('//')
      if url.scan(/(?<!src=)(\/\/)/).size > 1
        url.split(/(?<!src=)(\/\/)/).map { |l| l.gsub(/\/\//, '').gsub(/http:?/, '').strip }.reject(&:blank?).map { |l| "http://#{l}" }
      else
        "http:#{url}"
      end
    else
      if url.scan(/(?<!src=)(https?)?(\/\/)/).size > 1
        url.split(/(?<!src=)(http:?)?\/\//).map(&:strip).reject(&:blank?).map { |l| "http://#{l}" }
      else
        url
      end
    end
  end.flatten.reject { |url| url.blank? || url =~ /:(\/\/)?$/ }
end
row[key_mapping[:product_images]]
img_srcs = clean_urls(row[key_mapping[:product_images]])
' \n '.gsub(' ', '')
def clean_urls(url_csv)
  urls = url_csv.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).gsub(' ', '').split(/,(?=(https?:|\/\/))/)
  urls.map do |url|
    if url.starts_with?('//')
      if url.scan(/(?<!src=)(\/\/)/).size > 1
        url.split(/(?<!src=)(\/\/)/).map { |l| l.gsub(/\/\//, '').gsub(/http:?/, '').strip }.reject(&:blank?).map { |l| "http://#{l}" }
      else
        "http:#{url}"
      end
    else
      if url.scan(/(?<!src=)(https?)?(\/\/)/).size > 1
        url.split(/(?<!src=)(http:?)?\/\//).map(&:strip).reject(&:blank?).map { |l| "http://#{l}" }
      else
        url
      end
    end
  end.flatten.reject { |url| url.blank? || url =~ /:(\/\/)?$/ }
end
img_srcs = clean_urls(row[key_mapping[:product_images]])
row[key_mapping[:product_images]]
img_srcs = clean_urls(row[key_mapping[:product_images]])
edit -t
img_srcs = clean_urls(row[key_mapping[:product_images]])
edit -t
img_srcs = clean_urls(row[key_mapping[:product_images]])
exit
url = 'https://www.amazon.com/dp/B00VVKCA9C?_encoding=UTF8&ref_=de_a_smtd&showDetailTechData=1#technical-data'
url.scan(/(?<=dp\/).+(?=\W)/)
url.scan(/(?<=dp\/)(?=\W)/)
url.scan(/(?<=dp\/)[0-9A-Z]+(?=\W)/)
url.scan(/(?<=dp\/)[0-9A-Z]+(?=\W)/).first
url.scan(/(?<=dp\/)[0-9a-zA-Z]+(?=\W)/).first
url.scan(/(?<=dp\/)[0-9a-Z]+(?=\W)/).first
url.scan(/(?<=dp\/)[0-9A-z]+(?=\W)/).first
Identifier.id_types[:asin]
exit
url = 'https://www.amazon.com/XuanLei-2-2CM-4-Axis-Aircraft-Lights/dp/B01CY96S46/ref=sr_1_fkmr0_1?s=toys-and-games&ie=UTF8&qid=1465509120&sr=1-1-fkmr0&keywords=XuanLei+2.2CM+MINI+Drone+2.4G+RC'
exit
url = 'https://www.amazon.com/XuanLei-2-2CM-4-Axis-Aircraft-Lights/dp/B01CY96S46/ref=sr_1_fkmr0_1?s=toys-and-games&ie=UTF8&qid=1465509120&sr=1-1-fkmr0&keywords=XuanLei+2.2CM+MINI+Drone+2.4G+RC'
AmazonProductDetailJOb
AmazonProductDetailJon
AmazonProductDetailJob
AmazonProductDetailJob.new.peform(1,1,url, 0)
AmazonProductDetailJob.new.perform(1,1,url, 0)
l = Listing.last
l.identifiers
Identifier.last
Identifier.last(2)
url
l.destroy
exit
url
url = 'https://www.amazon.com/XuanLei-2-2CM-4-Axis-Aircraft-Lights/dp/B01CY96S46/ref=sr_1_fkmr0_1?s=toys-and-games&ie=UTF8&qid=1465509120&sr=1-1-fkmr0&keywords=XuanLei+2.2CM+MINI+Drone+2.4G+RC'
AmazonProductDetailJob.new.perform(1,1,url, 0)
Listing.last
Listing.last.identifiers
Listing.last.destroy
exit
url = 'https://www.amazon.com/XuanLei-2-2CM-4-Axis-Aircraft-Lights/dp/B01CY96S46/ref=sr_1_fkmr0_1?s=toys-and-games&ie=UTF8&qid=1465509120&sr=1-1-fkmr0&keywords=XuanLei+2.2CM+MINI+Drone+2.4G+RC'
Listing.last
AmazonProductDetailJob.new.perform(1,1,url, 0)
asin = url.scan(/(?<=dp\/)[0-9a-zA-Z]+(?=(\W)|$)/).first
url
asin = url.scan(/(?<=dp\/)[0-9a-zA-Z]+(?=\W)/).first
url = 'https://www.amazon.com/XuanLei-2-2CM-4-Axis-Aircraft-Lights/dp/B01CY96S46'
asin = url.scan(/(?<=dp\/)[0-9a-zA-Z]+(?=\W)/).first
asin = url.scan(/(?<=dp\/)[0-9a-zA-Z]+(?=\W|$)/).first
url = 'https://www.amazon.com/XuanLei-2-2CM-4-Axis-Aircraft-Lights/dp/B01CY96S46/ref=sr_1_fkmr0_1?s=toys-and-games&ie=UTF8&qid=1465509120&sr=1-1-fkmr0&keywords=XuanLei+2.2CM+MINI+Drone+2.4G+RC'
asin = url.scan(/(?<=dp\/)[0-9a-zA-Z]+(?=\W|$)/).first
Listing.last.destroy
exit
url = 'https://www.amazon.com/XuanLei-2-2CM-4-Axis-Aircraft-Lights/dp/B01CY96S46/ref=sr_1_fkmr0_1?s=toys-and-games&ie=UTF8&qid=1465509120&sr=1-1-fkmr0&keywords=XuanLei+2.2CM+MINI+Drone+2.4G+RC'
AmazonProductDetailJob.new.perform(1,1,url, 0)
l = Listing.last
l.identifiers
exit
source_category_ids = SourceCategory.where(source_id: 3, test_process: true).pluck(:id) if test_process == 'true'
source_category_ids = SourceCategory.where(source_id: 3, test_process: true).pluck(:id) 
source = Source.find 3
source.source_categories.where(id: source_category_ids)
source_cats = source.source_categories.where(id: source_category_ids)
source_cats.first
source_cats.first.parse_urll
source_cats.first.parse_url
URI.escape(_, '|')
exit
Scrape.last.destroy
SourceCategory.where("url like '%http://www.amazon.com/b/ref=sr_aj?node=2236628011&ajr=0%'")
url = _.first.url
'https://www.amazon.com/b/ref=sr_aj?node=2236628011&ajr=0'.gsub(/https/, '') == url.gsub(/http/, '')
'https://www.amazon.com/b/ref=sr_aj?node=2236628011&ajr=0'.gsub(/https?/, '') == url.gsub(/http?/, '')
s = Scrape.last
s.listings.last
l = _
l.identifiers
l.url
s.listings
s.listings.count
s.listings.select {|l| l.identifiers.count > 1 }
s.listings.select {|l| l.identifiers.count > 1 }.size
l = s.listings.select {|l| l.identifiers.count > 1 }.first
l.identifiers
l.url
SourceCategory.where("url like '%http://www.amazon.com/b/ref=sr_aj?node=2236628011&ajr=0%'")
SourceCategory.where("url like '%\u%'")
SourceCategory.where("url like '%\\u%'")
SourceCategory.where("url like '%http://www.amazon.com/b/ref=sr_aj?node=2236628011&ajr=0%'")
SourceCategory.where("url like '%\u0000%'")
SourceCategory.where("url like '%\u0000%'").count
SourceCategory.where("url like '%\u0000%'").destroy_all
exit
require 'jobs/capybara_job'
require 'capybara_phantomjs'
require '/Users/jonathan/rvx-rds/lib/scraping/capybara_phantomjs.rb'
exit
Identifier.last
id = _
id.products
p = _.first
p.identifiers
exit
Identifier.upc.last 5
Identifier.upc.last 10
Identifier.upc.last 6
Identifier.upc.last 5
Identifier.upc.last 4
Identifier.asin.count
exit
Identifier.upc.last 4
upc = '621112731905'
uri = URI.parse("https://api.upcitemdb.com/prod/trial/lookup?upc=#{upc}")
response = Net::HTTP.get_response(uri)
response.body
response.header
http = Net::HTTP.new(uri.host, uri.port)
response = http.request(Net::HTTP::Get.new(uri.request_uri))
respone.body
response.body
uri.port
uri.host
uri.schema
response = Net::HTTP.get_response(uri)
response['x-ratelimit-remaining']
response.body
response.body.as_json
JSON.parse(response.body)
response.body
JSON.parse(response.body)
id = Identifier.find_by(uniq_id: 'B00QRHDIPY')
id.listings.last
l = _
l.products
JSON.parse(response.body)
response.body
JSON.parse(response.body)
upc = Identifier.upc.last 4
upc = '799475028908'
uri = URI.parse("https://api.upcitemdb.com/prod/trial/lookup?upc=#{upc}")
response = Net::HTTP.get_response(uri)
response.body
JSON.parse(response.body)
exit
upc = '799475028908'
uri = URI.parse("https://api.upcitemdb.com/prod/trial/lookup?upc=#{upc}")
upc = '7994750'
uri = URI.parse("https://api.upcitemdb.com/prod/trial/lookup?upc=#{upc}")
response = Net::HTTP.get_response(uri)
JSON.parse(response.body)
upc = '799475028908'
uri = URI.parse("https://api.upcitemdb.com/prod/trial/lookup?upc=#{upc}")
response = Net::HTTP.get_response(uri)
JSON.parse(response.body)
upc = '799475028999'
uri = URI.parse("https://api.upcitemdb.com/prod/trial/lookup?upc=#{upc}")
response = Net::HTTP.get_response(uri)
JSON.parse(response.body)
upc = '799475011908'
uri = URI.parse("https://api.upcitemdb.com/prod/trial/lookup?upc=#{upc}")
response = Net::HTTP.get_response(uri)
JSON.parse(response.body)
response['x-ratelimit-remaining']
json = JSON.parse(response.body)
json['code']
upc = '799475028908'
uri = URI.parse("https://api.upcitemdb.com/prod/trial/lookup?upc=#{upc}")
response = Net::HTTP.get_response(uri)
json = JSON.parse(response.body)
json['code']
json
json['items']
json['items'].first
upc = "00047237010297"
uri = URI.parse("https://api.upcitemdb.com/prod/trial/lookup?upc=#{upc}")
response = Net::HTTP.get_response(uri)
json = JSON.parse(response.body)
upc = '00662705469621'
uri = URI.parse("https://api.upcitemdb.com/prod/trial/lookup?upc=#{upc}")
response = Net::HTTP.get_response(uri)
json = JSON.parse(response.body)
id = Identifier.find_by uniq_id: upc
Listing.where("data_hash like '%杜比%'")
Listing.where("data_hash like '%DD+%'")
exit
"--proxy=#{proxy}"
ENV['SOCKS_PROXY']
def pop_proxy
  Sidekiq.redis do |redis|
    used_proxies = redis.smembers(:'proxies:used')
    all_proxies = redis.smembers(:'proxies:all')
    if used_proxies.length == all_proxies.length
      used_proxies = []
      redis.del :'proxies:used'
    end
    available_proxies = all_proxies - used_proxies
    proxy = available_proxies.sample
    redis.sadd :'proxies:used', proxy
    proxy
  end
end
pop_proxy
exit
Identifier.last
Identifier.last.processed?
Identifier.processed
exit
Identifier.processed
Identifier.processed.count
id = Identifier.processed.last
id.products
id.listings
l = _.first
l.products
id
l.identifiers.upc
upc = '621112731905'
upc = l.identifier.upc.first
upc = l.identifiers.upc.first
uri = URI.parse("https://api.upcitemdb.com/prod/trial/lookup?upc=#{upc.uniq_id}")
response = Net::HTTP.get_response(uri)
json = JSON.parse(response.body)
exit
Identifier.asin.count
Identifier.asin.processed
Identifier.asin.processed.count
exit
Identifier.asin.processed.count
Identifier.last
Identifier.asin.processed.count
Identifier.upc.last
id = _
id.products
id.uniq_id
p = id.products.first
exit
id.last
id = Identifier.last
Identifier.upc.sample
Identifier.upc.count
Identifier.upc.sample
id
id.products
Identifier.processed
Identifier.processed.flat_map {|id| id.products.ids }
ids = _
Product.where(id: ids)
Product.where(id: ids).first
p = _
p.identifiers.upc
Identifier.asin.count
uri = URI.parse("https://api.upcitemdb.com/prod/trial/lookup?upc=#{718037772615}")
response = Net::HTTP.get_response(uri)
json = JSON.parse(response.body)
uri = URI.parse("https://api.upcitemdb.com/prod/trial/lookup?upc=#{017229132542}")
uri = URI.parse("https://api.upcitemdb.com/prod/trial/lookup?upc=017229132542")
response = Net::HTTP.get_response(uri)
json = JSON.parse(response.body)
remaining_requests = response['x-ratelimit-remaining']
Identifier.gint.sample
Identifier.gtin.sample
uri = URI.parse("https://api.upcitemdb.com/prod/trial/lookup?upc=00017817602853")
response = Net::HTTP.get_response(uri)
json = JSON.parse(response.body)
response['x-ratelimit-remaining']
Listing.all.select {|l| l.products.size == 0 }.sample
l.name
l = Listing.all.select {|l| l.products.size == 0 }.sample
l.name
l.url
json
Time.parse(1465604104)
Time.parse('1465604104')
time = 1465604104
Time.at(t).utc.strftime("%H:%M:%S")
Time.at(time).utc.strftime("%H:%M:%S")
Time.at(time).utc
Time.at(time)
exit
SourceCategory.where(source_id: 3, test_process: true)
Source.find 3
Source.find_by(name: 'amazonus')
SourceCategory.where(source_id: 4, test_process: true)
Source.first
Source.first.update id: -1
SourceCategory.where(source_id: 1)
SourceCategory.where(source_id: 1).count
SourceCategory.where(source_id: 1).destroy_all
Source.first
Source.first.update id: -1
exit
s = Scrape.last
s.Listings.count
s.listings.count
s.products.count
s.products
s.listings.count
s.products.count
s.listings.count
l = s.listings.last
l.identifiers
Identifier.upc
Identifier.upc14
Sidekiq.queue.new('product_listing_queue')
Sidekiq::Queue.new('product_listing_queue')
Sidekiq::Queue.new('product_listing_queue').size
Sidekiq::Queue.new('product_listing_queue').clear
s.listings.count
l = s.listings.last
l.products
l.identifiers
l.url
l = s.listings.first
l.products
l.identifiers
l.url
s.products.count
s.listings.count
625 / 971.to_f
l.images
l.images.count
l.identifiers
l = s.listings.sample
l.identifiers.first.processed_changed?
id = l.identifiers.first
id.processed?
id.processed = true
id.save
id.processed_changed?
id.processed = false
id.processed_changed?
id.save
id.processed_change
s.reload
Product.delete_index!
Listing.delete_index!
Listing.import
Product.import
exit
scrape_id = Scrape.last.id
scrape = Scrape.find(scrape_id)
duration = DateTime.now.to_i - scrape.created_at.to_i
logger.info "Update duration of scrape with id #{scrape_id} #{duration}"
scrape.update_attributes duration: duration, running: false
scrape = Scrape.find(scrape_id)
product_count = scrape.products.size
seller_count = scrape.sellers.size
scrape.update_attributes num_sellers: seller_count, num_products: product_count
s
s = Scrape.last
exit
Identifier.not_processed
Identifier.first
exit
Identifier.not_processed
url = Listing.last.url
url.scan(/\/dp.+$/, '')
url.scan(/\/dp.+$/)
url.gsub(/(?<=dp\/)/, '')
URI.parse(url)
uri = _
uri.domain
uri.host
"#{uri.scheme}://#{uri.host}/dp/#{asin}"
asin = url.scan(/(?<=dp\/)[0-9a-zA-Z]+(?=\W|$)/).first
"#{uri.scheme}://#{uri.host}/dp/#{asin}"
s = Scrape.last
s.listings.count
l = Listing.lat
l = Listing.last
source_category_ids = l.categories
exit
amazonus = Source.find 4
Identifier.find_by(uniq_id: 'B01BGC39VM')
Identifier.find_by(uniq_id: 'B01BGC39VM').listings
Identifier.find_by(uniq_id: 'B01BGC39VM').listings.count
Identifier.find_by(uniq_id: 'B01BGC39JY').listings.count
l = Identifier.find_by(uniq_id: 'B01BGC39VM').listings.first
l.categories
SourceCategory.find_by(category: _)
SourceCategory.find_by(source: source, category_id: 705)
SourceCategory.find_by(source_id: 4, category_id: 705)
source_category_id: _
source_category_id = SourceCategory.find_by(source_id: 4, category_id: 705)
source_category_id
scrape
url = l.url
url = l.original_url
source_category_id
source_category_id = _.id
url
Scrape.last
l.id
l
l.url
l.destroy
exit
scrape_id = 1
source_category_id = 792
url = "https://www.amazon.com/TCL-55US5800-55-Inch-Ultra-Smart/dp/B01BGC39VM/ref=sr_1_1?s=electronics&ie=UTF8&qid=1465604664&sr=1-1&refinements=p_n_feature_keywords_three_browse-bin%3A7688788011"
AmazonProductDetailJob
AmazonProductDetailJob.new.perform(scrape_id, source_category_id, url)
exit
scrape_id = 1
source_category_id = 792
url = "https://www.amazon.com/TCL-55US5800-55-Inch-Ultra-Smart/dp/B01BGC39VM/ref=sr_1_1?s=electronics&ie=UTF8&qid=1465604664&sr=1-1&refinements=p_n_feature_keywords_three_browse-bin%3A7688788011"
AmazonProductDetailJob.new.perform(scrape_id, source_category_id, url)
exit
scrape_id = 1
source_category_id = 792
url = "https://www.amazon.com/TCL-55US5800-55-Inch-Ultra-Smart/dp/B01BGC39VM/ref=sr_1_1?s=electronics&ie=UTF8&qid=1465604664&sr=1-1&refinements=p_n_feature_keywords_three_browse-bin%3A7688788011"
AmazonProductDetailJob.new.perform(scrape_id, source_category_id, url)
exit
scrape_id = 1
source_category_id = 792
url = "https://www.amazon.com/TCL-55US5800-55-Inch-Ultra-Smart/dp/B01BGC39VM/ref=sr_1_1?s=electronics&ie=UTF8&qid=1465604664&sr=1-1&refinements=p_n_feature_keywords_three_browse-bin%3A7688788011"
AmazonProductDetailJob.new.perform(scrape_id, source_category_id, url)
exit
scrape_id = 1
source_category_id = 792
url = "https://www.amazon.com/TCL-55US5800-55-Inch-Ultra-Smart/dp/B01BGC39VM/ref=sr_1_1?s=electronics&ie=UTF8&qid=1465604664&sr=1-1&refinements=p_n_feature_keywords_three_browse-bin%3A7688788011"
AmazonProductDetailJob.new.perform(scrape_id, source_category_id, url)
Listing.last 2
Listing.last(2).flat_map {|l| l.products }
l = Listing.last
l.url
l.original_url
l1, l2 = Listing.last 2
l1.screenshots
l1.screenshots.first.attachment.url
l2.screenshots.first.attachment.url
l1.screenshots.first.attachment.url
url = 'https://www.amazon.com/XuanLei-2-2CM-4-Axis-Aircraft-Lights/dp/B01CY96S46'
exit
url = 'https://www.amazon.com/XuanLei-2-2CM-4-Axis-Aircraft-Lights/dp/B01CY96S46'
source_category_id = 792
scrape_id = 1
AmazonProductDetailJob.new.perform(scrape_id, source_category_id, url)
Listing.last
l = _
l.screenshots.first.attachment.url
exit
l = Listing.last
l.destroy
url = 'https://www.amazon.com/XuanLei-2-2CM-4-Axis-Aircraft-Lights/dp/B01CY96S46'
source_category_id = 792
scrape_id = 1
AmazonProductDetailJob.new.perform(scrape_id, source_category_id, url)
exit
scrape_id = 1
source_category_id = 792
url = 'https://www.amazon.com/XuanLei-2-2CM-4-Axis-Aircraft-Lights/dp/B01CY96S46'
l = Listing.last
l.destroy
l = Listing.last
AmazonProductDetailJob.new.perform(scrape_id, source_category_id, url)
Listing.last 3
Listing.last(3).flat_map {|l| l.screenshots.first.attachment.url }
exit
s = Scrape.last
s.listings.count
s.products.count
s.listings.count
s.products.count
source = Source.find 4
SourceCategory.where(test_process: true, source: source)
SourceCategory.where(test_process: true, source: source).map(&:category)
Listing.where(url: 'https://www.amazon.com/dp/B00WGOO1TO')
l = _
l.products
l = l.first
l.products
p = _.first
p.listings
p.listings.count
p.listings.pluck(:url)
scrape = Scrape.last
source_category_id = 792
category = Category.find(SourceCategory.find(source_category_id).category_id)
category = Category.find(SourceCategory.find(source_category_id).category_id).to_sql
category = Category.find(SourceCategory.find(source_category_id).pluck(:category_id)).to_sql
category = Category.where(SourceCategory.where(id: source_category_id).pluck(:category_id)).to_sql
category = Category.where(id: SourceCategory.where(id: source_category_id).pluck(:category_id))
category = Category.where(id: SourceCategory.where(id: source_category_id).select(:category_id).limit(1))
category = Category.where(id: SourceCategory.where(id: source_category_id).select(:category_id).limit(1)).first
category = Category.where(id: SourceCategory.where(id: source_category_id).select(:category_id)).first
category = Category.where(id: SourceCategory.where(id: source_category_id).select(:category_id))
category = Category.where(id: SourceCategory.where(id: source_category_id).select(:category_id)).first
category.listings
category.listings.where("listings.scrape_id": 2)
category.listings.where("listings.scrape_id": 2).find_by(url: url)
url = 'https://www.amazon.com/XuanLei-2-2CM-4-Axis-Aircraft-Lights/dp/B01CY96S46'
category.listings.where("listings.scrape_id": 2).find_by(url: url)
url = Listing.last.url
category.listings.where("listings.scrape_id": 2).find_by(url: url)
l = Listing.last
url = l.url
source_category_id = l.categories.first
source_category_id = SourceCategory.where(category: l.categories.first, source_id: 4).id
source_category_id = SourceCategory.where(category: l.categories.first, source_id: 4).first.id
source_category_id
category = Category.where(id: SourceCategory.where(id: source_category_id).select(:category_id)).first
category.listings.where("listings.scrape_id": 2).find_by(url: url)
category.listings.where("listings.scrape_id": 2).find_by(url: url).to_sql
category.listings.where("listings.scrape_id": 2).find_by(url: url)
url
scrape_id
scrape_id = 2
source_category_id
Listing.includes(:categories).find_by('categories.id': Category.where(id: SourceCategory.where(id: source_category_id).select(:category_id)).limit(1), scrape_id: scrape_id, url: url )
Listing.includes(:categories).find_by('categories.id': Category.where(id: SourceCategory.where(id: source_category_id).select(:category_id)), scrape_id: scrape_id, url: url )
Listing.includes(:categories).where('categories.id': Category.where(id: SourceCategory.where(id: source_category_id).select(:category_id)), scrape_id: scrape_id, url: url )
Listing.includes(:categories).where('categories.id': Category.where(id: SourceCategory.where(id: source_category_id).select(:category_id)), scrape_id: scrape_id).find_by(url: url)
Listing.includes(:categories).where('categories.id': Category.where(id: SourceCategory.where(id: source_category_id).select(:category_id)), scrape_id: scrape_id, url: url )
Listing.includes(:categories).where('categories.id': Category.where(id: SourceCategory.where(id: source_category_id).select(:category_id)), scrape_id: scrape_id, url: url ).to_sql
s = Scrape.last
s.listings.count
s.products.count
category = Category.where(id: SourceCategory.where(id: source_category_id).select(:category_id))
Listing.includes(:categories).where('categories.id': category, scrape_id: scrape_id, url: url )
Listing.includes(:categories).where('categories.id': category, scrape_id: scrape_id, url: url).first
Listing.includes(:categories).where('categories.id': category, scrape_id: scrape_id, url: url)
Listing.includes(:categories).where('categories.id': category, scrape_id: scrape_id, url: url).first
Listing.includes(:categories).where('categories.id': category, scrape_id: scrape_id, url: url)
Listing.includes(:categories).where('categories.id': category, scrape_id: scrape_id, url: url).first
listings = Listing.includes(:categories).where('categories.id': category, scrape_id: scrape_id, url: url).limit(1)
listings = Listing.includes(:categories).where('categories.id': category, scrape_id: scrape_id, url: url)
listings = Listing.includes(:categories).where('categories.id': category, scrape_id: scrape_id, url: url).to_sql
listings = Listing.includes(:categories).where('categories.id': category, scrape_id: scrape_id, url: url)
listings = Listing.includes(:categories).where('categories.id': category, scrape_id: scrape_id, url: url).limit(1)
Listing.select('SELECT  DISTINCT `listings`.`id` FROM `listings` LEFT OUTER JOIN `category_listings` ON `category_listings`.`listing_id` = `listings`.`id` LEFT OUTER JOIN `categories` ON `categories`.`id` = `category_listings`.`category_id` WHERE `listings`.`scrape_id` = 2 AND `listings`.`url` = 'https://www.amazon.com/dp/B01GF1DSU6' AND `categories`.`id` IN (SELECT `categories`.`id` FROM `categories` WHERE `categories`.`id` IN (SELECT `source_categories`.`category_id` FROM `source_categories` WHERE `source_categories`.`id` = 789)) LIMIT 1')
Listing.find_by_sql("SELECT  DISTINCT `listings`.`id` FROM `listings` LEFT OUTER JOIN `category_listings` ON `category_listings`.`listing_id` = `listings`.`id` LEFT OUTER JOIN `categories` ON `categories`.`id` = `category_listings`.`category_id` WHERE `listings`.`scrape_id` = 2 AND `listings`.`url` = 'https://www.amazon.com/dp/B01GF1DSU6' AND `categories`.`id` IN (SELECT `categories`.`id` FROM `categories` WHERE `categories`.`id` IN (SELECT `source_categories`.`category_id` FROM `source_categories` WHERE `source_categories`.`id` = 789)) LIMIT 1")
Listing.find_by_sql("SELECT  DISTINCT `listings`.`*` FROM `listings` LEFT OUTER JOIN `category_listings` ON `category_listings`.`listing_id` = `listings`.`id` LEFT OUTER JOIN `categories` ON `categories`.`id` = `category_listings`.`category_id` WHERE `listings`.`scrape_id` = 2 AND `listings`.`url` = 'https://www.amazon.com/dp/B01GF1DSU6' AND `categories`.`id` IN (SELECT `categories`.`id` FROM `categories` WHERE `categories`.`id` IN (SELECT `source_categories`.`category_id` FROM `source_categories` WHERE `source_categories`.`id` = 789)) LIMIT 1")
Listing.find_by_sql("SELECT  DISTINCT `listings`.* FROM `listings` LEFT OUTER JOIN `category_listings` ON `category_listings`.`listing_id` = `listings`.`id` LEFT OUTER JOIN `categories` ON `categories`.`id` = `category_listings`.`category_id` WHERE `listings`.`scrape_id` = 2 AND `listings`.`url` = 'https://www.amazon.com/dp/B01GF1DSU6' AND `categories`.`id` IN (SELECT `categories`.`id` FROM `categories` WHERE `categories`.`id` IN (SELECT `source_categories`.`category_id` FROM `source_categories` WHERE `source_categories`.`id` = 789)) LIMIT 1")
Listing.find_by_sql("SELECT  DISTINCT `listings`.* FROM `listings` LEFT OUTER JOIN `category_listings` ON `category_listings`.`listing_id` = `listings`.`id` LEFT OUTER JOIN `categories` ON `categories`.`id` = `category_listings`.`category_id` WHERE `listings`.`scrape_id` = 2 AND `listings`.`url` = 'https://www.amazon.com/dp/B01GF1DSU6' AND `categories`.`id` IN (SELECT `categories`.`id` FROM `categories` WHERE `categories`.`id` IN (SELECT `source_categories`.`category_id` FROM `source_categories` WHERE `source_categories`.`id` = 789)) LIMIT 1").first
Listing.includes(:categories).where('categories: category, scrape_id: scrape_id, url: url)
      Listing.includes(:categories).where('categories': category, scrape_id: scrape_id, url: url)
Listing.includes(:categories).where('categories': category, scrape_id: scrape_id, url: url)
Listing.includes(:categories).where('categories': category, scrape_id: scrape_id, url: url)      Listing.includes(:categories).where('categories.id': category, scrape_id: scrape_id, url: url)
Listing.includes(:categories).where('categories.id': category, scrape_id: scrape_id, url: url)
kj      listings = Listing.includes(:categories).where('categories.id': category, scrape_id: scrape_id, url: url)
listings = Listing.includes(:categories).where('categories.id': category, scrape_id: scrape_id, url: url)
listings.first
listings.class
listings.present?
listings.empty?
listings = Listing.includes(:categories).where('categories.id': category, scrape_id: scrape_id, url: )
listings = Listing.includes(:categories).where('categories.id': category, scrape_id: scrape_id, url: 'wasd;lkjas;g')
listings.present?
listings = Listing.includes(:categories).where('categories.id': category, scrape_id: scrape_id, url: url).select(:id)
s.listings.count
l = s.listings.last
l.original_url
l.url
Listing.where(url: 'https://www.amazon.com/dp/B01E63ZNNA')
Listing.where(url: 'https://www.amazon.com/dp/B01E63ZNNA').count
s.listings.count
edit -t
args = [2, 845, "https://www.amazon.com/Audiopipe-Studio-Woofer-Wireless-0-00In/dp/B00T6XDTI2/ref=sr_1_108/188-3950033-0576732?s=aht&ie=UTF8&qid=1465845584&sr=1-108"]
args2 = [2, 849, "https://www.amazon.com/Audiopipe-Studio-Woofer-Wireless-0-00In/dp/B00T6XDTI2/ref=sr_1_108/188-3950033-0576732?s=aht&ie=UTF8&qid=1465845584&sr=1-108"]
args1 == args2
args\ == args2
args == args2
args == args
args
edit -t
edit
edit -t
args
args[0..2]
args[0..1]
args[0..2]
args1
args2
args[0..2] == args2[0..2]
edit -t
job_args
job_args.length
job_args.uniq.length
s = Scrape.last
s.listings.last
l = _
l.url
l.original_url
s.listings.count
listings = s.listings.all.select { |l| l.identifiers.size > 1 }
listings
listings.size
listings.first
l.identifiers
listings = s.listings.includes(:identifiers).all.reject { |l| l.identifiers.size < 2 }
listings
listings.size
l = listings.first
l.identifiers
l.url
listings.last
listings.map(&:url)
s = Scrape.last
s.listings.count
ProductManual.count
ProductManual.first
ProductManual.last
Products.pluck(:brand, :model)
Product.pluck(:brand, :model)
ProductManual.last
ProductManual.count
ProductManual.last
pm = _
pm
pm.attachment.url
pm.original_url
pm.reload
pm.attachment.url
ProductManual.last
ProductManual.count
ProductManual.last.attachment.url
ProductManual.last.original_url
url = ProductManual.last.original_url
ProductManual.count
ProductManual.last
pm
pm.product
pm.products
pm.products.first
ProductManual.first.attachment.url
ProductManual.count
ProductManual.last.attachment.url
ProductManual.last
pm = _
pm.products
ProductManual.count
ProductManual.all
exit
Product.find_by(brand: 'LG')
p = _.first
p = Product.find_by(brand: 'LG')
p.listings
p.listings.last
listing = _
ProductManual.where("original_url like '%manualslib%'")
ProductManual.where("original_url like '%manualslib%'").first.attachment.url
ProductManual.where("original_url like '%manualslib%'").first(2).last.attachment.url
pm = ProductManual.where("original_url like '%manualslib%'").first(2).last
pm.original_url
pm.listings
listing = pm.listings.first
product = pm.products.first
product_id, listing_id, manual_url = product.id, listing.id, pm.original_url
ParsePdfJob
ParsePdfJob.new.perform(product_id, listing_id, manual_url)
ProductManual.last
ProductManual.attachment.url
ProductManual.lat.attachment.url
ProductManual.last.attachment.url
ProductManual.find_by(manual_url: manual_url)
ProductManual.find_by(original_url: manual_url)
pm = _
pm.attactment.url
pm.attachment.url
exit
pm = ProductManual.find(6646)
pm.products
pm.listings
listing_id = pm.listings.first.id
product_id = pm.products.first.id
manual_url = pm.original_url
ParsePdfJob.new.perform(product_id, listing_id, manual_url)
pm.reload
pm.attachment.url
pdf = WickedPdf.new.pdf_from_string(pdf)
pdf
pdf = WickedPdf.new.pdf_from_string(nil)
pdf.nil?
pdf.empty:
pdf.empty?
pdf.content
pdf.nil?
ManualText.last
ManualText.first
ManualText.first.text.length
ManualText.first.product_manual
pm = _
pm.original_url
exit
Scrape.last.update running: false
:w
s = Scrape.last
l = s.listings.last
l.manuals
l.products
ProductManual.last
ProductManual.count
ProductManual.last
pm = _
pm.attachment.url
ProductManual.last
ProductManual.last.attachment.url
ProductManual.last
pm = _
pm.original_url
ProductManual.cunt
ProductManual.count
ProductManual.last
pm
ProductManual.last
Identifier.upc.sample
exit
Product.count
duplicates = []
Products.find_each do |p|
p = Product.first
p.identifiers
id = _.first
id.products
Identifier.find_each do |id|
  duplicates << id.id if id.products.size > 1
end
duplicates
p1 = Product.first
p2 = Product.second
p1.identifiers
p2.identifiers
Identifier.upc
exit
Scrape.running
Scrape.where(running: true)
Scrape.where(running: true).update_all running: false
Identifier.asin.count
Identifier.upc
Identifier.upc.count
Identifier.upc
Identifier.upc.size
Identifier.upc.count
edit -t
json
Identifier.upc.count
json
edit -t
Identifier.upc.count
Identifier.upc.sample
upc = Identifier.upc.sample.uniq_id
uri = URI.parse("https://api.upcitemdb.com/prod/trial/lookup?upc=#{upc}")
response = Net::HTTP.get_response(uri)
json = JSON.parse(response.body)
json['offers']
json
json['items']
json['items'].length
json['items'][0]
brand = json['items'][0]['brand']
brand = json['items'][0]['model']
brand = json['items'][0]['brand']
model = json['items'][0]['model']
Identifier.where(uniq_id: upc).products.first
Identifier.where(uniq_id: upc).first.products.first
Identifier.where(uniq_id: upc).count
l = Identifier.where(uniq_id: upc).first.listings
upc
l = Identifier.where(uniq_id: upc).first
id = l
id.products
id.listings
id
Identifier.asin.processed
Identifier.asin.processed.update_all processed: false
Identifier.asin.not_processed.count
Identifier.asin.processed.count
Identifier.upc.last
id = _
id.products
id.listings
Listing.count
Product.count
no_prods = Listing.all.reject {|l| l.products.size > 0 }
l = no_prods.first
l.identifiers
Identifier.upc.last
upc = '887276976204'
uri = URI.parse("https://api.upcitemdb.com/prod/trial/lookup?upc=#{upc}")
response = Net::HTTP.get_response(uri)
json = JSON.parse(response.body)
json['items'].length
json['items'][0]['offers'].length
json['items'][0]['offers']
json['items'][0]['offers'].select {|offer| offer['condition'] == "New" }.length
id = Identifier.find_by uniq_id: 'B01CQOV3YO'
id.listings
id.products
id.listings
Identifier.upc.count
upcs = Identifier.upc.last 500
upc = upcs.sample.uniq_id
uri = URI.parse("https://api.upcitemdb.com/prod/trial/lookup?upc=#{upc}")
response = Net::HTTP.get_response(uri)
json = JSON.parse(response.body)
upc
id = Identifier.find_by uniq_id: '711420604876'
id.products
id.listings
id.listings.first.name
l = _
l = id.listings.first
l.identifiers
response['x-ratelimit-remaining']
upc = Identifier.upc.sample.uniq_id
uri = URI.parse("https://api.upcitemdb.com/prod/trial/lookup?upc=#{upc}")
response = Net::HTTP.get_response(uri)
json = JSON.parse(response.body)
json
json['asin'
]
json
json['items'][0]['asin']
Identifier.find_by(uniq_id: "B010Q8G39W")
id = _
id.products
p = id.products.first
p.listings
p.listings.count
p.listings.map(&:name)
response['x-ratelimit-remaining']
exit
p = Product.last
p.products_matching_identifier
p.products_matching_identifiers
p.identifiers
id = p.identifiers.last
Product.sample.identifiers << id
Product.all.sample.identifiers << id
Product.all.sample.listings.first.identifiers << id
p.products_matching_identifiers
p.identifiers
id
id.products
p.reload
p.products_matching_identifiers
id.listings
id.listings.count
id.listings.map(&:name)
json
upc = Identifier.upc.sample.uniq_id
uri = URI.parse("https://api.upcitemdb.com/prod/trial/lookup?upc=#{upc}")
response = Net::HTTP.get_response(uri)
json = JSON.parse(response.body)
upc
id = Identifier.where(uniq_id: 'upc = Identifier.upc.sample.uniq_id
uri = URI.parse("https://api.upcitemdb.com/prod/trial/lookup?upc=#{upc}")
response = Net::HTTP.get_response(uri)
json = JSON.parse(response.body)
id = Identifier.where(uniq_id: "#{upc}")
id.products
id = id.first
id.products
id.listings
json['items'][0]
json['items'][0]['asin']
Identifier.where(uniq_id: json['items'][0]['asin'])
Identifier.where(uniq_id: json['items'][0]['asin']).first.products
Identifier.where(uniq_id: json['items'][0]['asin']).first.listings
Identifier.where(uniq_id: json['items'][0]['asin']).first.listings.size
Identifier.where(uniq_id: json['items'][0]['asin']).first.listings.pluck(:name)
Identifier.where(uniq_id: json['items'][0]['asin']).first.listings.size
Identifier.where(uniq_id: json['items'][0]['asin']).first.listings.count
Identifier.where(uniq_id: json['items'][0]['asin']).first.listings.map(&:name)
Identifier.where(uniq_id: json['items'][0]['asin']).first.listings.pluck(:name)
Identifier.where(uniq_id: json['items'][0]['asin']).first.listings.pluck(:name).to_sql
Identifier.where(uniq_id: json['items'][0]['asin']).first.listings.pluck(:name)
Identifier.where(uniq_id: json['items'][0]['asin']).first.listings.map(&:name)
Identifier.where(uniq_id: json['items'][0]['asin']).first.listings.map(&:created_at)
Identifier.where(uniq_id: json['items'][0]['asin']).first.listings.pluck(:created_at)
Identifier.asin.processed
Identifier.asin.processed.count
Identifier.asin.not_processed.count
Redis.current
connection = Redis.current
connection.smembers(:'proxies:used')
connection.smembers(:'proxies:all')
connection = Sidekiq.redis
Redis.current
connection = Redis.current
connection.smembers(:'amazonus:proxies:all')
connection.smembers(:'amazonus:proxies:all').count
connection.smembers(:'amazonus:proxies:used').count
redis = Redis::Namespace.new(source_name, redis: Redis.new(url: ENV['REDIS_URL']))
redis = Redis::Namespace.new('rds_shared_cache', redis: Redis.new(url: ENV['REDIS_URL']))
redis.smembers(:'proxies:used')
redis.keys
redis.redis
connection = redis.redis
connection.smembers(:'amazonus:proxies:all')
connection = redis
connection.class
namespace = connection
namespace.sadd
namespace.members
namespace.smembers
namespace.smembers :'proxies:all'
namespace.smembers :'ucpitemdb:all'
edit -t
proxies
namespace
full_proxies = proxies.map do |proxy|
Time.now
Time.now.to_i
Time.at(_)
full_proxies = proxies.map do |proxy|
full_proxies = proxies.map do |proxy| 
  "#{proxy}-#{Time.now.to_i}-#{200}"
end
((Time.now.to_i - full_proxies.first.split('-')[1].to_i) / 1000) > 10
((Time.now.to_i - full_proxies.first.split('-')[1].to_i) / 1000)
((Time.now.to_i - full_proxies.first.split('-')[1].to_i) / 1000.to_f)
((Time.now.to_i - full_proxies.first.split('-')[1].to_i))
(Time.now.to_i - full_proxies.first.split('-')[1].to_i) > 10
if (Time.now.to_i - full_proxies.first.split('-')[1].to_i) > 10 && full_proxies.first.split('-')[2] > 0
  puts "HEEYEY"
end
if (Time.now.to_i - full_proxies.first.split('-')[1].to_i) > 10 && full_proxies.first.split('-')[2].to_i > 0
  puts "HEEYEY"
end
namespace
namespace = Redis::Namespace.new("#{Rails.env}:shared_cache", redis: Redis.new(url: ENV['REDIS_URL']))
full_proxies
namespace.sadd :'upcitemdb', full_proxies
namespace
namespace.namespace
key = "upcitemdb"
"#{namespace.namespace}:key"
"#{namespace.namespace}:#{key}"
Redis.current.get "#{namespace.namespace}:#{key}"
Redis.current.get :"#{namespace.namespace}:#{key}"
Redis.current
namespace.smembers key
namespace.smembers(key)
f
namespace.del key
namespace.smembers(key)
namespace.sadd key, full_proxies
namespace.smembers(key)
response
response.headers
response
response.read_header
response.header
response.to_json
response.as_json
response['total']
json
response.as_json
response["x-ratelimit-remaining"]
namespace.smembers(key)
namespace.smembers(key).sample
proxy, last_used, remaining_limit = namespace.smembers.sample.split('-')
proxy, last_used, remaining_limit = namespace.smembers(key).sample.split('-')
Time.now.to_i - last_used.to_i > 10
Time.now.to_i - last_used.to_i 
Time.now.to_i - last_used.to_i > 0
Time.now.to_i - last_used.to_i > 10 && remaining_reqs.to_i > 0
proxy, last_used, remaining_reqs = namespace.smembers(key).sample.split('-')
Time.now.to_i - last_used.to_i > 10 && remaining_reqs.to_i > 0
conn = Redis.current
conn.get "#{key}"
key
namespace
namespace.namespace
full_key = "#{_}:#{key}"
conn.get full_key
full_key = "#{_}\:#{key}"
conn.get full_key
full_key = "#{_}:#{key}"
namespace.namespace
full_key = "#{_}\:#{key}"
conn.get full_key
full_key
conn.get full_key
conn.sscan full_key
conn.smembers(full_key)
conn.smembers(key)
conn.smembers(full_key)
response['X-RateLimit-Remaining']
response['X-RateLimit-Remaining'].zero?
Time.now
Time.now + 10.seconds
"#{Time.now} - #{Time.now + 10.seconds}"
"#{Time.now} - #{Time.now + 11.seconds}"
namespace = Redis::Namespace.new("#{Rails.env}:shared_cache", redis: Redis.new(url: ENV['REDIS_URL']))
key = "upcitemdb"
namespace
namespace.smembers(key)
proxy = nil
while proxy.nil?
  proxy, last_used, remaining_reqs = namespace.smembers(key).sample.split('-')
  if Time.now.to_i - last_used.to_i > 10 && remaining_reqs.to_i > 0
    new_remaining = remaining_reqs - 1
    next_time = Time.now + 30.seconds
    namespace.sadd "#{key}:used", [proxy, next_time, new_remaining]
  else
    proxy.nil?
  end
end
while proxy.nil?
  # TODO: Check last used time is greater than 10 secs and
  proxy, last_used, remaining_reqs = namespace.smembers(key).sample.split('-')
  if Time.now.to_i - last_used.to_i > 10 && remaining_reqs.to_i > 0
    # PROXY IS USABLE
    # new_remaining = response['X-RateLimit-Remaining'].to_i
    new_remaining = remaining_reqs.to_i - 1
    # next_time = new_remaining.zero? ? response['Retry-After'] : (Time.now + 10.seconds).to_i
    next_time = Time.now + 30.seconds
    namespace.sadd "#{key}:used", [proxy, next_time, new_remaining]
  else
    proxy.nil?
  end
end
namespace.smembers("#{key}:used")
full_key = "#{key}:used"
namespace.smembers(full_key)
namespace.smembers('upcitemdb')
namespace.smembers('upcitemdb:all')
namespace.del('upcitemdb')
namespace.smembers('upcitemdb')
namespace = Redis::Namespace.new("#{Rails.env}:shared_cache", redis: Redis.new(url: ENV['REDIS_URL']))
key = "upcitemdb"
edit -t
namespace.smembers('upcitemdb:all')
edit -t
namespace.smembers('upcitemdb:used')
edit -t
namespace.smembers('upcitemdb:used')
namespace.del('upcitemdb:used')
namespace.smembers('upcitemdb:used')
edit -t
namespace.smembers('upcitemdb:used')
namespace.smembers('upcitemdb:all')
key
used_proxies = namespace.smembers("#{key}:used")
used_proxies
all_proxies
all_proxies = namespace.smembers("#{key}:all")
namespace.del :"#{key}:all"
namespace.sadd "#{key}:all", used_proxies
namespace.del :"#{key}:used"
namespace.smembers('upcitemdb:used')
namespace.smembers('upcitemdb:all')
namespace.smembers('upcitemdb:all').count
edit -t
namespace.smembers('upcitemdb:all')
namespace.smembers('upcitemdb:all').size
2048 * 6
edit -t
200000 * 5
200000 * 5 / 60
200000 * 5 / 60 / 60
namespace.smembers('upcitemdb:all').reject {|proxy} proxy.split('-').last <= 0}
namespace.smembers('upcitemdb:all').reject {|proxy} proxy.split('-').last.to_i <= 0}
namespace.smembers('upcitemdb:all')
namespace.smembers('upcitemdb:all').class
namespace.smembers('upcitemdb:all').reject {|item| item.split("-").last == '0'}
namespace.smembers('upcitemdb:all').reject {|item| item.split("-").last == '0'}.count
200 * 2048
(10..20).sample
[10..22].sample
[10..22].class
[10..22]
[10..22].length
Array(10..25).sample
Array(111..25).sample
namespace.del('upcitemdb:all')
namespace.del('upcitemdb:used')
proxies
edit -t
proxies
namespace = Redis::Namespace.new("#{Rails.env}:shared_cache", redis: Redis.new(url: ENV['REDIS_URL']))
key = "upcitemdb"
proxy = proxies.first
namespace.sadd "#{key}:#{proxy}", Time.now
namespace.sget "#{key}:#{proxy}"
namespace.get "#{key}:#{proxy}"
namespace.redis.get "#{key}:#{proxy}"
namespace.smembers "#{key}:#{proxy}"
namespace.del "#{key}:#{proxy}"
namespace.smembers "#{key}:#{proxy}"
namespace.set "#{key}:#{proxy}", Time.now.to_i
namespace.get "#{key}:#{proxy}"
edit -t
namespace.get "#{key}:#{proxies.sample}"
edit -t
namespace.get "#{key}:#{proxies.sample}"
upc = Identifier.upc.sample.uniq_id
uri = URI.parse("https://api.upcitemdb.com/prod/trial/lookup?upc=#{upc}")
response = Net::HTTP.get_response(uri)
json = JSON.parse(response.body)
proxy = namespace.redis.smembers(:'proxies:all').sample
namespace.redis.smembers(:'proxies:all'
)
namespace.redis.smembers(:'proxies:all')
namespace.smembers(:'proxies:all')
Redis.current.smembers(:'proxies:all')
Redis.new { url: ENV['REDIS_URL'], namespace: ENV['REDIS_NAMESPACE'] }
client = Redis::Namespace.new ENV['REDIS_NAMESPACE'], redis: Redis.new(url: ENV['REDIS_URL'])
client.smembers(:'proxies:all')
namespace.redis.smembers(:'proxies:all')
.smembers(:'proxies:all')
Redis.smembers(:'proxies:all')
client.redis.smembers(:'proxies:all')
client.redis
Redis.current
Redis.current.smembers("#{ENV['REDIS_NAMESPACE']}")
Redis.current.smembers("#{ENV['REDIS_NAMESPACE']}:proxies:all")
proxy = namespace.redis.smembers("#{ENV['REDIS_NAMESPACE']}proxies:all").sample
Redis.current.smembers("#{ENV['REDIS_NAMESPACE']}:proxies:all")
Redis.current.smembers("#{ENV['REDIS_NAMESPACE']}:proxies:all")      proxy = Redis.current.smembers("#{ENV['REDIS_NAMESPACE']}:proxies:all").sample
proxy = Redis.current.smembers("#{ENV['REDIS_NAMESPACE']}:proxies:all").sample
last_used = namespace.get "#{key}:#{proxy}"
Time.now.to_i - last_used.to_i > 10
Time.now.to_i - last_used.to_i
Benchmark.bm do |bm|
  bm.report do
    1000.times do
      proxy = Redis.current.smembers("#{ENV['REDIS_NAMESPACE']}:proxies:all").sample
    end
  end
end
15.26 / 1000
rand(10...42)
rand
rand(11...25)
response
response['Retry-After']
response['Retry-After'].present
response['Retry-After'].present?
namespace = Redis::Namespace.new("#{Rails.env}:shared_cache", redis: Redis.new(url: ENV['REDIS_URL']))
key = "upcitemdb"
edit -t
responses
responses.size
responses.first
responses.select {|r| r['code'] != 'OK'}
responses.select {|r| r['code'] != 'OK'}.count
responses.select {|r| r['code'] == 'OK'}.count
proxy
URI.parse(proxy)
uri = URI.parse("http://#{proxy}")
uri.host
uri.port
uri
URI === uri
edit -t
proxy_host
proxy_pass
Net::HTTP.new(uri, nil, proxy_host, proxy_port, proxy_user, proxy_pass).start do |http|
  response = http.get(uri)
response = []
responses = []
Net::HTTP.new(uri, nil, proxy_host, proxy_port, proxy_user, proxy_pass).start do |http|
  response = http.get(uri)
  responses << JSON.parse(response.body)
  next_time = response['Retry-After'].present? ? response['Retry-After'] : (Time.now + rand(11...25).seconds).to_i
  namespace.set "#{key}:#{proxy}", next_time
end
uri
uri.to_s
uri = URI.parse("https://api.upcitemdb.com/prod/trial/lookup?upc=#{upc}")
upc = Identifier.upc.sample.uniq_id
uri = URI.parse("https://api.upcitemdb.com/prod/trial/lookup?upc=#{upc}")
proxy_uri = URI.parse("http://#{proxy}")
proxy_host = proxy_uri.host
proxy_port = proxy_uri.port
proxy_user = ENV['PROXY_USERNAME']
proxy_pass = ENV['PROXY_PASSWORD']
proxy = Redis.current.smembers("#{ENV['REDIS_NAMESPACE']}:proxies:all").sample
last_used = namespace.get "#{key}:#{proxy}"
if Time.now.to_i - last_used.to_i > 10 # proxy can be used
upc = Identifier.upc.sample.uniq_id
uri = URI.parse("https://api.upcitemdb.com/prod/trial/lookup?upc=#{upc}")
proxy_uri = URI.parse("http://#{proxy}")
proxy_host = proxy_uri.host
proxy_port = proxy_uri.port
proxy_user = ENV['PROXY_USERNAME']
proxy_pass = ENV['PROXY_PASSWORD']
req = Net::HTTP::Get.new(uri.to_s)
res = Net::HTTP.new(uri, nil, proxy_host, proxy_port, proxy_user, proxy_pass).start do |http|
  response = http.request(req)
  responses << JSON.parse(response.body)
  next_time = response['Retry-After'].present? ? response['Retry-After'] : (Time.now + rand(11...25).seconds).to_i
  namespace.set "#{key}:#{proxy}", next_time
end
res = Net::HTTP.new(uri.host, nil, proxy_host, proxy_port, proxy_user, proxy_pass).start do |http|
  response = http.request(req)
end
res.body
uri.host
uri
req = Net::HTTP::Get.new(uri)
res = Net::HTTP.new(uri.host, nil, proxy_host, proxy_port, proxy_user, proxy_pass).start do |http|
  response = http.request(req)
end
res.body
upc = Identifier.upc.sample.uniq_id
uri = URI.parse("https://api.upcitemdb.com/prod/trial/lookup?upc=#{upc}")
proxy_uri = URI.parse("http://#{proxy}")
proxy_host = proxy_uri.host
proxy_port = proxy_uri.port
proxy_user = ENV['PROXY_USERNAME']
proxy_pass = ENV['PROXY_PASSWORD']
proxy = Net::HTTP::Proxy(proxy_host, proxy_port, proxy_user, proxy_pass)
req = Net::HTTP::Get.new(uri.path)
proxy
proxy.class
proxy = Net::HTTP::Proxy(proxy_host, proxy_port, proxy_user, proxy_pass)
result = proxy.start(uri.host,uri.port) do |http|
  http.request(req)
end
result.body
uri = URI.parse("https://api.upcitemdb.com/prod/trial/lookup?upc=#{upc}")
proxy_uri = URI.parse("https://#{proxy}")
proxy_host = proxy_uri.host
proxy_port = proxy_uri.port
proxy_user = ENV['PROXY_USERNAME']
proxy_pass = ENV['PROXY_PASSWORD']
proxy = Net::HTTP::Proxy(proxy_host, proxy_port, proxy_user, proxy_pass)
req = Net::HTTP::Get.new(uri.path)
result = proxy.start(uri.host,uri.port) do |http|
  http.request(req)
end
result.body
ENV["http_proxy"]
uri = URI.parse("https://api.upcitemdb.com/prod/trial/lookup?upc=#{upc}")
uri.port
upc = Identifier.upc.sample.uniq_id
uri = URI.parse("https://api.upcitemdb.com/prod/trial/lookup?upc=#{upc}")
proxy_uri = URI.parse("https://#{proxy}")
proxy_host = proxy_uri.host
proxy_port = proxy_uri.port
proxy_user = ENV['PROXY_USERNAME']
proxy_pass = ENV['PROXY_PASSWORD']
request   = Net::HTTP.new(uri.host, uri.port, proxy_host, proxy_port, proxy_user, proxy_pass)
response  = request.get(uri)
response
response.body
request   = Net::HTTP.new(uri.host, uri.port, proxy_host, proxy_port, proxy_user, proxy_pass)
request.use_ssl = true
request.verify_mode = OpenSSL::SSL::VERIFY_NONE
reqest
request
response  = request.get(uri)
response.body
json = JSON.parse(response.body)
proxy
proxy = Redis.current.smembers("#{ENV['REDIS_NAMESPACE']}:proxies:all").sample
upc = Identifier.upc.sample.uniq_id
uri = URI.parse("https://api.upcitemdb.com/prod/trial/lookup?upc=#{upc}")
proxy_uri = URI.parse("https://#{proxy}")
proxy_host = proxy_uri.host
proxy_port = proxy_uri.port
proxy_user = ENV['PROXY_USERNAME']
proxy_pass = ENV['PROXY_PASSWORD']
request   = Net::HTTP.new(uri.host, uri.port, proxy_host, proxy_port, proxy_user, proxy_pass)
request.use_ssl = true
request.verify_mode = OpenSSL::SSL::VERIFY_NONE
response  = request.get(uri)
responses
responses << JSON.parse(response.body)
next_time = response['Retry-After'].present? ? response['Retry-After'] : (Time.now + rand(11...25).seconds).to_i
proxy
namespace.set "#{key}:#{proxy}", next_time
namespace.get "#{key}:#{proxy}"
request
request = Net::HTTP.new(uri.host, uri.port, proxy_host, proxy_port, proxy_user, proxy_pass) do |http|
  http.use_ssl = true
  http.verify_mode = OpenSSL::SSL::VERIFY_NONE
  http.request Net::HTTP::Get.new uri
end
request
res = Net::HTTP.start(uri.host, uri.port, proxy_host, proxy_port, proxy_user, proxy_pass, use_ssl: true ) do |http|
  response = http.request Net::HTTP::Get.new uri
end
res.body
next_time = response['Retry-After'].present? ? response['Retry-After'] : (Time.now + rand(11...25).seconds).to_i
edit -t
responses
response.last
responses.last
responses.select {|r| r['code'] != 'OK'}
not_ok = responses.select {|r| r['code'] != 'OK'}
not_ok.count
proxy = Redis.current.smembers("#{ENV['REDIS_NAMESPACE']}:proxies:all").sample
upc = Identifier.upc.sample.uniq_id
uri = URI.parse("https://api.upcitemdb.com/prod/trial/lookup?upc=#{upc}")
proxy_uri = URI.parse("https://#{proxy}")
proxy_host = proxy_uri.host
proxy_port = proxy_uri.port
proxy_user = ENV['PROXY_USERNAME']
proxy_pass = ENV['PROXY_PASSWORD']
request = Net::HTTP.new(uri.host, uri.port, proxy_host, proxy_port, proxy_user, proxy_pass)
request.proxy_address
proxy
request.use_ssl = true
request.verify_mode = OpenSSL::SSL::VERIFY_NONE
response  = request.get(uri)
reponse.headers
response.headers
response.header
response.body
JSON.parse(response.body)
response = []
next_time = response['Retry-After'].present? ? response['Retry-After'] : (Time.now + rand(11...25).seconds).to_i
response['Retry-After']
response
response  = request.get(uri)
response['Retry-After']
response['X-RateLimit-Remaining']
edit -t
responses
responses.map {|r| r['code']}
responses = []
edit 
responses
responses.map {|r| r['code']}
edit -t
responses.map {|r| r['code']}
last_used
Time.now.to_i - last_used.to_i
edit -t
responses.map {|r| r['code']}
time1 = Time.now
time2 = Time.now
time1.to_i - time2.to_i
time2.to_i - time1.to_i
proxies
namespace = Redis::Namespace.new("#{Rails.env}:shared_cache", redis: Redis.new(url: ENV['REDIS_URL']))
key = "upcitemdb"
proxies.each do |proxy|
  namespace.set "#{key}:#{proxy}", Time.now.to_i
end
proxy = Redis.current.smembers("#{ENV['REDIS_NAMESPACE']}:proxies:all").sample
last_used = namespace.get "#{key}:#{proxy}"
Time.now.to_i - _.to_i
responses = []
50.times do
  proxy = nil
  while proxy.nil?
    # TODO: Check last used time is greater than 10 secs and
    # proxy, last_used, remaining_reqs = namespace.smembers("#{key}:all").sample.split('-')
    proxy = Redis.current.smembers("#{ENV['REDIS_NAMESPACE']}:proxies:all").sample
    last_used = namespace.get "#{key}:#{proxy}"
    if Time.now.to_i - last_used.to_i > 30 # proxy can be used
      # maek request
      upc = Identifier.upc.sample.uniq_id
      uri = URI.parse("https://api.upcitemdb.com/prod/trial/lookup?upc=#{upc}")
      proxy_uri = URI.parse("https://#{proxy}")
      proxy_host = proxy_uri.host
      proxy_port = proxy_uri.port
      proxy_user = ENV['PROXY_USERNAME']
      proxy_pass = ENV['PROXY_PASSWORD']
      request = Net::HTTP.new(uri.host, uri.port, proxy_host, proxy_port, proxy_user, proxy_pass)
      request.use_ssl = true
      request.verify_mode = OpenSSL::SSL::VERIFY_NONE
      puts '*' * 80
      puts "PROXY: #{proxy}"
      response = request.get(uri)
      puts '*' * 80
      puts "COD      putsns      puts "        puts "COD  JS      puts "COD      putsns      puts "        puts "COry      puts "COD      putsns      puts "        puts "COD  JS      puts "COD      putsn        puts "COD      putsns      puts "        puts "COD  JS      puts "COD      puts "TOO SOON"
      proxy.nil?
    end
  end
edit -t
responses.map {|r| r['code']}
response
response['Retry-After']
response.as_json
response.body
Time.at(1465957386)
namespace
uri.to_s
res = nil
url = uri.to_s
open(url,
  proxy_http_basic_authentication: ["http://#{pop_proxy}", ENV['PROXY_USERNAME'], ENV['PROXY_PASSWORD']]
) do |html|
  res = html
end
proxy = Redis.current.smembers("#{ENV['REDIS_NAMESPACE']}:proxies:all").sample
edit -t
res
res.body
open(url,
  proxy_http_basic_authentication: ["http://#{proxy}", ENV['PROXY_USERNAME'], ENV['PROXY_PASSWORD']]
) do |html|
  res = html
  puts res.class
end
open(url,
  proxy_http_basic_authentication: ["http://#{proxy}", ENV['PROXY_USERNAME'], ENV['PROXY_PASSWORD']]
) do |html|
  res = html
  puts res
end
uri = URI.parse("https://api.upcitemdb.com/prod/trial/lookup?upc=#{upc}")
uri.request_uri
responses = []
proxy_ip = Redis.current.smembers("#{ENV['REDIS_NAMESPACE']}:proxies:all").sample
last_used = namespace.get "#{key}:#{proxy_ip}"
if Time.now.to_i - last_used.to_i > 30 # proxy can be used
  upc = Identifier.upc.sample.uniq_id
  uri = URI.parse("https://api.upcitemdb.com/prod/trial/lookup?upc=#{upc}")
  proxy_uri = URI.parse("https://#{proxy_ip}")
  proxy_host = proxy_uri.host
  proxy_port = proxy_uri.port
  proxy_user = ENV['PROXY_USERNAME']
  proxy_pass = ENV['PROXY_PASSWORD']
  proxy = Net::HTTP::Proxy(proxy_host, proxy_port, proxy_user, proxy_pass)
  response = proxy.start(uri.host, uri.port) { |http|
    # always connect to your.proxy.example:8080 using specified username
    # and password
    http.use_ssl = true
    http.verify_mode = OpenSSL::SSL::VERIFY_NONE
    req = Net::HTTP::Get.new(uri.path)
    http.request req
  }
  edn
edit -t
http://      uri = URI.parse("http://rvx:1Admin11@localhost:5000/api/v1/products?keywords=hdmi")data.beta.ruvixx.com/
uri = URI.parse("http://rvx:1Admin11@localhost:5000/api/v1/products?keywords=hdmi")
uri = URI.parse("http://rvx:1Admin11@data.ruvixx.com/api/v1/products?keywords=hdmi")
edit -t
response = request.get(uri)
proxy = Redis.current.smembers("#{ENV['REDIS_NAMESPACE']}:proxies:all").sample
last_used = namespace.get "#{key}:#{proxy}"
uri = URI.parse("https://data.ruvixx.com/api/v1/products?keywords=hdmi")
proxy_uri = URI.parse("https://#{proxy}")
proxy_host = proxy_uri.host
proxy_port = proxy_uri.port
proxy_user = ENV['PROXY_USERNAME']
proxy_pass = ENV['PROXY_PASSWORD']
request = Net::HTTP.new(uri.host, uri.port, proxy_host, proxy_port, proxy_user, proxy_pass)
request.use_ssl = true
request.verify_mode = OpenSSL::SSL::VERIFY_NONE
request.basic_auth 'rvx', '1Admin11'
puts '*' * 80
puts "PROXY: #{proxy}"
response = request.get(uri)
response.body
uri = URI.parse("https://data.ruvixx.com/api/v1/products?keywords=hdmi")
proxy_uri = URI.parse("https://#{proxy}")
proxy_host = proxy_uri.host
proxy_port = proxy_uri.port
proxy_user = ENV['PROXY_USERNAME']
proxy_pass = ENV['PROXY_PASSWORD']
html = Net::HTTP.new(uri.host, uri.port, proxy_host, proxy_port, proxy_user, proxy_pass)
html.use_ssl = true
html.verify_mode = OpenSSL::SSL::VERIFY_NONE
html.basic_auth 'rvx', '1Admin11'
request = Net::HTTP::Get.new(uri.request_uri)
edit -t
uri = URI.parse("https://data.ruvixx.com/api/v1/products?keywords=hdmi")
proxy_uri = URI.parse("https://#{proxy}")
proxy_host = proxy_uri.host
proxy_port = proxy_uri.port
proxy_user = ENV['PROXY_USERNAME']
proxy_pass = ENV['PROXY_PASSWORD']
html = Net::HTTP.new(uri.host, uri.port, proxy_host, proxy_port, proxy_user, proxy_pass)
request = Net::HTTP::Get.new(uri.request_uri)
request.use_ssl = true
request.verify_mode = OpenSSL::SSL::VERIFY_NONE
request.basic_auth 'rvx', '1Admin11'
response = html.get(request)
response = html.request(request)
resonse.body
response.body
uri = URI.parse("http://data.ruvixx.com/api/v1/products?keywords=hdmi")
proxy_uri = URI.parse("https://#{proxy}")
proxy_host = proxy_uri.host
proxy_port = proxy_uri.port
proxy_user = ENV['PROXY_USERNAME']
proxy_pass = ENV['PROXY_PASSWORD']
html = Net::HTTP.new(uri.host, uri.port, proxy_host, proxy_port, proxy_user, proxy_pass)
request = Net::HTTP::Get.new(uri.request_uri)
# request.use_ssl = true
# request.verify_mode = OpenSSL::SSL::VERIFY_NONE
request.basic_auth 'rvx', '1Admin11'
response = html.request(request)
response.body
edit -t
responses
responses.first
responses.length
repsonses.first == reponses[1]
responses.first == responses[1]
reponses[1]
responses[1]
responses[0]
responses[1]
proxies = []
begin
  proxies_file = "#{Rails.root}/proxy_list.txt"
  fail 'Proxy list not found' unless File.exist?(proxies_file)
  f = File.open(proxies_file, 'r')
  f.each_line do |line|
    l = line.chomp.strip
    proxies << l if l.present?
  end
ensure
  f.close
end
proxies.each do |proxy|
  namespace.set "#{key}:#{proxy}", Time.now.to_i
end
edit -t
responses
responses.last
responses.map {|r| r.code }
responses.map {|r| r['code'] }
response = request.get(uri)
response['code']
response.body
response.body['code']
edit -t
failures.size
failures
edit -t
failures
ips = failures.map {|f| f[:proxy] }
ips.size
ips.uniq
ips.uniq.size
failures.sort_by {|f| f[:proxy]}
failures.sort_by {|f| f[:time]}
10.seconds.ago
Time.now > 10.seconds.ago
Time.now > 10.seconds.from_now
10.seconds.ago > Time.now
proxy = Redis.current.smembers("#{ENV['REDIS_NAMESPACE']}:proxies:all").sample
last_used = namespace.get "#{key}:#{proxy}"
Time.at(last_used)
Time.at(last_used.to_i)
Time.at(last_used.to_i) > Time.now
Time.at(last_used.to_i) < Time.now
Time.at(last_used.to_i) < 10.seconds.ago
edit -t
failures.sort_by {|f| f[:time]}
edit -t
failures.sort_by {|f| f[:time]}
failures.select {|f| f[:code] == 'TOO_FAST'}.count
failures.map {|f| f[:proxy]}.uniq.count
failures.map {|f| f[:proxy]}.uniq
failures.map {|f| f[:proxy]}.uniq.size
failures.map {|f| f[:proxy]}.count
edit -t
failures.select {|f| f[:code] == 'TOO_FAST'}
proxies
edit -t
proxies2
proxies.uniq
proxies2.uniq
proxies2.uniq == proxies.uniq
proxies2.uniq | proxies.uniq
(proxies2.uniq | proxies.uniq).count
(proxies2.uniq & proxies.uniq).count
(proxies2.uniq & proxies.uniq)
(proxies2.uniq & proxies.uniq).count
failures.select {|f| f[:code] == 'TOO_FAST'}
subnets = proxies2.map {|p| p.split('.')[0..1].join('.')
subnets = proxies2.map {|p| p.split('.')[0..1].join('.') }
subnets.uniq
failures
failures.sort_by {|f| f[:time]}
edit -t
proxies.each do |proxy|
  namespace.set "#{key}:#{proxy}", Time.now.to_i
end
namespace.sismember('subnets:timeout', '38.121')
namespace.sadd('subnets:timeout', '38.121')
namespace.sismember('subnets:timeout', '38.121')
namespace.srem('subnets:timeout', '38.121')
namespace.sismember('subnets:timeout', '38.121')
namespace.sadd('subnets:available', Redis.current.smembers("#{ENV['REDIS_NAMESPACE']}:proxies:all"))
namespace.srem('subnets:available', Redis.current.smembers("#{ENV['REDIS_NAMESPACE']}:proxies:all"))
namespace.smembers('subnets:available')
namespace.smembers('proxies:available')
namespace
namespace.sadd('upcitemdb:proxies', Redis.current.smembers("#{ENV['REDIS_NAMESPACE']}:proxies:all"))
namespace.del('upcitemdb:proxies')
proxy = namespace.smembers('upcitemdb:proxies').sample
edit -t
failures
proxy
sub = proxy.split('.')[0..2].join('.')
sub = proxy.split('.')[0..1].join('.')
edit -t
failures
namespace.smembers('upcitemdb:proxies')
sub
failures
sub = '23.108'
namespace.smembers('upcitemdb:proxies').select {|p| p.split('.')[0..1].join('.') == sub
}
proxy.split('.')
proxy.split('.')[0..2]
edit -t
responses = []
failures = []
upc = Identifier.upc.sample.uniq_id
uri = URI.parse("https://api.upcitemdb.com/prod/trial/lookup?upc=#{upc}")
namespace.sadd('upcitemdb:proxies', Redis.current.smembers("#{ENV['REDIS_NAMESPACE']}:proxies:all"))
proxy = namespace.smembers('upcitemdb:proxies').sample
proxy_uri = URI.parse("https://#{proxy}")
proxy_host = proxy_uri.host
proxy_port = proxy_uri.port
proxy_user = ENV['PROXY_USERNAME']
proxy_pass = ENV['PROXY_PASSWORD']
last_used = namespace.get "#{key}:#{proxy.split('.')[0..2]}"
Time.at(last_used.to_i) < Time.now
edit -t
failures
edit -t
failures
proxy = "38.18.22.232:60000"
proxy_uri = URI.parse("https://#{proxy}")
proxy_host = proxy_uri.host
proxy_port = proxy_uri.port
proxy_user = ENV['PROXY_USERNAME']
proxy_pass = ENV['PROXY_PASSWORD']
last_used = namespace.get "#{key}:#{proxy.split('.')[0..1]}"
Time.at(_.to_f)
Time.at(_.to_i)
request = Net::HTTP.new(uri.host, uri.port, proxy_host, proxy_port, proxy_user, proxy_pass)
request.use_ssl = true
request.verify_mode = OpenSSL::SSL::VERIFY_NONE
response = request.get(uri)
response.body
response['X-RateLimit-Remaining']
response['Retry-After']
response['X-RateLimit-Reset']
Time.at(_.to_i)
failures
proxy = "38.18.22.84:60000"
proxy_uri = URI.parse("https://#{proxy}")
proxy_host = proxy_uri.host
proxy_port = proxy_uri.port
proxy_user = ENV['PROXY_USERNAME']
proxy_pass = ENV['PROXY_PASSWORD']
request = Net::HTTP.new(uri.host, uri.port, proxy_host, proxy_port, proxy_user, proxy_pass)
request.use_ssl = true
request.verify_mode = OpenSSL::SSL::VERIFY_NONE
response = request.get(uri)
response['X-RateLimit-Reset']
_ == "1466042389"
proxy = "38.18.19.88:60000"
edit -t
response['X-RateLimit-Reset']
proxy = "38.121.47.202:60000"
edit -t
response['X-RateLimit-Reset']
response['X-RateLimit-Limit']
response['X-RateLimit-Remaining']
edit -t
subs
namespace.sadd('upcitemdb:proxies', Redis.current.smembers("#{ENV['REDIS_NAMESPACE']}:proxies:all"))
edit -t
subs.count
edit -t
proxy = "23.105.161.61:60000"
sub = proxy.split('.')[0..2].join('.')
matching_subs = namespace.smembers('upcitemdb:proxies').select {|p| p.split('.')[0..2].join('.') == sub}
edit -t
response = []
proxy = "23.105.161.61:60000"
sub = proxy.split('.')[0..1].join('.')
matching_subs = namespace.smembers('upcitemdb:proxies').select {|p| p.split('.')[0..1].join('.') == sub}
matching_subs.count
edit -t
matching_subs.map {|p| p.split('.')[0..2].join('.') }
matching_subs.map {|p| p.split('.')[0..2].join('.') }.uniq
edit -t
proxies.map {|p| p.split('.')[0..2].join('.') }.uniq
14 * 200
responses
response = responses.first
json = JSON.parse(response.body)
response = responses[1]
json = JSON.parse(response.body)
response = responses[2]
json = JSON.parse(response.body)
response = responses[3]
json = JSON.parse(response.body)
UPC.all.count
Identifier.where.not(id_type: 0).sample
reset
id = 1155
identifier = Identifier.find(id)
reload!
ENV
Identifier
Identifier.connection
id
exit
Identifier
Identifier.connection
id = 1155
identifier = Identifier.find(id)
upc = identifier.uniq_id
edit -t
response = call_upc_item_db(upc)
edit -t
response = call_upc_item_db(upc)
edit -t
response = call_upc_item_db(upc)
json = JSON.parse(response.body)
json
json['items'][0]
json['items'][0]['upc'] == upc
json
json['items']
upc
upcs = [upc, '8806086449878']
item = json['items'][0]
Product.find_by(brand: item['brand'], model: item['model'])
Product.find_by(brand: item['brand'], model: nil)
Product.find_by(brand: item['brand'], model: '')
Product.find_or_create_by(brand: item['brand'], model: '')
p = _
p.destroy
json
json.to_json
json.as_json
json.to_json
response.body
json.to_json
json
reload!
Product.reload!
reload! Product
p = Product.find_or_create_by(brand: item['brand'], model: '')
Identifier.id_types
Identifier.id_types.keys
type = _.first
Identifier.id_types[type]
reload! Identifier
identifier
identifier.reload
identifier.related_identifiers
reload!
identifier.reload
identifier.related_identifiers
exit
identifier = Identifier.find 1155
identifier.related_identifiers
reload!
identifier = Identifier.find 1155
identifier.related_identifiers
reload!
identifier = Identifier.find 1155
l = Listing.all.sample
l.identifiers
l.related_listings
l.identifiers
l.identifiers.first.related_identifiers
Identifier.upc.count
Identifier.id_types[:asin]
Identifier.id_types[:asin, :upc]
edit -t
json[:items]
json = json.with_indifferent_access
json[:items]
json[:items][0]
json[:items][0][:offers]
json[:items][0][:offers][0]
Identifier.where.not(id_type: Identifier.id_types[:asin]).not_processed
Identifier.where.not(id_type: Identifier.id_types[:asin]).not_processed.count
Identifier.where.not(id_type: Identifier.id_types[:asin]).not_processed.limit(2000)
Identifier.where.not(id_type: Identifier.id_types[:asin]).not_processed.limit(100).count
Identifier.where.not(id_type: Identifier.id_types[:asin]).not_processed.limit(100)
identifier = _.first
identifier.processed?
exit
Identifier.processed.count
Identifier.asin.processed
Identifier.asin.processed.count
Identifier.asin.count
id = Identifier.asin.first
id.related_identifiers
id = Identifier.asin.processed.sample
id.related_identifiers
id.related_listings
id.related_identifiers
id.related_identifiers.flat_map(&:listings)
Listings.includes(:identifiers).where('listings.id': id.related_identifiers.ids << id.id)
Listing.includes(:identifiers).where('listings.id': id.related_identifiers.ids << id.id)
Listing.includes(:identifiers).where('listings.id': id.related_identifiers.ids << id.id).count
id.listings.count
id.listings
Listing.includes(:identifiers).where('listings.id': id.related_identifiers.ids << id.id)
Listing.includes(:identifiers).where('listings.id': id.related_identifiers.ids << id.id).destroy_all
reload!
p = Product.first
p.related_products
Product.all
reload!
asin = Identifier.asin.processed.sample
asin.update process_at: DateTime.now
asin.update processed_at: DateTime.now
asin.reload
Identifier.asin.processed.each do |asin|
  asin.update processed_at: asin.updated_at
end
exit
ENV['SCHEDULE_JOBS']
ENV['SCHEDULE_JOBS'].true
ENV['SCHEDULE_JOBS'].true?
exit
ENV['SCHEDULE_JOBS'].true?
ENV['SCHEDULE_JOBS']
ENV['SCHEDULE_JOBS'] == 'true'
ENV['SCHEDULE_JOBS']
exit
ENV['SCHEDULE_JOBS']
$LOAD_PATH
exit
AsinToUpcScheduleJob.new.perform
exit
AsinToUpcScheduleJob
exit
Identifier.asin.not_processed
Identifer.last
Identifier.last
id = _
id.listings
id.listings.count
l = id.listings.first
l.identifiers
namespace = Redis::Namespace.new("#{Rails.env}:shared_cache", redis: Redis.new(url: ENV['REDIS_URL']))
namespace.srandmember('upcitemdb:proxies')
proxy = '38.120.19.34:60000'
edit -t
uri = URI.parse("https://api.upcitemdb.com/prod/trial/lookup?upc=#{upc}")
request = proxy_req(proxy, uri)
namespace = Redis::Namespace.new("#{Rails.env}:shared_cache", redis: Redis.new(url: ENV['REDIS_URL']))
response = nil
proxy = '38.120.19.34:60000'
uri = URI.parse("https://api.upcitemdb.com/prod/trial/lookup?upc=#{upc}")
Identifier.where.not(id_type: Identifier.id_types[:asin]).not_processed.limit(50).first
upc = _
upc = upc.uniq_id
uri = URI.parse("https://api.upcitemdb.com/prod/trial/lookup?upc=#{upc}")
request = proxy_req(proxy, uri)
last_used = namespace.get "#{key}:#{proxy.split('.')[0..2]}"
key = "upcitemdb"
last_used = namespace.get "#{key}:#{proxy.split('.')[0..2]}"
sleep rand(10..15) if Time.at(last_used.to_i) < Time.now # proxy can be used
Time.at(last_used.to_i)
Time.at(last_used.to_i) < Time.now
Time.at(last_used.to_i) > Time.now
response = request.get(uri)
proxies = []
begin
  proxies_file = "#{Rails.root}/proxy_list.txt"
  fail 'Proxy list not found' unless File.exist?(proxies_file)
  f = File.open(proxies_file, 'r')
  f.each_line do |line|
    l = line.chomp.strip
    proxies << l if l.present?
  end
ensure
  f.close
end
Redis.current.del("#{ENV['REDIS_NAMESPACE']}:proxies:all")
Redis.current.sadd("#{ENV['REDIS_NAMESPACE']}:proxies:all", proxies)
namespace = Redis::Namespace.new("#{Rails.env}:shared_cache", redis: Redis.new(url: ENV['REDIS_URL']))
namespace.sadd("#{key}:proxies", Redis.current.smembers("#{ENV['REDIS_NAMESPACE']}:proxies:all"))
proxy = '38.18.15.24:60000'
request = proxy_req(proxy, uri)
response = request.get(uri)
respone
response
response = request.get(uri)
response['X-Limit'
response['X-Limit']
response['X-RateLimit-Remaining']
response.as_json
response['X-RateLimit-Reset']
Time.at(response['X-RateLimit-Reset'].to_i)
response['Retry-After'].to_i
response['Retry-After']
Time.at(response['X-RateLimit-Reset'].to_i) == Time.at(response['Retry-After'].to_f)
Time.at(Time.now + response['Retry_after'].to_f.seconds)
Time.at(response['X-RateLimit-Reset'].to_i)
Sidekiq::Limiter
all_proxies = Redis.current.smembers("#{ENV['REDIS_NAMESPACE']}:proxies:all"))
all_proxies = Redis.current.smembers("#{ENV['REDIS_NAMESPACE']}:proxies:all")
proxy.split('.')
proxy.split('.')[0..2]
proxy.split('.')[0..2].join('.')
edit -t
subs
subs.values
edit -t
namespace.smembers("#{key}:proxies")
names.lrange
namespavce.lrange
namespace.blpop("#{key}:proxies")
namespace.del("#{key}:proxies")
namespace.rpush("#{key}:proxies", proxy)
namespace.smemebers("#{key}:proxies")
namespace.smembers("#{key}:proxies")
namespace.values("#{key}:proxies")
namespace.keys("#{key}:proxies")
namespace.lrange("#{key}:proxies", 0, 1)
namespace.blpop("#{key}:proxies")
namespace.rpush("#{key}:proxies", proxy)
namespace.lrange("#{key}:proxies", 0, 1)
namespace.lrange("#{key}:proxies", 0, -1)
proxy = namespace.blpop("#{key}:proxies")
proxy
all_proxies = Redis.current.smembers("#{ENV['REDIS_NAMESPACE']}:proxies:all")
subs = {}
edit -t
namespace.del("#{key}:proxies")
subs
edit -t
proxy1 = namespace.blpop("#{key}:proxies")
proxy2 = namespace.blpop("#{key}:proxies")
proxy1
proxy1[1]
namespace.rpush("#{key}:proxies", proxy1[1])
namespace.rpush("#{key}:proxies", proxy2[1])
namespace.del("#{key}:proxies")
namespace.lrange("#{key}:proxies", 0, -1)
subs.values
subs.values.shuffle
proxy1 = namespace.blpop("#{key}:proxies")
namespace.lrange("#{key}:proxies", 0, -1)
proxy1 = namespace.blpop("#{key}:proxies", timeout: 2)
key, proxy1 = namespace.blpop("#{key}:proxies", timeout: 2)
key
key = 'upcitemdb'
_key, proxy1 = namespace.blpop("#{key}:proxies", timeout: 2)
proxy1
response
proxy = '38.18.15.24:60000'
request = proxy_req(proxy, uri)
uri
uri = URI.parse("https://api.upcitemdb.com/prod/trial/lookup?upc=#{Item.upc.sample.uniq_id}")
uri = URI.parse("https://api.upcitemdb.com/prod/trial/lookup?upc=#{Identifier.upc.sample.uniq_id}")
request = proxy_req(proxy, uri)
response
response = request.get(uri)
response['X-RateLimit-Remaining']
uri = URI.parse("https://api.upcitemdb.com/prod/trial/lookup?upc=#{Identifier.upc.sample.uniq_id}")
request = Net::HTTP.new(uri.host, uri.port)
response = request.get(uri)
response.as_json
request.use_ssl = true
request.verify_mode = OpenSSL::SSL::VERIFY_NONE
response = request.get(uri)
response['X-RateLimit-Remaining']
99.times do
  100 * 14
100 * 14
Identifier.where.not(id_type: Identifier.id_types[:asin]).not_processed.limit(1400).count
Identifier.where.not(id_type: Identifier.id_types[:asin]).not_processed.limit(50).count
99.times do
  uri = URI.parse("https://api.upcitemdb.com/prod/trial/lookup?upc=#{Identifier.upc.sample.uniq_id}")
  request = Net::HTTP.new(uri.host, uri.port)
  request.use_ssl = true
  request.verify_mode = OpenSSL::SSL::VERIFY_NONE
  response = request.get(uri)
  puts response['X-RateLimit-Remaining']
  puts response.code
  sleep 10
end
response
response['Retry-After']
last_used = namespace.get "#{key}:#{proxy.split('.')[0..2]}"
_redis_key, proxy = namespace.blpop("#{key}:proxies", timeout: 5)
edit -t
namespace = Redis::Namespace.new("#{Rails.env}:shared_cache", redis: Redis.new(url: ENV['REDIS_URL']))
reset_proxies
all_proxies = Redis.current.smembers("#{ENV['REDIS_NAMESPACE']}:proxies:all").shuffle
edit -t
response = nil
key = "upcitemdb"
_redis_key, proxy = namespace.blpop("#{key}:proxies", timeout: 3)
proxy.nil?
Time.at(last_used.to_i)
last_used.to_i
last_used.to_i - Time.now
last_used.to_i - Time.now.to_i
sleep _
last_used.to_i
time = last_used.to_i - Time.now.to_i
time.negative?
time.negative < 0
response
_redis_key, proxy = namespace.blpop("#{key}:proxies", timeout: 3)
uri = URI.parse("https://api.upcitemdb.com/prod/trial/lookup?upc=#{upc}")
def proxy_req(proxy, uri)
  proxy_host, proxy_pass, proxy_port, proxy_user = set_proxy(proxy)
  request = Net::HTTP.new(uri.host, uri.port, proxy_host, proxy_port, proxy_user, proxy_pass)
  request.use_ssl = true
  request.verify_mode = OpenSSL::SSL::VERIFY_NONE
  request
end
def set_proxy(proxy)
  proxy_uri = URI.parse("https://#{proxy}")
  proxy_host = proxy_uri.host
  proxy_port = proxy_uri.port
  proxy_user = ENV['PROXY_USERNAME']
  proxy_pass = ENV['PROXY_PASSWORD']
  return proxy_host, proxy_pass, proxy_port, proxy_user
end
edit -t
request = proxy_req(proxy, uri)
last_used = namespace.get "#{key}:#{proxy.split('.')[0..2]}"
sleep [(last_used.to_i - Time.now.to_i), 0].max + 1
response = request.get(uri)
if response.code != '200'
  logger.info "#{proxy} : #{response.code} - #{response}"
end
puts "#{proxy} : #{response.code} - #{response}"
puts "#{proxy} : #{response.code} - #{response.as_json}"
edit -t
set_proxies('upcitemdb')
namespace.lrange("#{key}:proxies", 0, -1)
edit -t
response = request.get(uri)
response['X-RateLimit-Limit']
response['X-RateLimit-Reset']
Time.at(response['X-RateLimit-Reset'].to_i)
proxy
last_used = namespace.get "#{key}:#{proxy.split('.')[0..2]}"
Time.at(last_used.to_i)
response['X-RateLimit-Remaining']
remaining == 0 && response['X-RateLimit-Limit'] == '100'
remaining = response['X-RateLimit-Remaining'].to_i
remaining == 0 && response['X-RateLimit-Limit'] == '100'
set_proxies('upcitemdb')
namespace.lrange("#{key}:proxies", 0, -1)
reset_proxies
set_proxies('upcitemdb')
Identifier.last.upc
Identifier.upc.last
Identifier.ean.last
set_proxies('upcitemdb')
Identifier.last
proxy = '38.120.17.186:60000
proxy = '38.120.17.186:60000'
edit -t
upc = Identifier.upc.sample
edit -t
identifier = upc
response = call_upc_item_db(uniq_id)
uniq_id = identifier.uniq_id
response = call_upc_item_db(uniq_id)
edit -t
response = call_upc_item_db(uniq_id)
key
set_proxies(key)
response = call_upc_item_db(uniq_id)
edit -t
response = call_upc_item_db(uniq_id)
edit -t
response = call_upc_item_db(uniq_id)
edit -t
response = call_upc_item_db(uniq_id)
edit -t
response = call_upc_item_db(uniq_id)
response.nil? || response.code != 200
response.code.class
response.nil? || response.code != '200'
response
Time.at(response['X-RateLimit-Reset'].to_i)
Identifier.last
Identifier.asin.last
id = _
id.listings
l = _.first
l.identifiers
l.url
identifier.id_type
identifier.upc!
identifier.upc
identifier.upc!
identifier.upc?
identifier.id_type
edit -t
identifier.reload
exit
identifier = Identifier.last
identifier.listing_identifier
ListingIdentifier.where(id_type: 0).count
ListingIdentifier.where(source_type: 0).count
ListingIdentifier.where(source_type: 1).count
ListingIdentifier.source_types[:api]
ProductListing.source_types[:api]
response
Identifier.ean.last
Identifier.upc.find_by(uniq_id: '792343349204')
id = _
identifier = _
uniq_id = identifier.uniq_id
edit -t
response = call_upc_item_db(uniq_id)
edit -t
response = call_upc_item_db(uniq_id)
response.body
response.body['items']
Oj.load(response.body)
Oj.parse(response.body)
json = Oj.load(response.body)
identifier.products.pluck(:brand, :model)
RvxSignal.last
RvxSignal.last.as_json
RvxSignal.where(brand: 'HISENSE', model: '65K680DW')
RvxSignal.where(brand: 'HISENSE', model: '65K680DW').count
RvxSignal.where(brand: 'HISENSE', model: '65K680DW')
RvxSignal.last.as_json
RvxSignal.where(brand: 'HISENSE', model: '65K680DW').pluck(:data_hash)
response
products = []
json['items'].each do |item|
  brand = item['brand']
  model = item['model']
  if brand && model
    products << Product.find_or_create_by(brand: brand, model: model)
  end
end
products
products.each do |product|
  identifier.listings.each do |listing|
    ProductListing.find_or_create_by(
      product_id: product.id,
      listing_id: listing.id,
      source_type: ProductListing.source_types[:api]
    )
  end
end
p = _.first
p.identifiers
p.listings
ProductListing.where(source_type: ProductListing.source_types[:api])
exit
Listing.where(scrape: Scrape.where(source: Source.find_by(name: 'amazonus'))).select(:id).find_each do |listing|
  CleanAmazonUrlJob.perform_async listing.id
end
exit
Listing.where(scrape: Scrape.where(source: Source.find_by(name: 'amazonus'))).select(:id).find_each do |listing|
  CleanAmazonUrlJob.perform_async listing.id
end
require 'clean_amazon_url_job'
require 'lib/tasks/clean_amazon_url_job'
require 'lib/tasks/clean_amazon_url_job.rb'
require 'tasks/clean_amazon_url_job.rb'
require 'tasks/clean_amazon_url_job'
Listing.where(scrape: Scrape.where(source: Source.find_by(name: 'amazonus'))).select(:id).find_each do |listing|
  CleanAmazonUrlJob.perform_async listing.id
end
Listing.where(scrape: Scrape.where(source: Source.find_by(name: 'amazonus'))).select(:id).size
Identifier.last
Identifier.asin.last
Listing.where(scrape: Scrape.where(source: Source.find_by(name: 'amazonus'))).select(:id).size
Listing.where(scrape: Scrape.where(source: Source.find_by(name: 'amazonus'))).select(:id).first
Listing.where(scrape: Scrape.where(source: Source.find_by(name: 'amazonus'))).select(:id).first.url
Listing.where(scrape: Scrape.where(source: Source.find_by(name: 'amazonus')).first.url
Listing.where(scrape: Scrape.where(source: Source.find_by(name: 'amazonus').first.url
Listing.where(scrape: Scrape.where(source: Source.find_by(name: 'amazonus'))).url
Listing.where(scrape: Scrape.where(source: Source.find_by(name: 'amazonus'))).first.url
Listing.where(scrape: Scrape.where(source: Source.find_by(name: 'amazonus'))).last.url
Listing.where(scrape: Scrape.where(source: Source.find_by(name: 'amazonus'))).select(:id).find_each do |listing|
  CleanAmazonUrlJob.perform_async listing.id
end
Listing.first
Listing.first.url
exit
Listing.first
Listing.first.url
Listing.pluck(:url)
Listing.pluck(:url).count
Listing.pluck(:url).uniq.count
Listing.pluck(:url, :scrape).uniq.count
Listing.pluck(:url, :scrape_id).uniq.count
Listing.pluck(:url, :scrape_id).count
exit
url = 'http://www.ebay.com/itm/USB3-0-HUB-4-Ports-Aluminum-High-Speed-For-Macbook-Pro-Mac-PC-Laptop-Silver-/351736705852'
scrape_id, source_category_id, url, retry_time = 1, 1, 'http://www.ebay.com/itm/USB3-0-HUB-4-Ports-Aluminum-High-Speed-For-Macbook-Pro-Mac-PC-Laptop-Silver-/351736705852', 0
EbayProductDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
exit
scrape_id, source_category_id, url, retry_time = 1, 1, 'http://www.ebay.com/itm/USB3-0-HUB-4-Ports-Aluminum-High-Speed-For-Macbook-Pro-Mac-PC-Laptop-Silver-/351736705852', 0
EbayProductDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l.url
exit
scrape_id, source_category_id, url, retry_time = 1, 1, 'http://www.ebay.com/itm/USB3-0-HUB-4-Ports-Aluminum-High-Speed-For-Macbook-Pro-Mac-PC-Laptop-Silver-/351736705852', 0
EbayProductDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
exit
scrape_id, source_category_id, url, retry_time = 1, 1, 'http://www.ebay.com/itm/USB3-0-HUB-4-Ports-Aluminum-High-Speed-For-Macbook-Pro-Mac-PC-Laptop-Silver-/351736705852', 0
EbayProductDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l.screenshots.first
l.screenshots.first.attachment.url
exit
EbayProductDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
scrape_id, source_category_id, url, retry_time = 1, 1, 'http://www.ebay.com/itm/USB3-0-HUB-4-Ports-Aluminum-High-Speed-For-Macbook-Pro-Mac-PC-Laptop-Silver-/351736705852', 0
EbayProductDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
scrape_id, source_category_id, url, retry_time = 2, 1, 'http://www.ebay.com/itm/USB3-0-HUB-4-Ports-Aluminum-High-Speed-For-Macbook-Pro-Mac-PC-Laptop-Silver-/351736705852', 0
EbayProductDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
Rails.root
l.screenshots
l.screenshots.first.attachment.url
l
jsmethod = "document.write(\"<sc\" + \"ript type=\" + \"'tex\" + \"t/jav\" + \"ascript'\" + \" src='http://pv.datacaciques.c\" + \"om/js/trackr.js?rc=\"+ parseInt(new Date/43200000) +\"&v=2&lid=351736705852'>\" + \"<\" + \"/sc\" + \"ript>\");"
jsmethod = JSON.parse("document.write(\"<sc\" + \"ript type=\" + \"'tex\" + \"t/jav\" + \"ascript'\" + \" src='http://pv.datacaciques.c\" + \"om/js/trackr.js?rc=\"+ parseInt(new Date/43200000) +\"&v=2&lid=351736705852'>\" + \"<\" + \"/sc\" + \"ript>\");")
jsmethod = "document.write(\"<sc\" + \"ript type=\" + \"'tex\" + \"t/jav\" + \"ascript'\" + \" src='http://pv.datacaciques.c\" + \"om/js/trackr.js?rc=\"+ parseInt(new Date/43200000) +\"&v=2&lid=351736705852'>\" + \"<\" + \"/sc\" + \"ript>\");".to_hrml
jsmethod = "document.write(\"<sc\" + \"ript type=\" + \"'tex\" + \"t/jav\" + \"ascript'\" + \" src='http://pv.datacaciques.c\" + \"om/js/trackr.js?rc=\"+ parseInt(new Date/43200000) +\"&v=2&lid=351736705852'>\" + \"<\" + \"/sc\" + \"ript>\");".to_html
jsmethod
jsmethod.html_safe
<%= @str %>
result = ERB.new(jsmethod).result
l
l.destroy
blacklist_urls = ['http://111.92.230.253/WebProductImage/template/4utoto/template-20111220/images/model/levelb1.png']
url_blacklist = [
  'https://plus.google.com', # Google Plus
  'http://www.google-analytics.com', # Google Analytics
  'http://www.google.com/aclk',
  'http://googleads.g.doubleclick.net', # Google Ads
  'http://aax-us-east.amazon-adsystem.com', # Amazon Ads
  # ' https : //is.aliexpress.com ' # Alibaba beacon times out alot
  # DH GATE ADS
  'https://googleads.g.doubleclick.net', # Google Ads
  'https://www.googleadservices.com',
  'https://cm.g.doubleclick.net/pixel',
  'http://ums.adtechus.com/',
  'http://rtb-csync.smartadserver.com/',
  'http://ads4.admatic.com.tr/',
  'http://ib.adnxs.com/',
  'http://t4.liverail.com/',
  'http://ads.admized.com/',
  'http://pixel.rubiconproject.com/ta',
  'https://ad.doubleclick.net/'
edit -t
message
message.scan(/http.+(,(?=http)|$)/)
message.scan(/https?.+(?=(,|http|$)/)
message
message.include?('resources still waiting')
message.gsub(/resources\sstill\swaiting\s?/, '')
urls = message.gsub(/resources\sstill\swaiting\s?/, '')
urls
message = "Request to 'http://www.ebay.com/itm/USB3-0-HUB-4-Ports-Aluminum-High-Speed-For-Macbook-Pro-Mac-PC-Laptop-Silver-/351736705852' failed to reach server, check DNS and/or server status - Timed out with the following resources still waiting http://www.besskystore.com/ebay/go_vovotrade/top.jpg,http://www.besskyebay.com/besskyebay7/0T771I_20140418034545308.jpg,http://www.besskymall.com/besskymall38/XYQ50615122A_20150615112416856.jpg,http://www.besskystore.com/ebay/go_vovotrade/d_08.gif,http://www.besskystore.com/ebay/go_vovotrade/ht_01b.gif,http://www.besskystore.com/ebay/go_vovotrade/ht_03b.gif,http://www.besskystore.com/ebay/go_vovotrade/store.gif,http://www.besskystore.com/ebay/go_vovotrade/ht_01.gif,http://www.besskystore.com/ebay/go_vovotrade/ht_03.gif,http://www.besskystore.com/ebay/go_vovotrade/camera.jpg,http://www.besskystore.com/ebay/go_vovotrade/iphone%20case.jpg,http://www.besskystore.com/ebay/go_vovotrade/leather.jpg,http://www.besskystore.com/ebay/go_vovotrade/battery.jpg,http://www.besskystore.com/ebay/go_vovotrade/gps.jpg,http://www.besskystore.com/ebay/go_vovotrade/cover.jpg,http://www.besskystore.com/ebay/go_vovotrade/laptop%20charger.jpg,http://www.besskystore.com/ebay/go_vovotrade/mouse.jpg,http://www.besskystore.com/ebay/go_vovotrade/card%20reader.jpg,http://www.besskystore.com/ebay/go_vovotrade/hub.jpg,http://www.besskystore.com/ebay/go_vovotrade/scale.jpg,http://www.besskystore.com/ebay/go_vovotrade/Thermometer.jpg,http://www.besskystore.com/ebay/go_vovotrade/LED.jpg,http://www.besskystore.com/ebay/go_vovotrade/Earring.jpg,http://www.besskystore.com/ebay/go_vovotrade/memory.jpg,http://www.besskystore.com/ebay/go_vovotrade/flashlight.jpg,http://www.besskystore.com/ebay/go_vovotrade/gift.jpg,http://www.besskystore.com/ebay/go_vovotrade/game.jpg,http://www.besskystore.com/ebay/go_vovotrade/au_delivery.jpg,http://www.besskystore.com/ebay/other/paypal.gif,http://www.besskystore.com/ebay/go_vovotrade/Bottom.jpg,http://www.besskyebay.com/besskyebay6/0V244A_20131221083126630.jpg,http://www.besskyebay.com/besskyebay6/0T652A_20131224083949740.jpg,http://www.besskyebay.com/besskyebay6/%23T252B_2014020908283870.jpg,http://www.besskyebay.com/besskyebay6/%23T251B_20131224084243707.jpg,http://www.besskyebay.com/besskyebay6/0V162A_20131217115221994.jpg,http://www.besskyebay.com/besskyebay7/0V182A_20140416031125450.jpg,http://www.besskyebay.com/besskyebay6/%23M294B_20140318035243328.JPG,http://www.besskyebay.com/besskyebay6/&A112G_20140315031403600.JPG,http://www.besskyebay.com/besskyebay1/%23V312B_20130527025413881.jpg,http://www.besskyebay.com/besskyebay6/%23X343F_20131226065312126.jpg,http://www.besskyebay.com/besskyebay6/0V381D_20131219053959984.jpg,http://www.besskyebay.com/besskyebay6/0U651F_20140326085328107.JPG,http://www.besskyebay.com/besskyebay7/LXL4040202E_20140403013809462.jpg,http://www.besskyebay.com/besskyebay7/LXL4041001I_20140411065605546.jpg,http://www.besskyebay.com/besskyebay7/ZLY4050604G_20140508070945223.jpg,http://www.besskyebay.com/besskyebay7/ZLY4050603G_20140508051454879.jpg,http://www.besskyebay.com/besskyebay7/ZLY4043001A_20140501035016134.jpg,http://www.besskyebay.com/besskyebay7/LSH4050701A_20140508015931744.jpg,http://www.besskyebay.com/besskyebay7/LKM4041201A_20140416102821925.jpg,http://www.besskyebay.com/besskyebay7/LXL4042202A_20140424065754353.jpg,http://www.besskyebay.com/besskyebay6/0E723A_20140115085716969.jpg,http://www.besskyebay.com/besskyebay6/0C471B_20140227105153901.jpg,http://www.besskyebay.com/besskyebay6/0I433E_20140121104037961.jpg,http://www.besskyebay.com/besskyebay6/0W233A_2014021808463377.jpg,http://www.besskymall.com/besskymall38/XYQ50615122A_20150615112416666.jpg,http://www.besskymall.com/besskymall38/XYQ50615122A_20150615112415390.jpg,http://www.besskymall.com/besskymall38/XYQ50615122A_20150615112415745.jpg,http://www.besskymall.com/besskymall38/XYQ50615122A_20150615112416760.jpg,http://www.besskymall.com/besskymall38/XYQ50615122A_20150615112419709.jpg,http://www.besskymall.com/besskymall38/XYQ50615122A_20150615112418290.jpg,http://www.besskystore.com/ebay/go_vovotrade/bj.jpg,http://www.besskystore.com/ebay/go_vovotrade/ht_02b.gif,http://www.besskystore.com/ebay/go_vovotrade/ht_02.gif"
urls = message.gsub(/\A.+resources\sstill\swaiting\s?/, '')
urls..split(/,(?=(https?:|\/\/))/)
urls.split(/,(?=(https?:|\/\/))/)
urls
urls.split(',')
urls = message.gsub(/\A.+resources\sstill\swaiting\     failed_resources = e.message.gsub(/\A.+resources\sstill\swaiting\s?/, '')s?/, '')
failed_resources = e.message.gsub(/\A.+resources\sstill\swaiting\s?/, '')
failed_resources = message.gsub(/\A.+resources\sstill\swaiting\s?/, '')
failed_resources = message.gsub(/\A.+resources\sstill\swaiting\s?/, '').split(',')
exit
url
url = 'http://www.ebay.com/itm/USB3-0-HUB-4-Ports-Aluminum-High-Speed-For-Macbook-Pro-Mac-PC-Laptop-Silver-/351736705852'
Listing.last
Listing.last.destroy
Listing.last
Listing.last.url
url = 'http://www.ebay.com/itm/USB3-0-HUB-4-Ports-Aluminum-High-Speed-For-Macbook-Pro-Mac-PC-Laptop-Silver-/351736705852'
EbayProductDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
scrape_id, source_category_id, url, retry_time = 2, 1, 'http://www.ebay.com/itm/USB3-0-HUB-4-Ports-Aluminum-High-Speed-For-Macbook-Pro-Mac-PC-Laptop-Silver-/351736705852', 0
EbayProductDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
Listing.last
Listing.last.url
exit
scrape_id, source_category_id, url, retry_time = 2, 1, 'http://www.ebay.com/itm/USB3-0-HUB-4-Ports-Aluminum-High-Speed-For-Macbook-Pro-Mac-PC-Laptop-Silver-/351736705852', 0
EbayProductDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
exit
scrape_id, source_category_id, url, retry_time = 2, 1, 'http://www.ebay.com/itm/USB3-0-HUB-4-Ports-Aluminum-High-Speed-For-Macbook-Pro-Mac-PC-Laptop-Silver-/351736705852', 0
EbayProductDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
exit
scrape_id, source_category_id, url, retry_time = 2, 1, 'http://www.ebay.com/itm/USB3-0-HUB-4-Ports-Aluminum-High-Speed-For-Macbook-Pro-Mac-PC-Laptop-Silver-/351736705852', 0
EbayProductDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l.url
url = 'http://www.ebay.com/itm/1080P-HD-USB-HDMI-Multi-TV-Media-Video-Player-Box-TV-video-MMC-RMVB-MP3-LO-/161885081078'
EbayProductDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l.url
l = Listing.last
seller_desc_url = @session.first(:seller_desc_iframe)['src']
seller_desc_url = 'http://vi.vipr.ebaydesc.com/ws/eBayISAPI.dll?ViewItemDescV4&item=401113081051&t=1461893033000&tid=10&category=43440&seller=tomtop_shop&excSoj=1&excTrk=1&lsite=0&ittenable=false&domain=ebay.com&descgauge=1'
html = open(seller_desc_url)
Nokogiri::HTML(html)
Nokogiri::HTML(html).css('body')
Nokogiri::HTML(html).css('body').text
Nokogiri::HTML(html)
html = open(seller_desc_url)
nodes = Nokogiri::HTML(html)
nodes.css('body')
nodes.css('body').text
nodes.css('body').inner_text
body = nodes.css('body')
body.text
nodes.inner_text
nodes.xpath('//text()')
body = nodes.css('body') { |config| config.noblanks }
body.inner_text
body.inner_text.strip
body.inner_text.strip.gsub(/\s+/, ' ')
text = ''
body.traverse |node|
if node.text? and !(node.text =~ /^\s*$/)
  text += node.text.strip.gsub(/\s+/, ' ')
end
body.content
body
nodes.content
body.inner_text
body.xpath("//script").remove
body.inner_text
body.xpath("//style").remove
body.inner_text
body.text
body.text.strip.gsub(/\s+/, ' ')
body.xpath("//style").remove
exit
url = http://www.ebay.com/itm/External-LCD-CRT-VGA-External-TV-Tuner-PC-Monitor-BOX-Digital-HD-Speaker-/262003573087
EbayProductDetailJob.new.perform(scrape_id, source_category_id, 'http://www.ebay.com/itm/External-LCD-CRT-VGA-External-TV-Tuner-PC-Monitor-BOX-Digital-HD-Speaker-/262003573087', retry_time)
scrape_id, source_category_id, url, retry_time = 2, 1, 'http://www.ebay.com/itm/External-LCD-CRT-VGA-External-TV-Tuner-PC-Monitor-BOX-Digital-HD-Speaker-/262003573087', 0
EbayProductDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
url = 'http://www.ebay.com/itm/19pin-USB-2-0-Female-Pin-Header-to-Dual-USB2-0-Port-T-Type-Internal-Adapter-/181967542508'
EbayProductDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l.data_hash[:seller_desc]
l.data_hash[:seller_desc].strip
l.data_hash[:seller_desc].strip[0]
l.data_hash[:seller_desc].strip[0].class
l.data_hash[:seller_desc].strip[0].utf
exit
scrape_id, source_category_id, url, retry_time = 2, 1, 'http://www.ebay.com/itm/External-LCD-CRT-VGA-External-TV-Tuner-PC-Monitor-BOX-Digital-HD-Speaker-/262003573087', 0
url = 'http://www.ebay.com/itm/Bluetooth-Smart-Wrist-Watch-Phone-Mate-For-Android-Samsung-iPhone-5-6-HTC-LG-BK-/272163482916'
EbayProductDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
exit
scrape_id, source_category_id, url, retry_time = 2, 1, 'http://www.ebay.com/itm/External-LCD-CRT-VGA-External-TV-Tuner-PC-Monitor-BOX-Digital-HD-Speaker-/262003573087', 0
url = 'http://www.ebay.com/itm/Portable-Swivel-USB-3-0-Flash-Drive-8GB-Memory-Pen-Stick-Storage-Colorful-U-Disk-/391213402136'
EbayProductDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
url
l.url
exit
scrape_id, source_category_id, url, retry_time = 2, 1, 'http://www.ebay.com/itm/External-LCD-CRT-VGA-External-TV-Tuner-PC-Monitor-BOX-Digital-HD-Speaker-/262003573087', 0
url = 'http://www.ebay.com/itm/Mini-1080P-Full-HD-LED-Projector-LCD-Smart-Home-Theater-AV-HDMI-Multimedia-US-CD-/222117432793'
EbayProductDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
edit -t
session = new_session(true)
edit -t
include Capybara::DSL
session = new_session(true)
cd new_session
ls
cd ..
new_session.class
edit -t
session = new_capy_session(true)
edit -t
session = new_capy_session(true)
session.driver
session.driver.browser
session.driver.browser["url_blacklist"]
browser = session.driver.browser
browser
browser.class
browser.client
exit
scrape_id, source_category_id, url, retry_time = 2, 1, 'http://www.ebay.com/itm/External-LCD-CRT-VGA-External-TV-Tuner-PC-Monitor-BOX-Digital-HD-Speaker-/262003573087', 0
url = 'http://www.ebay.com/itm/Mini-1080P-Full-HD-LED-Projector-LCD-Smart-Home-Theater-AV-HDMI-Multimedia-US-CD-/222117432793'
EbayProductDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l.screenshots.first.attachment.url
l.products
l.identifiers
@ignored_models = ["Does not apply", "N/A"]
@ignored_models.includes? "Does Not Apply"
@ignored_models.include? "Does Not Apply"
url = 'http://www.ebay.com/itm/Douk-Audio-Hi-End-2SK170J74-Stereo-Amplifier-High-power-250Wx2-HiFi-2-0CH-Amp-/272226344676'
exit
scrape_id, source_category_id, url, retry_time = 2, 1, 'http://www.ebay.com/itm/External-LCD-CRT-VGA-External-TV-Tuner-PC-Monitor-BOX-Digital-HD-Speaker-/262003573087', 0
url = 'http://www.ebay.com/itm/Douk-Audio-Hi-End-2SK170J74-Stereo-Amplifier-High-power-250Wx2-HiFi-2-0CH-Amp-/272226344676'
EbayProductDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l.identifiers
def serialize_data(obj, *keys)
  key = keys.map(&:to_sym).find { |k| obj.key?(k) } if obj.respond_to?(:key?)
  if key.present?
    obj[key]
  elsif obj.respond_to?(:each)
    r = nil
    obj.find { |*a| r = serialize_data(a.last, *keys) if a.last }
    r
  end
end
edit -t
l.data_hash
serialize_data l.data_hash, 'UPC'
@ignored_models.map(&:downcase).include? id.downcase
@ignored_models = ["Does not apply", "N/A"]
@ignored_models.map(&:downcase).include? id.downcase
id = serialize_data l.data_hash, 'UPC'
@ignored_models.map(&:downcase).include? id.downcase
id = serialize_data l.data_hash, 'MPN'
l.identifiers
l.identifiers.destroy_all
l.identifiers
exit
scrape_id, source_category_id, url, retry_time = 2, 1, 'http://www.ebay.com/itm/External-LCD-CRT-VGA-External-TV-Tuner-PC-Monitor-BOX-Digital-HD-Speaker-/262003573087', 0
url = 'http://www.ebay.com/itm/Modern-MODO-LED-Pendant-Lamp-Suspension-Chandelier-Ceiling-lights-Lighting-/152003991032'
EbayProductDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l.identifiers
exit
scrape_id, source_category_id, url, retry_time = 2, 1, 'http://www.ebay.com/itm/External-LCD-CRT-VGA-External-TV-Tuner-PC-Monitor-BOX-Digital-HD-Speaker-/262003573087', 0
url = 'http://www.ebay.com/itm/Broadcom-4311-Wireless-wifi-PCI-E-Card-For-HP-DELL-Laptop-Netowrk-Wlan-Wi-Fi-/180519002077'
EbayProductDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
exit
scrape_id, source_category_id, url, retry_time = 2, 1, 'http://www.ebay.com/itm/External-LCD-CRT-VGA-External-TV-Tuner-PC-Monitor-BOX-Digital-HD-Speaker-/262003573087', 0
url = 'http://www.ebay.com/itm/Broadcom-4311-Wireless-wifi-PCI-E-Card-For-HP-DELL-Laptop-Netowrk-Wlan-Wi-Fi-/180519002077'
EbayProductDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
edit -t
urls .uniq
urls.uniq
urls.uniq.count
urls.count
urls.uniq.count
urls = urls.uniq.count
urls.reject {|u| ['jpg, 'gif', 'png'].any? {|type| url.downcase.end_with? type } }
non_imgs = urls.reject {|u| ['jpg', 'gif', 'png'].any? {|type| url.downcase.end_with? type } }
edit -t
urls = urls.uniq
non_imgs = urls.reject {|u| ['jpg', 'gif', 'png'].any? {|type| url.downcase.end_with? type } }
non_imgs
non_imgs = urls.reject {|u| ['jpg', 'gif', 'png'].any? {|type| u.downcase.end_with? type } }
non_imgs
exi
texit
exit
scrape_id, source_category_id, url, retry_time = 2, 1, 'http://www.ebay.com/itm/External-LCD-CRT-VGA-External-TV-Tuner-PC-Monitor-BOX-Digital-HD-Speaker-/262003573087', 0
url = 'http://www.ebay.com/itm/3D-Virtual-Reality-VR-Video-Glasses-Head-Mount-Controller-Gamepad-For-Smartphone-/272250293331'
EbayProductDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
f = File.open("#{Rails.root}/log/phantomjs.log", "a")
f.open?
f.closed?
url = "http://i.ebayimg.com/images/g/I0AAAOSwyQtV7OWF/s-l1600.jpg"
file = OpenUri.open(url)
require 'open-uri'
OpenURI.open(url)
file = OpenURI.open_uri(url)
file.closed?
file.close
file.closed?
exit
GC.stat
Asset.last
img = Asset.last
original_url = img.original_url
img.attachment = nil
img.save
img.original_url
img.attachment
img.attachment.url
file = img.attachment = original_url
file
file.open?
file.closed?
img.save
img.attachment
img.attachment.closed?
EbayProductDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
exit
ProductImage.last
ProductImage.last.save_attachment
ProductImage.sample.save_attachment
ProductImage.all.sample.save_attachment
exit
ProductImage.all.sample.save_attachment
exit
ProductImage.all.sample.save_attachment
exit
url = ProductImage.all.sample.original_url
file = OpenURI.open_uri url
file.class
file.close
file.class
file.unlink
file.class
url = ProductImage.find(4077).original_url
file = OpenURI.open_uri url
file.size
file1 = file
file.close
file1
file.closed?
file.open
url1 = ProductImage.find(4077).original_url
url = ProductImage.find_by(original_url: 'https://images-na.ssl-images-amazon.com/images/I/51yrcRU%2BoAL._SL1500_.jpg')
url = ProductImage.find_by(original_url: 'https://images-na.ssl-images-amazon.com/images/I/51yrcRU%2BoAL._SL1500_.jpg').original_url
url2 = ProductImage.find_by(original_url: 'https://images-na.ssl-images-amazon.com/images/I/51yrcRU%2BoAL._SL1500_.jpg').original_url
url1 = ProductImage.find(4077).original_url
url2
Tempfile
Tempfile.is_a? File
nil.respond_to?(:unlink)
Logger
logger = Logger.new('/dev/null')
logger = StringIO.new('/dev/null')
logger.is_a?(IO)
IO
IO.class
File.open('/dev/null', &:close)
ProductScreenshot
ProductScreenshot.connection
ps = ProductScreenshot.last
cd ps
show save_attachment
show_method save_attachment
show-method save_attachment
show-method File.open
show-method open
exit
ps = ProductScreenshot.last
cd ps
show-method open
exit
File.exist? "/tmp/screenshots/92924df8-be41-49df-b18a-c9993f468057-seller.png"
File.open("/tmp/screenshots/92924df8-be41-49df-b18a-c9993f468057-seller.png")
"#{ENV['SCREENSHOTS_PATH']}/#{SecureRandom.uuid}-#{testing}.png"
"#{ENV['SCREENSHOTS_PATH']}/#{SecureRandom.uuid}-testing.png"
exit
target = _
target.kind_of?(URI)
exit
Listing.last
img = ProductImage.new
img.valid?
img.original_url = "http://dl.dropboxusercontent.com/u/75227659/SG6219-b.jpg?g"
img.save
img.save_attachment
exit
img = ProductImage.last
img.save_attachment
exit
img.save_attachment
img = ProductImage.last
img.save_attachment
exit
ProductImage.pluck(:url)
ProductImage.pluck(:original_url)
ProductImage.where('original_url like 'http:%'").pluck(:original_url)
ProductImage.where('original_url like 'http\:%'").pluck(:original_url)
ProductImage.where("original_url like 'http:%'").pluck(:original_url)
img.original_url
img = ProductImage.last
OpenURI.open_uri(img.original_url)
begin
  OpenURI.open_uri(img.original_url)
rescue => e
  p e
end
begin
  OpenURI.open_uri(img.original_url)
rescue => e
  p e.message
end
exit
img = ProductImage.last
img.save_attachment
exit
img = ProductImage.last
img.save_attachment
exit
img = ProductImage.last
img.save_attachment
img.attachment.url
exit
img = ProductImage.last
img.save_attachment
exit
img = ProductImage.last
img.save_attachment
img.attachment = nil
img.save
img
img.original_url = "http://dl.dropboxusercontent.com/u/75227659/SG6219-b.jpg?g"
img.save
img.save_attachment
exit
s = Scrape.last
s.products.count
s.listings.count
exit
ProductManual.last
pm = _
pm.product
pm.listings
listing.listing.products
l = pm.listings.first
l.manuals
l.manuals.first.attachment.url
pm.product_product_manuals
exit
Product.last
p = _
class_name = 'Product'
Product.to_s
model = class_name.classify
model = class_name.constantize
model.to_s
gst
exit
File.join('app', 'models', 'searchable'), glob: File.join('**', '*.rb')
File.join('app', 'models', 'searchable')
p = Product.last
p.update_document
exit
Product.delete_index!
Listing.delete_index!
Product.import
exit
ProductManual.confirmed
ProductManual.joins(:product_product_manuals).where('product_product_manuals.status': ProductProductManual.statuses[:confirmed]
ProductManual.joins(:product_product_manuals).where('product_product_manuals.status': ProductProductManual.statuses[:confirmed])
ProductManual.includes(:product_product_manuals).where('product_product_manuals.status': ProductProductManual.statuses[:confirmed])
ProductProductManual.last
ppm = _
ppm.status = 1
ppm.save
ProductManual.joins(:product_product_manuals).where('product_product_manuals.status': ProductProductManual.statuses[:confirmed])
ProductManual.includes(:product_product_manuals).where('product_product_manuals.status': ProductProductManual.statuses[:confirmed])
ppm.product
p = _
p.listings
l = p.listings.first
Product.delete_index!
Product.import
exit
Product.import
exit
Product.import
Listing.import
exit
exit
p = Proudct.last
p = Product.last
p.as_json(
  include: { listings: { only: :data_hash },
    sellers: { only: :name }
  }
)
p.as_json(
  include: { listings: { methods: [:as_indexed_json], only: [:as_indexed_json] }
  }
)
p.product_manuals
p.manuals
ProductManuals.confirmed
ProductManual.confirmed
ProductManual.confirmed.products
ProductManual.confirmed.listings
pm = ProductManual.confirmed.first
p = pm.products.first
p.as_json(
  include: { listings: { methods: [:as_indexed_json], only: [:as_indexed_json] }
  }
)
p.listings
p.listings.count
reload!
l.reload!
p.relaod!
p.reload
p.as_json(
  include: { listings: { methods: [:as_indexed_json], only: [:as_indexed_json] }
  }
)
exit
pm = ProductManual.confirmed.first
p = pm.products.first
p.as_json(
  include: { listings: { methods: [:as_indexed_json], only: [:as_indexed_json] }
  }
)
exit
p = ProductManual.confirmed.first.products.first
p.as_indexed_json
pm = ProductManual.confirmed.first
pm.text
ProductManual.first
pm = _
pm.product_product_manuals
pm = ProductManual.confirmed.first
pm.original_url = 'https://images-na.ssl-images-amazon.com/images/I/A1dTiG5NfYS.pdf'
pm.save
pm.product_product_manuals
pm.product_product_manuals.first.update(status: 0)
pm = ProductManual.first
p
p.manuals << pm
p.product_product_manuals.first
p.product_product_manuals
p.product_product_manuals.last
p.product_product_manuals.first
ppm = _
ppm.update status: 1
ProductManual.confirmed
pm
pm.text
p.as_indexed_json
p.related_products
p.identifiers
Identifier.first.products
p
Identifier.first.products << p
Identifier.first.products.listings << p
Identifier.first.products.listings
Identifier.first.products.first.listings
Identifier.first.products.first.listings.size
Identifier.first.products.first.listings.size.last.identifiers
Identifier.first.products.first.listings.last.identifiers
Identifier.first.products.first.listings.last.identifiers.last.products << p
Identifier.first.products.first.listings.last.products << p
ProductListing.last.destroy
exit
Product.delete_index!
Product.import
reload!
Product.import
reload!
exit
Product.import
reload!
exit
Product.import
reload!
Product.import
Product.sample
Product.last.id
Product.delete_index!
Product.import
p = Product.last
p2 = Product.first
p.listings.count
p2.listings.count
p.listings.first.identifiers
p.listings.first.products << p2
p.update_document
reload!
p.reload!
p.reload
p.update_document
exit
p = Product.last
p2 = Product.first
p.related_products
p.update_document
operator = 'AND'
terms = { keywords: params[:keywords].to_s }
terms = { keywords: 'Solar Panel Polycrstalline ethylene vinyl acetate' }
@products = []
filters = {}
per_page = 10
page = 1
search_response = Product.search_by(terms, operator, filters, { size: per_page, scroll: '3m' })
pids = search_response.results.map { |p| p.id }.compact
terms = { keywords: 'Solar Panel' }
search_response = Product.search_by(terms, operator, filters, { size: per_page, scroll: '3m' })
pids = search_response.results.map { |p| p.id }.compact
terms = { keywords: 'environmental conditions. Laminate framed with strong anodized aluminum profile with multiple holes for ease of Installation' }
search_response = Product.search_by(terms, operator, filters, { size: per_page, scroll: '3m' })
pids = search_response.results.map { |p| p.id }.compact
search_response.results.last
search_response.results.map {|p| p.id}
search_response.results.map {|p| [p.id, p.listings.map {|l| l.products.map{|p2| p2.id}
    }
search_response.results.map {|p| [p.products]}
search_response.results.map {|p| [p['products']}
search_response.results.map {|p| [p['products']]}
search_response.results.map {|p| [p['listings']]}
search_response.results.map {|p| p['listings']}
Oj.load(search_response.results)
search_response.results.map {|p| JSON.parse(p) }
search_response.class
JSON.parse(search_response)
search_response.to_s
search_response.results
search_response.results.response
search_response.results.response.to_s
search_response.results.response.as_json
reload!
search_response = Product.search_by(terms, operator, filters, { size: per_page, scroll: '3m' })
search_response.results.response.as_json
search_response = Product.search_by(terms, operator, filters, { size: per_page, scroll: '3m' })
search_response.results.response.as_json
related_listings = lambda do |listing|
  listings = listing.flat_map (&:related_listings)
listings = listings.flat_map {|l| related_listings.call(l)}
end
edit -t
l = Listing.last
l.related_listings
Listing.all.select {|l| l.related_listings.count > 0 }
listings = _
listings.count
l = Listings.first
l = listings.first
l.related_listings.count
related_listings.call(l)
related_listings = lambda do |listing|
  listings = listing.related_listings
  listings |= listings.flat_map { |l| related_listings.call(l) }
  listings
end
related_listings.call(l)
Product.listings
p
listings = Listing.all.select {|l| l.related_listings.count > 0 }
listings = Listing.all.select {|l| l.related_listings.count > 1 }
l = listings.first
l.related_listings
l.related_listings.pluck(:name)
l.name
l.url
l.related_listings.url
l.related_listings.pluck(:url)
l.related_listings.map(&:url)
listings = Listing.all.select {|l| l.related_listings.count > 1 && [l.url, l.related_listings.pluck(:url)].flatten.uniq.count > 1 }
l = listings.first
l.related_listings.map(&:url)
l.ulr
l.url
listings.last
l = listings.last
l.related_listings.map(&:url)
l.url
l.data_hash
l.identifiers
Identifier.where(uniq_id: %w(Does does not Not Apply apply))
Identifier.where(uniq_id: %w(Does does not Not Apply apply)).size
ids = Identifier.where(uniq_id: %w(Does does not Not Apply apply))
ListingIdentifier.where(identifier: ids)
ListingIdentifier.where(identifier: ids).count
ListingIdentifier.where(identifier: ids).destroy_all
ids.each(&:destroy)
listings = Listing.all.select {|l| l.related_listings.count > 1 && [l.url, l.related_listings.pluck(:url)].flatten.uniq.count > 1 }
Excon
exit
l = Listing.last
l.images
img =l.images.first
img.listings
img.listings.size
s = Scrape.last
ProductManual.last
ProductManual.last.product
ProductManual.last.products
Asset.find 6969
pm = _
pm.confirmed?
pm.product_product_manuals
s = Scrape.last
s.update running: false
source = Source.find_by(name: 'ebay')
source.source_categories.where(test_process: true)
p = Product.last
p.as_json(
  only: [:id, :brand, :model]
)
p.as_json(
  only: [:id, :brand, :model]
p.as_json(
  only: [:id, :brand, :model],
  include: [listings: {only: [:name, :data_hash] } ]
)
p.as_json(
  only: [:id, :brand, :model],
  include: [listings: {only: [:id, :name, :data_hash] } ]
)
p.as_json(
  only: [:id, :brand, :model],
  include: [listings: {only: [:id, :name, :data_hash] } ]
)
exit
p = Product.all.sample
p.as_indexed_json
p_indexed_json = _
p.as_json(
  only: [:id, :brand, :model],
  include: {
    scrapes: { only: [:id] },
    sources: { only: [:id] },
    categories: { only: [:id] },
    listings: { only: [:id, :name, :data_hash] },
    manuals: { methods: [:text], only: [:id, :original_url, :text] }
  }
)
l = p.listings.first
l.as_json
l.as_json.to_s
p.as_json
reload!
exit
p = Product.last
p.as_indexed_json
p.as_indexed_json.to_json
q
p.index_document
reload!
p.reload
p.as_indexed_json
p.index_document
reload!
p.reload
p.as_indexed_json
p.index_document
exit
p = Product.last
p.index_document
exit
p = Product.last
p.index_document
exit
p = Product.last
p.index_document
exit
p = Product.last
p.index_document
Product.delete_index!
Product.import
exit
Product.import
exit
Product.import
exit
Product.import
exit
Product.import
exit
Product.import
Product.delete_index!
Product.import
Redis.current
redis = Redis.current
redis.flushall
Scrape.last.update running: false
p = Product.all.sample
p.as_indexed_json
Product.delete_index!
p.as_indexed_json
exit
Product.delete_index!
Product.import
p
product = Product.last
product.as_indexed_json
xi
xtei
exit
product = Product.last
product.as_json
product.as_indexed_json
product = Product.last
product.as_indexed_json
product = Product.last
product.as_indexed_json
Product.delete_index!
Product.import
product = Proudct.last
product = Product.last
ProductProductManual.where(product: product)
pm = ProductManual.first
product.manuals << pm
ProductProductManual.last
ProductProductManual.last.confirmed
ProductProductManual.confirmed
product.manuals
exit
l = Listing.last
l.url
l = Listing.last
ENV['AWS_BUCKET'[
ENV['AWS_BUCKET']
ENV['AWS_ACCESS_KEY_ID']
ENV['AWS_SECRET_ACCESS_KEY']
ENV['AWS_REGION']
l = Listing.last
l.images
img = ProductImage.last
img.listings.last.url
img.attachment.url
l = Listing.last
l.images
l.url
rails c
exit
p = ProductProductManual.confirmed.first.product
p.manuals
p.manuals.size
p.confirmed_manuals
p.confirmed_manuals.size
p.manuals.uniq.size
p.manuals
reload!
p.reload
p.manuals
p.confirmed_manuals
exit
p = ProductProductManual.confirmed.first.product
p.manuals
p.confirmed_manuals
p.confirmed_manuals.count
ProductProductManual.confirmed.count
ProductProductManual.confirmed
exit
p = Product.find 809
p.confirmed_manuals.count
p.product_product_manuals
ProductProductManual.confirmed
p.confirmed_manuals.count
p.confirmed_manuals
exit
p = Product.find 809
p.confirmed_manuals
p.confirmed_manuals.uniq
p.manuals
p.manuals.confirmed
exit
p = Product.find 809
p.manuals.confirmed
p.confirmed_manuals
p.manuals.confirmed.to_sql
exit
p = Product.find 809
p.manuals.confirmed.to_sql
p.manuals.confirmed
p.manuals.confirmed.to_sql
exit
p = Product.find 809
p.manuals.confirmed
p.confirmed_manuals
p.manuals
exit
p = Product.find 809
p.confirmed_manuals
exit
Product.delete_index!
Product.import
exit
p = Product.find 809
p.as_indexed_json
Product.import
edit -t
product.manuals
product.reload
product.manuals
ProductManual.last
pm = _
pm.product
pm.products
p = _.first
p.manuals
p.manuals.confirmed
p = Product.all.sample
p.listings.count
p.listings.pluck(:url)
p.listings.pluck(:data_hash)
redis = Redis.current
redis.flushall
s = Scrape.last
ProductImage.count
s.update running: false
Product.delete_index!
Product.import
p = Product.first
p.as_indexed_json
_.as_json
p
p.manuals
p = ProductManual.confirmed.first.products.first
p.manuals
manuals = _
manuals.class
manuals.select{ |m| m.status == 1 }
p
p.as_indexed_json
exit
p = ProductManual.confirmed.first.products.first
p.manuals
manuals = )
p.manuals
p.manuals.includes(:product_product_manuals)
manuals = _
manuals.confirmed
manuals.where('product_product_manuals.status': 1)
exit
Product.delete_index!
Listing.delete_index!
Product.import
exit
Product.import
Listing.import
Seller.find 1128
seller = _
seller.image
SellerImage.last
l = Listing.last
l.as_indexed_json
l = Listing.all.sample
l.as_indexed_json
exit
ops = AWS::OpsWorks.new
ops.client
client = ops.client
edit -t
client.user_agent_string
client.send(:user_agent_string)
client = Aws::OpsWorks::Client.new
client = AWS::OpsWorks::Client.new
client.user_agent_string
client.stop_stack(stack_id: '0c956f97-e97a-4a4b-a21a-f99f574520ee')
exit
Product.last
p = _
p.as_indexed_json
p.index_document
Product.import
Product.connection
Product.import
exit
Product.import
exit
"::#{'Listing'}"
"::#{'Listing'}".constantize
Listing.connection
"::#{'Listing'}".constantize
exit
Listing.delete_index!
Listing.import
exit
Listing.import
exit
Product.delete_index!
Listing.delete_index!
Product.import
Listing.import
exit
Listing.with_confirmed_manuals
exit
Listing.with_confirmed_manuals
listings = _
listings.map(&:manuals)
exit
Listing.klass
Listing.my_klass
Listing.ancestors
exit
Listing.my_klass
exit
Listing.my_class
exit
l = Listing.first
l.update scrape_id: 2
l.update_document
Listing.find 2
Listing.first(5).pluck(:id)
Listing.first(5).map(&:id)
Source.find 1
Source.find(1)
Source.all
:index
:index.to_s
ProductManual.confirmed?
ProductManual.confirmed
ProductManual.confirmed.first
pm = _
pm.status_changed?
pm.last
ProductManual.last
pm.status_changed?
pm.status
exit
pm = ProductManual.last
pm.confirmed?
pm
pm.product
pm.products
ppm = pm.product_product_manual
ppm = pm.product_product_manuals.first
[ppm.product] | ppm.product.listings
[ppm.product_id] | ppm.product.listings.ids
ppm
exit
ppm = pm.product_product_manuals.first
ppm = ProductProductManuals.find 13
ppm = ProductProductManual.find 13
exit
ppm = ProductProductManual.find 13
ppm.update(status: 1)
ENV["REDIS_NAMESPACE"]
ppm
ppm.status_changed?
exit
ppm = ProductProductManual.find 13
ppm.status = 0
ppm.status_changed?
ppm.save
ppm.status_changed?
ppm.previous_changes
ppm.status = nil
ppm.save
ppm.previous_changes
exit
ppm = ProductProductManual.find 13
ppm.status = 1
ppm.save
ppm.status = nil
ppm.save
ppm.status = 1
ppm.save
ppm.created_at == ppm.updated_at
ppm.status_changed?
ppm.previous_changes
ppm.previous_changes['status'].present?
exit
ppm = ProductProductManual.find 13
ppm.status = nil
ppm.save
exit
ppm = ProductProductManual.find 13
ppm.status = 1
ppm.save
ppm.update_indexes
ppm.status = nil
exit
ppm = ProductProductManual.find 13
exit
ppm = ProductProductManual.find 13
ppm.status = nil
ppm.save
case :update_manual
case :update_manual.to_s
when /update/
  puts "hey"
else
  puts "nooo"
end
exit
options={}
load_image = options[:load_images] || true
load_image
exit
Product.import
exit
gst
exit
US $219.99
currency_string = 'US $219.99'
raw_price_string = 'US $219.99'
raw_price_string.scan(/[[:alpha:]]{2,3}/).try(:first)
Currency.first
symbol = raw_price_string.delete('^A-z0-9')
symbol = raw_price_string.delete('^A-z0-9.')
symbol = raw_price_string.scan([^\W\s\d])
symbol = raw_price_string.scan(/[^\W\s\d]/)
raw_price_string
symbol = raw_price_string.scan(/[^\d\s\.\w]/).first
iso_code = raw_price_string.scan(/[[:alpha:]]{2,3}/).first
Currency.where("iso_code like '#{iso_code}%'").find_by(symbol: symbol)
raw_price_string.gsub(/\A[[:alpha:]]{2,3}?\$/, '')
raw_price_string.gsub(/[[:alpha:]]{2,3}?\$/, '')
raw_price_string.gsub(/[[:alpha:]]{2,3}?\s+?\$/, '')
exit
scrape_id = 1
Scrape.first
scrape_id, source_category_id, url, retry_time = 1, 1, 'http://www.ebay.com/itm/Samsung-Galaxy-Note-4-32GB-SM-N910T-GSM-Unlocked-4G-LTE-Android-Smartphone/252178725754?hash=item3ab705fb7a', 0
EbayProductDetailJob
EbayProductDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
Listing.last
Listing.find_by(url: url)
exit
scrape_id, source_category_id, url, retry_time = 1, 1, 'http://www.ebay.com/itm/Samsung-Galaxy-Note-4-32GB-SM-N910T-GSM-Unlocked-4G-LTE-Android-Smartphone/252178725754?hash=item3ab705fb7a', 0
EbayProductDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
exit
scrape_id, source_category_id, url, retry_time = 1, 1, 'http://www.ebay.com/itm/Samsung-Galaxy-Note-4-32GB-SM-N910T-GSM-Unlocked-4G-LTE-Android-Smartphone/252178725754?hash=item3ab705fb7a', 0
EbayProductDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
exit
EbayProductDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
scrape_id, source_category_id, url, retry_time = 1, 1, 'http://www.ebay.com/itm/Samsung-Galaxy-Note-4-32GB-SM-N910T-GSM-Unlocked-4G-LTE-Android-Smartphone/252178725754?hash=item3ab705fb7a', 0
EbayProductDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
scrape_id, source_category_id, url, retry_time = 1, 1, 'http://www.ebay.com/itm/Samsung-Galaxy-S5-SM-G900A-16GB-White-UNLOCKED-GSM-Smartphone-AT-T-TMOBILE-NEW-/151925779195?hash=item235f7b6afb:g:LasAAOSwNphWbxQt', 0
EbayProductDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
exit
scrape_id, source_category_id, url, retry_time = 1, 1, 'http://www.ebay.com/itm/Samsung-Galaxy-S5-SM-G900A-16GB-White-UNLOCKED-GSM-Smartphone-AT-T-TMOBILE-NEW-/151925779195?hash=item235f7b6afb:g:LasAAOSwNphWbxQt', 0
Listing.last
Listing.last.destroy
scrape_id, source_category_id, url, retry_time = 1, 1, 'http://www.ebay.com/itm/Samsung-Galaxy-S5-SM-G900A-16GB-White-UNLOCKED-GSM-Smartphone-AT-T-TMOBILE-NEW-/151925779195?hash=item235f7b6afb:g:LasAAOSwNphWbxQt', 0
EbayProductDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
exit
scrape_id, source_category_id, url, retry_time = 1, 1, 'http://www.ebay.com/itm/Samsung-Galaxy-S5-SM-G900A-16GB-White-UNLOCKED-GSM-Smartphone-AT-T-TMOBILE-NEW-/151925779195?hash=item235f7b6afb:g:LasAAOSwNphWbxQt', 0
Listing.last.destroy
EbayProductDetailJob.new.perform(scrape_id, source_category_id, url, retry_time)
l = Listing.last
l.sellers
l.scrape_listing_sellers
l.id
sls = ScrapeListingSeller.last
sls.min_price
sls.min_price_cents
Monetize.parse("#{nil} #{nil}").exchange_to(:USD)
Monetize.parse("#{nil} #{nil}").exchange_to(:USD).to_f
min = nil
exchanged_min = Monetize.parse("#{nil} #{min}").exchange_to(:USD) if min.present?
exchanged_min
exit
p = Product.all.sampe
p = Product.all.sample
p.as_indexed_json
Product.mappings
Product.mappings.to_hash
p.listings
exit
Figaro
exit
Figaro.env('REDIS_NAMESPACE')
Figaro.env.redis_namespace
Figaro.env.redis_namespace = "amazonus"
ENV["REDIS_NAMESPACE"]
ENV["REDIS_NAMESPACE"] = 'amazonus'
ENV["REDIS_NAMESPACE"]
develop_token = ENV["BOX_DEVELOPER_TOKEN"]
client = exit
box_client = BoxClient.new
box_client = BoxClient.new.get_client
exit
client = BoxClient.get_client
exit
client = BoxClient.get_client
namespace = Redis::Namespace.new(
  "#{Rails.env}:shared_cache",
  redis: Redis.new(url: ENV['REDIS_URL'])
)
developer_token = namespace.get 'BOX_DEVELOPER_TOKEN' || ENV['BOX_DEVELOPER_TOKEN']
exit
client = BoxClient.get_client
refresh_token = namespace.get('BOX_REFRESH_TOKEN')
namespace = Redis::Namespace.new(
  "#{Rails.env}:shared_cache",
  redis: Redis.new(url: ENV['REDIS_URL'])
)
refresh_token = namespace.get('BOX_REFRESH_TOKEN')
client.root_folder_items
client.folder_from_id('id')
client.folder_from_id("6940480094")
name = "SunPower via Google"
folder_name = "SunPower via Google"
client.folder_from_path(folder_name)
client.folder_from_path("Hand Scrapes/#{folder_name}")
folder = client.folder_from_path("Hand Scrapes/#{folder_name}")
folder.folders
folder.class
folder
folder = client.folder_from_path("Hand Scrapes/#{folder_name}")['id']
folder = client.folder_from_id(id)
folder_id = folder
folder = client.folder_from_id(folder_id)
folder = client.folder_from_id(folder_id)['item_collection']
id = "8554918757"
folder = client.folder_from_id(id)
folder['path']
folder.path
folder
id = "8554901185"
folder = client.folder_from_id(id)
exit
client = BoxClient.new
client = client.client
exit
client = BoxClient.new
client
client.client
exit
client = BoxClient.new
client.client
exit
client = BoxClient.new
client.client
exit
client = BoxClient.new
client.client
client.get_client
exit
client = BoxClient.new
client.client
client.get_client
exit
client = BoxClient.new
client.client
client.get_client
exit
client = BoxClient.new
client.client
client.get_client
client.client
exit
client = BoxClient.new
client.client
client.developer_refresh_token
exit
client = BoxClient.new
client.clas
client.class
client = client.client
client.root_folder_items
items = _
items.class
Boxr::ROOT
folder = client.folder_from_path('Hand Scrapes/SunPower via Google/Sunpower June 20016')
client.access_token
client.root_folder_items
exit
client = BoxClient.new.client
client.root_folder_items
folder = client.folder_from_path('Hand Scrapes/SunPower via Google/Sunpower June 20016')
folder = client.folder_from_path('Hand Scrapes/SunPower via Google/Sunpower June 2016')
folder.class
folder.entries
folder = client.folder_from_id("8554901185")
folder.entries
folder.class
folder.item_collection
edit -t
namespace.get('BOX_DEVELOPER_TOKEN')
exit
client = BoxClient.new.client
exit
client = BoxClient.new.client
client.developer_token
exit
client = BoxClient.new
folder = client.folder_from_path('Hand Scrapes/SunPower via Google/Sunpower June 20016')
exit
client = BoxClient.new
folder = client.folder_from_path('Hand Scrapes/SunPower via Google/Sunpower June 20016')
folder = client.folder_from_path('Hand Scrapes/SunPower via Google/Sunpower June 2016')
folder = client.folder_from_id("8554901185")
folder.item_collection
folder.item_collection.entries
folder = client.folder_from_path('Hand Scrapes/SunPower via Google/Sunpower June 2016')
folder.id
folder = client.folder_from_path('Hand Scrapes/SunPower via Google/Sunpower June 2016')
client = BoxClient.new
folder = client.folder_from_path('Hand Scrapes/SunPower via Google/Sunpower June 2016')
client
namespace = client.namespace
namespace.get('BOX_DEVELOPER_TOKEN')
exit
client = BoxClient.new
folder = client.folder_from_path('Hand Scrapes/SunPower via Google/Sunpower June 2016')
ext
exit
@namespace = Redis::Namespace.new(
  "#{Rails.env}:shared_cache",
  redis: Redis.new(url: ENV['REDIS_URL'])
)
@namespace.get('BOX_DEVELOPER_TOKEN')
@namespace.set('BOX_DEVELOPER_TOKEN', 'M1pn0TROg9Atu6yjUHNYCu0l7hODhQHy')
@namespace.get('BOX_DEVELOPER_TOKEN')
@namespace.get('BOX_REFRESH_TOKEN')
eixt
exit
client = BoxClient.new
exit
Base64.uuid
SecureRandom.uuid.length
exit
client = BoxClient.new
client2 = client.client
box_client = client2
client.namespace
client.namespace.get('BOX_DEVELOPER_TOKEN')
exit
client = BoxClient.new
client.namespace.get('BOX_DEVELOPER_TOKEN')
folder = client.folder_from_path('Hand Scrapes/SunPower via Google/Sunpower June 2016')
folder.item_collection
folder.item_collection.first
folder.item_collection.entries
folder.item_collection.entries.each
folder.item_collection.entries.first
folder.item_collection.entries.select {|item| item.type == 'file'}
box = client.client
Boxr::Client.FILES_URI
box.FILES_URI
sheet1 = folder.item_collection.entries.select {|item| item.type == 'file'}.first
file = box.download_file(sheet1)
file.class
File.open('/Users/jonathan/Desktop/box_test.xlsx', 'w') { |f| f.write(file) }
file.class
Encoding.default_external
File.open('/Users/jonathan/Desktop/box_test.xlsx', 'w') { |f| f.write(file.encode('URF-8', 'ISO-8859-15') }
File.open('/Users/jonathan/Desktop/box_test.xlsx', 'w') { |f| f.write(file.encode('URF-8', 'ISO-8859-15')) }
File.open('/Users/jonathan/Desktop/box_test.xlsx', 'w') { |f| f.write(file.encode('UTF-8', 'ISO-8859-15')) }
folder.item_collection.entries.select {|item| item.type == 'file'}
folder = client.folder_from_path('Hand Scrapes/SunPower via Google/Sunpower June 2016')
folder.item_collection.entries.select {|item| item.type == 'file'}
file = folder.item_collection.entries.select {|item| item.type == 'file'}[1]
sheet = file
file = box.download_file(sheet)
box.refresh_token
client
file = client.download_file(sheet)
file = client.client.download_file(sheet)
client.get_client
file = client.client.download_file(sheet)
@namespace = Redis::Namespace.new(
  "#{Rails.env}:shared_cache",
  redis: Redis.new(url: ENV['REDIS_URL'])
)
@namespace.get('BOX_REFRESH_TOKEN')
box = client.client
box.refresh_token = 'jonathan
'
box.instance_variables
exit
client = BoxClient.get_client
client.refresh_token
folder = client.folder_from_path('Hand Scrapes/SunPower via Google/Sunpower June 2016')
exit
namespace = Redis::Namespace.new(
  "#{Rails.env}:shared_cache",
  redis: Redis.new(url: ENV['REDIS_URL'])
)
namespace.set(ENV['BOX_DEVELOPER_TOKEN'], access_token)
namespace.set('BOX_DEVELOPER_TOKEN', ENV['BOX_DEVELOPER_TOKEN'])
developer_token = namespace.get('BOX_DEVELOPER_TOKEN') || ENV['BOX_DEVELOPER_TOKEN']
developer_refresh_token = namespace.get('BOX_REFRESH_TOKEN')
client = BoxClient.get_client
folder = client.folder_from_path('Hand Scrapes/SunPower via Google/Sunpower June 2016')
id = folder.id
client.folder_from_id(id)
folder = _
folder.item_collection
folder.item_collection.entries
files = folder.item_collection.entries.select {|item| item.type == 'file'}
files.last.id
kl = _
client.file_from_id(kl)
file = _
client.create_shared_link_for_file(file)
file = _
dl_url = file.shared_link.url
dl_url = file.shared_link.download_url
%x( wget #{dl_url} -P /Users/jonathan/ )
client.refesh_token
client
resp = %x( curl https://api.box.com/oauth2/token -d 'grant_type=authorization_code&code=#{ENV['BOX_DEVELOPER_TOKEN']}&client_id=#{'BOX_CLIENT_ID'}&client_secret=#{'BOX_CLIENT_SECRET'}' -X POST )
client
client.create_shared_link_for_file(file)
resp = %x( curl https://api.box.com/oauth2/token -d 'grant_type=authorization_code&code=#{'q2a 852'}&client_id=#{'BOX_CLIENT_ID'}&client_secret=#{'BOX_CLIENT_SECRET'}' -X POST )
exit
Product.includes(:scrapes).where('scrapes.id': scrape_id).select(:id).find_in_batches(batch_size: 500) do |products|
  ElasticsearchIndexerJob.perform_async('update', 'Product', products.map(&:id))
end
s = Scrape.last
Scrape.all
Scrape.all.map {|s| [s.id, s.product.count]}
Scrape.all.map {|s| [s.id, s.products.count]}
scrape_id = 1
Product.includes(:scrapes).where('scrapes.id': scrape_id).select(:id).find_in_batches(batch_size: 500) do |products|
  ElasticsearchIndexerJob.perform_async('update', 'Product', products.map(&:id))
end
exit
Listing.where(scrape_id: scrape_id).select(:id).find_in_batches(batch_size: 500) do |listings|
  ElasticsearchIndexerJob.perform_async('update', 'Listing', listings.map(&:id))
end
scrape_id = 1
Listing.where(scrape_id: scrape_id).select(:id).find_in_batches(batch_size: 500) do |listings|
  ElasticsearchIndexerJob.perform_async('update', 'Listing', listings.map(&:id))
end
s = Scrape.last
s = Scrape.first
s.listings.count
exit
scrape_id = Scrape.first
scrape_id = Scrape.first.id
Product.includes(:scrapes).where('scrapes.id': scrape_id).select(:id).find_in_batches(batch_size: 500) do |products|
  ElasticsearchIndexerJob.perform_async('update', 'Product', products.map(&:id))
end
as_indexed_json.select { |k,v| changed_attributes.keys.map(&:to_s).include? k.to_s }
p = Product.first
p.instance_variable_get(:@__changed_attribtes)
changed_attributes = :model
p.as_indexed_json.select { |k,v| changed_attributes.keys.map(&:to_s).include? k.to_s }
changed_attributes = {:model => 'Something'}
p.as_indexed_json.select { |k,v| changed_attributes.keys.map(&:to_s).include? k.to_s }
changed_attributes = {:model => 'Something', :brand => 'else' }
p.as_indexed_json.select { |k,v| changed_attributes.keys.map(&:to_s).include? k.to_s }
exit
p = Product.first
Product.prepare_records([p], 'update')
exit
index_name = "#{Person.index_name}_#{SecureRandom.hex}"
index_name = "#{Product.index_name}_#{SecureRandom.hex}"
Product.__elasticsearch__.document_type
client = Product.__elasticsearch__.client
Product.__elasticsearch__.create_index! index: index_name, force: true
client.indices
edit -t
exit
Product.count
client.indices.delete(index: Product.index_name) rescue nil
old_indices = client.indices.get_alias(name: Product.index_name).map do |key, val|
  { index: key, name: val['aliases'].keys.first }
end
client.indices.get_alias(name: Product.index_name)
client.indices.put_alias(index: index_name, name: Product.index_name)
index_name = "#{Product.index_name}_#{SecureRandom.hex}"
edit -t
client.indices.delete(index: Product.index_name) rescue nil
old_indices = client.indices.get_alias(name: Product.index_name).map do |key, val|
  { index: key, name: val['aliases'].keys.first }
end
client.indices.put_alias(index: index_name, name: Product.index_name)
edit -t
client.indices.delete(index: Product.index_name) rescue nil
client.indices.put_alias(index: index_name, name: Product.index_name)
client = Product.__elasticsearch__.client
edit -t
old_indices = client.indices.get_alias(name: Product.index_name).map do |key, val|
  { index: key, name: val['aliases'].keys.first }
end
edit -t
Product.index_name
client.indices
client.indices.get_alias
client.indices.put_alias(index: Product.index_name, name: index_name)
:w
old_indices = client.indices.get_alias(name: Product.index_name)
old_indices = client.indices.get_alias(name: index_name)
old_indices = client.indices.get_alias
Entity.index_name
client = Entity.__elasticsearch__.client
client
client.aliases
client.indices
client.indices.get_aliases
Product.index_name
Scrape.first
Scrape.second
Scrape.find 3
Scrape.find 4
s = _
s.products.count
Product.delete_index!
Product.import
client
client = Product.__elasticsearch__.client
client
client.indices
client.indices.class
Product.index_name
client.indices.get_alias(name: Product.index_name)
alias_name
alias_name = 'products_development_6cc5b51bbc44d5421c474b0a4920f25f'
client.indices.put_alias(index: alias_name, name: Product.index_name)
client.indices.delete(index: Product.index_name) rescue nil
client.indices.put_alias(index: alias_name, name: Product.index_name)
client.indices.get_alias
:w
alias_name = "#{Product.index_name}_#{SecureRandom.hex}"
client = Product.__elasticsearch__.client
Product.__elasticsearch__.create_index! index: alias_name, force: true
edit -t
client.indices.delete(index: Product.index_name) rescue nil
old_indices = client.indices.get_alias(name: Product.index_name).map do |key, val|
  { index: key, name: val['aliases'].keys.first }
end
client.indices.put_alias(index: alias_name, name: Product.index_name)
old_indices.each do |index|
  client.indices.delete_alias(index)
  client.indices.delete(index: index[:index])
end
old_indices = client.indices.get_alias(name: Product.index_name).map do |key, val|
  { index: key, name: val['aliases'].keys.first }
end
Product.index_name
old_indices = client.indices.get_alias(name: Product.index_name).map do |key, val|
  { index: key, name: val['aliases'].keys.first }
end
old_indices.first[:index]
alias_name = "#{Product.index_name}_#{SecureRandom.hex}"
edit -t
old_indices = client.indices.get_alias(name: Product.index_name).map do |key, val|
  { index: key, name: val['aliases'].keys.first }
end
old_index = old_indices.first[:index]
client.indices.update_aliases body: {
  actions: [
    { remove: { index: old_index, alias: Product.index_name } },
    { add:    { index: alias_name, alias: Product.index_name } }
  ]
}
client.indicies.get_alias
client.indices.get_alias
alias_name
Product.index_name
client.indices.get_alias
client.indices.delete(index: old_index)
client.indices.get_alias
alias_name = "#{Product.index_name}_#{Time.now}"
alias_name = "#{Product.index_name}_#{Time.now.to_i}"
alias_name = "#{Product.index_name}_#{Time.now.strftime('%Y%m%d%S')}"
alias_name = "#{Product.index_name}_#{Time.now.strftime('%Y%m%d%S%L')}"
client = Product.__elasticsearch__.client
xtei
exit
alias_name = "#{Product.index_name}_#{Time.now.strftime('%Y%m%d%S%L')}"
edit -t
old_indices = client.indices.get_alias(name: Product.index_name).map do |key, val|
  { index: key, name: val['aliases'].keys.first }
end
old_index = old_indices.first[:index]
client.indices.update_aliases body: {
  actions: [
    { remove: { index: old_index, alias: Product.index_name } },
    { add:    { index: alias_name, alias: Product.index_name } }
  ]
}
client.indices.delete(index: old_index)
exit
client.indices.delete(index: Product.index_name) rescue nil
old_indices = client.indices.get_alias(name: klass.index_name).map do |key, val|
  { index: key, name: val['aliases'].keys.first }
end
klass = Product
old_indices = client.indices.get_alias(name: klass.index_name).map do |key, val|
  { index: key, name: val['aliases'].keys.first }
end
client.indices.delete(index: klass.index_name) rescue nil
client.indices.delete(index: 'products_development_2016062708127') rescue nil
Listing.count
Product.count
hash = {}
hash.delete(:index_name)
Product.index_name
p = Product.first
p.update model: "UN65KU6300FXZ-A"
changed_attributes = p.instance_variable_get(:@__changed_attributes)
p.index_document
Source.find 6
Product.index_name
Product.__elasticsearch__.index_name
exit
s = Scrape.last
s = Scrape.where(running: true).last
s.update(running: false)
s = Scrape.last
s.listings.last
s.listings.count
s.listings.map {|l| l.images.count }
exit
client = Redis.current
client.queues
client.keys
client.keys.select {|k| k.include?('dhgate') }
queue_key = "dhgate:queue:product_listing_queue"
client.lrange(queue_key, 0, -1)
jobs = _
queue = Sidekiq::Queue.new("product_listing_queue")
jobs.map {|j| j['jid']}
jobs.map {|j| Oj.load(j)}
jobs.map {|j| Oj.load(j)['jid']}
job_ids = _
queue.each do |job|
  job.delete if job_ids.include?(job.jid)
end
client.lrange(queue_key, 0, -1)
client.keys
client.lrange(queue_key, 0, -1)
client.keys.select {|k| k.include?('dhgate') }
s
s = Scrape.alst
s = Scrape.last
s.listings.count
q = Sidekiq::Queue.new('product_listing_queue')
q.pause!
q.paused?
q.clear
exit
client = Product.__elasticsearch__.client
client.indices
client.indices.get_alias
index_names = client.indices.get_alias.keys
index_names.select! {|i| i.include?('products_development') }
_.each do {|name| client.indices.delete(index: name) }
index_names.select! {|i| i.include?('products_development') }
index_names
index_names.select! {|i| i.include?('products_development') }
names = index_names.select! {|i| i.include?('products_development') }
index_names
index_names.each do {|name| client.indices.delete(index: name) }
index_names.each {|name| client.indices.delete(index: name) }
s = Scrape.last
exit
Listing.include_indexed_fields.where(scrape_id: scrape_id).find_in_batches(batch_size: 50) do |records|
  Listing.bulk_index(records, options)
end
scrape_id = 1
Listing.include_indexed_fields.where(scrape_id: scrape_id).find_in_batches(batch_size: 50) do |records|
  Listing.bulk_index(records, options)
end
Listing.include_indexed_fields.where(scrape_id: scrape_id).find_in_batches(batch_size: 50) do |records|
  Listing.bulk_index(records)
end
s = Scrape.first
l = s.listings.first
l.name
l.name = l.name + " JonathanNewman"
l.save
Listing.include_indexed_fields.where(scrape_id: scrape_id).find_in_batches(batch_size: 50) do |records|
  Listing.bulk_index(records)
end
exit
docs = [2255125,
  1263690,
  2327577,
  2677375,
3663185]
docs.sum
exit
ProductDetailJob
job = ProductDetailJob.new
exit
ProductDetailJob
AmazonProductDetailJOb
AmazonProductDetailJob
job = DhGateProductDetailJob.new
job = DhGateListingDetailJob.new
job.exist?
exit
job = DhGateListingDetailJob.new
exit
job = DhGateListingDetailJob.new
job.exist?(94, 548, "http://www.dhgate.com/product/high-quality-a-grade-4-29-4-33-w-polycrystalline/373990166.html#s60-19-1b;searl|868920554", 1)
exit
DhGateListingDetailJob.exist?(94, 548, "http://www.dhgate.com/product/high-quality-a-grade-4-29-4-33-w-polycrystalline/373990166.html#s60-19-1b;searl|868920554", 1)
exit
l = Listing.first
l.url
require 'httparty'
require httparty
include HTTParty
HTTParty
require 'httparty
require 'httparty'
exit
exit
require 'httparty'
l = Listing.first
response = HTTParty.get(l.url)
response.body
Nokogiri.html(response.body)
Nokogiri::HTML(response.body)
Nokogiri::HTML(response.body.gsub(/\s+/, ' ')
Nokogiri::HTML(response.body.gsub(/\s+/, ' '))
nodes = _
image_srcs = from_mfg.xpath('//img/@src').map(&:value)
image_srcs = nodes.xpath('//img/@src').map(&:value)
nodes.css('h3.productDescriptionSource').map { |el| el.text.strip }
nodes.css('div.productDescriptionWrapper').map { |el| el.text.strip }
l.url
Listing.where(scrape_id: 1).each do |l|
  response = HTTParty.get(l.url)
  nodes = Nokogiri::HTML(response.body.gsub(/\s+/, ' '))
Listing.where(scrape_id: 1).map do |l|
  response = HTTParty.get(l.url)
  nodes = Nokogiri::HTML(response.body.gsub(/\s+/, ' '))
  nodes.css('div.productDescriptionWrapper').map { |el| el.text.strip }
Listing.where(scrape_id: 1).map do |l|
Listing.where(scrape_id: 1).each do |l|
  response = HTTParty.get(l.url)
  nodes = Nokogiri::HTML(response.body.gsub(/\s+/, ' '))
  puts  nodes.css('div.productDescriptionWrapper').map { |el| el.text.strip }
  sleep rand(1..4)
end
Listing.first(5).map(&:url)
Listing.where(scrape_id: 1).each do |l|
  response = HTTParty.get(l.url)
  nodes = Nokogiri::HTML(response.body.gsub(/\s+/, ' '))
  desc = nodes.css('div.productDescriptionWrapper').map { |el| el.text.strip }
  puts l.id
  puts l.data_hash[:'Product Description'].strip.gsub(/\s+/, ' ') == desc.strip.gsub(/\s+/, ' ')
end
l = Listing.find 3
l.data_hash
l.data_hash[:'Product Description'].strip.gsub(/\s+/, ' ') 
response = HTTParty.get(l.url)
nodes = Nokogiri::HTML(response.body.gsub(/\s+/, ' '))
desc = nodes.css('div.productDescriptionWrapper').map { |el| el.text.strip }
l
l.url
l.data_hash
l.data_hash[:General]
Listing.where(scrape_id: 1).each do |l|
  response = HTTParty.get(l.url)
  nodes = Nokogiri::HTML(response.body.gsub(/\s+/, ' '))
  desc = nodes.css('div.productDescriptionWrapper').map { |el| el.text.strip }
puts (l.data_hash[:'Product Description'] || l.data_hash[:General][:'Product Description']).strip.gsub(/\s+/, ' ') == desc.strip.gsub(/\s+/, ' ') rescue nil
end
exit
ProductManual.count
ProductManual.all.map(&:text)
ManualText.destroy_all
edit -t
ManualText.count
ManualText.last
ManualText.destroy_all
Yomu.server(:text)
ProductManual.all.each { |pm| pm.text }
Yomo.server_pid
Yomu.server_pid
Yomu.call(:@@server_pid)
Yomu.class_variable_get(:@@server_pid)
Yomu.kill_server!
exit
Benchmark.bm do |x|
  ManualText.destroy_all
  x.report("no-server") do
    ProductManual.all.each { |pm| pm.text }
  end
  ManualText.destroy_all
  Yomu.server(:text)
  x.report("server") do
    ProductManual.all.each { |pm| pm.text }
  end
  Yomu.kill_server!
end
ProductManual.count
ManualText.count
ManualText.all
ProductManual
ProductManual.confirmed
pm = ProductManual.first
pm.text
exit
pm = ProductManual.first
exit
pm = ProductManual.first
pm.as_indexed_json
exit
pm = ProductManual.first
pm.as_indexed_json
json = _
json.as_json
exit
pm = ProductManual.first
ProductManual.include_indexed_fields
exit
ProductManual.include_indexed_fields.to_sql
ProductManual.include_indexed_fields
exit
ProductManual.include_indexed_fields
exit
ProductManual.include_indexed_fields
pm = _.first
pm.as_indexed_json
ManualText.size
ManualText.count
ProdcutManual.count
ProductManual.count
exit
ProductManual.processed.count
ProductManual.processed
ProductManual.all.each(&:text)
ProductManual.processed.count
pm = ProductManual.first
pm.text
ProductManual.each do |pm| pm.as_indexed_json end
ProductManual.all.each do |pm| pm.as_indexed_json end
ProductManual.all.map(&:as_indexed_json)
manuals = _
manuals.flat_map {|pm| pm['products].map {|p| p['id'] } }
manuals.flat_map {|pm| pm['products'].map {|p| p['id'] } }
manuals.flat_map {|pm| pm['products'].map {|p| p['id'] } }.uniq
exit
operator = 'AND'
terms = { keywords: 'led' }
filters = {}
per_page = 10
page = 1
filters[:brand] = 'Samsung'
terms
opts = { size: per_page, scroll: '3m' }
fields = Product.query_fields
terms
fields
operator
filters
Product.reflect_on_all_associations
Product.reflect_on_all_associations.first
Product.reflect_on_all_associations.first.name
Product.reflect_on_all_associations.first.klass
ProductListing.connection
Product.reflect_on_all_associations.first.klass
Product.reflect_on_all_associations.first.klass.attribute_names
fields
search_fields = fields[:search_fields]
edit -t
terms.values.reject(&:blank?).present?
query[:query][:filtered][:query].delete(:match_all)
query
query[:query][:filtered][:query].delete(:bool) if terms.values.reject(&:blank?).blank?
query
definition = query
filter_fields = fields[:filter_fields]
terms[:keywords].present?
definition[:query][:filtered][:query][:bool][:filter][0][:multi_match][:type]
definition[:query][:filtered][:query][:bool][:filter][0][:multi_match][:type] = 'cross_fields'
definition
terms[:brand].nil?
filters[:brand].nil?
definition[:query][:filtered][:query][:bool][:filter][0][:multi_match][:fields] << filter_fields[:brand] if filters[:brand].nil?
definition[:query][:filtered][:query][:bool][:filter][0][:multi_match][:fields] << filter_fields[:model] if filters[:model].nil?
filter_fields.present?
definition[:query][:filtered][:filter][:bool][:must]
definition[:query][:filtered][:filter][:bool][:must] ||= []
definition[:query][:filtered][:filter][:bool][:must] << { term: { filter_fields[:brand] => filters[:brand] } } if filters[:brand].present?
definition[:query][:filtered][:filter][:bool][:must] << { term: { filter_fields[:model] => filters[:model] } } if filters[:model].present?
definition[:query][:filtered][:filter][:bool][:must] << { term: { filter_fields[:scrape_id] => filters[:scrape_id] } } if filters[:scrape_id].present?
definition
opts
search_response = Product.search(definition, opts)
search_response.results
search_response.results.total
definition
definition[:query][:filtered]
definition[:query][:filtered][:filter]
definition[:query][:filtered][:filter][:bool][:must] = []
search_response = Product.search(definition, opts)
search_response.results.total
Product.where(brand: 'Samsung')
query
filters[:brand]
definition[:query][:filtered][:query][:bool][:must] << { term: { filter_fields[:brand] => filters[:brand] } } if filters[:brand].present?
definition[:query][:filtered][:query][:bool][:must] = { term: { filter_fields[:brand] => filters[:brand] } } if filters[:brand].present?
filters
filters.keys
filters.present?
{}.present?
'1'.downcase
exit
img = Asset.last
img = ProductImage.all.sample
im.save_attachment
img.save_attachment
exit
img = ProductImage.all.sample
img.save_attachment
img = ProductImage.all.sample; img.save_attachment
exit
s = Scrape.lat
s = Scrape.last
exit
s = Scrape.last
exit
s = Scrape.last
exit
s = Scrape.last
s.products.count
s.listings.count
s.id
l = Listing.find 1362824
l.images
l.images.count
l.images.last
seller = Seller.find 120388
seller.listings
l.sellers
Listing.joins(:sellers).where('sellers.id': seller.id).count
Listing.joins(:sellers).where('sellers.id': seller.id).first
Listing.joins(:sellers).where('sellers.id': seller.id).to_sql
Listing.joins(:source, :sellers).where('sellers.id': seller.id).to_sql
Listing.joins(:source, :sellers).where('sellers.id': seller.id, 'sources.id': 3).to_sql
Listing.joins(:source, :sellers).where('sellers.id': seller.id, 'sources.id': 3).count
Listing.joins(:source, :sellers).where('sellers.id': seller.id, 'sources.id': 3)
Listing.joins(:source, :sellers).where('sellers.id': seller.id, 'sources.id': 3).pluck(:url)
Listing.joins(:source, :sellers).where('sellers.id': seller.id, 'sources.id': 3).pluck(:url).uniq
Listing.joins(:source, :sellers).where('sellers.id': seller.id, 'sources.id': 3).pluck(:url).uniq.size
Listing.joins(:source, :sellers).where('sellers.id': seller.id, 'sources.id': 3).pluck(:url).uniq
Seller.count
Listing.joins(:source, :sellers).where('sellers.id': seller.id, 'sources.id': 3).to_sql
Listing.joins(:source, :sellers).where('sellers.id': seller.id, 'sources.id': 3).select('DISTINCT(listings.url)').to_sql
Listing.joins(:source, :sellers).where('sellers.id': seller.id, 'sources.id': 3).select('DISTINCT(listings.url)').size
Listing.joins(:source, :sellers).where('sellers.id': seller.id, 'sources.id': 3).select('DISTINCT(url)').size
Listing.joins(:source, :sellers).where('sellers.id': seller.id, 'sources.id': 3).select('SELECT DISTINCT(url)').size
Listing.joins(:source, :sellers).where('sellers.id': seller.id, 'sources.id': 3).distinct_on(:url).count
Listing.joins(:source, :sellers).where('sellers.id': seller.id, 'sources.id': 3).distinct_on(:url)
Listing.joins(:source, :sellers).where('sellers.id': seller.id, 'sources.id': 3).distinct_on(:url).load
Listing.joins(:source, :sellers).where('sellers.id': seller.id, 'sources.id': 3).select(:url).distinct
Listing.joins(:source, :sellers).where('sellers.id': seller.id, 'sources.id': 3).select(:url).distinct.count
Listing.joins(:source, :sellers).where('sellers.id': seller.id).select(:url).distinct.count
Sellers.first(100)
sellers = Seller.first(100)
sellers.map { |seller| Listing.joins(:sellers).where('sellers.id': seller.id).select(:url).distinct.count }
edit -t
seller_counts
xi
xt
rails c
edit -t
Seller.where(name: 'Amazon.com')
sql = 
sql = "SELECT
    `sellers`.`name`, count(distinct(`listings`.`url`)) as my_count
FROM
    `listings`
        INNER JOIN
    `scrape_listing_sellers` ON `scrape_listing_sellers`.`listing_id` = `listings`.`id`
        INNER JOIN
    `sellers` ON `sellers`.`id` = `scrape_listing_sellers`.`seller_id`
group by
  `sellers`.`id`
order by my_count DESC"
sql.gsub!(/\s+/, ' ')
records_array = ActiveRecord::Base.connection.execute(sql)
records_array.values
records_array.as_json
sql
Seller.where(name: ['', nil]).size
Seller.where(name: [' ', '', nil]).size
blank_seller = Seller.where(name: [' ', '', nil]).first
seller.listings
blank_seller.listings
Listing.joins(:sellers).where('sellers.id': blank_seller.id)
Listing.joins(:sellers).where('sellers.name': ['', ' ', nil])
Listing.joins(:sellers).where('sellers.name': ['', ' ', nil]).first
Listing.connection
Listing.joins(:sellers).where('sellers.name': ['', ' ', nil]).first
exit
Listing.joins(:sellers).where('sellers.name': ['', ' ', nil]).first
l = _
l.url
l.created_at
Listing.joins(:sellers).where('sellers.name': ['', ' ', nil]).to_sql
l = Listing.find '1315481'
l = Listing.find 1315481
l.url
l.sellers
l.url
l.scrape_listing_sellers
sls = l.scrape_listing_sellers.first
sls.min_price
sls.min_price.to_s
exit
client = Redis.client
client = Redis.current
client.flushall
exit
url = 'https://www.amazon.com%2FKodak-8612-Easy-Share-ZD8612%2Fdp%2FB0026JQ8AE%2Fref%3Dsr_1_51%2F190-5093485-9023129%3Fs%3Dphoto%26ie%3DUTF8%26qid%3D1467307338%26sr%3D1-51-spons%26psc%3D1&qualifier=1571477&id=8360549776393545372&widgetName=sp_btf_browse'
Rails.env
Rails.env.development?
exit
Scrape.last
Scrape.last.source
scrape_id = 6
Scrape.last.source.source_categories.active
Scrape.last.source.source_categories.active.first
source_category_id = 3
url = 'https://www.amazon.com%2FKodak-8612-Easy-Share-ZD8612%2Fdp%2FB0026JQ8AE%2Fref%3Dsr_1_51%2F190-5093485-9023129%3Fs%3Dphoto%26ie%3DUTF8%26qid%3D1467307338%26sr%3D1-51-spons%26psc%3D1&qualifier=1571477&id=8360549776393545372&widgetName=sp_btf_browse'
AmazonProductDetailJob.new.perform(scrape_id, source_category_id, url, 0)
exit
url = 'https://www.amazon.com%2FKodak-8612-Easy-Share-ZD8612%2Fdp%2FB0026JQ8AE%2Fref%3Dsr_1_51%2F190-5093485-9023129%3Fs%3Dphoto%26ie%3DUTF8%26qid%3D1467307338%26sr%3D1-51-spons%26psc%3D1&qualifier=1571477&id=8360549776393545372&widgetName=sp_btf_browse'
source_category_id = 3
scrape_id = 6
AmazonProductDetailJob.new.perform(scrape_id, source_category_id, url, 0)
exit
url = 'https://www.amazon.com%2FKodak-8612-Easy-Share-ZD8612%2Fdp%2FB0026JQ8AE%2Fref%3Dsr_1_51%2F190-5093485-9023129%3Fs%3Dphoto%26ie%3DUTF8%26qid%3D1467307338%26sr%3D1-51-spons%26psc%3D1&qualifier=1571477&id=8360549776393545372&widgetName=sp_btf_browse'
source_category_id = 3
scrape_id = 6
AmazonProductDetailJob.new.perform(scrape_id, source_category_id, url, 0)
AmazonProductDetailJob
AmazonProductDetailJob.new
job = _
job.get_detail_url(url)
exit
url = 'https://www.amazon.com%2FKodak-8612-Easy-Share-ZD8612%2Fdp%2FB0026JQ8AE%2Fref%3Dsr_1_51%2F190-5093485-9023129%3Fs%3Dphoto%26ie%3DUTF8%26qid%3D1467307338%26sr%3D1-51-spons%26psc%3D1&qualifier=1571477&id=8360549776393545372&widgetName=sp_btf_browse'
job = AmazonProductDetailJob.new
job.get_detail_url(url)
exit
url = 'https://www.amazon.com%2FKodak-8612-Easy-Share-ZD8612%2Fdp%2FB0026JQ8AE%2Fref%3Dsr_1_51%2F190-5093485-9023129%3Fs%3Dphoto%26ie%3DUTF8%26qid%3D1467307338%26sr%3D1-51-spons%26psc%3D1&qualifier=1571477&id=8360549776393545372&widgetName=sp_btf_browse'
job = AmazonProductDetailJob.new
job.get_detail_url(url)
exit
AmazonProductDetailJob
AmazonProductDetailJob.new
job = _
job.methods
cd job
ls
show-source get_detail_url
exit
job = AmazonProductDetailJob.new
cd job
show-source get_detail_url
show-source parse_asin_from_url
exit
job = AmazonProductDetailJob.new
cd job
show-source parse_asin_from_url
url = 'https://www.amazon.com%2FKodak-8612-Easy-Share-ZD8612%2Fdp%2FB0026JQ8AE%2Fref%3Dsr_1_51%2F190-5093485-9023129%3Fs%3Dphoto%26ie%3DUTF8%26qid%3D1467307338%26sr%3D1-51-spons%26psc%3D1&qualifier=1571477&id=8360549776393545372&widgetName=sp_btf_browse'
exit
url
url = 'https://www.amazon.com%2FKodak-8612-Easy-Share-ZD8612%2Fdp%2FB0026JQ8AE%2Fref%3Dsr_1_51%2F190-5093485-9023129%3Fs%3Dphoto%26ie%3DUTF8%26qid%3D1467307338%26sr%3D1-51-spons%26psc%3D1&qualifier=1571477&id=8360549776393545372&widgetName=sp_btf_browse'
job.parse_asin_from_url(url)
exit
url = 'https://www.amazon.com%2FKodak-8612-Easy-Share-ZD8612%2Fdp%2FB0026JQ8AE%2Fref%3Dsr_1_51%2F190-5093485-9023129%3Fs%3Dphoto%26ie%3DUTF8%26qid%3D1467307338%26sr%3D1-51-spons%26psc%3D1&qualifier=1571477&id=8360549776393545372&widgetName=sp_btf_browse'
job = AmazonProductDetailJob.new
job.parse_asin_from_url(url)
exit
url = 'https://www.amazon.com%2FKodak-8612-Easy-Share-ZD8612%2Fdp%2FB0026JQ8AE%2Fref%3Dsr_1_51%2F190-5093485-9023129%3Fs%3Dphoto%26ie%3DUTF8%26qid%3D1467307338%26sr%3D1-51-spons%26psc%3D1&qualifier=1571477&id=8360549776393545372&widgetName=sp_btf_browse'
job = AmazonProductDetailJob.new
job.parse_asin_from_url(url)
exit
job = AmazonProductDetailJob.new
url = 'https://www.amazon.com%2FKodak-8612-Easy-Share-ZD8612%2Fdp%2FB0026JQ8AE%2Fref%3Dsr_1_51%2F190-5093485-9023129%3Fs%3Dphoto%26ie%3DUTF8%26qid%3D1467307338%26sr%3D1-51-spons%26psc%3D1&qualifier=1571477&id=8360549776393545372&widgetName=sp_btf_browse'
job.parse_asin_from_url(url)
exit
job = AmazonProductDetailJob.new
url = 'https://www.amazon.com%2FKodak-8612-Easy-Share-ZD8612%2Fdp%2FB0026JQ8AE%2Fref%3Dsr_1_51%2F190-5093485-9023129%3Fs%3Dphoto%26ie%3DUTF8%26qid%3D1467307338%26sr%3D1-51-spons%26psc%3D1&qualifier=1571477&id=8360549776393545372&widgetName=sp_btf_browse'
job.parse_asin_from_url(url)
AmazonHelpers
AmazonHelpers.parent
exit
job = AmazonProductDetailJob.new
url = 'https://www.amazon.com%2FKodak-8612-Easy-Share-ZD8612%2Fdp%2FB0026JQ8AE%2Fref%3Dsr_1_51%2F190-5093485-9023129%3Fs%3Dphoto%26ie%3DUTF8%26qid%3D1467307338%26sr%3D1-51-spons%26psc%3D1&qualifier=1571477&id=8360549776393545372&widgetName=sp_btf_browse'
job.parse_asin_from_url(url)
require 'lib/scraping/amazon_helpers.rb'
exit
require 'lib/scraping/amazon_helpers.rb'
require './lib/scraping/amazon_helpers.rb'
exit
job = AmazonProductDetailJob.new
job.parse_asin_from_url(url)
url = 'https://www.amazon.com%2FKodak-8612-Easy-Share-ZD8612%2Fdp%2FB0026JQ8AE%2Fref%3Dsr_1_51%2F190-5093485-9023129%3Fs%3Dphoto%26ie%3DUTF8%26qid%3D1467307338%26sr%3D1-51-spons%26psc%3D1&qualifier=1571477&id=8360549776393545372&widgetName=sp_btf_browse'
job.parse_asin_from_url(url)
job.methods - 1.methods
show-source job.get_detail_url
job.extend(AmazonHelpers)
require './lib/scraping/amazon_helpers.rb'
job.extend(AmazonHelpers)
jon.parse_asin_from_url(url)
job.parse_asin_from_url(url)
job.get_detail_url(url)
AmazonProductDetailJob.ancestors
exit
AmazonProductDetailJob.ancestors
exit
AmazonProductDetailJob.ancestors
job = AmazonProductDetailJob.new
exit
job = AmazonProductDetailJob.new
exikt
exit
job = AmazonProductDetailJob.new
exit
job = AmazonProductDetailJob.new
job.parse_asin_from_url
url = 'https://www.amazon.com%2FKodak-8612-Easy-Share-ZD8612%2Fdp%2FB0026JQ8AE%2Fref%3Dsr_1_51%2F190-5093485-9023129%3Fs%3Dphoto%26ie%3DUTF8%26qid%3D1467307338%26sr%3D1-51-spons%26psc%3D1&qualifier=1571477&id=8360549776393545372&widgetName=sp_btf_browse'
job.parse_asin_from_url
job.parse_asin_from_url url
job.get_detail_url url
exit
url = 'https://www.amazon.com%2FKodak-8612-Easy-Share-ZD8612%2Fdp%2FB0026JQ8AE%2Fref%3Dsr_1_51%2F190-5093485-9023129%3Fs%3Dphoto%26ie%3DUTF8%26qid%3D1467307338%26sr%3D1-51-spons%26psc%3D1&qualifier=1571477&id=8360549776393545372&widgetName=sp_btf_browse'
job = AmazonProductDetailJob.new
job.get_detail_url url
exit
job = AmazonProductDetailJob.new
exit
job = AmazonProductDetailJob.new
job.get_detail_url 
exit
job = AmazonProductDetailJob.new
job.get_detail_url 
url = 'https://www.amazon.com%2FKodak-8612-Easy-Share-ZD8612%2Fdp%2FB0026JQ8AE%2Fref%3Dsr_1_51%2F190-5093485-9023129%3Fs%3Dphoto%26ie%3DUTF8%26qid%3D1467307338%26sr%3D1-51-spons%26psc%3D1&qualifier=1571477&id=8360549776393545372&widgetName=sp_btf_browse'
job.get_detail_url uel
job.get_detail_url url
exit
url = 'https://www.amazon.com%2FKodak-8612-Easy-Share-ZD8612%2Fdp%2FB0026JQ8AE%2Fref%3Dsr_1_51%2F190-5093485-9023129%3Fs%3Dphoto%26ie%3DUTF8%26qid%3D1467307338%26sr%3D1-51-spons%26psc%3D1&qualifier=1571477&id=8360549776393545372&widgetName=sp_btf_browse'
job = AmazonProductListingJob.new
exit
url = 'https://www.amazon.com%2FKodak-8612-Easy-Share-ZD8612%2Fdp%2FB0026JQ8AE%2Fref%3Dsr_1_51%2F190-5093485-9023129%3Fs%3Dphoto%26ie%3DUTF8%26qid%3D1467307338%26sr%3D1-51-spons%26psc%3D1&qualifier=1571477&id=8360549776393545372&widgetName=sp_btf_browse'
job = AmazonProductListingJob.new
job.get_detail_url url
job.parse_asin_from_url url
detail_url = job.get_detail_url url
job.parse_asin_from_url detail_url
job = AmazonProductDetailJob.new
url
source_category_id = 3
scrape_id = 6
job.perform(scrape_id, source_category_id, url, 0)
l = Listing.last
l.products
l.manuals
pm = _.first
pm.product_product_manual
pm.product_product_manuals
pm.text
exit
AmazonProductDetailJob
exit
AmazonProductDetailJob
job = AmazonProductDetailJob.new
job.parse_asin_from_url
url = 'https://www.amazon.com%2FKodak-8612-Easy-Share-ZD8612%2Fdp%2FB0026JQ8AE%2Fref%3Dsr_1_51%2F190-5093485-9023129%3Fs%3Dphoto%26ie%3DUTF8%26qid%3D1467307338%26sr%3D1-51-spons%26psc%3D1&qualifier=1571477&id=8360549776393545372&widgetName=sp_btf_browse'
job.get_detail_url url
job.parse_asin_from_url _
url = 'https://www.amazon.com/Tagital%C2%AE-Android-Bluetooth-Supported-Keyboard/dp/B01D4EFINW/ref=sr_1_178/183-1786699-9565421?s=pc&ie=UTF8&qid=1467307439&sr=1-178&refinements=p_n_operating_system_browse-bin%3A3077590011%22'
URI.decode(url)
url = 'https://www.amazon.com/UHURU\u{ae}-Rechargeable-Bluetooth-Noiseless-Computer/dp/B019MR4Y3U/ref=sr_1_68/187-3396881-9570553?s=pc&ie=UTF8&qid=1467307444&sr=1-68'
URI.decode(url)
url = 'https://www.amazon.com/UHURU%C2%AE-Rechargeable-Bluetooth-Noiseless-Computer/dp/B019MR4Y3U/ref=sr_1_68/187-3396881-9570553?s=pc&ie=UTF8&qid=1467307444&sr=1-68'
job
job = AmazonProductDetailJob.new
job.get_detail_url url
URI.decode(url)
URI.encode(_)
url = URI.encode(URI.decode(original_url))
url = URI.encode(URI.decode(url))
url.scan(/(?<=dp\/)[0-9a-zA-Z]+(?=\W|$)/).first
asin = _
uri = URI.parse(url)
"#{uri.scheme}://#{uri.host}/dp/#{asin}"
url = 'https://www.amazon.com%2FYaesu-FT-450D-Get-Radio-Bundle%2Fdp%2FB019EGCMDI%2Fref%3Dsr_1_51%2F175-8610911-9310230%3Fs%3Delectronics%26ie%3DUTF8%26qid%3D1467307342%26sr%3D1-51-spons%26psc%3D1&qualifier=1571473&id=1432274920441392355&widgetName=sp_btf_browse'
url = URI.encode(URI.decode(url))
asin = url.scan(/(?<=dp\/)[0-9a-zA-Z]+(?=\W|$)/).first
uri = URI.parse(url)
"#{uri.scheme}://#{uri.host}/dp/#{asin}"
url = 'https://www.amazon.com%2FYaesu-FT-450D-Get-Radio-Bundle%2Fdp%2FB019EGCMDI%2F'
URI.parse(url)
CGI.parse(url)
CGI.unescape(url)
url = 'https://www.amazon.com/UHURU%C2%AE-Rechargeable-Bluetooth-Noiseless-Computer/dp/B019MR4Y3U/ref=sr_1_68/187-3396881-9570553?s=pc&ie=UTF8&qid=1467307444&sr=1-68'
CGI.unescape(url)
CGI.escape(url)
CGI.encode
CGI.escape(url)
CGI.escape(CGI.unescape(url))
CGI.unescape(url)
str = _
str.    url.encode(Encoding)
str.encode(Encoding.find('ASCII'))
encoding_options = {
  :invalid           => :replace,  # Replace invalid byte sequences
  :undef             => :replace,  # Replace anything not defined in ASCII
  :replace           => '',        # Use a blank for those replacements
  :universal_newline => true       # Always break lines with \n
}
url.encode(Encoding.find('ASCII'), encoding_options)
url
CGI.unescape(url).encode(Encoding.find('ASCII'), encoding_options)
:W
CGI.unescape(url).encode(Encoding.find('ASCII'), replace: '')
url
CGI.unescape(url)
CGI.escape(CGI.unescape(url))
url
url = 'https://www.amazon.com%2FYaesu-FT-450D-Get-Radio-Bundle%2Fdp%2FB019EGCMDI%2F'
url.gsub(/%2F/, /\//)
url.gsub(/%2F/, '/')
url = https://www.amazon.com%2FYaesu-FT-450D-Get-Radio-Bundle%2Fdp%2FB019EGCMDI%2F
url = 'https://www.amazon.com%2FYaesu-FT-450D-Get-Radio-Bundle%2Fdp%2FB019EGCMDI%2F'
url.gsub(/%2F/, '/')
uri = URI.parse(url)
url = 'https://www.amazon.com/UHURU%C2%AE-Rechargeable-Bluetooth-Noiseless-Computer/dp/B019MR4Y3U'
uri = URI.parse(url)
url.gsub!(/%2F/, '/')
url
uri = URI.parse(url)
url = 'https://www.amazon.com%2FYaesu-FT-450D-Get-Radio-Bundle%2Fdp%2FB019EGCMDI%2F'
url.gsub!(/%2F/, '/')
uri = URI.parse(url)
edit -t
url =     # https://www.amazon.com%2FYaesu-FT-450D-Get-Radio-Bundle%2Fdp%2FB019EGCMDI%2F  -- escaped slashes
url = https://www.amazon.com%2FYaesu-FT-450D-Get-Radio-Bundle%2Fdp%2FB019EGCMDI%2F'
url = 'https://www.amazon.com%2FYaesu-FT-450D-Get-Radio-Bundle%2Fdp%2FB019EGCMDI%2F'
get_detail_url(url)
exit
Sidekiq.redis
Sidekiq.redis do |r|
  r.client
end
Sidekiq.redis do |r|
  p r.namespace
end
exit
Sidekiq.redis do |r|
  p r.namespace
end
exit
Redis.current
Redis.current.client
Redis.current.client.namespace
Redis.current.client
Sidekiq
Sidekiq.client
exit
Sidekiq::Client
Sidekiq::Client.name
Sidekiq::Client.name.namespace_name
Sidekiq::Client.namespace_name
Sidekiq.redis
Sidekiq.redis do |r|
  puts r
end
Sidekiq.redis do |r|
  p r
end
exit
s = Scrape.last
s.products.count
s.listings.count
s.running = false
s.save
exit
s = Scrape.last
s.listings.count
s.products.count
exit
s = Scrape.last
s.listings.count
exit
Product.import
Listing.import
exit
[1, 2,3,4,5].none? {|n| n > 6 }
[1, 2,3,4,5].none? {|n| n > 5 }
[1, 2,3,4,5].none? {|n| n < 5 }
Listing.last
l = _
l.screenshots.last
img = _
img.attachment.url
img.attachment.attributes
img.attachment_file_size
img.attachment_file_name
img.attachment_updated_at
img.attachment.url
img.attachment_file_size?
img.attachment.exists?
img.attachment.file?
img.original_url
img.original_filename
img.attachment.original_filename
exit
s = Scrape.last
s.listings.count
s.products.count
l = s.listings.first
l.screenshots.first
exit
img = ProductImage.first
img.call(:pop_proxy)
exit
img = ProductImage.first
img.pop_proxy
proxy = _
URI.parse(proxy)
URI.parse('http://' + proxy)
uri = _
uri.port
proxy_uri = URI.parse('http://' + proxy)
original_url = img.original_url
ENV['PROXY_USERNAME']
file = OpenURI.open_uri(original_url, :proxy_http_basic_authentication => [proxy_uri, ENV['PROXY_USERNAME'], ENV['PROXY_PASSWORD']])
img.attachment = file
file.close
file.unlink
img.reload
img.attachment.url
exit
img = ProductImage.first
exit
img = ProductImage.first
img.save_attachment
exit
img = ProductImage.first
img.save_attachment
exit
img = ProductImage.first
img.save_attachment
exit
href = '/gp/slredirect/picassoRedirect.html/ref=pa_sp_btf_browse_lawngarden_sr_pg45_3?ie=UTF8&adId=A02391262PSOK5HU48YR1&url=https%3A%2F%2Fwww.amazon.com%2FEfficiency-Poweradd-Charger-External-Batteries%2Fdp%2FB00M16KLOG%2Fref%3Dsr_1_1083%3Fs%3Dlawn-garden%26ie%3DUTF8%26qid%3D1467751084%26sr%3D1-1083-spons%26psc%3D1&qualifier=1127730&id=5218998071441586819&widgetName=sp_btf_browse'
URI.decode(href)
URI.decode(href).scan(/https?/)
URI.decode(href).split(/https?/)
:W
URI.decode(href).split(/https?/).last
'https' + URI.decode(href).split(/https?/).last
'https' + URI.decode(href).scan(/https?.+/).last
URI.decode(href).scan(/https?.+/).last
href = 'https://www.amazon.com/gp/slredirect/picassoRedirect.html/ref=pa_sp_btf_browse_lawngarden_sr_pg45_3?ie=UTF8&adId=A02391262PSOK5HU48YR1&url=https%3A%2F%2Fwww.amazon.com%2FEfficiency-Poweradd-Charger-External-Batteries%2Fdp%2FB00M16KLOG%2Fref%3Dsr_1_1083%3Fs%3Dlawn-garden%26ie%3DUTF8%26qid%3D1467751672%26sr%3D1-1083-spons%26psc%3D1&qualifier=1127143&id=5501038372109604359&widgetName=sp_btf_browse'
URI.decode(href).scan(/https?.+/).last
URI.decode(href).scan(/https?.+/)
URI.decode(href).scan(/https?.+(?=(http)/)
URI.decode(href).scan(/https?.+(?=(http))/)
URI.decode(href).scan(/https?.+/).last
href
"https" + URI.decode(href).split(/https?:/).last
href
https://www.amazon.com/gp/slredirect/picassoRedirect.html/ref=pa_sp_btf_browse_lawngarden_sr_pg45_3?ie=UTF8&adId=A02391262PSOK5HU48YR1&url=http%3A%2F%2Fwww.amazon.com%2FEfficiency-Poweradd-Charger-External-Batteries%2Fdp%2FB00M16KLOG%2Fref%3Dsr_1_1083%3Fs%3Dlawn-garden%26ie%3DUTF8%26qid%3D1467751672%26sr%3D1-1083-spons%26psc%3D1&qualifier=1127143&id=5501038372109604359&widgetName=sp_btf_browse
href = 'https://www.amazon.com/gp/slredirect/picassoRedirect.html/ref=pa_sp_btf_browse_lawngarden_sr_pg45_3?ie=UTF8&adId=A02391262PSOK5HU48YR1&url=http%3A%2F%2Fwww.amazon.com%2FEfficiency-Poweradd-Charger-External-Batteries%2Fdp%2FB00M16KLOG%2Fref%3Dsr_1_1083%3Fs%3Dlawn-garden%26ie%3DUTF8%26qid%3D1467751672%26sr%3D1-1083-spons%26psc%3D1&qualifier=1127143&id=5501038372109604359&widgetName=sp_btf_browse'
"https" + URI.decode(href).split(/https?:/).last
"https:" + URI.decode(href).split(/https?:/).last
"https:" + URI.decode(_).split(/https?:/).last
exit
[      '*plus.google.com*',
  '*google-analytics*',
  '*amazon-adsystem.com*',
  '*googleads*',
  '*sync.adaptv.advertising.com*',
  '*ads.yahoo.com*',
  '*openx.net*',
  '*adtechus*',
  '*adsystem*',
  '*doubleclick.net*',
  '*smartadserver*',
  '*admatic*',
  '*adnxs*',
  '*liverail.com/',
  '*ads.admized*',
  '*rubiconproject*',
]
_.sort
exit
l = Listing.last
l.listing_url
exit
url = 'https://www.amazon.com/dp/B01BGC39VM'
AmazonProductDetailJob.perform_async(scrape_id, source_category_id, url)
Scrape.last
s = _
s.listings.count
s.listings.pluck(:url)
url
scrape_id
scrape_id = s.id
source_category_id = s.source.source_categories.sample.id
SourceCategory.find(945).category
AmazonProductDetailJob.perform_async(scrape_id, source_category_id, url)
ENV
url
exit
scrape_id = Scrape.last
scrape_id = Scrape.last.id
url = 'https://www.amazon.com/dp/B01BGC39VM'
source_category_id = s.source.source_categories.sample.id
s = Scrape.last
source_category_id = s.source.source_categories.sample.id
AmazonProductDetailJob.perform_async(scrape_id, source_category_id, url)
Sidekiq.namespace
Sidekiq.redis do |r|
  p r.namespace
end
exit
s = Scrape.last
source_category_id = s.source.source_categories.sample.id
url = 'https://www.amazon.com/dp/B01BGC39VM'
scrape_id = Scrape.last.id
AmazonProductDetailJob.perform_async(scrape_id, source_category_id, url)
exit
"#{[Rails.env, ENV['REDIS_NAMESPACE']].join('_')"
"#{[Rails.env, ENV['REDIS_NAMESPACE']].join('_')}"
AmazonProductDetailJob.perform_async(scrape_id, source_category_id, url)
1800 / 60
120 * 60
7200 / 60
60 * 60
2 * 3600
2.hours
2.hours.to_i
4.hours.to_i
24.hours.to_i
eixt
exit
q = Sidekiq::Queue.new('product_detail_queue')
scrape_id = Scrape.last.id
url = 'https://www.amazon.com/dp/B01BGC39VM'
source_category_id = s.source.source_categories.sample.id
source_category_id = Scrape.last.source.source_categories.sample.id
AmazonProductDetailJob.perform_async(scrape_id, source_category_id, url)
exit
scrape_id = Scrape.last.id
source_category_id = Scrape.last.source.source_categories.sample.id
url = 'https://www.amazon.com/dp/B01BGC39VM'
AmazonProductDetailJob.perform_async(scrape_id, source_category_id, url)
l = Listing.last
exit
url = 'https://www.amazon.com/dp/B01GFP73MG'
source_category_id = Scrape.last.source.source_categories.sample.id
scrape_id = Scrape.last.id
img = Asset.find 7997
ProductManual.first
%r{\Aapplication/pdf\Z}
AmazonProductDetailJob.perform_async(scrape_id, source_category_id, url)
l = Listing.last
exit
pm = ProductManual.last
exit
ProductManual.last(10).map(&:original_url)
url = 'http://rhfg.tk/Lt-55164-Lcd-Tv-55-Inch-User-Manual.pdf'
pm = ProductManual.last(2).first
pm.destroy
pm = ProductManual.create(original_url: 'http://rhfg.tk/Lt-55164-Lcd-Tv-55-Inch-User-Manual.pdf')
pm.save_attachment
pm
pm.valid?
pm.errors.full_messages
pm.attachment
exit
pm = ProductManual.last
pm.attachment.valid?
pm.attachment.errors
pm.save_attachment
pm.attachment.errors
pm.attachment.valid?
exit
url = 'http://www.ebay.com/itm/Canon-EOS-6D-DSLR-Camera-24-105mm-IS-Lens-16GB-Tripod-Case-/350709146760'
scrape_id = Scrape.last.id
ebay = Source.find_by(name: 'ebay')
source.scrapes
ebay.scrapes
scrape_id = 6
source_category_id = ebay.source_categories.sample.id
EbayProductDetailJob
EbayProductDetailJob.new.perform(scrape_id, source_category_id, url)
l = Listing.last
l.url
l.screenshots.first.attachment.url
exit
Scrape.where(running: true).update_all(running: false)
ebay = Scrape.last
ebay.listings.count
ebay.listings.where("data_hash not like '%Item specifics%'").first
l = ebay.listings.first
l.url
l.screenshots.first
l.screenshots.first.attachment.url
l.data_hash
l.url
l.products
ebay.products.count
ebay.listings.count
ebay.listings.select {|l| l.products.count == 0 }
no_prods = _
no_prods.first
ebay.listings.where("data_hash not like '%Item specifics%'").first
ebay.reload
ebay.listings.where("data_hash not like '%Item specifics%'").first
ebay.listings.where("data_hash not like '%Item specifics%'").count
ebay = Scrape.last
ebay.listings.where("data_hash not like '%Item specifics%'").count
exit
Product.reindex_alias
Listing.reindex_alias
l = Listing.count
l = Listing.all.sample
l.as_indexed_json
operator = 'AND'
terms = { keywords: 'HDMI' }
@listings = []
params = {}
per_page = params[:per_page] || 10
page = params[:page] || 1
filters = {}
Listing.query_fields[:filter_fields].keys.each do |key|
  filters[key] = params[key].downcase.split(',').map(&:strip) if params[key].present?
end
filters
terms
search_response = Listing.search_by(terms, operator, filters, { size: per_page, scroll: '3m' })
search_response.records
search_response.class
search_response.results
search_response.results.class
search_response.results.records
search_response.results
search_response.results.first
rails c
exit
operator = 'AND'
edit -t
search_response.results.first
Listing.connection
search_response = Listing.search_by(terms, operator, filters, { size: per_page, scroll: '3m' })
search_response.results.first
l = Listing.first
l.as_indexed_json
l = Listing.last
l.as_indexed_json
exit
Listing.reindex_alias
Product.reindex_alias
l = Listing.last
l.as_json(
  only: [:id, :name, :scrape_id],
  include: {
    source: { only: [:id] },
    categories: { only: [:id] },
    products: { only: [:id, :brand, :model] },
    identifiers: { only: [:id, :uniq_id] },
  }
)
attrs = _
attrs[:data_hash] = hash_to_s(l.data_hash) if l.data_hash.present?
def hash_to_s(obj)
  return obj unless obj.is_a? Hash
  obj.map do |k, v|
    case v.class
    when Hash
      v
    when Array
      { k => v.join(', ') }
    else
      { k => v }
    end
  end.inject({}) { |a, e| a.merge(e || {}) }.values.join(' ')
end
attrs[:data_hash] = hash_to_s(l.data_hash) if l.data_hash.present?
attrs
attrs.delete_if {|k, v| v.blank? }
attrs
exit
Listing.reindex_alias
exit
Listing.reindex_alias
Product.reindex_alias
Product.delete_index!
Listing.delete_index!
Product.delete_index!
client = Product.__elasticsearch__.client
client.indices.delete(index: 'listings_development_2016070857783')
exit
Listing.reindex_alias
Product.reindex_alias
exit
Product.reindex_alias
Listing.reindex_alias
operator = 'AND'
terms = { keywords: 'HDMI' }
@listings = []
params = {}
per_page = params[:per_page] || 10
page = params[:page] || 1
filters = {}
Listing.query_fields[:filter_fields].keys.each do |key|
  filters[key] = params[key].downcase.split(',').map(&:strip) if params[key].present?
end
search_response = Listing.search_by(terms, operator, filters, { size: per_page, scroll: '3m' })
search_response.results.first
exit
l.as_indexed_json
operator = 'AND'
terms = { keywords: 'HDMI' }
@listings = []
params = {}
per_page = params[:per_page] || 10
page = params[:page] || 1
filters = {}
Listing.query_fields[:filter_fields].keys.each do |key|
  filters[key] = params[key].downcase.split(',').map(&:strip) if params[key].present?
end
search_response = Listing.search_by(terms, operator, filters, { size: per_page, scroll: '3m' })
search_response.results.first
exit
operator = 'AND'
terms = { keywords: 'HDMI' }
@listings = []
params = {}
per_page = params[:per_page] || 10
page = params[:page] || 1
filters = {}
Listing.query_fields[:filter_fields].keys.each do |key|
  filters[key] = params[key].downcase.split(',').map(&:strip) if params[key].present?
end
search_response = Listing.search_by(terms, operator, filters, { size: per_page, scroll: '3m' })
search_response.results.first
listings = Listing.includes(:products).all.select {|l| l.products.size > 1 }
l = listings.first
l.products
terms = { keywords: 'JU6500' }
@listings = []
params = {}
per_page = params[:per_page] || 10
page = params[:page] || 1
filters = {}
Listing.query_fields[:filter_fields].keys.each do |key|
  filters[key] = params[key].downcase.split(',').map(&:strip) if params[key].present?
end
search_response = Listing.search_by(terms, operator, filters, { size: per_page, scroll: '3m' })
search_response.results.first
search_response.results
exit
terms = { keywords: 'JU6500' }
@listings = []
params = {}
per_page = params[:per_page] || 10
page = params[:page] || 1
filters = {}
Listing.query_fields[:filter_fields].keys.each do |key|
  filters[key] = params[key].downcase.split(',').map(&:strip) if params[key].present?
end
search_response = Listing.search_by(terms, operator, filters, { size: per_page, scroll: '3m' })
operator = 'AND'
search_response = Listing.search_by(terms, operator, filters, { size: per_page, scroll: '3m' })
search_response.results
Listing.connect
Listing.connection
search_response = Listing.search_by(terms, operator, filters, { size: per_page, scroll: '3m' })
search_response.results
search_response.results.first
exit
operator = 'AND'
terms = { keywords: 'JU6500' }
@listings = []
params = {}
per_page = params[:per_page] || 10
page = params[:page] || 1
filters = {}
Listing.query_fields[:filter_fields].keys.each do |key|
  filters[key] = params[key].downcase.split(',').map(&:strip) if params[key].present?
end
search_response = Listing.search_by(terms, operator, filters, { size: per_page, scroll: '3m' })
search_response.results
search_response.results.all
search_response.results.first
search_response.results
exit
operator = 'AND'
terms = { keywords: 'JU6500' }
@listings = []
params = {}
per_page = params[:per_page] || 10
page = params[:page] || 1
filters = {}
Listing.query_fields[:filter_fields].keys.each do |key|
  filters[key] = params[key].downcase.split(',').map(&:strip) if params[key].present?
end
search_response = Listing.search_by(terms, operator, filters, { size: per_page, scroll: '3m' })
search_response.results.first
search_response.results
exit 
operator = 'AND'
terms = { keywords: 'JU6500' }
@listings = []
params = {}
per_page = params[:per_page] || 10
page = params[:page] || 1
filters = {}
Listing.query_fields[:filter_fields].keys.each do |key|
  filters[key] = params[key].downcase.split(',').map(&:strip) if params[key].present?
end
search_response = Listing.search_by(terms, operator, filters, { size: per_page, scroll: '3m' })
search_response.results.first
search_response.results
search_response.results.flat_map {|l| l['products.id']}
search_response.results.first
l = search_response.results.first
l.id
l.products
l.fields
l.fields['products.id']
search_response.results.flat_map {|l| l.field['products.id'] }
search_response.results.flat_map {|l| l.fields['products.id'] }
search_response.results.flat_map {|l| l.fields['products.id'] }.uniq
listings = Listing.all.select {|l| l.products.count == 0 }
l = listings.first
terms = { keywords: 'Bluetooth SpeakerElegant' }
exit
listings = Listing.all.select {|l| l.products.count == 0 }
l = listings.first
terms = { keywords: 'Bluetooth SpeakerElegant' }
@listings = []
params = {}
per_page = params[:per_page] || 10
page = params[:page] || 1
filters = {}
Listing.query_fields[:filter_fields].keys.each do |key|
  filters[key] = params[key].downcase.split(',').map(&:strip) if params[key].present?
end
search_response = Listing.search_by(terms, operator, filters, { size: per_page, scroll: '3m' })
operator = 'AND'
search_response = Listing.search_by(terms, operator, filters, { size: per_page, scroll: '3m' })
search_response.results.first
l.id
search_response.results
exit
operator = 'AND'
terms = { keywords: 'Bluetooth SpeakerElegant' }
@listings = []
params = {}
per_page = params[:per_page] || 10
page = params[:page] || 1
filters = {}
Listing.query_fields[:filter_fields].keys.each do |key|
  filters[key] = params[key].downcase.split(',').map(&:strip) if params[key].present?
end
search_response = Listing.search_by(terms, operator, filters, { size: per_page, scroll: '3m' })
exit
operator = 'AND'
terms = { keywords: 'Bluetooth SpeakerElegant' }
@listings = []
params = {}
per_page = params[:per_page] || 10
page = params[:page] || 1
filters = {}
Listing.query_fields[:filter_fields].keys.each do |key|
  filters[key] = params[key].downcase.split(',').map(&:strip) if params[key].present?
end
search_response = Listing.search_by(terms, operator, filters, { size: per_page, scroll: '3m' })
search_response.results.first
terms = { keywords: 'HDMI' }
search_response = Listing.search_by(terms, operator, filters, { size: per_page, scroll: '3m' })
search_response.results.first
Product.count
p = Proudct.find 870
p = Product.find 870
p.listings.count
Product.count
exit
terms = { keywords: 'HDMI' }
operator = 'AND'
terms = { keywords: params[:keywords].to_s }
@products = []
params = {}
operator = 'AND'
terms = { keywords: 'HDMI' }
@products = []
per_page = params[:per_page] || 10
page = params[:page] || 1
filters = {}
Product.query_fields[:filter_fields].keys.each do |key|
  filters[key] = params[key].downcase.split(',').map(&:strip) if params[key].present?
end
filters = {}
Listing.query_fields[:filter_fields].keys.each do |key|
  filters[key] = params[key].downcase.split(',').map(&:strip) if params[key].present?
end
search_response = Listing.search_by(terms, operator, filters, { size: per_page, scroll: '3m', type: 'products' })
exit
params = {}
operator = 'AND'
terms = { keywords: 'HDMI' }
@products = []
per_page = params[:per_page] || 10
page = params[:page] || 1
filters = {}
Listing.query_fields[:filter_fields].keys.each do |key|
  filters[key] = params[key].downcase.split(',').map(&:strip) if params[key].present?
end
search_response = Listing.search_by(terms, operator, filters, { size: per_page, scroll: '3m', type: 'products' })
search_response.results.first
exit
params = {}
operator = 'AND'
terms = { keywords: 'HDMI' }
@products = []
per_page = params[:per_page] || 10
page = params[:page] || 1
filters = {}
Listing.query_fields[:filter_fields].keys.each do |key|
  filters[key] = params[key].downcase.split(',').map(&:strip) if params[key].present?
end
search_response = Listing.search_by(terms, operator, filters, { size: per_page, scroll: '3m', type: 'products' })
search_response.results.first
exit
params = {}
operator = 'AND'
terms = { keywords: 'HDMI' }
@products = []
per_page = params[:per_page] || 10
page = params[:page] || 1
filters = {}
Listing.query_fields[:filter_fields].keys.each do |key|
  filters[key] = params[key].downcase.split(',').map(&:strip) if params[key].present?
end
search_response = Listing.search_by(terms, operator, filters, { size: per_page, scroll: '3m', type: 'products' })
Listing.connection
search_response = Listing.search_by(terms, operator, filters, { size: per_page, scroll: '3m', type: 'products' })
search_response.results.first
search_response.results
search_response.results.first
search_response.results
search_response.results.response
search_response.results.response['aggregations']
search_response.results.response.response
search_response.results.response.response['aggregations']
search_response.results.response.response['aggregations']['product_ids']
search_response.results.response.response['aggregations']['product_ids']['buckets'].map(&:'key')
search_response.results.response.response['aggregations']['product_ids']['buckets']
search_response.results.response.response['aggregations']['product_ids']['buckets'].map {|h| h['key']}
ids = search_response.results.response.response['aggregations']['product_ids']['buckets'].map {|h| h['key']}
params = {}
operator = 'AND'
terms = { keywords: 'HDMI' }
@products = []
per_page = params[:per_page] || 10
page = params[:page] || 1
filters = {}
Listing.query_fields[:filter_fields].keys.each do |key|
  filters[key] = params[key].downcase.split(',').map(&:strip) if params[key].present?
end
search_response = Listing.search_by(terms, operator, filters, { size: per_page, scroll: '3m', type: 'products' })
ids = search_response.results.response.response['aggregations']['product_ids']['buckets'].map {|h| h['key']}
scroll_id = search_response.response._scroll_id
scroll_response = Listing.__elasticsearch__.client.scroll(scroll_id: scroll_id, scroll: '3m')
ids = search_response.results.response.response['aggregations']['product_ids']['buckets'].map {|h| h['key']}
exit
p = Product.last
p.destroy
Product.last
p = _
p.destroy
exit
p = Product.last
p.destroy
exit
p = Product.last
p.destroy
exit
ENV['SLACK_WEBHOOK']
notifier = Slack::Notifier.new ENV['SLACK_WEBHOOK']
exit
ENV['SLACK_WEBHOOK']
notifier = Slack::Notifier.new ENV['SLACK_WEBHOOK']
notifier.channel
notifier.username = 'TEST'
notifier.channel = '#engineering'
notifier.ping 'TESTING'
exit
scrape = Scrape.last
notifier = Slack::Notifier.new(ENV['SLACK_WEBHOOK'], channel: '#rvx-rds', username: 'bot')
notifier.ping "Scrape for #{scrape.source.name} - #{scrape.created_at} just complete at #{scrape.completed_at}"
notifier = Slack::Notifier.new(ENV['SLACK_WEBHOOK'], channel: '#rvx-rds', username: "#{[Rails.env, ENV['REDIS_NAMESPACE']].join('_')}")
notifier.ping "Scrape for #{scrape.source.name} - #{scrape.created_at} just complete at #{scrape.completed_at}"
exit
url = "http://www.dhgate.com/product/s20i-20a-12v-24v-240w-solar-panel-charge/229825638.html"
Source.all
Source.find(6).scrapes
scrape_id = 9
Source.find(6).source_categories.where(test_process: true)
source_category_id = 3
SourceCategory.find(3)
Category.find(6)
source_category_id
scrape_id
source_category_id = 3
scrape_id = 9
url = "http://www.dhgate.com/product/s20i-20a-12v-24v-240w-solar-panel-charge/229825638.html"
exit
source_category_id = 3
scrape_id = 9
url = "http://www.dhgate.com/product/s20i-20a-12v-24v-240w-solar-panel-charge/229825638.html"
DhGateListingDetailJob.new.perform(scrape_id, source_category_id, url)
l = Listing.last
l.name
l.products
sls = l.scrape_listing_sellers
sls = sls.first
sls.min_price
sls.min_price.to_s
sls.currency
exit
rails c
edit -t
url = "http://www.ibuypicture.com/justfor2011/ELC-XC230-a.jpg"
download_via_proxy(url)
def download_via_proxy(url)
  proxy_uri = URI.parse("http://#{pop_proxy}")
  OpenURI.open_uri(url, :proxy_http_basic_authentication => [proxy_uri, ENV['PROXY_USERNAME'], ENV['PROXY_PASSWORD']])
rescue => e
  p e.message
end
download_via_proxy(url)
msg = "503 Service Unavailable"
msg =~ /\A4\d{2}/
msg =~ /\A5\d{2}/
if msg =~ /\A5\d{2}/
  puts "SKIP"
end
msg =~ /\A[45]\d{2}/
img.class
exit
url = "http://www.ibuypicture.com/justfor2011/ELC-XC230-a.jpg"
img = Asset.create(original_url: url)
img.destroy
img = ProductImage.create(original_url: url)
exit
Scrape.where(running: true)
Scrape.where(running: true).update_all(running: false)
fk = Source.find_by name: 'flipkart'
fk.source_categories
fk.source_categories.count
fk.source_categories.active.count
s = Scrape.last
s.listings.count
s.products.count
l = s.listings.last
l.images
fk.source_categories.where(test_process: true)
s.listings.count
s.products.count
Listing.last
l = _
l.products
sls = ScrapeListingSeller.last
sls.min_price.to_f
sls.exchanged_min.to_f
exit
url = "https://images-na.ssl-images-amazon.com/images/I/310qMEd7cvL.jpg"
Net::HTTP
uri = URI.parse(url)
edit -t
pop_proxy
proxy = )_
proxy = pop_proxy
proxy_port = proxy.split(':').last
proxy_addr = proxy.split(':').take(4)
proxy_addr
proxy_addr, proxy_port = pop_proxy.split(':')
proxy_addr
url
file = Net::HTTP.new(url, nil, proxy_addr, proxy_port).start {|http| http.get(url, '') }
uri = URI.parse url
uri.host
uri.path
uri = URI.parse(url)
Net::HTTP.new(uri.host, uri.port, proxy_addr, proxy_port, ENV['PROXY_USERNAME'], ENV['PROXY_PASSWORD']).start do |http|
  http.get(uri.path)
end
res = _
res.body
Net::HTTP.new(uri.host, nil, proxy_addr, proxy_port, ENV['PROXY_USERNAME'], ENV['PROXY_PASSWORD']).start do |http|
res = Net::HTTP.new(uri.host, nil, proxy_addr, proxy_port, ENV['PROXY_USERNAME'], ENV['PROXY_PASSWORD']).start do |http|
  http.get(uri.path)
end
res.body
res.close
uri.request_uri
proxy_addr
proxy_addr = "http://#{proxy_addr}"
uri = URI.parse(url)
Net::HTTP.new(uri.host, nil, proxy_addr, proxy_port, ENV['PROXY_USERNAME'], ENV['PROXY_PASSWORD']).start do |http|
  # always proxy via your.proxy.addr:8080
  http.get(uri.request_uri)
end
Net::HTTP.new(uri.host, nil, proxy_addr, proxy_port, ENV['PROXY_USERNAME'], ENV['PROXY_PASSWORD']).start do |http|
  http.get(uri.request_uri)
end
proxy_uri = URI.parse("http://#{pop_proxy}")
f = OpenURI.open_uri(url,
  proxy_http_basic_authentication: [proxy_uri, ENV['PROXY_USERNAME'], ENV['PROXY_PASSWORD']],
  ssl_verify_mode: OpenSSL::SSL::VERIFY_NONE
)
exit
Product.count
products = Product.first(1006).select { |p| p.sources.count > 1 }
p = products.last
p.listings
p.listings.map(&:source)
exit
users = { {name: 'Jon'}, {name: 'Jeff'}, {name: 'Jonathan'}, {name: 'Alex'}, {name: 'Viresh'} }
users = { users: {name: 'Jon'}, {name: 'Jeff'}, {name: 'Jonathan'}, {name: 'Alex'}, {name: 'Viresh'} }
users = { users: {{name: 'Jon'}, {name: 'Jeff'}, {name: 'Jonathan'}, {name: 'Alex'}, {name: 'Viresh'} }}
users = { users:  [{ name: 'Jon' }, { name: 'Jeff' }, { name: 'Jonathan' }, { name: 'Alex' }, { name: 'Viresh' }] } 
users.grep(/^J/)
users[:users].grep(/^J/)
users[:users].values.grep(/^J/)
users[:users].map(&:name).grep(/^J/)
users[:users].map(&:[], :name).grep(/^J/)
users[:users].map(&:[:name]).grep(/^J/)
users[:users].map{|u| u[:name]}.grep(/^J/)
users[:users].map{|u| u[:name]}.grep(/\A^J/)
users[:users].map{|u| u[:name]}.grep(/A/)
exit
1/2r
7/8r
7/8r.to_f
1/3r
1/3r.to_f
4523/1235r
rational = _
rational = 3/4r
rational = 6/8r
1+2i
exit
ProductManual.count
ManualText
ManualText.where(text: nil)
ManualText.ids
ManualText.ids.count
ProductManual.where.not(manual_text: ManualText.all).to_sql
ProductManual.where.not(manual_text: ManualText=).to_sql
ProductManual.where.not(manual_text: ManualText).to_sql
ProductManual.where.not(manual_text: ManualText.all).to_sql
ProductManual.where.not(manual_text: ManualText.all).ids
ProductManual.where.not(manual_text: ManualText.all).first
ProductManual.where.not(manual_text: ManualText.ids).first
ProductManual.where.not(manual_text: ManualText.all)
ProductManual.joins(:manual_text).count
ProductManual.where.not(id: ProductManual.joins(:manual_text))
pm = _.first
pm.text
mannuals = ProductManual.where.not(id: ProductManual.joins(:manual_text))
pm.attachment_content_type
pm.original_url
pm = ProductManual.find 6908
pm.text
url = pm.original_url
def download_via_proxy(url)
  proxy_uri = URI.parse("http://#{pop_proxy}")
  OpenURI.open_uri(
    url,
    proxy_http_basic_authentication: [proxy_uri, ENV['PROXY_USERNAME'], ENV['PROXY_PASSWORD']],
    ssl_verify_mode: OpenSSL::SSL::VERIFY_NONE
  )
end
def pop_proxy
  Sidekiq.redis do |redis|
    used_proxies = redis.smembers(:'proxies:used')
    all_proxies = redis.smembers(:'proxies:all')
    if used_proxies.length == all_proxies.length
      used_proxies = []
      redis.del :'proxies:used'
    end
    available_proxies = all_proxies - used_proxies
    proxy = available_proxies.sample
    redis.sadd :'proxies:used', proxy
    proxy
  end
end
def pop_user_agent
  Sidekiq.redis { |redis| redis.srandmember :'useragents:all' }
end
file = download_via_proxy(original_url)
file = download_via_proxy(url)
pdf = Yomu.new(file)
pdf.text
pm = ManualText.last.manual
pm = ManualText.last.product_manual
file.close
file = download_via_proxy(pm.original_url)
pdf = Yomu.new(file)
pdf.text
ManualText.last
file
file.close
content = pdf.text.strip.gsub(/\s+/, ' ').squeeze('.').downcase.slice(0, 100000)
file
pdf
exit
pm = ManualText.last.product_manual
file = download_via_proxy(pm.original_url)
include RvxRds::Proxy
file = download_via_proxy(pm.original_url)
pdf = Yomu.new(file)
pdf.text
pdf
ProductManual
exit
pm = ProductManual.last
pm.name
exit
pm = ProductManual.last
pm.text
ProductManual.first
pm = _
pm.text
pm = ProductManual.where.not(id: ManualText.pluck(:product_manual_id)).first
pm.original_url = nil
pm.save
pm.errors
pm = ProductManual.where.not(id: ManualText.pluck(:product_manual_id)).sample
pm.original_url = nil
pm.save
url = pm.attachment.url || pm.original_url
pm.attachment = nil
pm.save
url = pm.attachment.url || pm.original_url
pm.    url = attachment.url || original_url
url = (attachment.file? && attachment.url) || original_url
url = (pm.attachment.file? && pm.attachment.url) || pm.original_url
if url.nil?
  exit
pm = ProductManual.find 6969
pm.original_url
pm.text
pm.text(reprocess: true)
exit
pm = ProductManual.find 6969
pm.text(reprocess: true)
pm = ProductManual.find 6969
exit
pm = ProductManual.find 6969
pm.text(reprocess: true)
exit
pm = ProductManual.find 6969
pm.text(reprocess: true)
exit
pm = ProductManual.find 6969
pm.text(reprocess: true)
reload!
pm = ProductManual.find 6969
pm.text(reprocess: true)
exit
pm
pm = ProductManual.find 6969
pm.text(reprocess: true, pdf: false)
Yomu.server(:text)
ProductManual.find_each {|pm| pm.text(pdf: true, reprocess: true, overwrite: true)}
exit
Yomu.server(:text)
ProductManual.find_each {|pm| pm.text(pdf: true, reprocess: true, overwrite: true)}
http://s3.amazonaws.com/rvx-rds/development/manuals/uploads/000/010/757/original/open-uri20160713-20751-1dzasrp?1468446360
exit
Yomu.server(:text)
ProductManual.find_each {|pm| pm.text(pdf: true, reprocess: true, overwrite: true)}
exit
Yomu.server(:text)
Yomu.kill_server!
Yomu.server(:text)
exit
Yomu.kill_server!
Yomu.server(:text)
ProductManual.find_each {|pm| pm.text(pdf: true, reprocess: true, overwrite: true)}
ManualText.count
ProductManual.count
ManualText.pluck(:text)
ManualText.first
ManualText.first.product_manual
pm = _
Yomu.kill_server!
include RvxRds::Proxy
url = (pm.attachment.file? && pm.attachment.url) || pm.original_url
file = download_via_proxy(url)
pdf = Yomu.new(file)
content = pdf.text.strip.gsub(/\s+/, ' ').squeeze('.').downcase.slice(0, 100000)
exit
Yomu.server(:text)
ProductManual.find_each {|pm| pm.text(pdf: true, reprocess: true, overwrite: true)}
exit
Yomu.server(:text)
ProductManual.find_each {|pm| pm.text(pdf: true, reprocess: true, overwrite: true)}
ManualText.count
ProductManual.count
ManualText.pluck(:text)
exit
Identifier.asin.first
asin = _
asin.products
asin.products.class
asin.products.to_a
exit
namespace = Redis::Namespace.new("#{Rails.env}:shared_cache", redis: Redis.new(url: ENV['REDIS_URL']))
response = nil
key = 'upcitemdb'
_redis_key, proxy = namespace.blpop("#{key}:proxies", timeout: 10)
exit
namespace = Redis::Namespace.new("#{Rails.env}:shared_cache", redis: Redis.new(url: ENV['REDIS_URL']))
response = nil
key = 'upcitemdb'
_redis_key, proxy = namespace.blpop("#{key}:proxies", timeout: 10)
edit -t
set_proxies('upcitemdb')
response = nil
key = 'upcitemdb'
namespace = Redis::Namespace.new("#{Rails.env}:shared_cache", redis: Redis.new(url: ENV['REDIS_URL']))
_redis_key, proxy = namespace.blpop("#{key}:proxies", timeout: 10)
_redis_key, proxy = namespace.blpop(["#{key}:proxies"], timeout: 10)
exit
_redis_key, proxy = namespace.blpop("#{key}:proxies", read_timeout: 10)
namespace = Redis::Namespace.new("#{Rails.env}:shared_cache", redis: Redis.new(url: ENV['REDIS_URL']))
key = 'upcitemdb'
_redis_key, proxy = namespace.blpop("#{key}:proxies", read_timeout: 10)
_redis_key, proxy = namespace.blpop("#{key}:proxies", read_timeout: 10, timeout: 5)
_redis_key, proxy = namespace.blpop("#{key}:proxies", read_timeout: 10, connect_timeout: 5)
_redis_key, proxy = namespace.blpop("#{key}:proxies", timeout: 10)
proxy = "38.120.19.38:60000"
namespace.rpush("#{key}:proxies", proxy)
_redis_key, proxy = namespace.blpop("#{key}:proxies", timeout: 10)
namespace.rpush("#{key}:proxies", proxy)
_redis_key, proxy = namespace.blpop("#{key}:proxies", timeout: 10)
namespace.rpush("#{key}:proxies", proxy)
_redis_key, proxy = namespace.blpop("#{key}:proxies", timeout: 10)
namespace.rpush("#{key}:proxies", proxy)
_redis_key, proxy = namespace.blpop("#{key}:proxies", timeout: 10)
namespace.rpush("#{key}:proxies", proxy)
_redis_key, proxy = namespace.blpop("#{key}:proxies", timeout: 10)
namespace.rpush("#{key}:proxies", proxy)
_redis_key, proxy = namespace.blpop("#{key}:proxies", timeout: 10)
namespace.rpush("#{key}:proxies", proxy)
last_used = namespace.get "#{key}:#{proxy.split('.')[0..2]}"
last_used.to_i - Time.now.to_i
([(last_used.to_i - Time.now.to_i), 0].max)
exit
Redis.current
Sidekiq.current
namespace = Redis::Namespace.new("#{[Rails.env, ENV['REDIS_NAMESPACE']].join('_')}")
Redis.new(url: ENV['REDIS_URL'])
exit
"#{[Rails.env, ENV['REDIS_NAMESPACE']].join('_')}"
redis
redis = Redis.current
redis.flushall
exit
s = Scrape.last
s.products.count
s.listings.count
l = s.listings.last
l.products
s.products.count
s.listings.pluck(:url)
s.listings.pluck(:original_url)
l =- s.listings[5]
l = s.listings[5]
l = s.listings[6]
l.screenshots
l.screenshots.first.attachment.url
s.listings.pluck(:id, :url)
l = Listing.find 2251
l.screenshots.first.attachment.url
s = Scrape.last
scrape_id = s.id
restart = false
s
UpdateScrapeCounterJob.perform_async scrape_id
scrape = Scrape.find(scrape_id)
scrape.products.include_indexed_fields.find_in_batches(batch_size: 100) do |records|
  Product.bulk_index(records)
end
scrape.listings.include_indexed_fields.find_in_batches(batch_size: 250) do |records|
  Listing.bulk_index(records)
end
xit
exit
RdsApiCache
cache = RdsApiCache.new
cache.fetch('hello')
exit
cache = RdsApiCache.new
cache.fetch('hello')
cache.fetch('hello') {}
h = Hashie::Mash.new
params = Hashie::Mash.new
params[:brands] = 'Sony'
params[:keywords] = 'HDMI'
params
params.to_s
params.to_json
params.to_json.to_s
params[:artist] = 'Bach'
params
params.to_json
params.sort.to_json
params.sort.to_h
params.sort.to_h.to_s
params[:keywords
params[:keywords]
params.to_query
params.sort.to_query
params.sort.to_h.to_query
params[:keywords] = 'HDMI,bluetooth'
params.sort.to_h.to_query
filtered_params = params.reject {|k,v| [:cloud, :scroll_id, :keywords, :type, :using].include? k }
filtered_params
params
params.class
params.keys
params.slice(:keywords)
params
filtered_params = params.reject {|k,v| [:brands, :keywords, :artist].include? k }
params
filtered_params = params.select {|k,v| [:brands, :keywords, :artist].include? k }
params
params[:cloud] = "HDMI"
params
params.class
filtered_params = params.reject {|k,v| [:brands, :keywords, :artist, :cloud].include? k }
filtered_params = params.reject {|k,v| [:brands, :keywords, :artist, 'cloud'].include? k }
params.symbolize_keys
params
params = params.symbolize_keys
p params
filtered_params = params.reject {|k,v| [:brands, :keywords, :artist, :cloud].include? k }
params[:page] = 3
page = params.delete(:page) || 1
key_params = params.sort.to_h
key = "#{key_params.to_param}:#{page}"
key = "products:#{key_params.to_param}:#{page}"
cache = RdsApiCache.new
cache.connection
cache.call(:connection)
cache.send(:connection)
def prefix
  "#{Rails.env}_#{ENV['REDIS_NAMESPACE']}:api_cache"
end
def full_key(key, options={})
  "#{prefix}:#{key}"
end
key
connection = cache.send(:connection)
val = Products.first 100
val = Product.first 100
val = Oj.dump val
prefix
full_key(key)
options = {}
existing_val = connection.get full_key(key, options)
connection.set full_key(key, options), val, :ex => expires_in
expires_in = options[:expires_in] || 2.weeks
connection.set full_key(key, options), val, :ex => expires_in
existing_val = connection.get full_key(key, options)
Oj.load(_)
val = Products.first(100).map {|p| ProductSerializer.new(p) }
val = Product.first(100).map {|p| ProductSerializer.new(p) }
val = Product.first(100).map {|p| ProductSerializer.new(p) }.as_json
OAOBOei
xtei
xt
val = Product.include_indexed_fields.first(100).map {|p| ProductSerializer.new(p) }
val = Product.include_indexed_fields.first(100).map {|p| ProductSerializer.new(p) }.as_json
cache = RdsApiCache.new
params = Hashie::Mash.new
params[:page = 1]
params[:page] = 1
params[:keywords] = 'hdmi'
params
page = params.delete(:page) || 1
key_params = params.sort.to_h
key = "products:#{key_params.to_param}:#{page}"
options = {}
expires_in = options[:expires_in] || 2.weeks
def prefix
  "#{Rails.env}_#{ENV['REDIS_NAMESPACE']}:api_cache"
end
def full_key(key, options={})
  "#{prefix}:#{key}"
end
existing_val = connection.get full_key(key, options)
@connection ||= Redis.new
connection = _
existing_val = connection.get full_key(key, options)
val
val.class
val.as_json
val.to_json
val = Oj.dump val
connection.set full_key(key, options), val, :ex => expires_in
Oj.load(val)
existing_val = connection.get full_key(key, options)
Oj.load(existing_val)
params
page
key
existing_val = connection.get full_key(key, options)
key
Oj.load(existing_val)
cahced_response = Oj.load(existing_val)
cached_response
cached_response = Oj.load(existing_val)
cached_response
resp = {products: cached_response.map { } }
resp = {products: cached_response.map {|p| p} }
resp = {products: cached_response.map {|p| p.value } }
p = cached_response.first
resp = {products: cached_response.map {|p| p['product'] } }
exit
params = Hashie::Mash.new
params[:keywords] = 'hdmi'
params[:page] = 3
params[:brands] = 'lg'
params
params.to_s
Oj.load(params.to_s
Oj.load(params.to_s)
Oj.dump(params)
str = _
Oj.load(str)
h = _
h.class
str.class
params
page = params.delete(:page)
page
params
str = Oj.dump(params.merge!({ page: page }))
params = Oj.load(str)
params.class
DateTime.now
DateTime.now.to_i
DateTime.at(_)
Time.at(1469039361)
_.class
Time.now
DateTime.now
migrate_timestamp = '20150608044134'
Time.now.strftime("%Y%m%d%H%M%S")
params
params.class
params.select {|k, v| %w(keywords brands models source_ids scrape_ids category_ids).include? k }
_.class
params.select {|k, v| %w(keywords brands models source_ids scrape_ids category_ids).include? k }.sort
params.select {|k, v| %w(keywords brands models source_ids scrape_ids category_ids).include? k }.sort.to_h
params
params.class
params.slice(:keywords, :brands)
h = _
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
pids = Product.limit(200).ids
Product.include_api_info.where(id: pids).to_sql
exit
pids = Product.limit(200).ids
Product.include_api_info.where(id: pids).to_sql
Product.include_api_info.where(id: pids).limit(1).to_sql
Product.include_api_info.where(id: pids)
params = Hashie::Mash.new({keywords: 'hdmi', brands: 'lg'})
params.class
params = Oj.load(params).to_h
page = 1
Oj.dump(params.merge!({ type: 'products', page: page })
Oj.dump(params.merge!({ type: 'products', page: page }))
params = _
params = Oj.load(params).to_h
operator = 'AND'
terms = { keywords: params[:keywords].to_s }
params
params = Oj.load(params)
params.to_s
params = Oj.load(params)
params = Hashie::Mash.new({keywords: 'hdmi', brands: 'lg'})
Oj.dump(params.merge!({ type: 'products', page: page })
Oj.dump(params.merge!({ type: 'products', page: page }))
page
params = Oj.load(params)
params = Oj.dump(params.merge!({ type: 'products', page: page }))
params = Oj.load(params)
operator = 'AND'
terms = { keywords: params[:keywords].to_s }
per_page = 200
page = params[:page] || 1
scroll_id = params[:scroll_id]
filters = {}
Product.query_fields[:filter_fields].keys.each do |key|
  filters[key] = params[key].downcase.split(',').map(&:strip) if params[key].present?
end
if params[:using] == 'any'
  keywords = [params[:keywords].to_s, params[:brand], params[:model]].join(' ')
  filters.delete_if { |k, _v| [:brand, :model].include?(k) }
  terms = { keywords: keywords }
  operator = 'OR'
end
search_response = Product.search_by(terms, operator, filters, { size: per_page, scroll: '3m' })
pids = search_response.results.map { |p| p.id }.compact
total = search_response.results.total
scroll_id = search_response.response._scroll_id
products = Product.include_api_info.where(id: pids)
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
timestamp = params[:timestamp] || Time.now.strftime("%Y%m%d%H%M%S")
key = "products:#{cache_params}:#{page}:#{timestamp}"
cache = RdsApiCache.new
cache.set_value(key, ProductSerializer(products))
cache.set_value(key, ProductSerializer.new(products))
reload!
cache.set_value(key, ProductSerializer.new(products))
exit
params = Hashie::Mash.new({keywords: 'hdmi', brands: 'lg'})
Oj.dump(params.merge!({ type: 'products', page: page }))
page = 1
Oj.dump(params.merge!({ type: 'products', page: page }))
params = Oj.load(params)
params = Oj.dump(params.merge!({ type: 'products', page: page }))
params = Oj.load(params)
operator = 'AND'
terms = { keywords: params[:keywords].to_s }
per_page = 200
page = params[:page] || 1
scroll_id = params[:scroll_id]
filters = {}
Product.query_fields[:filter_fields].keys.each do |key|
  filters[key] = params[key].downcase.split(',').map(&:strip) if params[key].present?
end
if scroll_id
  scroll_response = Product.__elasticsearch__.client.scroll(scroll_id: params[:scroll_id], scroll: '3m')
  pids = scroll_response['hits']['hits'].map { |l| l['_id'] }
  total = scroll_response['hits']['total']
  scroll_id = params[:scroll_id]
else
  search_response = Product.search_by(terms, operator, filters, { size: per_page, scroll: '3m' })
  pids = search_response.results.map { |p| p.id }.compact
  total = search_response.results.total
  scroll_id = search_response.response._scroll_id
end
products = Product.include_api_info.where(id: pids)
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
timestamp = params[:timestamp] || Time.now.strftime("%Y%m%d%H%M%S")
key = "products:#{cache_params}:#{page}:#{timestamp}"
cache = RdsApiCache.new
cache.set_value(key, ProductSerializer.new(products))
ProductSerializer.new(products)
ProductSerializer.new(products).as_json
cache.set_value(key, products.map {|p| ProductSerializer.new(p) }.as_json)
total
total / per_page
total 250
total = 250
total / per_page
PopulateApiCacheJob.new.perform params.merge({ scroll_id: scroll_id, timestamp: timestamp, page: page + 1 }) unless (total / per_page).zero?
exit
Oj.load(nil)
val = nil
Oj.load(val) if val.present?
exit
redis = Redis.current
redis.flushall
exit
params = Hashie::Mash({keywords: 'led'})
params = Hashie::Mash.new({keywords: 'led'})
page = params.delete(:page) || 1
per_page = 200
timestamp = params.delete(:timestamp)
key_params = params.sort.to_h
key = "products:#{key_params.to_param}:#{page}" #TODO HANDLE WHEN TIMESTAMP IS GIVEN OR NOT
key
cache = RdsApiCache.new
cached_response = cache.fetch(key) {}
PopulateApiCacheJob.perform_async Oj.dump(params.merge!({ type: 'products', page: page }))
redis
redis = Redis.current
redis.keys
redis.keys.select {|k| k.inlcude?('api_cache') }
redis.keys.select {|k| k.include?('api_cache') }
timestampe = ;20160720124516'
timestampe = '20160720124516'
redis.flushall
params
params.merge!({ type: 'products', page: page })
params
params = Oj.dump(params)
params = Oj.load(params)
operator = 'AND'
terms = { keywords: params[:keywords].to_s }
per_page = 200
page = params[:page] || 1
scroll_id = params[:scroll_id]
filters = {}
Product.query_fields[:filter_fields].keys.each do |key|
  filters[key] = params[key].downcase.split(',').map(&:strip) if params[key].present?
end
if params[:using] == 'any'
  keywords = [params[:keywords].to_s, params[:brand], params[:model]].join(' ')
  filters.delete_if { |k, _v| [:brand, :model].include?(k) }
  terms = { keywords: keywords }
  operator = 'OR'
end
search_response = Product.search_by(terms, operator, filters, { size: per_page, scroll: '3m' })
pids = search_response.results.map { |p| p.id }.compact
total = search_response.results.total
scroll_id = search_response.response._scroll_id
products = Product.include_api_info.where(id: pids)
products.count
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
timestamp = params[:timestamp] || Time.now.strftime("%Y%m%d%H%M%S")
key = "products:#{cache_params}:#{page}:#{timestamp}"
cache = RdsApiCache.new
cache.set_value(key, products.map {|p| ProductSerializer.new(p) }.as_json)
(total / per_page).zero?
params
params.merge({ scroll_id: scroll_id, timestamp: timestamp, page: page + 1 })
params.merge!({ scroll_id: scroll_id, timestamp: timestamp, page: page + 1 })
Oj.dump(params)
params = _
params = Oj.load(params)
operator = 'AND'
terms = { keywords: params[:keywords].to_s }
per_page = 200
page = params[:page] || 1
scroll_id = params[:scroll_id]
if scroll_id
  scroll_response = Product.__elasticsearch__.client.scroll(scroll_id: params[:scroll_id], scroll: '3m')
  pids = scroll_response['hits']['hits'].map { |l| l['_id'] }
  total = scroll_response['hits']['total']
else
  filters = {}
  Product.query_fields[:filter_fields].keys.each do |key|
    filters[key] = params[key].downcase.split(',').map(&:strip) if params[key].present?
  end
  if params[:using] == 'any'
    keywords = [params[:keywords].to_s, params[:brand], params[:model]].join(' ')
    filters.delete_if { |k, _v| [:brand, :model].include?(k) }
    terms = { keywords: keywords }
    operator = 'OR'
  end
  search_response = Product.search_by(terms, operator, filters, { size: per_page, scroll: '3m' })
  pids = search_response.results.map { |p| p.id }.compact
  total = search_response.results.total
  scroll_id = search_response.response._scroll_id
end
total
products = Product.include_api_info.where(id: pids)
products.count
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
timestamp = params[:timestamp] || Time.now.strftime("%Y%m%d%H%M%S")
params
key = "products:#{cache_params}:#{page}:#{timestamp}"
cache = RdsApiCache.new
cache.set_value(key, products.map {|p| ProductSerializer.new(p) }.as_json)
(total / (per_page * page)).zero?
PopulateApiCacheJob.perform_async Oj.dump(params.merge({ scroll_id: scroll_id, timestamp: timestamp, page: page + 1 }))
params = Oj.dump(params.merge({ scroll_id: scroll_id, timestamp: timestamp, page: page + 1 }))
params = Oj.load(params)
operator = 'AND'
terms = { keywords: params[:keywords].to_s }
per_page = 200
page = params[:page] || 1
scroll_id = params[:scroll_id]
scroll_response = Product.__elasticsearch__.client.scroll(scroll_id: params[:scroll_id], scroll: '3m')
pids = scroll_response['hits']['hits'].map { |l| l['_id'] }
'products'
'products'.constantize
'products'.singularize
'products'.singularize.constantize
Product
'products'.singularize.capitalize.constantize
model = _
"#{}
"#{model}"
"#{model}Serializer"
exit
params = Hashie::Mash.new({keywords: "led"})
params = Hashie::Mash.new({keywords: "led", type: 'products'})
type = params.delete(:type)
model = type.singularize.capitalize.constantize
Product.connection
per_page = 200
page = params[:page] || 1
scroll_id = params[:scroll_id]
cache = Redis.current
Redis.new
cache = Redis.new
edit -t
search_response = model.search_by(*prepare_search_params(model, params), { size: per_page, scroll: '3m' })
pids = search_response.results.map { |p| p.id }.compact
prepare_search_params(model, params)
def prepare_search_params(model, params)
  operator = 'AND'
  terms = { keywords: params[:keywords].to_s }
  filters = {}
  model.query_fields[:filter_fields].keys.each do |key|
    filters[key] = params[key].downcase.split(',').map(&:strip) if params[key].present?
  end
  if params[:using] == 'any'
    keywords = [params[:keywords].to_s, params[:brand], params[:model]].join(' ')
    filters.delete_if { |k, _v| [:brand, :model].include?(k) }
    terms = { keywords: keywords }
    operator = 'OR'
  end
  return terms, operator, filters
end
search_response = model.search_by(*prepare_search_params(model, params), { size: per_page, scroll: '3m' })
pids = search_response.results.map { |p| p.id }.compact
total = search_response.results.total
scroll_id = search_response.response._scroll_id
records = model.include_api_info.where(id: pids)
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
version = params[:version] || Time.now.strftime("%Y%m%d%H%M%S")
prefix = "#{type}:#{cache_params.to_param}"
key = "#{prefix}:#{version}:#{page}"
cache.set_value(key, records.map {|r| "#{model}Serializer".constantize.new(r) }.as_json)
cache = RdsApiCache.new
cache.set_value(key, records.map {|r| "#{model}Serializer".constantize.new(r) }.as_json)
(total / (per_page * page)).zero?
page = page+1
params
params.merge!({ scroll_id: scroll_id, timestamp: timestamp, type: type, page: page + 1 })
params.merge!({ scroll_id: scroll_id, timestamp: version, type: type, page: 2 })
type = params.delete(:type)
model = type.singularize.capitalize.constantize
per_page = 200
page = params[:page] || 1
scroll_response = model.__elasticsearch__.client.scroll(scroll_id: params[:scroll_id], scroll: '3m')
Product.find 66
Product.import
cache
cache.clear_cache!
params
params[:page] = 1
params
params.delet(:scroll_id)
params
params.delete(:scroll_id)
params
params.delete(:timestamp)
params
params[:type] = 'products'
type = params.delete(:type)
model = type.singularize.capitalize.constantize
per_page = 200
page = params[:page] || 1
scroll_id = params[:scroll_id]
search_response = model.search_by(*prepare_search_params(model, params), { size: per_page, scroll: '3m' })
pids = search_response.results.map { |p| p.id }.compact
total = search_response.results.total
scroll_id = search_response.response._scroll_id
records = model.include_api_info.where(id: pids)
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
version = params[:version] || Time.now.strftime("%Y%m%d%H%M%S")
prefix = "#{type}:#{cache_params.to_param}"
key = "#{prefix}:#{version}:#{page}"
cache.set_value(key, records.map {|r| "#{model}Serializer".constantize.new(r) }.as_json)
params.merge!({ scroll_id: scroll_id, timestamp: version, type: type, page: page + 1 })
type = params.delete(:type)
model = type.singularize.capitalize.constantize
per_page = 200
page = params[:page] || 1
scroll_id = params[:scroll_id]
scroll_response = model.__elasticsearch__.client.scroll(scroll_id: scroll_id, scroll: '3m')
pids = scroll_response['hits']['hits'].map { |l| l['_id'] }
total = scroll_response['hits']['total']
records = model.include_api_info.where(id: pids)
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
version = params[:version] || Time.now.strftime("%Y%m%d%H%M%S")
prefix = "#{type}:#{cache_params.to_param}"
key = "#{prefix}:#{version}:#{page}"
cache.set_value(key, records.map {|r| "#{model}Serializer".constantize.new(r) }.as_json)
(total / (per_page * page)).zero?
params.merge!({ scroll_id: scroll_id, timestamp: version, type: type, page: page + 1 })
type = params.delete(:type)
model = type.singularize.capitalize.constantize
per_page = 200
page = params[:page] || 1
scroll_id = params[:scroll_id]
scroll_response = model.__elasticsearch__.client.scroll(scroll_id: scroll_id, scroll: '3m')
pids = scroll_response['hits']['hits'].map { |l| l['_id'] }
total = scroll_response['hits']['total']
records = model.include_api_info.where(id: pids)
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
version = params[:version] || Time.now.strftime("%Y%m%d%H%M%S")
prefix = "#{type}:#{cache_params.to_param}"
key = "#{prefix}:#{version}:#{page}"
cache.set_value(key, records.map {|r| "#{model}Serializer".constantize.new(r) }.as_json)
(total / (per_page * page)).zero?
def update_versions(prefix, new_version)
  current_version = cache.fetch("#{prefix}}:current_version")
  cache.set_value("#{prefix}:old_version", current_version) if current_version.present?
  cache.set_value("#{prefix}:current_version", new_version)
  old_version = cache.fetch("#{prefix}}:old_version")
  cache.clear_cache!("#{prefix}:#{old_version}") if old_version.present?
end
update_versions(prefix, version)
current_version = cache.fetch("#{prefix}}:current_version")
cache.set_value("#{prefix}:old_version", current_version) if current_version.present?
cache.set_value("#{prefix}:current_version", new_version)
cache.set_value("#{prefix}:current_version", version)
old_version = cache.fetch("#{prefix}}:old_version")
cache.clear_cache!("#{prefix}:#{old_version}") if old_version.present?
params
prefix
cache.clear_cache!(prefix)
connection = Redis.current
key
prefix
def full_key(key, options={})
  "#{prefix}:#{key}"
end
full_key
prefix
key = prefix
def prefix
  "#{Rails.env}_#{ENV['REDIS_NAMESPACE']}:api_cache"
end
prefix =     "#{Rails.env}_#{ENV['REDIS_NAMESPACE']}:api_cache"
prefix
full_key(key)
connection.del full_key(key)
key
key = key + ':*'
connection.del full_key(key)
full_key(key)
connection.keys(full_key(key)).each {|k| connection.del(k) }
Object.eval("Product")
eval("Product")
type
%w(products listings).include?(type)
model = type.gsub(/s$/, '').constantize
model = type.classify.constantize
params
params = Hashie::Mash.new({keywords: 'led'})
params.merge!({ type: 'products', page: 1 })
page = params.delete(:page) || 1
cache = RdsApiCache.new
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
version = params[:version]
version = params[:version] || cache.fetch("products:#{cache_params.to_param}:current_version")
params
params = Oj.dump(params.merge!({ type: 'products', page: page }))
cache = RdsApiCache.new
params = Oj.load(params)
type = params.delete(:type)
raise "Unrecognized type: #{type}" unless %w(products listings).include?(type)
model = type.classify.constantize
per_page = 200
page = params[:page] || 1
scroll_id = params[:scroll_id]
search_response = model.search_by(*prepare_search_params(model, params), { size: per_page, scroll: '3m' })
pids = search_response.results.map { |p| p.id }.compact
total = search_response.results.total
scroll_id = search_response.response._scroll_id
records = model.include_api_info.where(id: pids)
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
version = params[:version] || Time.now.strftime("%Y%m%d%H%M%S")
prefix = "#{type}:#{cache_params.to_param}"
key = "#{prefix}:#{version}:#{page}"
cache.set_value(key, records.map {|r| "#{model}Serializer".constantize.new(r) }.as_json)
v = version
(total / (per_page * page)).zero?
params = Oj.dump(params.merge!({ scroll_id: scroll_id, version: version, type: type, page: page + 1 }))
params = Oj.load(params)
type = params.delete(:type)
raise "Unrecognized type: #{type}" unless %w(products listings).include?(type)
model = type.classify.constantize
per_page = 200
page = params[:page] || 1
scroll_id = params[:scroll_id]
scroll_response = model.__elasticsearch__.client.scroll(scroll_id: scroll_id, scroll: '3m')
pids = scroll_response['hits']['hits'].map { |l| l['_id'] }
total = scroll_response['hits']['total']
records = model.include_api_info.where(id: pids)
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
version = params[:version] || Time.now.strftime("%Y%m%d%H%M%S")
version == v
prefix = "#{type}:#{cache_params.to_param}"
key = "#{prefix}:#{version}:#{page}"
cache.set_value(key, records.map {|r| "#{model}Serializer".constantize.new(r) }.as_json)
params = Oj.dump(params.merge!({ scroll_id: scroll_id, version: version, type: type, page: page + 1 }))
params = Oj.load(params)
type = params.delete(:type)
raise "Unrecognized type: #{type}" unless %w(products listings).include?(type)
model = type.classify.constantize
per_page = 200
page = params[:page] || 1
scroll_id = params[:scroll_id]
if scroll_id
  scroll_response = model.__elasticsearch__.client.scroll(scroll_id: scroll_id, scroll: '3m')
  pids = scroll_response['hits']['hits'].map { |l| l['_id'] }
  total = scroll_response['hits']['total']
else
  search_response = model.search_by(*prepare_search_params(model, params), { size: per_page, scroll: '3m' })
  pids = search_response.results.map { |p| p.id }.compact
  total = search_response.results.total
  scroll_id = search_response.response._scroll_id
end
records = model.include_api_info.where(id: pids)
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
# timestamp for expiration
version = params[:version] || Time.now.strftime("%Y%m%d%H%M%S")
prefix = "#{type}:#{cache_params.to_param}"
key = "#{prefix}:#{version}:#{page}"
cache.set_value(key, records.map {|r| "#{model}Serializer".constantize.new(r) }.as_json)
def update_versions(prefix, new_version)
  current_version = cache.fetch("#{prefix}}:current_version")
  cache.set_value("#{prefix}:old_version", current_version) if current_version.present?
  cache.set_value("#{prefix}:current_version", new_version)
  old_version = cache.fetch("#{prefix}}:old_version")
  cache.clear_cache!("#{prefix}:#{old_version}") if old_version.present?
end
update_versions(prefix, version)
current_version = cache.fetch("#{prefix}}:current_version")
cache.set_value("#{prefix}:old_version", current_version) if current_version.present?
cache.set_value("#{prefix}:current_version", new_version)
cache.set_value("#{prefix}:current_version", version)
params
params = Hashie::Mash.new({keywords: 'led'})
page = params.delete(:page) || 1
cache = RdsApiCache.new
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
version = params[:version] || cache.fetch("products:#{cache_params.to_param}:current_version")
key = "products:#{cache_params.to_param}:#{version}:#{page}"
cached_body = cache.fetch(key)
p cache.fetch(key)
cached_body = cache.fetch(key)
version_data = {
  version: version,
  total: total,
  per_page: per_page
}
cache.set_value("#{prefix}:current_version", version_data)
info = cache.fetch("products:#{cache_params.to_param}:current_version")
info.class
version
version_info
info
version = version_info
version_info = info
new_version =  version_info[:version]
current_version = cache.fetch("#{prefix}}:current_version")
prefix
cache
reload!
cache.reload
cache = RdsApiCache
cache = RdsApiCache.new
new_version =  version_info[:version]
current_version = cache.fetch("#{prefix}}:current_version")
"#{prefix}}:current_version"
current_version = cache.fetch("#{prefix}:current_version")
cache.set_value("#{prefix}:current_version", new_version)
cache.set_value("#{prefix}:#{new_version}", version_info)
version
version_info
exit
products = Product.first(10)
products.as_json
Oj.dump(products)
products)
products
products.as_json
json = {"a":"Alpha","b":true,"c":12345,"d":[true,[false,[-123456789,null],3.9676,["Something else.",false],null]],"e":{"zero":null,"one":1,"two":2,"three":[3],"four":[0,1,2,3,4]},"f":null,"g":{"json_class":"One::Two::Three::Empty"},"h":{"a":{"b":{"c":{"d":{"e":{"f":{"g":null}}}}}}},"i":[[[[[[[null]]]]]]]}
json = '{"a":"Alpha","b":true,"c":12345,"d":[true,[false,[-123456789,null],3.9676,["Something else.",false],null]],"e":{"zero":null,"one":1,"two":2,"three":[3],"four":[0,1,2,3,4]},"f":null,"g":{"json_class":"One::Two::Three::Empty"},"h":{"a":{"b":{"c":{"d":{"e":{"f":{"g":null}}}}}}},"i":[[[[[[[null]]]]]]]}'
Oj.load(json)
JSON.parse(json)
_.as_json
_.to_s
products.to_s
products.as_json
p products.as_json
products.to_json
versino
version
key = "products:#{cache_params.to_param}:#{version}:#{page}"
cached_body = cache.fetch(key)
products = _
products.first
p = _
ProductSerializer.new(p).as_json
model
"#{model}Serializer".constantize.new(p) }.as_json
p
p = Product.first
"#{model}Serializer".constantize.new(p) }.as_json
cache.keys
products = Product.first(10)
{products: records.map {|r| "#{model}Serializer".constantize.new(r)['product'] }.as_json }
p = Product.first
r = _
"#{model}Serializer".constantize.new(r)
r.as_json
"#{model}Serializer".constantize.new(r)
"#{model}Serializer".constantize.new(r).as_json
Oj.dump("#{model}Serializer".constantize.new(r))
records = products
{products: records.map {|r| "#{model}Serializer".constantize.new(r).as_json['product'] } }.as_json
cache.clear_cache!
redis = Redis.new
redis.flushall
cache_params
version
version_info = cache.fetch("products:#{cache_params.to_param}:#{version}")
cache
version
version = '20160720162651'
version_info = cache.fetch("products:#{cache_params.to_param}:#{version}")
version_info.class
"string"
key
connection = redis.new
connection = Redis.new
options = {}
val = connection.get full_key(key, options)
key
version
key = "products:#{cache_params.to_param}:#{version}:#{page}"
val = connection.get full_key(key, options)
val
recoexit
exit
model = Product
Product.connect
Product.connection
model = Product
p = Product.last
r = p
"#{model}Serializer".constantize.new(r).as_json['product'] } }
"#{model}Serializer".constantize.new(r).as_json['product'] } 
"#{model}Serializer".constantize.new(r).as_json['product']  
{products: ["#{model}Serializer".constantize.new(r).as_json['product'] }]
{products: ["#{model}Serializer".constantize.new(r).as_json['product'] }]}
records = Product.first 2
{products: records.map {|r| "#{model}Serializer".constantize.new(r).as_json['product'] } }
obj = _
obj.class
{products: records.map {|r| "#{model}Serializer".constantize.new(r).as_json['product'] } }.merge({pagination: {version: 1, page: 1, per_page: 10, total: 2 })
{products: records.map {|r| "#{model}Serializer".constantize.new(r).as_json['product'] } }.merge({pagination: {version: 1, page: 1, per_page: 10, total: 2 } }
{products: records.map {|r| "#{model}Serializer".constantize.new(r).as_json['product'] } }.merge({pagination: {version: 1, page: 1, per_page: 10, total: 2 } } )
redis.flushall
redis = Redis.new
redis.flushall
pids
params = Hashie::Mash({keywords: 'hdmi'})
params = Hashie::Mash.new({keywords: 'hdmi'})
type = params.delete(:type)
type = 'products'
model = type.classify.constantize
per_page = 200
page = params[:page] || 1
search_response = model.search_by(*prepare_search_params(model, params), { size: per_page, scroll: '3m' })
def prepare_search_params(model, params)
  operator = 'AND'
  terms = { keywords: params[:keywords].to_s }
  filters = {}
  model.query_fields[:filter_fields].keys.each do |key|
    filters[key] = params[key].downcase.split(',').map(&:strip) if params[key].present?
  end
  if params[:using] == 'any'
    keywords = [params[:keywords].to_s, params[:brand], params[:model]].join(' ')
    filters.delete_if { |k, _v| [:brand, :model].include?(k) }
    terms = { keywords: keywords }
    operator = 'OR'
  end
  return terms, operator, filters
end
def update_versions(prefix, new_version)
  current_version = cache.fetch("#{prefix}:current_version")
  cache.set_value("#{prefix}:old_version", current_version) if current_version.present?
  cache.set_value("#{prefix}:current_version", new_version)
  old_version = cache.fetch("#{prefix}}:old_version")
  cache.clear_cache!("#{prefix}:#{old_version}") if old_version.present?
end
search_response = model.search_by(*prepare_search_params(model, params), { size: per_page, scroll: '3m' })
pids = search_response.results.map { |p| p.id }.compact
total = search_response.results.total
scroll_id = search_response.response._scroll_id
records = model.include_api_info.where(id: pids)
records.as_json
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
version = params[:version] || Time.now.strftime("%Y%m%d%H%M%S")
prefix = "#{type}:#{cache_params.to_param}"
key = "#{prefix}:#{version}:#{page}"
meta_data = { pagination: { page: page, per_page: per_page, total: total }, version: version }
body = { products: records } 
body = { products: records.map { |r| "#{model}Serializer".constantize.new(r) } }.as_json
cache = RdsApiCache
cache = RdsApiCache.new
cache.set_value(key, body.merge(meta_data))
exit
params = Hashie::Mash({keywords: 'hdmi', type: 'products'})
params = Hashie::Mash.new({keywords: 'hdmi', type: 'products'})
type = params.delete(:type)
model = type.classify.constantize
model
model.call
Product.connection
model
per_page = 200
page = params[:page] || 1
search_response = model.search_by(*prepare_search_params(model, params), { size: per_page, scroll: '3m' })
pids = search_response.results.map { |p| p.id }.compact
total = search_response.results.total
scroll_id = search_response.response._scroll_id
search_response = model.search_by(*prepare_search_params(model, params), { size: per_page, scroll: '3m' })
def prepare_search_params(model, params)
  operator = 'AND'
  terms = { keywords: params[:keywords].to_s }
  filters = {}
  model.query_fields[:filter_fields].keys.each do |key|
    filters[key] = params[key].downcase.split(',').map(&:strip) if params[key].present?
  end
  if params[:using] == 'any'
    keywords = [params[:keywords].to_s, params[:brand], params[:model]].join(' ')
    filters.delete_if { |k, _v| [:brand, :model].include?(k) }
    terms = { keywords: keywords }
    operator = 'OR'
  end
  return terms, operator, filters
end
def update_versions(prefix, new_version)
  current_version = cache.fetch("#{prefix}:current_version")
  cache.set_value("#{prefix}:old_version", current_version) if current_version.present?
  cache.set_value("#{prefix}:current_version", new_version)
  old_version = cache.fetch("#{prefix}}:old_version")
  cache.clear_cache!("#{prefix}:#{old_version}") if old_version.present?
end
search_response = model.search_by(*prepare_search_params(model, params), { size: per_page, scroll: '3m' })
pids = search_response.results.map { |p| p.id }.compact
total = search_response.results.total
scroll_id = search_response.response._scroll_id
records = model.include_api_info.where(id: pids)
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
version = params[:version] || Time.now.strftime("%Y%m%d%H%M%S")
prefix = "#{type}:#{cache_params.to_param}"
key = "#{prefix}:#{version}:#{page}"
meta_data = { pagination: { page: page, per_page: per_page, total: total }, version: version }
body = { products: records.map { |r| "#{model}Serializer".constantize.new(r).as_json['product'] } }
params
params[:type] = 'listings'
type = params.delete(:type)
model = type.classify.constantize
per_page = 200
page = params[:page] || 1
scroll_id = params[:scroll_id]
search_response = model.search_by(*prepare_search_params(model, params), { size: per_page, scroll: '3m' })
pids = search_response.results.map { |p| p.id }.compact
total = search_response.results.total
scroll_id = search_response.response._scroll_id
records = model.include_api_info.where(id: pids)
exit
params = Hashie::Mash.new({keywords: 'hdmi', type: 'listings'})
type = params.delete(:type)
model = type.classify.constantize
Listing.connection
model
per_page = 200
page = params[:page] || 1
scroll_id = params[:scroll_id]
search_response = model.search_by(*prepare_search_params(model, params), { size: per_page, scroll: '3m' })
pids = search_response.results.map { |p| p.id }.compact
def prepare_search_params(model, params)
  operator = 'AND'
  terms = { keywords: params[:keywords].to_s }
  filters = {}
  model.query_fields[:filter_fields].keys.each do |key|
    filters[key] = params[key].downcase.split(',').map(&:strip) if params[key].present?
  end
  if params[:using] == 'any'
    keywords = [params[:keywords].to_s, params[:brand], params[:model]].join(' ')
    filters.delete_if { |k, _v| [:brand, :model].include?(k) }
    terms = { keywords: keywords }
    operator = 'OR'
  end
  return terms, operator, filters
end
def update_versions(prefix, new_version)
  current_version = cache.fetch("#{prefix}:current_version")
  cache.set_value("#{prefix}:old_version", current_version) if current_version.present?
  cache.set_value("#{prefix}:current_version", new_version)
  old_version = cache.fetch("#{prefix}}:old_version")
  cache.clear_cache!("#{prefix}:#{old_version}") if old_version.present?
end
search_response = model.search_by(*prepare_search_params(model, params), { size: per_page, scroll: '3m' })
pids = search_response.results.map { |p| p.id }.compact
total = search_response.results.total
scroll_id = search_response.response._scroll_id
records = model.include_api_info.where(id: pids)
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
version = params[:version] || Time.now.strftime("%Y%m%d%H%M%S")
prefix = "#{type}:#{cache_params.to_param}"
key = "#{prefix}:#{version}:#{page}"
meta_data = { pagination: { page: page, per_page: per_page, total: total }, version: version }
body = { products: records.map { |r| "#{model}Serializer".constantize.new(r).as_json['product'] } }
body = { products: records.map { |r| "#{model}Serializer".constantize.new(r) } }.as_json
type
type.singularize
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
l = Listing.first
l = Listing.include_api_info.first
l.sellers
{:i* Closing connection 0
l.sellers
object = l
object.sellers
min, max = object.scrape_listing_sellers.to_a.select{|sls| sls.seller_id == seller.id}.pluck(:exchanged_min, :exchanged_max).flatten.compact.minmax
seller = object.sellers.first
min, max = object.scrape_listing_sellers.to_a.select{|sls| sls.seller_id == seller.id}.pluck(:exchanged_min, :exchanged_max).flatten.compact.minmax
object.scrape_listing_sellers
sls = object.scrape_listing_sellers.first
sls.seller
object.scrape_listing_sellers
object.scrape_listing_sellers.flat_map {|sls| [sls.exchanged_min, sls.exchanged_max]}
min, max = object.scrape_listing_sellers.to_a.select{|sls| sls.seller_id == seller.id}.flat_map {|sls| [sls.exchanged_min, sls.exchanged_max]}.compact.minmax
exit
params = Hashie::Mash.new({keywords: 'hdmi', type: 'listings'})
cache = RdsApiCache
cache = RdsApiCache.new
def prepare_search_params(model, params)
  operator = 'AND'
  terms = { keywords: params[:keywords].to_s }
  filters = {}
  model.query_fields[:filter_fields].keys.each do |key|
    filters[key] = params[key].downcase.split(',').map(&:strip) if params[key].present?
  end
  if params[:using] == 'any'
    keywords = [params[:keywords].to_s, params[:brand], params[:model]].join(' ')
    filters.delete_if { |k, _v| [:brand, :model].include?(k) }
    terms = { keywords: keywords }
    operator = 'OR'
  end
  return terms, operator, filters
end
type = params.delete(:type)
model = type.classify.constantize
Listing.connection
per_page = 200
page = params[:page] || 1
scroll_id = params[:scroll_id]
search_response = model.search_by(*prepare_search_params(model, params), { size: per_page, scroll: '3m' })
pids = search_response.results.map { |p| p.id }.compact
total = search_response.results.total
scroll_id = search_response.response._scroll_id
records = model.include_api_info.where(id: pids)
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
version = params[:version] || Time.now.strftime("%Y%m%d%H%M%S")
prefix = "#{type}:#{cache_params.to_param}"
key = "#{prefix}:#{version}:#{page}"
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
Listing.reload
Listing.reload!
reload
reload!
exit
params = Hashie::Mash.new({keywords: 'hdmi', type: 'listings'})
def prepare_search_params(model, params)
  operator = 'AND'
  terms = { keywords: params[:keywords].to_s }
  filters = {}
  model.query_fields[:filter_fields].keys.each do |key|
    filters[key] = params[key].downcase.split(',').map(&:strip) if params[key].present?
  end
  if params[:using] == 'any'
    keywords = [params[:keywords].to_s, params[:brand], params[:model]].join(' ')
    filters.delete_if { |k, _v| [:brand, :model].include?(k) }
    terms = { keywords: keywords }
    operator = 'OR'
  end
  return terms, operator, filters
end
type = params.delete(:type)
model = type.classify.constantize
Listing.connection
per_page = 200
page = params[:page] || 1
search_response = model.search_by(*prepare_search_params(model, params), { size: per_page, scroll: '3m' })
pids = search_response.results.map { |p| p.id }.compact
records = model.include_api_info.where(id: pids)
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
version = params[:version] || Time.now.strftime("%Y%m%d%H%M%S")
prefix = "#{type}:#{cache_params.to_param}"
key = "#{prefix}:#{version}:#{page}"
meta_data = { pagination: { page: page, per_page: per_page, total: total }, version: version }
total = search_response.results.total
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
version = params[:version] || Time.now.strftime("%Y%m%d%H%M%S")
prefix = "#{type}:#{cache_params.to_param}"
key = "#{prefix}:#{version}:#{page}"
meta_data = { pagination: { page: page, per_page: per_page, total: total }, version: version }
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
records = model.include_api_info.where(id: pids)
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
version = params[:version] || Time.now.strftime("%Y%m%d%H%M%S")
prefix = "#{type}:#{cache_params.to_param}"
key = "#{prefix}:#{version}:#{page}"
meta_data = { pagination: { page: page, per_page: per_page, total: total }, version: version }
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
exit
params = Hashie::Mash.new {keywords: 'hdmi', type: 'listings'}
params = Hashie::Mash.new({keywords: 'hdmi', type: 'listings'})
type = params.delete(:type)
model = type.classify.constantize
per_page = 200
page = params[:page] || 1
Listing.connection
scroll_id = params[:scroll_id]
search_response = model.search_by(*prepare_search_params(model, params), { size: per_page, scroll: '3m' })
j  def prepare_search_params(model, params)
  operator = 'AND'
  terms = { keywords: params[:keywords].to_s }
  filters = {}
  model.query_fields[:filter_fields].keys.each do |key|
    filters[key] = params[key].downcase.split(',').map(&:strip) if params[key].present?
  end
  if params[:using] == 'any'
    keywords = [params[:keywords].to_s, params[:brand], params[:model]].join(' ')
    filters.delete_if { |k, _v| [:brand, :model].include?(k) }
    terms = { keywords: keywords }
    operator = 'OR'
  end
  return terms, operator, filters
end
def prepare_search_params(model, params)
  operator = 'AND'
  terms = { keywords: params[:keywords].to_s }
  filters = {}
  model.query_fields[:filter_fields].keys.each do |key|
    filters[key] = params[key].downcase.split(',').map(&:strip) if params[key].present?
  end
  if params[:using] == 'any'
    keywords = [params[:keywords].to_s, params[:brand], params[:model]].join(' ')
    filters.delete_if { |k, _v| [:brand, :model].include?(k) }
    terms = { keywords: keywords }
    operator = 'OR'
  end
  return terms, operator, filters
end
search_response = model.search_by(*prepare_search_params(model, params), { size: per_page, scroll: '3m' })
pids = search_response.results.map { |p| p.id }.compact
total = search_response.results.total
scroll_id = search_response.response._scroll_id
records = model.include_api_info.where(id: pids)
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
version = params[:version] || Time.now.strftime("%Y%m%d%H%M%S")
prefix = "#{type}:#{cache_params.to_param}"
key = "#{prefix}:#{version}:#{page}"
meta_data = { pagination: { page: page, per_page: per_page, total: total }, version: version }
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
listing = Listing.include_api_info.first
listing.products
l = Listing.includes(:products).first
l.products
l = Listing.include_api_info.first
l.products
l.categories
exit
l = Listing.includes(:products).first
l.products
l = Listing.include_api_info.first
l.products
l = Listing.include_api_info.first
l.categories
exit
l = Listing.include_api_info.first
l.products
exit
l = Listing.include_api_info.first
l.products
exit
l = Listing.include_api_info.first
l.products
l = Listing.include_api_info.first
l = Listing.include_api_info.to_sql
exit
l = Listing.include_api_info.first
l.products
exit
l = Listing.include_api_info.first
l.products
exit
l = Listing.include_api_info.first
l.products
lids = Listing.limit(100).pluck(:id)
lids
listings = Listing.include_api_info.where(id: lids)
listings = Listing.include_api_info.where(id: lids).to_sql
listings = Listing.where(id: lids).include_api_info.to_sql
listings = Listing.where(id: lids).include_api_info
listings.count
Listing.connection
listings.count
Listing.count
exit
lids = Listing.limit(10).pluck(:id)
listings = Listing.where(id: lids).include_api_info
exit
lids = Listing.limit(10).pluck(:id)
listings = Listing.where(id: lids).include_api_info
listings = Listing.where(id: lids).include_api_info.to_sql
exit
lids = Listing.limit(10).pluck(:id)
listings = Listing.where(id: lids).include_api_info.to_sql
listings = Listing.where(id: lids).include_api_info
l  = listings.first
l.categories
l.products
exit
lids = Listing.limit(10).pluck(:id)
listings = Listing.where(id: lids).include_api_info
listings.count
l = listings.first
exit
lids = Listing.limit(10).pluck(:id)
listings = Listing.where(id: lids).include_api_info
l = listings.first
l.products
exit
lids = Listing.limit(10).pluck(:id)
listings = Listing.where(id: lids).include_api_info
l = listings.first
l.products
l.id
Listing.include_api_info.limit(200)
lids = Listing.limit(10).pluck(:id)
listings = Listing.where(id: lids).include_api_info
listings.each {|l| l.products }
listings.each {|l| puts l.products }
listings.each {|l| puts l.products.as_json }; return nil
listings.each {|l| puts l.products.as_json }
listings[0].products
listings[1].products
listings[3].products
listings = Listing.where(id: lids).include_api_info
listings[3].products
listings[3].categories
listings[3].listing_url
exit
lids = Listing.limit(10).pluck(:id)
listings = Listing.where(id: lids).include_api_info
listings[3].listing_url
listings[3].products
params = Hashie::Mash.new({keywords: 'hdmi', type: 'listings'})
type = params.delete(:type)
model = type.classify.constantize
per_page = 200
page = params[:page] || 1
search_response = model.search_by(*prepare_search_params(model, params), { size: per_page, scroll: '3m' })
def prepare_search_params(model, params)
  operator = 'AND'
  terms = { keywords: params[:keywords].to_s }
  filters = {}
  model.query_fields[:filter_fields].keys.each do |key|
    filters[key] = params[key].downcase.split(',').map(&:strip) if params[key].present?
  end
  if params[:using] == 'any'
    keywords = [params[:keywords].to_s, params[:brand], params[:model]].join(' ')
    filters.delete_if { |k, _v| [:brand, :model].include?(k) }
    terms = { keywords: keywords }
    operator = 'OR'
  end
  return terms, operator, filters
end
search_response = model.search_by(*prepare_search_params(model, params), { size: per_page, scroll: '3m' })
pids = search_response.results.map { |p| p.id }.compact
total = search_response.results.total
scroll_id = search_response.response._scroll_id
records = model.include_api_info.where(id: pids)
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
version = params[:version] || Time.now.strftime("%Y%m%d%H%M%S")
prefix = "#{type}:#{cache_params.to_param}"
key = "#{prefix}:#{version}:#{page}"
meta_data = { pagination: { page: page, per_page: per_page, total: total }, version: version }
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
cache.set_value(key, body.merge(meta_data))
cache = RdsApiCache.new
cache.set_value(key, body.merge(meta_data))
exit
params = Hashie::Mash.new({keywords: 'hdmi', type: 'listings'})
params = Hashie::Mash.new({keywords: 'hdmi', type: 'products'})
type = params.delete(:type)
model = type.classify.constantize
Product.connectino
Product.connection
model = type.classify.constantize
per_page = 200
page = params[:page] || 1
search_response = model.search_by(*prepare_search_params(model, params), { size: per_page, scroll: '3m' })
def prepare_search_params(model, params)
  operator = 'AND'
  terms = { keywords: params[:keywords].to_s }
  filters = {}
  model.query_fields[:filter_fields].keys.each do |key|
    filters[key] = params[key].downcase.split(',').map(&:strip) if params[key].present?
  end
  if params[:using] == 'any'
    keywords = [params[:keywords].to_s, params[:brand], params[:model]].join(' ')
    filters.delete_if { |k, _v| [:brand, :model].include?(k) }
    terms = { keywords: keywords }
    operator = 'OR'
  end
  return terms, operator, filters
end
search_response = model.search_by(*prepare_search_params(model, params), { size: per_page, scroll: '3m' })
pids = search_response.results.map { |p| p.id }.compact
total = search_response.results.total
scroll_id = search_response.response._scroll_id
records = model.include_api_info.where(id: pids)
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
version = params[:version] || Time.now.strftime("%Y%m%d%H%M%S")
prefix = "#{type}:#{cache_params.to_param}"
key = "#{prefix}:#{version}:#{page}"
meta_data = { pagination: { page: page, per_page: per_page, total: total }, version: version }
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
exit
params = Hashie::Mash.new({keywords: 'hdmi', type: 'products'})
type = params.delete(:type)
model = type.classify.constantize
Product.connection
per_page = 200
page = params[:page] || 1
def prepare_search_params(model, params)
  operator = 'AND'
  terms = { keywords: params[:keywords].to_s }
  filters = {}
  model.query_fields[:filter_fields].keys.each do |key|
    filters[key] = params[key].downcase.split(',').map(&:strip) if params[key].present?
  end
  if params[:using] == 'any'
    keywords = [params[:keywords].to_s, params[:brand], params[:model]].join(' ')
    filters.delete_if { |k, _v| [:brand, :model].include?(k) }
    terms = { keywords: keywords }
    operator = 'OR'
  end
  return terms, operator, filters
end
def update_versions(prefix, new_version)
  current_version = cache.fetch("#{prefix}:current_version")
  cache.set_value("#{prefix}:old_version", current_version) if current_version.present?
  cache.set_value("#{prefix}:current_version", new_version)
  old_version = cache.fetch("#{prefix}}:old_version")
  cache.clear_cache!("#{prefix}:#{old_version}") if old_version.present?
end
search_response = model.search_by(*prepare_search_params(model, params), { size: per_page, scroll: '3m' })
pids = search_response.results.map { |p| p.id }.compact
total = search_response.results.total
scroll_id = search_response.response._scroll_id
records = model.include_api_info.where(id: pids)
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
version = params[:version] || Time.now.strftime("%Y%m%d%H%M%S")
prefix = "#{type}:#{cache_params.to_param}"
key = "#{prefix}:#{version}:#{page}"
meta_data = { pagination: { page: page, per_page: per_page, total: total }, version: version }
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
records = model.include_api_info.where(id: pids)
exit
params = Hashie::Mash.new({keywords: 'hdmi', type: 'products'})
type = params.delete(:type)
model = type.classify.constantize
per_page = 200
page = params[:page] || 1
Product.connection
scroll_id = params[:scroll_id]
def prepare_search_params(model, params)
  operator = 'AND'
  terms = { keywords: params[:keywords].to_s }
  filters = {}
  model.query_fields[:filter_fields].keys.each do |key|
    filters[key] = params[key].downcase.split(',').map(&:strip) if params[key].present?
  end
  if params[:using] == 'any'
    keywords = [params[:keywords].to_s, params[:brand], params[:model]].join(' ')
    filters.delete_if { |k, _v| [:brand, :model].include?(k) }
    terms = { keywords: keywords }
    operator = 'OR'
  end
  return terms, operator, filters
end
search_response = model.search_by(*prepare_search_params(model, params), { size: per_page, scroll: '3m' })
pids = search_response.results.map { |p| p.id }.compact
total = search_response.results.total
scroll_id = search_response.response._scroll_id
records = model.include_api_info.where(id: pids)
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
version = params[:version] || Time.now.strftime("%Y%m%d%H%M%S")
prefix = "#{type}:#{cache_params.to_param}"
key = "#{prefix}:#{version}:#{page}"
meta_data = { pagination: { page: page, per_page: per_page, total: total }, version: version }
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
exit
edit -t
params = Hashie::Mash.new({keywords: 'hdmi', type: 'products'})
model = type.classify.constantize
type = params.delete(:type)
model = type.classify.constantize
Product.connection
model = type.classify.constantize
per_page = 200
page = params[:page] || 1
scroll_id = params[:scroll_id]
search_response = model.search_by(*prepare_search_params(model, params), { size: per_page, scroll: '3m' })
pids = search_response.results.map { |p| p.id }.compact
total = search_response.results.total
scroll_id = search_response.response._scroll_id
records = model.include_api_info.where(id: pids)
records.size
records.last
exit
edit -t
params = Hashie::Mash.new({keywords: 'hdmi', type: 'products'})
type = params.delete(:type)
model = type.classify.constantize
Product.connection
per_page = 200
page = params[:page] || 1
scroll_id = params[:scroll_id]
search_response = model.search_by(*prepare_search_params(model, params), { size: per_page, scroll: '3m' })
pids = search_response.results.map { |p| p.id }.compact
total = search_response.results.total
scroll_id = search_response.response._scroll_id
records = model.include_api_info.where(id: pids)
records.last
exit
edit -t
params = Hashie::Mash.new({keywords: 'hdmi', type: 'products'})
type = params.delete(:type)
model = type.classify.constantize
Product.connection
per_page = 200
page = params[:page] || 1
scroll_id = params[:scroll_id]
search_response = model.search_by(*prepare_search_params(model, params), { size: per_page, scroll: '3m' })
pids = search_response.results.map { |p| p.id }.compact
total = search_response.results.total
scroll_id = search_response.response._scroll_id
records = model.include_api_info.where(id: pids)
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
version = params[:version] || Time.now.strftime("%Y%m%d%H%M%S")
prefix = "#{type}:#{cache_params.to_param}"
key = "#{prefix}:#{version}:#{page}"
meta_data = { pagination: { page: page, per_page: per_page, total: total }, version: version }
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
exit
params = Hashie::Mash.new({keywords: 'hdmi', type: 'products'})
edit -t
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
exit
edit -t
params = Hashie::Mash.new({keywords: 'hdmi', type: 'products'})
edit -t
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
products = _
p = products['products'].first
p['sellers']
p
p[:sellers]
seller = {:id=>1442, :name=>"Anyagreenenergy", :image_url=>"NO SELLER IMAGE", :min_price_cents=>#<BigDecimal:7fdf192d6978,'0.1764E4',9(36)>}
seller = p[:sellers].sample
p[:sellers] << seller
p[:sellers]
p[:sellers].uniq
exit
edit -t
params = Hashie::Mash.new({keywords: 'hdmi', type: 'products'})
edit -t
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
l = Listing.include_api_info.first
l.products
exit
edit -t
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
exit
edit -t
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
edit -t
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
exit
edit -t
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
records = model.where(id: pids)
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
version = params[:version] || Time.now.strftime("%Y%m%d%H%M%S")
prefix = "#{type}:#{cache_params.to_param}"
key = "#{prefix}:#{version}:#{page}"
meta_data = { pagination: { page: page, per_page: per_page, total: total }, version: version }
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
records = model.includes(:listings).where(id: pids)
edit -t
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
load '/Users/jonathan/rvx-rds/app/serializers/product_serializer.rb'
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
records = model.includes({listings: :scrape}).where(id: pids)
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
records = model.includes(:listings, :scrapes).where(id: pids)
type
type.to_sym
records = model.includes(:listings, :scrapes).where(id: pids).references(type.to_sym)
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
load '/Users/jonathan/rvx-rds/app/serializers/product_serializer.rb'
records = model.includes(:listings, :scrapes).where(id: pids).references(type.to_sym)
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
records = model.includes(:listings, :scrapes, :images).where(id: pids).references(type.to_sym)
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
records = model.includes({ listings: :images }, :scrapes).where(id: pids).references(type.to_sym)
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
records = model.includes({ listings: :images }, :images, :scrapes).where(id: pids).references(type.to_sym)
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
load '/Users/jonathan/rvx-rds/app/serializers/product_serializer.rb'
records = model.includes({ listings: [:images, :screenshots] }, :images, :screenshots, :scrapes).where(id: pids).references(type.to_sym)
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
records = model.includes({ listings: [:images] }, :images, :screenshots, :scrapes).where(id: pids).references(type.to_sym)
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
load '/Users/jonathan/rvx-rds/app/serializers/product_serializer.rb'
records = model.includes({ listings: [:images] }, :images, :screenshots, :scrapes).where(id: pids).references(type.to_sym)
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
records = model.includes({ listings: [:images] }, :images, :screenshots, {scrapes: [:sources]}).where(id: pids).references(type.to_sym)
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
records = model.includes({ listings: [:images] }, :images, :screenshots, :scrapes, :sources).where(id: pids).references(type.to_sym)
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
records = model.includes({ listings: [:images] }, :images, :screenshots, {scrapes: :source}, :sources).where(id: pids).references(type.to_sym)
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
records = model.includes({ listings: [:images, {scrape: :source}] }, :images, :screenshots, {scrapes: :source}, :sources).where(id: pids).references(type.to_sym)
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
edit -t
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
records = model.includes({ listings: [:images, { scrape: :source }] }, :images, :screenshots, { scrapes: :source }, :sources).preload(:listings, :sources).where(id: pids).references(type.to_sym)
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
records = model.includes({ listings: [:images, { scrape: :source }] }, :images, :screenshots, { scrapes: :source }, :sources).preload({listings: :source}).where(id: pids).references(type.to_sym)
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
load '/Users/jonathan/rvx-rds/app/serializers/product_serializer.rb'
edit -t
records = model.includes({ listings: [:images, { scrape: :source }] }, :images, :screenshots, { scrapes: :source }, :sources).preload({listings: :source}).where(id: pids).references(type.to_sym)
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
xtei
exit
edit -t
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
records = model.includes({ listings: [:images, { scrape: :source }] }, :sellers, :images, :screenshots, { scrapes: :source }, :sources).preload({listings: [:images, { scrape: :source }]}, {sellers: :images}).where(id: pids).references(type.to_sym)
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
records = model.includes({ listings: [:images, { scrape: :source }] }, :sellers, :images, :screenshots, { scrapes: :source }, :sources).preload({listings: [:images, { scrape: :source }]}, :sources, {sellers: :images}).where(id: pids).references(type.to_sym)
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
records = model.includes({ listings: [:images, { scrape: :source }] }, {sellers: :images}, :images, :screenshots, { scrapes: :source }, :sources).preload({ listings: [:images, { scrape: :source }] }, {sellers: :images}, :images, :screenshots, { scrapes: :source }, :sources).where(id: pids).references(type.to_sym)
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
load '/Users/jonathan/rvx-rds/app/serializers/product_serializer.rb'
records = model.includes({ listings: [:images, { scrape: :source }] }, {sellers: :images}, :images, :screenshots, { scrapes: :source }, :sources).preload({ listings: [:images, { scrape: :source }] }, {sellers: :images}, :images, :screenshots, { scrapes: :source }, :sources).where(id: pids).references(type.to_sym)
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
load '/Users/jonathan/rvx-rds/app/serializers/product_serializer.rb'
records = model.includes({ listings: [:images, { scrape: :source }] }, {sellers: :images}, :images, :screenshots, { scrapes: :source }, :sources).preload({ listings: [:images, { scrape: :source }] }, {sellers: :images}, :images, :screenshots, { scrapes: :source }, :sources).where(id: pids).references(type.to_sym)
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
load '/Users/jonathan/rvx-rds/app/serializers/product_serializer.rb'
edit -t
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
xG
Gexit
exit
edit -t
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
load '/Users/jonathan/rvx-rds/app/serializers/product_serializer.rb'
edi t-t
edit -t
load '/Users/jonathan/rvx-rds/app/serializers/product_serializer.rb'
edit -t
load '/Users/jonathan/rvx-rds/app/serializers/product_serializer.rb'
edit -t
records = model.includes({ listings: [:images, { scrape: :source }, :source] }, {sellers: :images}, :images, :screenshots, { scrapes: :source }, :sources).where(id: pids).references(type.to_sym)
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
records = model.includes({ listings: [:images, :scrape, :source] }, {sellers: :images}, :images, :screenshots, { scrapes: :source }, :sources).where(id: pids).references(type.to_sym)
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
records = model.includes({ listings: [:scrape, :source] }, {sellers: :images}, :images, :screenshots, { scrapes: :source }, :sources).where(id: pids).references(type.to_sym)
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
records = model.includes({ listings: [:scrape, :source] }, {sellers: :images}, :images, :screenshots, :scrapes, :sources).where(id: pids).references(type.to_sym)
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
records = model.includes({ listings: [:source] }, {sellers: :images}, :images, :screenshots, :scrapes, :sources).where(id: pids).references(type.to_sym)
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
exit
edit -t
params = Hashie::Mash.new({ keywords: 'hdmi', type: 'listings' })
edit -t
Listing.include_api_info.to_sql
Listing.include_api_info.first(10)
load '/Users/jonathan/rvx-rds/app/models/listing.rb'
Listing.include_api_info.first(10)
exit
edit -t
redis = Redis.new
redis.flushall
cache = RdsApiCache.new
params = Hashie::Mash.new({"keywords"=>"led"})
page = params.delete(:page) || 1
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
version = params[:version] || cache.fetch("products:#{cache_params.to_param}:current_version")
key = "products:#{cache_params.to_param}:#{version}:#{page}"
cached_body = cache.fetch(key)
cached_body
exit
cache = RdsApiCache.new
params = Hashie::Mash.new({"keywords"=>"led"})
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
page = params.delete(:page) || 1
version = params[:version] || cache.fetch("products:#{cache_params.to_param}:current_version")
key = "products:#{cache_params.to_param}:#{version}:#{page}"
cached_body = cache.fetch(key)
str = '{"products"=>[{:id=>1, :name=>"Goldi Green Goldi 012PM Solar Pan"}]}'
Oj.load(str)
redis = Redis.new
edit -t
val = redis.get full_key(key)
str = _
str[11]
str[12]
str.class
Oj.load(str)
Oj.dump(str)
Oj.load(_)
redis
redis.flushall
edit -t
exit
edit -t
esit
exit
val = nil
Oj.dump(val)
cache = RdsApiCache.new
cache.fetch("#{type}:#{cache_params.to_param}:in_progress")
type = 'products'
cache_params = {'keywords' => 'hdmi' }
cache.fetch("#{type}:#{cache_params.to_param}:in_progress")
redis = Redis.new
redis.flushall
{"keywords"=>"led"}
h = _
h.class
h.to_param
key
params = Hashie::Mash.new({'keywords': 'led'})
type = 'listings'
page = params.delete(:page) || 1
cache = RdsApiCache.new
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
version = params[:version] || cache.fetch("#{type}:#{cache_params.to_param}:current_version")
key = "#{type}:#{cache_params.to_param}:#{version}:#{page}"
cached_body = cache.fetch(key)
edit -t
val = connection.get full_key(key, options)
val = connection.get full_key(key)
val = redis.get full_key(key)
val[10093]
val[10090..94]
val[10090]
val[10091]
val[10092]
val[10094]
val[10095]
val[10093]
val
redis
redis.flushall
type
params
type = params.delete(:type)
type
type = 'listings'
model = type.classify.constantize
Listing.connection
per_page = 200
page = params[:page] || 1
search_response = model.search_by(*prepare_search_params(model, params), { size: per_page, scroll: '3m' })
edit -t
records = model.include_api_info.where(id: pids)
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
version = params[:version] || Time.now.strftime("%Y%m%d%H%M%S")
prefix = "#{type}:#{cache_params.to_param}"
key = "#{prefix}:#{version}:#{page}"
meta_data = { pagination: { page: page, per_page: per_page, total: total }, version: version }
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json[type.singularize] } }
JSON.generate(body)
body.class
body.is_a? Number
1
123.is_a?(Number)
123.is_a?(Integer)
params.calss
params.class
params.is_a?(Hash)
redis
redis.flushall
exit
key
edit -t
key = "#{type}:#{cache_params.to_param}:#{version}:#{page}"
body = redis
def prefix
  "#{Rails.env}_#{ENV['REDIS_NAMESPACE']}:api_cache"
end
def full_key(key, options={})
  "#{prefix}:#{key}"
end
redis.get full_key(key)
thing = _
thing.class
JSON.parse(thing)
thing
exit
edit -t
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r) } }
Oj.dump(body)
JSON.generate(body)
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json(root: false) } }
body.class
puts body
body.class
body.to_json.class
body.to_json
puts body.to_json
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json(root: false)} }
redis
redis = Redis.new
redis.flushall
meta_data
val = body.merge(meta_data).to_json
JSON.parse(val)
val
val.class
JSON.generate(val)
key
edit -t
val = redis.get full_key(key, options)
val = redis.get full_key(key)
redis.flushall
q        type = 'products'
type = 'products'
page = params.delete(:page) || 1
cache = RdsApiCache.new
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
version = params[:version] || cache.fetch("#{type}:#{cache_params.to_param}:current_version")
params = {'keywords' => 'led' }
version = params[:version] || cache.fetch("#{type}:#{cache_params.to_param}:current_version")
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
params = {:keywords => 'led' }
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
version = params[:version] || cache.fetch("#{type}:#{cache_params.to_param}:current_version")
key = "#{type}:#{cache_params.to_param}:current_version"
redis
val = redis.get full_key(key)
Oj.dump('hello')
val = redis.get full_key(key)
JSON.parse(    val = JSON.generate(val) if val.is_a?(Hash)
JSON.parse(val)
redis.flushall
edit -t
version = params[:version] || cache.fetch("#{type}:#{cache_params.to_param}:current_version").try(:[], :version)
"#{type}:#{cache_params.to_param}:current_version"
version = params[:version] || cache.fetch("#{type}:#{cache_params.to_param}:current_version").try(:[], 'version')
params
params.class
type
model = type.classify.constantize
per_page = 20
page = params[:page] || 1
search_response = model.search_by(*prepare_search_params(model, params), { size: per_page, scroll: '3m' })
pids = search_response.results.map { |p| p.id }.compact
q      total = search_response.results.total
total = search_response.results.total
scroll_id = search_response.response._scroll_id
records = model.include_api_info.where(id: pids)
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
version = params[:version] || Time.now.strftime("%Y%m%d%H%M%S")
prefix = "#{type}:#{cache_params.to_param}"
key = "#{prefix}:#{version}:#{page}"
meta_data = { pagination: { page: page, per_page: per_page, total: total }, version: version }
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json(root: false)} }
Oj.dump(body) {:mode => :compat }
Oj.dump(body, {:mode => :compat })
Oj.load(_)
redis
redis.flushall
exit
url = 'http://www.dhgate.com/product/23000mah-usb-external-rechargeable-portable/212329682.html#s21-16-1b;searl|795349086'
DhGateListingDetailJob.new.perform(1,1,url)
l = Listing.last
include Sidekiq::Worker
include CapybaraJob
# include ScrapeBatchJob
include ImageScrapeable
include DetailScrapeable
edit -t
@model_key = ['Model', 'Model Number']
@brand_key = ['Brand', 'Brand Name', 'Popular Brand','Chinese Brand']
edit -t
include SellersScrapeable
scrape_id = 1
source_category_id = 1
url
logger.info "Product: #{url}#{retry_time > 0 ? ', retry: ' + retry_time.to_s : ''}"
retry_time = 1
logger.info "Product: #{url}#{retry_time > 0 ? ', retry: ' + retry_time.to_s : ''}"
detail_url = get_detail_url(url)
url_md5 = Digest::MD5.hexdigest(detail_url)
listing_url = ListingUrl.find_or_create_by(url_md5: url_md5, url: detail_url)
listing = Listing.find_by(scrape_id: scrape_id, url: detail_url)
visit_page detail_url
visit_page
detail_url = get_detail_url(url)
visit_page detail_url
show-method visit_page
cd
require 'capybara/poltergeist'
include CapybaraPhantomJs
include CapybaraJob
visit_page detail_url
edit -t
include Capybara::DSL
include RvxRds::Proxy
visit_page detail_url
exit
redis = redis.new
redis = Redis.new
redis.flushall
exit
url = "http://www.dhgate.com/product/10pcs-6-35-stereo-headset-socket-gold-plated/372819964.html#s20-9-1b;searl|3371814035"
DhGateListingDetailJob.new.perform(1,1,url)
exit
url = "http://www.dhgate.com/product/10pcs-6-35-stereo-headset-socket-gold-plated/372819964.html#s20-9-1b;searl|3371814035"
DhGateListingDetailJob.new.perform(1,1,url)
exit
url = "http://www.dhgate.com/product/10pcs-6-35-stereo-headset-socket-gold-plated/372819964.html#s20-9-1b;searl|3371814035"
DhGateListingDetailJob.new.perform(1,1,url)
ProductScreenshot.last
ProductScreenshot.last.save_attachment
screenshot = ProductScreenshot.last
screenshot.attachment.url
exit
url = "http://www.dhgate.com/product/10pcs-6-35-stereo-headset-socket-gold-plated/372819964.html#s20-9-1b;searl|3371814035"
DhGateListingDetailJob.new.perform(1,1,url)
exit
redis = Redis.new
exit
redis = Redis.new
key = 'development_rvx-rds:api_cache:listings'
keys = redis.keys(key)
keys = redis.keys("#{key}:*")
key = 'development_rvx-rds:api_cache'
keys = redis.keys("#{key}:*")
first = _.first
first.split(':')
first.split(':')[2..3]
keys.map {|k| k.split(':')[2..3] }
keys.map {|k| k.split(':')[2..3] }.uniq
keys.map {|k| k.split(':')[2..3] }.uniq.to_h
keys.map {|k| k.split(':')[2..3] }.uniq
pairs = keys.map {|k| k.split(':')[2..3] }.uniq
params = {}
pairs.each do |pair|
  params[pair[0]] || []
  params[pair[0]] << pair[1]
end
params[pair[0]] ||= []
pairs.each do |pair|
  params[pair[0]] ||= []
  params[pair[0]] << pair[1]
end
params
exit
edit -t
params
edit -t
type_params
edit -t
type_params.first
type_params.first[1]
type_params.first[1].first
params = "keywords=sunpower+module"
CGI.parse(params)
CGI.unescape(CGI.parse(params))
CGI.parse(params)
CGI.parse(params).values
params.map { |k, str| [k, str.first.gsub(/\s/, '+')] }.to_h
params = CGI.parse(params)
params.map { |k, str| [k, str.first.gsub(/\s/, '+')] }.to_h
params = _
Oj.dump(params.merge!({ type: type, version: version })
Oj.dump(params.merge!({ type: type, version: version }))
type = 'listings'
version = '23242511'
Oj.dump(params.merge!({ type: type, version: version }))
pairs
redis
redis.flushall
edit -t
pairs
edit -t
redis
redis.flushall
edit -t
redis.flushall
edit -t
redis.flushall
edit -t
redis
redis.flushall
params
params.to_param
params['keywords'] = 'sunpower module'
parmas
params
params.to_param
redis.flushall
cache = RdsApiCache.new
prefix = 'listings:keywords=dolby+digital:current_version'
current_version = cache.fetch("#{prefix}:current_version").try(:[], :version)
current_version = cache.fetch("#{prefix}:current_version").try(:[], 'version')
prefix
current_version = cache.fetch("#{prefix}:current_version")
key = prefix
def prefix
  "#{Rails.env}_#{ENV['REDIS_NAMESPACE']}:api_cache"
end
def full_key(key, options={})
  "#{prefix}:#{key}"
end
key
full_key(key)
redis
redis.get full_key(key)
cache = RdsApiCache.new
keys = cache.keys
pairs = keys.map { |k| k.split(':')[2..3] }.uniq
type_params = Hash.new { |h, k| h[k] = [] }
pairs.each do |pair|
  type_params[pair[0]] << pair[1]
end
type, params_list = type_params.first
version = Time.now.strftime("%Y%m%d%H%M%S")
params = params_list.first
params = CGI.parse(params)
params = params.map { |k, str| [k, str.first] }.to_h.sort.to_h.symbolize_keys
"#{type}:#{params.to_param}:in_progress"
params = Oj.dump(params.merge!({ type: type, version: version })
params = Oj.dump(params.merge!({ type: type, version: version }))
params = Oj.load(params)
type = params.delete(:type)
edit -t
model
Listing.connection
edit -t
prefix
connection = Redis.new
key = "#{prefix}:current_version"
edit -t
val = connection.get full_key(key, options)
options = {}
val = connection.get full_key(key, options)
prefix = "#{type}:#{cache_params.to_param}"
cache
current_version = cache.fetch("#{prefix}:current_version").try(:[], :version)
current_version = cache.fetch("#{prefix}:current_version")
current_version = cache.fetch("#{prefix}:current_version").try(:[], 'version')
val = connection.get full_key(key, options)
Oj.load(val) unless val.nil?
resp = _
resp.class
exit
cache = RdsApiCache.new
keys = cache.keys
pairs = keys.map { |k| k.split(':')[2..3] }.uniq
type_params = Hash.new { |h, k| h[k] = [] }
pairs.each do |pair|
  type_params[pair[0]] << pair[1]
end
type_params.each do |type, params_list|
  version = Time.now.strftime("%Y%m%d%H%M%S")
  type_params.each do |type, params_list|
type, params_list =  type_params.first
params = params_list.last
params = CGI.parse(params)
params = params.map { |k, str| [k, str.first] }.to_h.sort.to_h.symbolize_keys
params = Oj.dump(params.merge!({ type: type, version: version })
params = Oj.dump(params.merge!({ type: type, version: version }))
version = Time.now.strftime("%Y%m%d%H%M%S")
params = Oj.dump(params.merge!({ type: type, version: version }))
edit -t
prefix
cache
cache.keys(prefix)
prefix
connection = Redis.new
connection.keys("#{prefix}:*")
prefix
key = "#{Rails.env}_#{ENV['REDIS_NAMESPACE']}:api_cache:#{prefix}"
key = "#{Rails.env}_#{ENV['REDIS_NAMESPACE']}:api_cache:#{prefix}:*"
connection.keys(key)
prefix = "keywords=dolby+digital:20160721190546"
cache.clear_cache!(nil, pattern: prefix)
prefix = "listings:keywords=dolby+digital:20160721190546"
cache.clear_cache!(nil, pattern: prefix)
exit
connection ||= Redis.new
key = 'development_rvx-rds:api_cache:listings:keywords=dolby+digital:20160722120740:1'
val = connection.get key
val = Oj.load(val)
val = Oj.dump(val, {mode: :compat})
gzip_val = ActiveSupport::Gzip.compress(val)
connection.set full_key(key, options), gzip_val, :ex => expires_in
connection.set (key, gzip_val, :ex => expires_in
connection.set(key, gzip_val, :ex => expires_in)
connection.set(key, gzip_val, :ex => 2.weeks)
key
gzip_val
connection.set(key, gzip_val, :ex => 2.weeks)
val = connection.get key
val = Oj.load(val)
unzip_val = ActiveSupport::Gzip.decompress(val)
Oj.load(unzip_val)
val
unzip_val = ActiveSupport::Gzip.decompress(val)
val =     Oj.load(unzip_val)
val
gzip_val = ActiveSupport::Gzip.compress(val)
connection.set key, gzip_val, :ex => 2.days
key
key = "listings:keywords=dolby+digital:20160722120740:1"
cache = RdsApiCache.new
cache.fetch(key)
unzip_val
Oj.load(unzip_val)
key
edit =t
edit -t
connection ||= Redis.new
key
val = connection.get full_key(key, options)
options = {}
val = connection.get full_key(key, options)
unzip_val = ActiveSupport::Gzip.decompress(val)
Oj.load(unzip_val)
JSON.parse(unzip_val)
Oj.dump(unzip_val)
Oj.load(unzip_val)
val = connection.get full_key(key, options)
qqq
q    unzip_val = ActiveSupport::Gzip.decompress(val)
unzip_val = ActiveSupport::Gzip.decompress(val)
exijjexit
exit
edit -t
params = Hashie::Mash.new({:keyords => 'dolby digital'})
type = 'listings'
page = params.delete(:page) || 1
cache = RdsApiCache.new
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
params = Hashie::Mash.new({:keywords => 'dolby digital'})
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
version = params[:version] || cache.fetch("#{type}:#{cache_params.to_param}:current_version").try(:[], 'version')
cache.fetch("#{type}:#{cache_params.to_param}:current_version")
version = _
JSON.parse(version)
Oj.load(version)
exit
connection ||= Redis.new
key = 'beta_rvx-rds:api_cache:listings::20160722221322:14'
val = connection.get key
unzip_val = ActiveSupport::Gzip.decompress(val)
val =     Oj.load(unzip_val)
val['pagination']
q = Sidekiq::Queue.new('api_cache_queue')
q.pause
q.pause!
q.size
q.jobs
q.pause!
q.paused?
q.unpause!
exit
redis = Redis.current
redis.keys
redis.client.call(['client', 'list'])
list = redis.client.call(['client', 'list'])
list.class
p list
list.split('\n')
list.split("\n")
str = "id=1973410 addr=10.0.1.70:46890 fd=7 name= age=9772 idle=9772 flags=N db=0 sub=0 psub=1 multi=-1 qbuf=0 qbuf-free=0 obl=0 oll=0 omem=0 events=r cmd=psubscribe"
str.scan(/addr=(.*?) .*? idle=(\d*)/)
str =~ (/addr=(.*?) .*? idle=(\d*)/)
str.match(/addr=(.*?) .*? idle=(\d*)/)
matches = str.match(/addr=(.*?) .*? idle=(\d*)/)
matches.first
matches[0]
matches[1]
matches[2]
conn = 'id=1972929 addr=10.0.1.70:36632 fd=2737 name= age=79649 idle=8476 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=0 obl=0 oll=0 omem=0 events=r cmd=exec'
idle_max = 3600
conn.ends_with?('psubscribe')
match_data = conn.match(/addr=(.*?) .*? idle=(\d*)/)
if match_data[2] >= idle_max
  connection.client.call(["client", "kill", match_data[1]])
end
if match_data[2].to_i >= idle_max
  connection.client.call(["client", "kill", match_data[1]])
end
if match_data[2].to_i >= idle_max
  redis.client.call(["client", "kill", match_data[1]])
end
redis.client.disconnect
redis.client.connection
redis.client.connected?
exit
connection ||= Redis.new.client
connection.connected?
idle_max = 3600
connection.client.call(["client", "list"]).split("\n").each do |conn|
  next if conn.ends_with?('psubscribe')
  match_data = conn.match(/addr=(.*?) .*? idle=(\d*)/)
  if match_data[2] >= idle_max
    connection.client.call(["client", "kill", match_data[1]])
  end
end
@connection ||= Redis.new
connection ||= Redis.new
edit -t
connection ||= Redis.new
connection = Redis.new
edit -t
exit
connection = Redis.new
connection.client
exit
idle_max = 3600
client = Redis.current.client
edit -t
conn = client.call(['client', 'list']).split("\n").first
match_data = conn.match(/addr=(.*?) .*? idle=(\d*)/)
conn = 'id=1973424 addr=10.0.1.182:53992 fd=9 name= age=8504 idle=1421 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=0 obl=0 oll=0 omem=0 events=r cmd=exec'
match_data = conn.match(/addr=(.*?) .*? idle=(\d*)/)
conn.ends_with?('psubscribe')
puts "addr: #{match_data[1]}"
client = Redis.current.client
client.connected?
client.call(['client', 'kill', 'addr', match_data[1]])
edit -t
client.connected?
client.disconnect
params = Hashie.new({keywords: 'HE-AAC'})
params = Hashie::Mash.new({keywords: 'HE-AAC'})
params.map(&:downcase)
1.downcase
1.downcase if 1.respond_to? :downcase
params.each {|k,v| v.downcase! }
params
exit
{
  # search_fields: ['listings.name', 'listings.data_hash', 'product_manuals.text'],
  search_fields: ['listings.name', 'listings.data_hash'],
  filter_fields: {
    brands: 'brand',
    models: 'model',
    scrape_ids: 'scrapes.id',
    source_ids: 'sources.id',
    category_ids: 'categories.id'
  }
}
params = Hashie::Mash.new({keywords: 'hdmi', brand: 'Sony'})
type = 'products'
model = type.classify.constantize
Product.connection
model = type.classify.constantize
page = params[:page] || 1
scroll_id = params[:scroll_id]
edit -t
terms, operator, filters = prepare_search_params(model, params)
terms
params
params[:brands] = ['Sony', 'LG']
params
params.to_param
terms, operator, filters = prepare_search_params(model, params)
params['brands'] = 'Sony,Lg'
terms, operator, filters = prepare_search_params(model, params)
opts = { size: PER_PAGE, scroll: '3m' }
opts = { size: 20, scroll: '3m' }
fields = model.query_fields
edit -t
exit
edit -t
RestClient
exit
edit -t
response
edit -t
response.headers
response.body
ro = Oj.load(response)
total = ro[:pagination][:total]
ro
ro.keys
ro.keysro[:pagination]
ro[:pagination]
exit
edit -t
ro = Oj.load(response)
ro[:pagination]
ro.keys
ro['pagination']
ro
ro[:products]
ro.class
ro = Hashie::Mash.new(ro)
total = ro[:pagination][:total]
per_page = ro[:pagination][:per_page]
version = ro[:version]
results = ro[search_params[:type].to_sym]
results.map { |p| Hashie::Mash.new(p)}
response
Hashie::Mash.new(response)
Hashie::Mash.new(Oj.load(response))
ro
ro.class
ro[:products].class
ro[:products].first.class
params = Hashie::Mash.new({keywords: 'hdmi', brands: 'sony,lg'})
, "brand
type = 'products'
page = params.delete(:page) || 1
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
cache_params.each {|_k, v| v.respond_to?(:downcase) ? v.downcase! : v}
Oj.dump(params.merge!({ type: type, version: version }))
cache = RdsApiCache.new
keys = cache.keys
pairs = keys.map { |k| k.split(':')[2..3] }.uniq
type_params = Hashie::Mash.new { |h, k| h[k] = [] }
type_params.class
pairs.each do |pair|
  type_params[pair[0]] << pair[1]
end
type, params_list = type_params.first
type
params
params_list = params
version = Time.now.strftime("%Y%m%d%H%M%S")
type, params_list = type_params.first
response.headers
type_params
params_list
version
params = params_list.first
params = CGI.parse(params)
params = params.map { |k, str| [k, str.first] }.to_h.sort.to_h.symbolize_keys
Hash::Mash.new(params.map { |k, str| [k, str.first] }.to_h.sort)
params = Hashie::Mash.new(params.map { |k, str| [k, str.first] }.to_h.sort) #.to_h.symbolize_keys
params = Hashie::Mash.new(params.map { |k, str| [k, str.first] }.to_h.sort.to_h) #.to_h.symbolize_keys
params = params_list.first
params = CGI.parse(params)
params = Hashie::Mash.new(params.map { |k, str| [k, str.first] }.to_h.sort.to_h) #.to_h.symbolize_keys
Oj.dump(params.merge!({ type: type, version: version })
Oj.dump(params.merge!({ type: type, version: version }))
params
params.class
params.to_h
params.sybolize_keys
params
params.sybolize_keys!
params
params.class
params = params.to_h
params.symbolize_keys!
params
Oj.dump(params)
saved = Oj.dump(params)
params
params.class
params.delete(:symbolize_keys)
params
params.delete(:sybolize_keys)
params
saved = Oj.dump(params)
params = Oj.load(saved)
params
params = Hashie.mash.new(params)
params = Hashie::Mash.new(params)
type = 'products'
page = params.delete(:page) || 1
cache_params = params.to_h.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
cache_params = Hashie::Mash.new(params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h)
cache_params.class
params
cache
cache_params
page = params[:page] || 1
type = params.delete(:type)
model = type.classify.constantize
search_response = model.search_by(*prepare_search_params(model, params), { size: PER_PAGE, scroll: '3m' })
operator = 'AND'
terms = { keywords: params[:keywords].to_s }
filters = {}
model.query_fields[:filter_fields].keys.each do |key|
  filters[key] = params[key].downcase.split(',').map(&:strip) if params[key].present?
end
if params[:using] == 'any'
  keywords = [params[:keywords].to_s, params[:brand], params[:model]].join(' ')
  filters.delete_if { |k, _v| [:brand, :model].include?(k) }
  terms = { keywords: keywords }
  operator = 'OR'
end
def prepare_search_params(model, params)
  operator = 'AND'
  terms = { keywords: params[:keywords].to_s }
  filters = {}
  model.query_fields[:filter_fields].keys.each do |key|
    filters[key] = params[key].downcase.split(',').map(&:strip) if params[key].present?
  end
  if params[:using] == 'any'
    keywords = [params[:keywords].to_s, params[:brand], params[:model]].join(' ')
    filters.delete_if { |k, _v| [:brand, :model].include?(k) }
    terms = { keywords: keywords }
    operator = 'OR'
  end
  return terms, operator, filters
end
search_response = model.search_by(*prepare_search_params(model, params), { size: PER_PAGE, scroll: '3m' })
PER_PAGE = 200
search_response = model.search_by(*prepare_search_params(model, params), { size: PER_PAGE, scroll: '3m' })
ids = search_response.results.map { |p| p.id }.compact
ids.count
total = search_response.results.total
scroll_id = search_response.response._scroll_id
records = model.include_api_info.where(id: ids)
records.count
Product.connection
ActiveRecord::Base.connection.reconnect!
exit
params = Hashie::Mash.new({keywords: 'hdmi'})
type = params.delete(:type)
type = 'products'
model = type.classify.constantize
Product.connection
page = params[:page] || 1
scroll_id = params[:scroll_id]
edit -t
search_response = model.search_by(*prepare_search_params(model, params), { size: PER_PAGE, scroll: '3m' })
PER_PAGE = ENV['CACHE_PAGE_SIZE'].try(:to_i) || 200
search_response = model.search_by(*prepare_search_params(model, params), { size: PER_PAGE, scroll: '3m' })
ids = search_response.results.map { |p| p.id }.compact
total = search_response.results.total
scroll_id = search_response.response._scroll_id
records = model.include_api_info.where(id: ids)
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
version = params[:version] || Time.now.strftime("%Y%m%d%H%M%S")
prefix = "#{type}:#{cache_params.to_param}"
key = "#{prefix}:#{version}:#{page}"
meta_data = { pagination: { page: page, per_page: PER_PAGE, total: total }, version: version }
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json(root: false)} }
body.class
body.merge(meta_data)
body.merge(meta_data).to_json
val = _
val = Oj.dump(val, { mode: :compat })
Oj.load(val)
cache = RdsApiCache.new
keys = cache.keys
pairs = keys.map { |k| k.split(':')[2..3] }.uniq
type_params = Hashie::Mash.new { |h, k| h[k] = [] }
pairs.each do |pair|
  type_params[pair[0]] << pair[1]
end
type, params_list = type_params
params_list.alst
params_list.last
type, params_list = type_params.first
type_params
type_params.each do |type, params_list|
  p type
  p params_list
  puts '****'
end
type_params = Hash.new { |h, k| h[k] = [] }
pairs.each do |pair|
  type_params[pair[0]] << pair[1]
end
type, params_list = type_params.first
type_params
version = Time.now.strftime("%Y%m%d%H%M%S")
params = params_list.first
params = CGI.parse(params)
params = params.map { |k, str| [k, str.first] }.to_h.sort.to_h.symbolize_keys
"#{type}:#{params.to_param}:in_progress"
params
Hashie::Mash.new(params)
Oj.dump(params.merge!({ type: type, version: version }))
params = Hashie::Mash.new(params)
Oj.dump(params.merge!({ type: type, version: version }))
params = params_list.first
params = CGI.parse(params)
params = Hashie::Mash.new(params.map { |k, str| [k, str.first] }.to_h.sort.to_h.symbolize_keys)
params.class
"#{type}:#{params.to_param}:in_progress"
params.class
params.to_param
params.to_params
params.to_param
params.to_param!
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
cache_params = cache_params.map! {|_k, v| v.respond_to?(:downcase) ? v.downcase! : v}.to_param
cache_params = cache_params.map {|_k, v| v.respond_to?(:downcase) ? v.downcase! : v}.to_param
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
cache_params = cache_params.map {|_k, v| v.respond_to?(:downcase) ? v.downcase! : v}.to_param
cache_params.map {|_k, v| v.respond_to?(:downcase) ? v.downcase! : v}.to_param
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
cache_params.map {|_k, v| v.respond_to?(:downcase) ? v.downcase! : v}.to_param
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
cache_params.map {|_k, v| v.respond_to?(:downcase) ? v.downcase! : v}
cache_params.each {|_k, v| v.respond_to?(:downcase) ? v.downcase! : v}
cache_params['keywords'] = 'Dolby Digital'
cache_params
cache_params = cache_params.each {|_k, v| v.respond_to?(:downcase) ? v.downcase! : v}.to_param
cache_params = {}
cache_params = cache_params.each {|_k, v| v.respond_to?(:downcase) ? v.downcase! : v}.to_param
Oj.dump({version: 'asdfasdfas', { mode: :compat })
Oj.dump({version: 'asdfasdfas'}, { mode: :compat })
data = _
Oj.load(data)
exit
operator = 'AND'
terms = { keywords: 'hdmi' }
params = Hashie::Mash.new
per_page = params[:per_page] || 10
page = params[:page] || 1
filters = {}
Product.query_fields[:filter_fields].keys.each do |key|
  filters[key] = params[key].downcase.split(',').map(&:strip) if params[key].present?
end
terms
operator
filter
opts = { size: per_page, scroll: '3m' }
fields = Product.query_fields
query = Product.generate_query_by_keywords(fields, terms, operator, filters)
query = Product.generate_query_by_keywords(fields, terms, operator, filters).as_json
query = Product.generate_query_by_keywords(fields, terms, operator, filters).to_json
query = Product.generate_query_by_keywords(fields, terms, operator, filters).as_json
query = Product.generate_query_by_keywords(fields, terms, operator, filters).to_json
exit
edit -t
fields
edit -t
exit
edit -t
fields
Product.query_fields
search_fields = fields[:search_fields]
edit -t
fields
edit -t
fields = self.query_fields
edit -t
fields = Product.query_fields
search_fields = fields[:search_fields]
query = {
  fields: ['id'],
  query: {
    filtered: {
      query: {
        bool: {
          filter: [
            {
              multi_match: {
                query: terms[:keywords],
                operator: operator,
                fields: search_fields
              }
            }
          ]
        },
        match_all: {}
      },
      filter: {
        bool: {
          must: []
        }
      }
    }
  },
  sort: [
    '_doc'
  ]
}
query[:query][:filtered][:query].delete(:match_all) if terms.values.reject(&:blank?).present?
query[:query][:filtered][:query].delete(:bool) if terms.values.reject(&:blank?).blank?
query
definition = query
definition
exit
edit -t
exit
operator = 'AND'
terms = { keywords: 'hdmi' }
params = Hashie::Mash.new
per_page = params[:per_page] || 10
page = params[:page] || 1
filters = {}
Product.query_fields[:filter_fields].keys.each do |key|
  filters[key] = params[key].downcase.split(',').map(&:strip) if params[key].present?
end
opts = { size: per_page, scroll: '3m' }
fields = Product.query_fields
query = Product.generate_query_by_keywords(fields, terms, operator, filters)
edit -t
operator = 'AND'
terms = { keywords: 'hdmi' }
params = Hashie::Mash.new
per_page = params[:per_page] || 10
page = params[:page] || 1
filters = {}
Listing.query_fields[:filter_fields].keys.each do |key|
  filters[key] = params[key].downcase.split(',').map(&:strip) if params[key].present?
end
opts = { size: per_page, scroll: '3m' }
fields = Listing.query_fields
query = Listing.generate_query_by_keywords(fields, terms, operator, filters)
exit
edit -t
query
query.as_json
Scrape.last.name
exit
Product.reindex_alias
exit
Listing.reindex_alias
Source.all
exit
exit
Listing.reindex_alias
exit
Listing.reindex_alias
Product.reindex_alias
exit
Listing.reindex_alias
exit
Listing.reindex_alias
exit
def expire!( ttl=3600, *keys)
  connection = Redis.new
  keys.each do |key|
    connection.keys("#{key}*").each { |k| connection.expire(k, ttl) }
  end
end
*keys, ttl = "key1", "key2", 2222
keys
ttl
keys
ttl
def expire!(*keys, ttl=3600)
connection = Redis.new
keys.each do |key|
  connection.keys("#{key}*").each { |k| connection.expire(k, ttl) }
end
def expire!(*keys, ttl)
  connection = Redis.new
  keys.each do |key|
    connection.keys("#{key}*").each { |k| connection.expire(k, ttl) }
  end
end
exit
def expire!(*keys, ttl)
  connection = Redis.new
  keys.each do |key|
    connection.keys("#{key}*").each { |k| connection.expire(k, ttl) }
  end
end
connection = Redis.current
connection.keys
cache = RdsApiCache.new
cache.keys
keys = cache.keys
*keys
keys
keys.class
cache.expire!(*keys, 1.minutes)
edit -t
exit
edit -t
types
keywords
cache = RdsApiCache.new
version = Time.now.strftime("%Y%m%d%H%M%S")
type = types.first
keyword = keywords.first
params = Hashie::Mash.new({keywords: keyword})
cache.set_value("#{type}:#{params.to_param}:in_progress", { version: version }, ex: 6.hours)
keywords.uniq
keyword
keywords
cache = RdsApiCache.new
version = Time.now.strftime("%Y%m%d%H%M%S")
types.each do |type|
  keywords.uniq!.map(&:downcase).each do |keyword|
    params = Hashie::Mash.new({keywords: keyword})
    cache.set_value("#{type}:#{params.to_param}:in_progress", { version: version }, ex: 6.hours)
    PopulateApiCacheJob.perform_async Oj.dump(params.merge!({ type: type, version: version }))
  end
end
keywords
keywords.uniq!
keywords.map!(&:downcase)
keywords.map!(&:downcase).uniq!
keywords.map!(&:downcase).uniq
types.each do |type|
  keywords.uniq.map!(&:downcase).each do |keyword|
    params = Hashie::Mash.new({keywords: keyword})
    cache.set_value("#{type}:#{params.to_param}:in_progress", { version: version }, ex: 6.hours)
    PopulateApiCacheJob.perform_async Oj.dump(params.merge!({ type: type, version: version }))
  end
end
cache.keys
eix
texit
exit
cache = RdsApiCache.new
cache.expire!(key, 1.minunte)
key = 'development_rvx-rds:api_cache'
cache.expire!(key, 1.minute)
Time.now
Sleep 30
sleep 30
edit -t
Object.const_get('products'.classify)
'products'.classify
Object.const_get('products'.classify).connection
q = Sidekiq.queue('api_cache_queue').new
q = Sidekiq::Queue.new('api_cache_queue')
q.jobs
q.count
Redis = Redis.current
redis = Redis.current
redis.flushall
edit -t
keywords.count
_ * 2
keywords.uniq
keywords.uniq.count
* 2
keywords.uniq.map(&:downcase)
keywords.uniq.map(&:downcase).uniq
exit
redis = Redis.current.flushall
keywords = [
  'thx',
  '4k',
  'uhd',
  'dolby digital',
  'hdmi',
  'dts',
  'srs',
  'he-aac',
  'dimmable',
  'rgbw',
  'aac',
  'hdr',
  'local dimming,',
  'hdr,',
  'high dynamic range',
  'multizone backlight',
  'full array backlight',
  'uhd dimming',
  'pixel dimming',
  'backlight scanning'
]
types = ['products', 'listings']
cache = RdsApiCache.new
version = Time.now.strftime("%Y%m%d%H%M%S")
type = types.first
keywords = keywords.first
keyword = keywords.first
keywords
keywords = [
  'thx',
  '4k',
  'uhd',
  'dolby digital',
  'hdmi',
  'dts',
  'srs',
  'he-aac',
  'dimmable',
  'rgbw',
  'aac',
  'hdr',
  'local dimming,',
  'hdr,',
  'high dynamic range',
  'multizone backlight',
  'full array backlight',
  'uhd dimming',
  'pixel dimming',
  'backlight scanning'
]
keyword = keywords.first
params = Hashie::Mash.new({ keywords: keyword })
cache.set_value("#{type}:#{params.to_param}:in_progress", { version: version }, ex: 6.hours)
exit
PopulateApiCacheJob.new.perform Oj.dump(params.merge!({ type: type, version: version }))
model = Object.const_get(type.classify).try(:connection)
exit
keywords = [
  'thx',
  '4k',
  'uhd',
  'dolby digital',
  'hdmi',
  'dts',
  'srs',
  'he-aac',
  'dimmable',
  'rgbw',
  'aac',
  'hdr',
  'local dimming,',
  'hdr,',
  'high dynamic range',
  'multizone backlight',
  'full array backlight',
  'uhd dimming',
  'pixel dimming',
  'backlight scanning'
]
types = ['products', 'listings']
cache = RdsApiCache.new
version = Time.now.strftime("%Y%m%d%H%M%S")
type = types.first
keyword = keywords.first
params = Hashie::Mash.new({ keywords: keyword })
cache.set_value("#{type}:#{params.to_param}:in_progress", { version: version }, ex: 6.hours)
PopulateApiCacheJob.new.perform Oj.dump(params.merge!({ type: type, version: version }))
model = Object.const_get(type.classify)
model.connection
params = Oj.dump(params.merge!({ type: type, version: version }))
params = Oj.load(params)
type = params.delete(:type)
raise "Unrecognized type: #{type}" unless %w(products listings).include?(type)
model = Object.const_get(type.classify)
model.try(:connection)
page = params[:page] || 1
scroll_id = params[:scroll_id]
search_response = model.search_by(*prepare_search_params(model, params), { size: PER_PAGE, scroll: '3m' })
edit -t
search_response = model.search_by(*prepare_search_params(model, params), { size: PER_PAGE, scroll: '3m' })
PER_PAGE = ENV['CACHE_PAGE_SIZE'].try(:to_i) || 200
cache = RdsApiCache.new
search_response = model.search_by(*prepare_search_params(model, params), { size: PER_PAGE, scroll: '3m' })
ids = search_response.results.map { |p| p.id }.compact
total = search_response.results.total
scroll_id = search_response.response._scroll_id
records = model.include_api_info.where(id: ids)
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
version = params[:version] || Time.now.strftime("%Y%m%d%H%M%S")
prefix = "#{type}:#{cache_params.to_param}"
key = "#{prefix}:#{version}:#{page}"
meta_data = { pagination: { page: page, per_page: PER_PAGE, total: total }, version: version }
body = { type => records.map { |r| "#{model}Serializer".constantize.new(r).as_json(root: false) } }
body = { type => records.map { |r| Object.const_get("#{model}Serializer").new(r).as_json(root: false) } }
cache.set_value(key, body.merge(meta_data))
(total / (PER_PAGE * page.to_i)).zero?
total
PER_PAGE
0 / 200
update_versions(prefix, version)
def update_versions(prefix, new_version)
  cache = RdsApiCache.new # REMOVE
  current_version = cache.fetch("#{prefix}:current_version").try(:[], 'version')
  old_version = cache.fetch("#{prefix}:old_version").try(:[], 'version')
  cache.set_value("#{prefix}:current_version", { version: new_version })
  cache.clear_cache!("#{prefix}:in_progress")
  cache.clear_cache!(nil, pattern: "#{prefix}:#{old_version}") if old_version.present?
  cache.set_value("#{prefix}:old_version", { version: current_version }) if current_version.present?
  cache.expire!("#{prefix}:old_version", "#{prefix}:#{old_version}", 2.hours) if old_version.present?
end
current_version = cache.fetch("#{prefix}:current_version").try(:[], 'version')
prefix
old_version = cache.fetch("#{prefix}:old_version").try(:[], 'version')
cache.set_value("#{prefix}:current_version", { version: new_version })
new_version = version
cache.set_value("#{prefix}:current_version", { version: new_version })
cache.clear_cache!("#{prefix}:in_progress")
cache.clear_cache!(nil, pattern: "#{prefix}:#{old_version}") if old_version.present?
cache.set_value("#{prefix}:old_version", { version: current_version }) if current_version.present?
cache.expire!("#{prefix}:old_version", "#{prefix}:#{old_version}", 2.hours) if old_version.present?
cache.expire!("#{prefix}:old_version", "#{prefix}:#{old_version}", 2) if old_version.present?
old_version
params
version = Time.now.strftime("%Y%m%d%H%M%S")
params = Hashie::Mash.new({ keywords: keyword })
cache.set_value("#{type}:#{params.to_param}:in_progress", { version: version }, ex: 6.hours)
params = Oj.dump(params.merge!({ type: type, version: version }))
params = Oj.load(params)
type = params.delete(:type)
model = Object.const_get(type.classify)
model.try(:connection)
page = params[:page] || 1
scroll_id = params[:scroll_id]
search_response = model.search_by(*prepare_search_params(model, params), { size: PER_PAGE, scroll: '3m' })
ids = search_response.results.map { |p| p.id }.compact
total = search_response.results.total
scroll_id = search_response.response._scroll_id
records = model.include_api_info.where(id: ids)
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
version = params[:version] || Time.now.strftime("%Y%m%d%H%M%S")
prefix = "#{type}:#{cache_params.to_param}"
key = "#{prefix}:#{version}:#{page}"
meta_data = { pagination: { page: page, per_page: PER_PAGE, total: total }, version: version }
body = { type => records.map { |r| Object.const_get("#{model}Serializer").new(r).as_json(root: false) } }
cache.set_value(key, body.merge(meta_data))
new_version = version
current_version = cache.fetch("#{prefix}:current_version").try(:[], 'version')
old_version = cache.fetch("#{prefix}:old_version").try(:[], 'version')
cache.set_value("#{prefix}:current_version", { version: new_version })
cache.clear_cache!("#{prefix}:in_progress")
cache.set_value("#{prefix}:old_version", { version: current_version }) if current_version.present?
current_version = cache.fetch("#{prefix}:current_version").try(:[], 'version')
old_version = cache.fetch("#{prefix}:old_version").try(:[], 'version')
cache.expire!("#{prefix}:old_version", "#{prefix}:#{old_version}", 1.minute) if old_version.present?; Time.now
sleep 1.minute
connection = Redis.new
key1, key2, ttl = "#{prefix}:old_version", "#{prefix}:#{old_version}", 1.minute
def expire!(*keys, ttl)
  keys.each do |key|
    connection.keys("#{full_key(key)}*").each { |k| connection.expire(k, ttl) }
  end
end
*keys = key1, key2
keys
ttl
2.minutes
2.minutes.to_i
2.minutes
2.minutes.class
2.minutes.to_i
ttl = 1.minutes.to_i
keys.each do |key|
  connection.keys("#{full_key(key)}*").each { |k| connection.expire(k, ttl.to_i) }
end
def prefix
  "#{Rails.env}_#{ENV['REDIS_NAMESPACE']}:api_cache"
end
def full_key(key, options={})
  "#{prefix}:#{key}"
end
prefix
exit
exit
edit -t
keyword = keywords.first
type = 'products'
params = Hashie::Mash.new({ keywords: keyword })
cache.set_value("#{type}:#{params.to_param}:in_progress", { version: version }, ex: 6.hours)
cache = RdsApiCache.new
version = Time.now.strftime("%Y%m%d%H%M%S")
cache.set_value("#{type}:#{params.to_param}:in_progress", { version: version }, ex: 6.hours)
PopulateApiCacheJob.new.perform Oj.dump(params.merge!({ type: type, version: version }))
redis = Redis.new
redis.flushall
exit
edit -t
keyword = 'thx'
type = 'products'
cache = RdsApiCache.new
version = Time.now.strftime("%Y%m%d%H%M%S")
params = Hashie::Mash.new({ keywords: keyword })
cache.set_value("#{type}:#{params.to_param}:in_progress", { version: version }, ex: 6.hours)
PopulateApiCacheJob.new.perform Oj.dump(params.merge!({ type: type, version: version }))
keyword = 'thx'
type = 'products'
cache = RdsApiCache.new
version = Time.now.strftime("%Y%m%d%H%M%S")
params = Hashie::Mash.new({ keywords: keyword })
cache.set_value("#{type}:#{params.to_param}:in_progress", { version: version }, ex: 6.hours)
PopulateApiCacheJob.new.perform Oj.dump(params.merge!({ type: type, version: version }))
sleep 50
redis
Redis.current.flush_all
Redis.current.flushall
exit
keyword = 'thx'
type = 'products'
cache = RdsApiCache.new
version = Time.now.strftime("%Y%m%d%H%M%S")
params = Hashie::Mash.new({ keywords: keyword })
cache.set_value("#{type}:#{params.to_param}:in_progress", { version: version }, ex: 6.hours)
PopulateApiCacheJob.perform_async Oj.dump(params.merge!({ type: type, version: version }))
1800 / 60
cache = RdsApiCache.new
version = Time.now.strftime("%Y%m%d%H%M%S")
types.each do |type|
  keywords.each do |keyword|
    params = Hashie::Mash.new({ keywords: keyword })
    cache.set_value("#{type}:#{params.to_param}:in_progress", { version: version }, ex: 6.hours)
    PopulateApiCacheJob.perform_async Oj.dump(params.merge!({ type: type, version: version }))
  end
end
edit -t
Product.connection
Redis.new.flushall
exit
edit -t
keywords.size
Redis.current.flushall
exit
edit -t
Redis.current.flushall
exit
edit -t 
keywords = [
  'thx',
  '4k',
  'uhd',
  'dolby digital',
  'hdmi',
  'dts',
  'srs',
  'he-aac',
  'dimmable',
  'rgbw',
  'aac',
  'hdr',
  'local dimming',
  'high dynamic range',
  'multizone backlight',
  'full array backlight',
  'uhd dimming',
  'pixel dimming',
  'backlight scanning'
]
keywords.map {|k| k.gsub(',', '')
keywords.map {|k| k.gsub(',', '') }
exit
edit -t
'
exit
Redis.current.flushall
edit -t
Redis.current.flushall
edit -t
Redis.current.flushall
exit
edit -t
10.minutes.to_i
2.weeks.to_i
exit
connection = Redis.current
connection.keys
connection.keys("#{key}*")
key = 'beta_rvx-rds:api_cache'
connection.keys("#{key}*")
connection.keys("#{key}*").count
connection.keys("#{key}*").each {|k| connection.del k }
exit
keys '*', 100
exit
es = Product.__elasticsearch__
Product.connection
es = Product.__elasticsearch__
keyword = 'thx'
type = 'listings'
cache = RdsApiCache.new
version = Time.now.strftime("%Y%m%d%H%M%S")
params = Hashie::Mash.new({ keywords: keyword })
params.merge!({ type: type, version: version })
type = params.delete(:type)
model = Object.const_get(type.classify)
page = params[:page] || 1
scroll_id = params[:scroll_id]
search_response = model.search_by(*prepare_search_params(model, params), { size: PER_PAGE, scroll: '10m' })
ids = search_response.results.map { |p| p.id }.compact
total = search_response.results.total
scroll_id = search_response.response._scroll_id
search_response = model.search_by(*prepare_search_params(model, params), { size: PER_PAGE, scroll: '10m' })
edit -t
search_response = model.search_by(*prepare_search_params(model, params), { size: PER_PAGE, scroll: '10m' })
PER_PAGE = 100
search_response = model.search_by(*prepare_search_params(model, params), { size: PER_PAGE, scroll: '10m' })
ids = search_response.results.map { |p| p.id }.compact
total = search_response.results.total
scroll_id = search_response.response._scroll_id
scroll_response = model.__elasticsearch__.client.scroll(scroll_id: scroll_id, scroll: '10m')
Product.__elasticsearch__.clear_scroll(scroll_id: scroll_id)
scroll_response = model.__elasticsearch__.client.scroll(scroll_id: scroll_id, scroll: '10m')
scroll_id = search_response.response._scroll_id
exit
keyword = 'led'
type = 'listings'
cache = RdsApiCache.new
version = Time.now.strftime("%Y%m%d%H%M%S")
params = Hashie::Mash.new({ keywords: keyword })
params.merge!({ type: type, version: version })
type = params.delete(:type)
model = Object.const_get(type.classify)
Listing.connection
page = params[:page] || 1
search_response = model.search_by(*prepare_search_params(model, params), { size: PER_PAGE, scroll: '10m' })
edit -t
search_response = model.search_by(*prepare_search_params(model, params), { size: PER_PAGE, scroll: '10m' })
ids = search_response.results.map { |p| p.id }.compact
total = search_response.results.total
scroll_id = search_response.response._scroll_id
PER_PAGE = 200
search_response = model.search_by(*prepare_search_params(model, params), { size: PER_PAGE, scroll: '10m' })
scroll_id
ids = search_response.results.map { |p| p.id }.compact
total = search_response.results.total
scroll_id = search_response.response._scroll_id
Product.clear_scroll(scroll_id)
client = Elasticsearch::Model.client
client.methods
client.clear_scroll scroll_id
client.clear_scroll scroll_id: scroll_id
search_response = model.search_by(*prepare_search_params(model, params), { size: PER_PAGE, scroll: '10m' })
ids = search_response.results.map { |p| p.id }.compact
total = search_response.results.total
scroll_id = search_response.response._scroll_id
client.clear_scroll scroll_id: scroll_id
scroll_response = model.__elasticsearch__.client.scroll(scroll_id: scroll_id, scroll: '10m')
search_response = model.search_by(*prepare_search_params(model, params), { size: PER_PAGE, scroll: '10m' })
scroll_id = search_response.response._scroll_id
Product.__elasticsearch__.client
Product.__elasticsearch__.client.clear_scroll scroll_id: scroll_id
scroll_response = model.__elasticsearch__.client.scroll(scroll_id: scroll_id, scroll: '10m')
scroll_id
Product.client
exit
keyword = 'led'
type = 'listings'
cache = RdsApiCache.new
version = Time.now.strftime("%Y%m%d%H%M%S")
params = Hashie::Mash.new({ keywords: keyword })
params.merge!({ type: type, version: version })
type = params.delete(:type)
model = Object.const_get(type.classify)
page = params[:page] || 1
scroll_id = params[:scroll_id]
search_response = model.search_by(*prepare_search_params(model, params), { size: PER_PAGE, scroll: '10m' })
edit -t
search_response = model.search_by(*prepare_search_params(model, params), { size: PER_PAGE, scroll: '10m' })
ids = search_response.results.map { |p| p.id }.compact
total = search_response.results.total
scroll_id = search_response.response._scroll_id
model.clear_scroll! scroll_id
scroll_response = model.__elasticsearch__.client.scroll(scroll_id: scroll_id, scroll: '10m')
'0'.zero?
total = search_response.results.total
exit
keys '*'
exit
edit -t
exit
edit -t
Redis.current
Redis.current.flushall
edit -t
keywords.count
keywords.count * 2
edit -t
exit
edit -t
Redis.current.flushall
exit
edit -t
exit
edit -t
exit
edit -t
exit
redis = Redis.current
redis.keys
redis.keys.length
redis.client
exit
keywords = [
  'thx',
  '4k',
  'uhd',
  'dolby digital',
  'hdmi',
  'dts',
  'srs',
  'he-aac',
  'dimmable',
  'rgbw',
  'aac',
  'hdr',
  'local dimming',
  'high dynamic range',
  'multizone backlight',
  'full array backlight',
  'uhd dimming',
  'pixel dimming',
'backlight scanning']
edit -t
keywords.count * 2
exit
edit -t
exit
edit -t
exit
edit -t
exit
edit -t
redis = Redis.current
redis.keys
redis.flushall
exit
edit -t
Redis.current.flushall
exit
Redis.current.flushall
exit
edit -t
Redis.current.flushall
exit
edit -t
exit
edit -t
Redis.current.flushall
edit -t
Redis.current.flushall
exit
connection = Redis.niw
connection = Redis.new
connection.quit
connection = Redis.new
connection.keys
connection.quit
connection = Redis.new
connection.flushall
exit
edit -t
exit
edit -t
Redis.current.flushall
exit
edit -t
Oj.load({"^o":"Hashie::Mash","self":{"keywords":"4k","type":"products","version":"20160801123017"})
Oj.load({"^o":"Hashie::Mash","self":{"keywords":"4k","type":"products","version":"20160801123017"}})
Oj.load('{"^o":"Hashie::Mash","self":{"keywords":"4k","type":"products","version":"20160801123017"}}')
_.class
Redis.current.flushall
exit
edit -t
exit
Redis.current.flushall
exit
edit -t
exit
edit -t
Redis.current.flushall
exit
edit -t
Redis.current.flushall
exit
edit -t
params = {:keywords=>"uhd dimming", :type=>"listings", :version=>"20160801131250"}
JSON.dump(params)
params = _
params = JSON.load(params).symbolize_keys
type = params[:type]
exit
edit -t
Redis.current.flushall
exit
edit -t
params = {:keywords=>"uhd dimming", :type=>"listings", :version=>"20160801131250"}
JSON.dump(params)
params = JSON.dump(params)
params = JSON.load(params)
params = params.symbolize_keys
type = params[:type]
model = Object.const_get(type.classify)
model.connection
page = params[:page] || 1
scroll_id = params[:scroll_id]
search_response = model.search_by(*prepare_search_params(model, params), { size: PER_PAGE, scroll: '8m' })
edit -t
params
params[:keywords] = 'listings'
search_response = model.search_by(*prepare_search_params(model, params), { size: PER_PAGE, scroll: '8m' })
edit -t
search_response = model.search_by(*prepare_search_params(model, params), { size: PER_PAGE, scroll: '8m' })
PER_PAGE = 200
search_response = model.search_by(*prepare_search_params(model, params), { size: PER_PAGE, scroll: '8m' })
ids = search_response.results.map { |p| p.id }.compact
total = search_response.results.total
scroll_id = search_response.response._scroll_id
records = model.include_api_info.where(id: ids)
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
version = params[:version] || Time.now.strftime("%Y%m%d%H%M%S")
prefix = "#{type}:#{cache_params.to_param}"
key = "#{prefix}:#{version}:#{page}"
meta_data = { pagination: { page: page, per_page: PER_PAGE, total: total }, version: version }
body = { type => records.map { |r| Object.const_get("#{model}Serializer").new(r).as_json(root: false) } }
Oj.dump(params.merge!({ scroll_id: scroll_id, version: version, type: type, page: page + 1 }))
params = _
JSON.load(params)
params = JSON.load(params).symbolize_keys
params = {:keywords=>"uhd dimming", :type=>"listings", :version=>"20160801131250"}
params[:keywords] = 'led'
params
JSON.dump(params.merge!({ scroll_id: scroll_id, version: version, type: type, page: page + 1 }))
params = _
params = JSON.load(params).symbolize_keys
exit
edit -t
exit
Redis.current
redis = _
redis.keys
redis.flushall
exit
edit -t
exit
edit -t
exit
edit -t
params = "{\"keywords\":\"hdmi\",\"type\":\"listings\",\"version\":\"20160801153953\"}"
params = Oj.load(params)
type = params[:type]
params = Hashie::Mash.new(params)
type = params[:type]
edit -t
exit
arr = ['Something', 'something']
arr.uniq(&:downcase!)
arr.uniq(&:downcase)
arr.map!(&:downcase).uniq!
'Some String'.downcase!.gsub!(' ', '_').to_sym
'Some String'.downcase!.gsub!(' ', '_').to_sym!
'Some String'.downcase!.gsub!(' ', '_').to_sym
hash = { a: 'something', b: 'something', c: 'something' }
has.map! {|k, v| v.upcase! }
hash.map! {|k, v| v.upcase! }
hash
arr = Array.new(100) { hash }
arr.each_with_index.map! {|val, idx| [ val[:a], val[:b] ] }
arr2 = []
arr.length.times do |i|
  arr2 << [arr[i][:a], arr[i][:b]]
end
arr2
arr
arr.length.times do |i|
Listing
Listing.first
l = _
l.products.count
ProductImage.first
ProductImage.find_by_sql('Select *').first
ProductImage.find_by_sql('Select * FROM product_images').first
ProductImage.find_by_sql('Select * FROM assets WHERE type = ProductImage').first
ProductImage.find_by_sql("Select * FROM assets WHERE type = 'ProductImage'").first
ProductImage.first
ProductImage.last
ProductImage.find_by_sql("Select * FROM assets WHERE type = 'ProductImage'").first
img = ProductImage.find_by_sql("Select * FROM assets WHERE type = 'ProductImage'").first
img = ProductImage.find_by_sql("Select id, original_url, attachment_file_size, type FROM assets WHERE type = 'ProductImage'").first
Listing.includes(:products).first(10)
Benchmark.bm do
time = Benchmark.realtime do
  Product.all.include(:listings).load
end
time = Benchmark.realtime do
  Product.all.include(:listings).load
end
time = Benchmark.realtime do
  Product.all.includes(:listings).load
end
time = Benchmark.realtime do
  Product.joins(:listings).to_sql
Product.joins(:listings).to_sql
query = _
time = Benchmark.realtime do
  Product.find_by_sql(query).load
end
time = Benchmark.realtime do
  Product.find_by_sql(query)
end
products = Product.find_by_sql(query)
p = products.first
p.listings
Listing.count
require '/Users/jonathan/ruby-programming/ruby-opt/wrapper.rb'
measure do
  Listing.all.each do |l|
    l
  end
end
measure do
  Listing.find_in_batches do |listings|
    listings.count.times do |i|
      listings.pop
    end
  end
end
exit
arr = Array.new(10) { 'x' * 1000 }
edit -t
measure(true) do
  arr.length.times do |idx|
    el = arr.unshift
  end
end
measure(true) do
  arr.each_with_index do |el, idx|
    el
  end
end
arr
arr.lenght
arr.length
arr = Array.new(10000) { 'x' * 1000 }
measure(true) do
  arr.each_with_index do |el, idx|
    el << 'x'
  end
end
arr = Array.new(10000) { 'x' * 1000 }
edit
edit -t
edit
exit
include Sidekiq::Worker
include CapybaraJob
include Sidekiq::Worker
include CapybaraJob
require 'capybara_product_details_selectors'
require 'jobs/capybara_job'
require 'jobs/scrape_batch_job'
require 'image_scrapeable'
require 'detail_scrapeable'
require 'overview_scrapeable'
require 'amazon_helpers'
url = 'https://www.amazon.com/dp/B00XMUU5KU/'
show-method get-naked
get-naked
test-ansi
nyan-cat
edit -
edit
exit
edit -t
measure do
  AmazonProductDetailJob.new.peform(scrape_id, source_category_id, url)
end
measure do
  AmazonProductDetailJob.new.perform(scrape_id, source_category_id, url)
end
Listing.last
Listing.last.destroy
SourceCategory.find(source_category_id)
source_category = SourceCategory.find source_category_id
category_id = source_category.category_id
source_category_id
source_category_id = SourceCategory.where(source: source).first.id
source_category = SourceCategory.find source_category_id
source_category.parent_id
edit -t
Listing.last
Listing.destroy
Listing.last.destroy
edit -t
edi t-t
measure do
  AmazonProductDetailJob.new.perform(scrape_id, source_category_id, url)
end
Listing.last
Listing.last.destroy
exit
edit -t
measure do
  AmazonProductDetailJob.new.perform(scrape_id, source_category_id, url)
end
edit -t
measure do
  AmazonProductDetailJob.new.perform(scrape_id, source_category_id, url)
end
exit
edit -t
Listing.last
edit -t
Listing.last.destroy
edit -t
Listing.last
edit -t
listing1 == listing2
Identifier.last
listing1.data_hash
listing1
listing1[:General]
listing1[:data_hash][:General]
listing1[:data_hash][:General][:"From the Manufacturer"]
listing2[:data_hash][:General][:"From the Manufacturer"] == _
edit -t
edit
rand(1..100)
edit
arr
reset
exit
Listing.last
Listing.last.destroy
edit
exit
edit
Listing.last
edit
Listing.last
edit -t
Listing.last.screenshots.count
Listing.last.screenshots.first.attachment.url
exit
edit
exit
edit
exit
edit
Listing.last
edit
exit
Listing.last
exit
edit
Listing.last
edit -t
listing1.each do |k, v|
  if listing2[k] != v
    puts k
  end
end
edit -t
compare_hash(listing1, listing2)
edit
edit -t
compare_hash(listing1, listing2)
edit
Listing.last
l = Listing.last
l.screenshots.last.attachment.url
exit
1294213120 - 117882404
1176330716
1_176_330_716 / (1024 * 1024)
1024 * 1024
1024 * 1024 * 1024
1_176_330_716 / (1024 * 1024 * 1024)
1_176_330_716 / (1024 * 1024 * 1024).to_f
exit
edit
edit -t
Listing.last.destroy
edit -t
edit
9 * 20
Listing.last
Listing.last.url
edit
exit
Source.to_h
Source.all.to_h
Source.all.to_json
hash = JSON.parse(Source.all.to_json)
Source.all.select(:id, :pretty_name)
Source.all.select(:id, :pretty_name).map! {|s| {s.id => s.pretty_name }}
Source.all.select(:id, :pretty_name).load.map! {|s| {s.id => s.pretty_name }}
Source.all.select(:id, :pretty_name).to_a.map! {|s| {s.id => s.pretty_name }}
Oj.load(Source.all.select(:id, :pretty_name).to_json)
Source.pluck(:id)
Source.includes(:scrapes).select(:id).to_a.map! {|s| {s.id => s.scrapes.order(id: :desc).pluck(:id) }}
source_scrape_map = Source.includes(:scrapes).select(:id).to_a.map! {|s| {s.id => s.scrapes.order(id: :desc).pluck(:id) }}
source_scrape_map.each {|sm| puts sm }
edit
exit
exi
exit
Product.first
p = Product.include_api_info.find(672598)
p.listings.count
p = Product.include_api_info.find(672598)
p.listings.size
p = Product.includes(:listings).find(672598)
exit
p = Product.joins({ listings: [:source, :images] }).find(672598)
p.listings.size
edit -t
p = Product.joins({ listings: [:source, :images] }).preload(:images).find(672598)
Product.joins({ listings: [:source, :images] }).to_sql
p = Product.joins({ listings: [:source, :images] }).find(672598)
p.images.size
p.listings.size
exit
p = Product.include_api_info.find(672598)
p.listings.count
ProductSerializer.new(p).as_json(root: false)
p
p = Product.find p.id
p.listings.count
p.listings.distinct(:url).to_sql
p.listings.pluck(:url).uniq.count
exit
"{\"^o\":\"Hashie::Mash\",\"self\":{\"keywords\":\"dolby digital\",\"type\":\"products\",\"version\":\"20160802235715\"}"
params = _
@api_cache = RdsApiCache.new
params = Hashie::Mash.new(Oj.load(params))
params = "{\"^o\":\"Hashie::Mash\",\"self\":{\"keywords\":\"dolby digital\",\"type\":\"products\",\"version\":\"20160802235715\"}}"
params = Hashie::Mash.new(Oj.load(params))
type = params[:type]
raise "Unrecognized type: #{type}" unless %w(products listings).include?(type)
model = Object.const_get(type.classify)
page = params[:page] || 1
scroll_response = model.__elasticsearch__.client.scroll(scroll_id: scroll_id, scroll: '10m')
scroll_id = params[:scroll_id]
scroll_response = model.__elasticsearch__.client.scroll(scroll_id: scroll_id, scroll: '10m')
scroll_id = params[:scroll_id]
search_response = model.search_by(*prepare_search_params(model, params), { size: PER_PAGE, scroll: '10m' })
edit -t
search_response = model.search_by(*prepare_search_params(model, params), { size: PER_PAGE, scroll: '10m' })
PER_PAGE = 400
search_response = model.search_by(*prepare_search_params(model, params), { size: PER_PAGE, scroll: '10m' })
ids = search_response.results.map { |p| p.id }.compact
Product.connection
search_response = model.search_by(*prepare_search_params(model, params), { size: PER_PAGE, scroll: '10m' })
ids = search_response.results.map { |p| p.id }.compact
total = search_response.results.total
scroll_id = search_response.response._scroll_id
keywords
params
scroll_id = search_response.response._scroll_id
records = model.include_api_info.where(id: ids)
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
version = params[:version] || Time.now.strftime("%Y%m%d%H%M%S")
prefix = "#{type}:#{cache_params.to_param}"
key = "#{prefix}:#{version}:#{page}"
meta_data = { pagination: { page: page, per_page: PER_PAGE, total: total }, version: version }
body = { type => records.map { |r| Object.const_get("#{model}Serializer").new(r).as_json(root: false) } }
Product.count
params = "{\"^o\":\"Hashie::Mash\",\"self\":{\"keywords\":\" rgbw\",\"type\":\"products\",\"version\":\"20160802235715\"}}"
edit -t
Product.count
exit
"{\"^o\":\"Hashie::Mash\",\"self\":{\"keywords\":\" backlight scanning\",\"type\":\"products\",\"version\":\"20160802235715\"}}"
api_cache_queue  less than a minute ago  NoMethodError: undefined method `flat_map' for nil:NilClass
Processor: rds-dashboard-beta-02:17780
Retry NowDelete
Delete All
params = "{\"^o\":\"Hashie::Mash\",\"self\":{\"keywords\":\" backlight scanning\",\"type\":\"products\",\"version\":\"20160802235715\"}}"
PER_PAGE = ENV['CACHE_PAGE_SIZE'].try(:to_i) || 200
edit -t
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
version = params[:version] || Time.now.strftime("%Y%m%d%H%M%S")
prefix = "#{type}:#{cache_params.to_param}"
key = "#{prefix}:#{version}:#{page}"
meta_data = { pagination: { page: page, per_page: PER_PAGE, total: total }, version: version }
body = { type => records.map { |r| Object.const_get("#{model}Serializer").new(r).as_json(root: false) } }
params
params.delete('version')
params
edit -t
params['keywords'] = 'backlight scanning'
params
edit -t
ids
search_response = model.search_by(*prepare_search_params(model, params), { size: PER_PAGE, scroll: '10m' })
params['keywords'] = 'hdmi'
edit -t
exit
params = "{\"^o\":\"Hashie::Mash\",\"self\":{\"keywords\":\" backlight scanning\",\"type\":\"products\",\"version\":\"20160802235715\"}}
  PER_PAGE = ENV['CACHE_PAGE_SIZE'].try(:to_i) || 200
params = "{\"^o\":\"Hashie::Mash\",\"self\":{\"keywords\":\" backlight scanning\",\"type\":\"products\",\"version\":\"20160802235715\"}}"
PER_PAGE = ENV['CACHE_PAGE_SIZE'].try(:to_i) || 200
edit -t
@api_cache = RdsApiCache.new
api_cache = RdsApiCache.new
edit -t
exit
edit -t
params
type = params[:type]
model = Object.const_get(type.classify)
page = params[:page] || 1
scroll_id = params[:scroll_id]
search_response = model.search_by(*prepare_search_params(model, params), { size: PER_PAGE, scroll: '10m' })
ids = search_response.results.map { |p| p.id }.compact
edit -t
params = "{\"^o\":\"Hashie::Mash\",\"self\":{\"keywords\":\"dolby digital\",\"type\":\"products\",\"version\":\"20160802235715\"}}"
p = Product.first
['a', 'b', '
c']
arr = _
arr[-2..-1]
arr
arr2 = []
arr2[-2..-1]
{ "delete" : { "_index" : "test", "_type" : "type1", "_id" : "2" } }
Product.count
exit
Product.index
Product.index_name
{ "delete" : { "_index" : Product.index_name, "_type" : "type1", "_id" : "2" } }
{ "delete" : { "_index" : Product.index_name, "_type" : "products", "_id" : p.id } }
Product.where.not(id: ProductListing.pluck(:id).find_in_batches do |products|
    products.map |p| {{ "delete": { "_index": Product.index_name, "_type": "products", "_id": p.id } }}
Product.where.not(id: ProductListing.pluck(:id).find_in_batches do |products|
    products.map {|p| { "delete": { "_index": Product.index_name, "_type": "products", "_id": p.id } }}
Product.where.not(id: ProductListing.pluck(:id).find_in_batches do |products|
    products.map {|p| { "delete": { "_index": Product.index_name, "_type": "products", "_id": p.id } } }
  end
Product.where.not(id: ProductListing.pluck(:id)).find_in_batches do |products|
  products.map {|p| { "delete": { "_index": Product.index_name, "_type": "products", "_id": p.id } } }
end
Product.where.not(id: ProductListing.pluck(:id)).find_in_batches do |products|
string = ''
Product.where.not(id: ProductListing.pluck(:id)).find_in_batches do |products|
  products.each {|p| str << "{ 'delete' : { '_index' : #{Product.index_name}, '_type' : 'products', '_id' : #{p.id} } }" }
end
Product.where.not(id: ProductListing.pluck(:id)).find_in_batches do |products|
  products.each {|p| string << "{ 'delete' : { '_index' : #{Product.index_name}, '_type' : 'products', '_id' : #{p.id} } }" }
end
string
string = ''
Product.where.not(id: ProductListing.pluck(:id)).find_in_batches do |products|
  products.each {|p| string << "{ 'delete' : { '_index' : #{Product.index_name}, '_type' : 'products', '_id' : #{p.id} } }\n" }
end
string
query = _
client = Product.__elasticsearch__.client
query
client.bulk(query)
client.bulk(body: query)
Product.where.not(id: ProductListing.pluck(:id)).find_in_batches do |products|
edit -t
arr2[-2..-1]
arr2 = [\
arr2 = []
arr2[-2..-1]
arr2[-2..-1].map {|p| puts p}
arr2 = [1, 2, 3, 4]
arr2[-10..-1]
arr2
arr2[-10..-1]
arr2.take(100)
arr2 = Array.new(1000) { a }
arr2 = Array.new(1000) { 'a' }
arr2.length
arr2.take(100).length
arr2 = []
arr2.take(100)
exit
params = Hashie::Mash.new({keywords: 'led', type: 'products'})
params
edit -t
@api_cache = RdsApiCache.new
type = params[:type]
raise "Unrecognized type: #{type}" unless %w(products listings).include?(type)
model = Object.const_get(type.classify)
page = params[:page] || 1
scroll_id = params[:scroll_id]
if scroll_id
  scroll_response = model.__elasticsearch__.client.scroll(scroll_id: scroll_id, scroll: '10m')
  ids = scroll_response['hits']['hits'].map { |l| l['_id'] }
  total = scroll_response['hits']['total']
else
  search_response = model.search_by(*prepare_search_params(model, params), { size: PER_PAGE, scroll: '10m' })
  ids = search_response.results.map { |p| p.id }.compact
  total = search_response.results.total
  scroll_id = search_response.response._scroll_id
end
last_page = (total / (PER_PAGE * page.to_i)).zero?
version = params[:version] || Time.now.strftime("%Y%m%d%H%M%S")
# unless last_page
#   # Start next page
#   PopulateApiCacheJob.perform_async Oj.dump(params.merge!({ scroll_id: scroll_id, version: version, type: type, page: page + 1 }))
edit -t
PER_PAGE = 200
edit -t
measure(true) do
  records = model.include_api_info.where(id: ids).to_a
  records.size.times do |i|
    r = records.shift
    body[type] << Object.const_get("#{model}Serializer").new(r).as_json(root: false)
  end
end
body = { type => [ ] }
measure(true) do
  records = model.include_api_info.where(id: ids).to_a
  records.size.times do |i|
    r = records.shift
    body[type] << Object.const_get("#{model}Serializer").new(r).as_json(root: false)
  end
end
body
body['products].length
body['products'].length
total
params['keywords'] = 'hdmi'
params
params['type'] 'listings'
params['type'] = 'listings'
edit -t
total
measure(true) do
  records = model.include_api_info.where(id: ids).to_a
  records.size.times do |i|
    r = records.shift
    body[type] << Object.const_get("#{model}Serializer").new(r).as_json(root: false)
  end
end
GC.enable
GC.start
params
edit -t
measure(true) do
  records = model.include_api_info.where(id: ids).to_a
  records.size.times do |i|
    r = records.shift
    body[type] << Object.const_get("#{model}Serializer").new(r).as_json(root: false)
  end
measure(true) do
  body = { type => records.map { |r| Object.const_get("#{model}Serializer").new(r).as_json(root: false) } }
end
measure(true) do
  records = model.include_api_info.where(id: ids).to_a
  body = { type => records.map { |r| Object.const_get("#{model}Serializer").new(r).as_json(root: false) } }
end
PER_PAGE
PER_PAGE = 50
params
measure do
@api_cache = RdsApiCache.new
edit -t
GC.start
memory_before = `ps -o rss= -p #{Process.pid}`.to_i/1024
measure(true) do
edit -t
records = model.include_api_info.where(id: ids)
GC.enable && GC.start
memory_before = `ps -o rss= -p #{Process.pid}`.to_i/1024
GC.start
records = nil
GC.start
memory_before = `ps -o rss= -p #{Process.pid}`.to_i/1024
edit -t
edit
memory_before = `ps -o rss= -p #{Process.pid}`.to_i/1024
GC.start
memory_before = `ps -o rss= -p #{Process.pid}`.to_i/1024
GC.start(full_mark: true, immediate_sweep: true, immediate_mark: false)
memory_before = `ps -o rss= -p #{Process.pid}`.to_i/1024
GC.start(full_mark: true, immediate_sweep: true, immediate_mark: true)
memory_before = `ps -o rss= -p #{Process.pid}`.to_i/1024
edit
body
body[type]
body[type].length
PER_PAGE = 200
edit -t
edit
records = model.include_api_info.where(id: ids)
edit
PER_PAGE = 500
edit
params
parms.delete(:page, :scroll_id, :version)
params.delete(:page, :scroll_id, :version)
params.delete(:page)
params.delete(:scroll_id)
params.delete(:version)
params
edit
total
edit
edit -t
edit
client = Mysql2::Client.new(:host => "localhost", :username => "root")
results = client.query('SELECT * FROM products')
client
results = client.query('SELECT * FROM products')
client
Product.all.to_sql
results = client.query('SELECT * FROM products')
client = Mysql2::Client.new(:host => "localhost", :username => "root")
results = client.query('SELECT * FROM products')
client = Mysql2::Client.new(:host => "localhost", :username => "root", database: 'rvx-rds')
results = client.query('SELECT * FROM products')
results.class
results.count
results.all
results.first
measure do
  client = Mysql2::Client.new(:host => "localhost", :username => "root", database: 'rvx-rds')
results.to_a
measure do
  client = Mysql2::Client.new(:host => "localhost", :username => "root", database: 'rvx-rds')
  results = client.query('SELECT * FROM products')
  results.to_a
end
measure do
  Product.all.to_a
end
Product.connection
measure do
  Product.all.to_a
end
measure do
  Product.all.to_a
end
Product.all
measure do
results = client.query('select * from products')
exit
measure do 
  client = Mysql2::Client.new(:host => "localhost", :username => "root", database: 'rvx-rds', :reconnect = true)
measure do 
  client = Mysql2::Client.new(:host => "localhost", :username => "root", database: 'rvx-rds', reconnect: true)
  results = client.query('SELECT * FROM products')
  results.to_a
end
edit
Product.include_api_info.first
p = Product.first
client
client = Mysql2::Client.new(:host => "localhost", :username => "root", database: 'rvx-rds', reconnect: true)
edit
results.to_h
results.to_a
edit
products.to_h
products.to_a
Product.includes(:listings).first.to_sql
Product.includes(:listings).to_sql
Product.joins(:listings).to_sql
edit
edit -t
edit
measure do
  Product.joins("left outer join listings").to_a
end
edit
Product.limit(100).joins(:listings).to_a
Product.joins(:listings).limit(100).to_a
products = Product.limit(200).joins(:listings).merge( Listing.limit(100))
p = products.first
p.listings
products = Product.limit(200).joins(:listings).merge( joins(:listings))
products = Product.limit(200).joins(:listings).merge(-> joins(:listings))
products = Product.limit(200).joins(:listings).merge(-> { joins(:listings) })
p = products.first
p.listings.size
exit
products = Product.includes_api_info
products = Product.include_api_info
products = Product.include_api_info.limit(200)
Listing.limit(10).joins(:source).select('url','source.name', 'source.pretty_name').map
Listing.limit(10).joins(:source).select('url','sources.name', 'sources.pretty_name').map
Listing.limit(10).joins(:source).select('url','sources.name', 'sources.pretty_name').map do |listing|
  {
    source_name: listing.source.name,
    pretty_name: listing.source.pretty_name,
    url: listing.url
  }
end.uniq
edit
l = Listing.limit(10).joins(:source).select('url','sources.name', 'sources.pretty_name').first
edit
Listing.limit(10).includes(:source).select('url','sources.name', 'sources.pretty_name').map do |listing|
edit
exit
edit
exit
Product.limit(0).all
edit
p = Product.first
p.limited_listings.first
p.limited_listings
p.limited_listings.first
exit
require ​'ruby-prof'
require  'ruby-prof
'
xit
exit
require 'ruby-prof'
edit
pwd
edit
RubyProf.stop
edit
RubyProf.stop
edit
exit
Product.count
Product.reindex_alias
Listing.reindex_alias
exit
edit -t
RubyProf.stop
edit -t
Sidekiq::Queue.new('api_cache_queue')
q = _
q.size
q.clear
edit -t
eixt
exit
edit
exit
edit
exit
"16.7 million".parse_csv
params = {keywords: _.first}
params.to_param
params = Hashie::Mash.new({keywords: '16.7+million'})
page = params.delete(:page) || 1
cache = RdsApiCache.new
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
cache_params = cache_params.each { |_k, v| v.respond_to?(:downcase) ? v.downcase! : v }.to_param
params = Hashie::Mash.new({keywords: '16.7 million'})
page = params.delete(:page) || 1
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
cache_params = cache_params.each { |_k, v| v.respond_to?(:downcase) ? v.downcase! : v }.to_param
version = params[:version] || cache.fetch("#{type}:#{cache_params}:current_version").try(:[], 'version')
type = 'products'
version = params[:version] || cache.fetch("#{type}:#{cache_params}:current_version").try(:[], 'version')
"#{type}:#{cache_params}:current_version"
exit
params = Hashie::Mash.new({keywords: ' '})
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
cache_params = cache_params.each { |_k, v| v.respond_to?(:downcase) ? v.downcase! : v }.reject!{|k,v| v.blank?}.to_param
params = Hashie::Mash.new({keywords: 'hdmi'})
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
cache_params = cache_params.each { |_k, v| v.respond_to?(:downcase) ? v.downcase! : v }.reject!{|k,v| v.blank?}.to_param
cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h
cache_params = cache_params.each { |_k, v| v.respond_to?(:downcase) ? v.downcase! : v }.reject{|k,v| v.blank?}.to_param
redis = Redis.current
redis.keys
exit
params = Hashie::Mash.new({keywords: ' HDMI,'})
edit
params
edit
cleanp = clean_params(params)
edit
params = Hashie::Mash.new({keywords: ' HDMI,'})
cleaned_params = clean_params(params)
params
params.class
params['page'] = 2
cleaned_params = clean_params(params)
5.is_a? Numeric
def clean_params(params)
  params.each do |k, v|
    next if v.is_a? Numeric
    params[k] = v.split(',').reject(&:blank?).join(',') # for when commas are entered followed by nothing
    params[k].strip!; params[k].downcase!; params[k].blank?
  end
end
clean_params(params)
exit
ids = Product.limit(200).ids
Product.include_api_info.where(id: ids).to_sql
Product.include_api_info.where(id: ids)
2016080355110 > 2016080303051
exit
keyword = 'led'
type = 'listings'
cache = RdsApiCache.new
version = Time.now.strftime("%Y%m%d%H%M%S")
params = Hashie::Mash.new({ keywords: keyword })
params.merge!({ type: type, version: version })
cache.set_value("#{type}:#{params.to_param}:in_progress", { version: version }, ex: 6.hours)
PopulateApiCacheJob.new.perform Oj.dump(params.merge!({ type: type, version: version }))
keyword = 'led'
type = 'listings'
cache = RdsApiCache.new
version = Time.now.strftime("%Y%m%d%H%M%S")
params = Hashie::Mash.new({ keywords: keyword })
cache.set_value("#{type}:#{params.to_param}:in_progress", { version: version }, ex: 6.hours)
params.merge!({ type: type, version: version })
PopulateApiCacheJob.new.perform Oj.dump(params.merge!({ type: type, version: version }))
edit -t
measure do
  keyword = 'led'
  type = 'listings'
  cache = RdsApiCache.new
  version = Time.now.strftime("%Y%m%d%H%M%S")
  params = Hashie::Mash.new({ keywords: keyword })
  cache.set_value("#{type}:#{params.to_param}:in_progress", { version: version }, ex: 6.hours)
  PopulateApiCacheJob.new.perform Oj.dump(params.merge!({ type: type, version: version }))
end
edit
exit
edit
exit
edit
exit
edit
version, scroll = nil
version
scroll
exit
edit
GC.enable
GC.start
728 * 2
exit
edit
exit
search_params ||= 200
search_params
search_params ||= 124
edit -t
params = {keywords: 'hdmi'}
model = Product
search_params ||= prepare_search_params(model, params)
search_params
search_params = nil
search_params ||= prepare_search_params(model, params)
terms, operator, filters = prepare_search_params(model, params)
terms
operator
filters
terms, operator, filters ||= prepare_search_params(model, params)
params =     params = Hashie::Mash.new(Oj.load(params))
params = Hashie::Mash.new(Oj.load(params))
params = Hashie::Mash.new({keywords: 'led', type: 'products'})
type = params[:type]
model = Object.const_get(type.classify)
cache_params, last_page, prefix, scroll_id, search_params, total, version = nil
page = params[:page] || 1
scroll_id ||= params[:scroll_id]
search_params ||= prepare_search_params(model, params)
search_response = model.search_by(*search_params, { size: PER_PAGE, scroll: '10m' })
PER_PAGE = 200
search_response = model.search_by(*search_params, { size: PER_PAGE, scroll: '10m' })
ids = search_response.results.map { |p| p.id }.compact
ids = search_response.results.map { |p| p.id }.compact!
ids = search_response.results.map { |p| p.id }.compact!; search_response.results.map { |p| p.id }
total ||= search_response.results.total
ids = search_response.results.map! { |p| p.id }.compact
ids = search_response.results
ids = search_response.results.results
ids = search_response.results.results.class
ids = search_response.results.results.map! {|r| r['_id'] }
ids = search_response.results.results
ids = search_response.results.results.class
ids = search_response.results.results.map! {|r| r['_id'] }
search_response.results.results.map! { |p| p.id }
search_response.results.results.first.class
scroll_id = search_response.response._scroll_id
scroll_response = model.__elasticsearch__.client.scroll(scroll_id: scroll_id, scroll: '10m')
total ||= scroll_response['hits']['total']
ids = scroll_response['hits']['hits'].map! { |l| l['_id'] }
exit
edit
edit -t
edit
exit
edit -t
edit
PopulateApiCacheJob.reload!
self.send(:include, PopulateApiCacheJob)
require 'populate_api_cache_job'
edit -t
edit
exit
edit
require 'populate_api_cache_job'
edit
require 'populate_api_cache_job'
load 'populate_api_cache_job'
load '/Users/jonathan/rvx-rds/app/jobs/populate_api_cache_job.rb'
edit
load '/Users/jonathan/rvx-rds/app/jobs/populate_api_cache_job.rb'
edit
exit
pp GC.stat
exit
edit -t
edit
p = Product.first.lazy
p = Product.first
p.load
p.touch
p.load_dependency
p.load_dependency(:listings)
redis = Redis.current
redis.connected?
redis.connect
redis.connected?
redis.quit
redis.connected?
redis.anything
redis.connected?
exit
edit
GC.stat
puts ObjectSpace.count_objects
GC.start
puts ObjectSpace.count_objects
exit
GC::INTERNAL_CONSTANTS
exit
GC::INTERNAL_CONSTANTS
edit
edit -t
GC.stats
GC.stat
exit
GC.stat
GC.stat[:heap_length]
GC.stat[:heap_sorted_length]
pp GC.stat; nil
GC.stat[:heap_allocated_pages]
GC.start
GC.stat[:heap_allocated_pages]
GC.stat
edit
:wq
edit
x
x = nil
edit
str = '
str = 'x'
ObjectSpace.memsize_of(str)
require 'objspace'
ObjectSpace.memsize_of(str)
records = Product.include_api_info.limit(100).load
ObjectSpace.memsize_of(records)
records = []
ObjectSpace.memsize_of(records)
GC.stat
GC.start
GC.stat
edit
exit
Product.count
edit
exit
GC.stat
exit
GC.stat
GC.start
GC.stat
x = Array.new(350000) { Object.new}; nil
GC.stat
x = nil
GC.stat
y = Array.new(350000) { Object.new}; nil
GC.stat
31039425 / 1028
31039425 / 1028 / 1028
GC::INTERNAL_CONSTANTS[:HEAP_OBJ_LIMIT]
exit
v = 'hdmi,,bluetooth'
v.gsub(/,(,|$|\s+)/,'').strip
v.gsub(/,{2+}/,',')gsub(/,(,|$|\s+)/,'').strip
v.gsub(/,{2+}/,',').gsub(/,(,|$|\s+)/,'').strip
v.gsub(/,{2,}/,',').gsub(/,(,|$|\s+)/,'').strip
v = 'hdmi,,bluetooth   '
v.gsub(/,{2,}/,',').gsub(/,(,|$|\s+)/,'').strip
v = 'hdmi, ,bluetooth   '
v.gsub(/,{2,}/,',').gsub(/,(,|$|\s+)/,'').strip
v = 'hdmi, ,  bluetooth   '
v.gsub(/,{2,}/,',').gsub(/,(,|$|\s+)/,'').strip
v.gsub(/,\s*{2,}/,',').gsub(/,(,|$|\s+)/,'').strip
v = 'hdmi, ,  bluetooth   '
v.gsub(/,\s*{2,}/,',').gsub(/,(,|$|\s+)/,'').strip
v.gsub(/,\s+{2,}/,',').gsub(/,(,|$|\s+)/,'').strip
v = 'hdmi,,bluetooth   '
v.gsub(/,\s+{2,}/,',').gsub(/,(,|$|\s+)/,'').strip
reg = /(?:^|,)(?=[^"]|(")?)"?((?(1)[^"]*|[^,"]*))"?(?=,|$)/
v.match(reg)
v =~ reg
v.match reg
v
v.gsub(/[\s]{1,}?|,/, ',')
v.gsub(/[\s+]{1,}?|,/, ',')
v.gsub(/[\s+]{1,}?|,{1,}/, ',')
v.gsub(/,{1,}(?=\s+)?/, ',')
v
v = 'hdmi, ,bluetooth , something,   '
v.gsub(/,{1,}(?=\s+)?/, ',')
v.gsub(/,{1,}?(?=\s+)?/, ',')
v.gsub(/,{1,}?\s+?/, ',')
v.gsub(/,{1,}?\s+?,{1,}/, ',')
v.gsub(/\s+,{1,}?\s+?,{1,}/, ',')
v.gsub(/\s+?,{1,}?\s+?,{1,}/, ',')
v.gsub(/\s+?,,{1,}?\s+?,{1,}/, ',')
v.gsub(/\s+?,(,{1,}?|\s+?),{1,}/, ',')
v.gsub(/\s+?,(,{1,}?|\s+?)/, ',')
v.gsub(/\s+?,(,{1,}?|\s+?),{1,}?/, ',')
v.gsub(/\s+?,(,{1,}?|\s+?)/, ',')
v.gsub(/\s+?,(,{1,}?|\s+?),{1,}/, ',')
v.gsub(/\s+?,{1,}?\s+?,{1,}/, ',')
v.gsub(/,\s*,/, ',')
v.gsub(/\s*?,\s*?,/, ',')
v.gsub(/\s*?,\s*?,?/, ',')
v
v.gsub(/\s*?,\s*?,?/, ',')
v.gsub(/\s+,/, ',')
v.gsub(/\s+,/, ',').gsub(/,\s+/, ',')
v.gsub(/\s+,/, ',').gsub(/,\s+/, ',').gsub(/,{1,}/, ',')
v.gsub(/\s+,/, ',').gsub(/,\s+/, ',').gsub(/,{1,}/, ',').gsub(/,$/, '')
v.gsub(/\s+,/, ',').gsub(/,\s+/, ',').gsub(/,{1,}/, ',').gsub(/,$/, '').gsub(/\s+/, ' ')
v.gsub(/\s+,/, ',').gsub(/,\s+/, ',').gsub(/,{1,}/, ',').gsub(/,$/, '').gsub(/\s+/, ' ').gsub(/\A\s+/, '')
exit
edit -t
times.map(&:to_f)
times = times.map(&:to_f)
times
123.132.to_f
123.132.to_i
times.map!(&:to_i)
times
exit
DateTime.now.strftime("%Y%m%d%H%M%S")
Date.strptime(_, "%Y%m%d%H%M%S")
lastused = _
lastused < Date.now - 24.hours
lastused < DateTime.now - 24.hours
str = "20160808103254"
DateTime.strptime(str, "%Y%m%d%H%M%S") 
DateTime.strptime(str, '%Y%m%d%H%M%S') 
val = Oj.dump('hello')
Oj.load(val)
val = Oj.dump(true)
Oj.load(val)
DateTime.strptime(nil, "%Y%m%d%H%M%S") 
DateTime.strptime(nil.to_s, "%Y%m%d%H%M%S") 
cache = RdsApiCache.new
key = 'listings:keywords=led'
last_seen_at = cache.fetch("#{key}:last_seen")
exit
nil > 0
page = 1
page = 100
empty = {type => [], pagination: { page: page, per_page: 200, total: 1 }, version: 1  }.to_json
type = 'products'
empty = {type => [], pagination: { page: page, per_page: 200, total: 1 }, version: 1  }.to_json
exit
hash = {b: 'b', c: 'c', a: 'a'}
params = Hashe::Mash.new(hash)
params = Hashie::Mash.new(hash)
params.class
hash = params.slice(:a, :b, :c)
hash
hash.class
params = Hashie::Mash.new(source_ids: '1,2,3', keywords: 'led')
params.slice(:brands, :category_ids, :keywords, :models, :scrape_ids, :source_ids)
_.class
params.slice(:brands, :category_ids, :keywords, :models, :scrape_ids, :source_ids).sort
params.slice(:brands, :category_ids, :keywords, :models, :scrape_ids, :source_ids).sort!
params
params.sort!
params
params.slice(:brands, :category_ids, :keywords, :models, :scrape_ids, :source_ids).to_param
redis =     # cache_params = params.slice(:keywords, :brands, :models, :source_ids, :scrape_ids, :category_ids).sort.to_h.to_param
redis = Redis.current.freeze
redis = Redis.new
api_cache = RdsApiCache.new.freeze
api_cache.frozen?
api_cache = 'somethign else'
API_CACHE = RdsApiCache.new.freeze
API_CACHE = 'somethign else'
API_CACHE
require 'benchmark/ips'
Benchmark.ips do |x|
edit
items.each_slice(10) do |chunk|
averages = []
items.each_slice(10) do |chunk|
  averages << (chunk.reduce(:+) / 10.0)
end
averages
items.each_slice(5) do |chunk|
  averages << (chunk.reduce(:+) / 5.0)
averages = [\
averages = []
items.each_slice(5) do |chunk|
  averages << (chunk.reduce(:+) / 5.0)
end
averages
exit
require "json"
require "benchmark"
def measure(no_gc=false, &block)
  if no_gc
    GC.disable
  else
    # collect memory allocated during library loading
    # and our own code before the measurement
    GC.enable
    GC.start
  end
  memory_before = `ps -o rss= -p #{Process.pid}`.to_i/1024
  gc_stat_before = GC.stat
  time = Benchmark.realtime do
    yield
  end
  puts ObjectSpace.count_objects
  unless no_gc
    GC.start(full_mark: true, immediate_sweep: true, immediate_mark: true)
  end
  puts ObjectSpace.count_objects
  gc_stat_after = GC.stat
  memory_after = `ps -o rss= -p #{Process.pid}`.to_i/1024
  puts({
      RUBY_VERSION => {
        gc: no_gc ? 'disabled' : 'enabled',
        time: time.round(2),
        gc_count: gc_stat_after[:count] - gc_stat_before[:count],
        memory: "%d MB" % (memory_after - memory_before)
      }
  }.to_json)
end
measure do
  100.times do { ['Brand'.freeze, 'Brand Name'.freeze] }
measure do
  100.times { ['Brand'.freeze, 'Brand Name'.freeze] }
end
measure(true) do
  100.times { ['Brand'.freeze, 'Brand Name'.freeze] }
end
measure(true) do
  100.times { ['Brand', 'Brand Name'] }
end
GC.enable
GC.start
measure(true) do
  100.times { ['Brand', 'Brand Name'] }
end
measure(true) do
  10000.times { ['Brand', 'Brand Name'] }
end
measure(true) do
  100000.times { ['Brand', 'Brand Name'] }
end
exit
url = 'https://www.amazon.com/LEDENET-Lighting-Controller-Limitless-Long-range/dp/B01AUPDLAU/ref=sr_1_4452/192-4604941-0263538?s=hi&ie=UTF8&qid=1467846931&sr=1-4452'
scrape_id = 1
source_category_id = 1
Scrape.find 1
source = Source.find 4
source.source_categories.first
source.source_categories.active.first
souce_category_id = 791
scrape_id
url
url = 'https://www.amazon.com/LEDENET-Lighting-Controller-Limitless-Long-range/dp/B01AUPDLAU/ref=sr_1_4452/192-4604941-0263538?s=hi&ie=UTF8&qid=1467846931&sr=1-4452'
scrape_id = 1
source_category_id = 791
exit
url = 'https://www.amazon.com/LEDENET-Lighting-Controller-Limitless-Long-range/dp/B01AUPDLAU/ref=sr_1_4452/192-4604941-0263538?s=hi&ie=UTF8&qid=1467846931&sr=1-4452'
scrape_id = 1
source_category_id = 791
AmazonProductDetailJob
AmazonProductDetailJob.peform_async(scrape_id, source_category_id, url)
AmazonProductDetailJob.perform_async(scrape_id, source_category_id, url)
l = Listing.last
l.products
Listing.last
l = Listing.last
nil.strip
l = Listing.last
l.destroy
AmazonProductDetailJob.perform_async(scrape_id, source_category_id, url)
l = Listing.last
Listing.where("data_hash like '%Part Number%'".count
Listing.where("data_hash like '%Part Number%'").count
Listing.joins(:products).where("data_hash like '%Part Number%'").count
Listing.joins(:products).where("data_hash like '%Part Number%'").first
_.products
Listing.where.not(id: Listing.joins(:products).where("data_hash like '%Part Number%'").ids).count
Listing.where(scrape_id: Scrape.where(source: Source.find_by(name: 'amazonus')).where.not(id: Listing.joins(:products).where("data_hash like '%Part Number%'").ids).count
Listing.where(scrape_id: Scrape.where(source: Source.find_by(name: 'amazonus'))).where.not(id: Listing.joins(:products).where("data_hash like '%Part Number%'").ids).count
Listing.where(scrape_id: Scrape.where(source: Source.find_by(name: 'amazonus'))).where("data_hash like '%Part Number%'").where.not(id: Listing.joins(:products).ids).count
ProductListing.uniq.pluck(:listing_id).to_sql
ProductListing.uniq.pluck(:listing_id)
exit
l = Listing.last
ProductListing.where(listing_id: l.id)
ProductListing.where(listing_id: l.id).destroy_all
l.reload
l.products
PopulateAmazonProductsJob.new.perform(l.id)
load '/Users/jonathan/rvx-rds/app/jobs/populate_amazon_products_job.rb'
PopulateAmazonProductsJob.new.perform(l.id)
l.reload
l.products
scrape_ids = Scrape.where(source: Source.find_by(name: 'amazonus')).ids
listing_ids = ProductListing.uniq.pluck(:listing_id); nil
listing_ids.count
Listing.where(scrape_id: scrape_ids).where("data_hash like '%Part Number%'").where.not(id: listing_ids).select(:id).find_in_batches(batch_size: 50) do |listings|
  listings.each { |l| PopulateAmazonProductsJob.perform_async(l.id)}
end
listing_ids
Listing.first
Listing.first.products
exit
scrape_ids = Scrape.where(source: Source.find_by(name: 'amazonus')).ids
listing_ids = ProductListing.uniq.pluck(:listing_id); nil
Listing.where(scrape_id: scrape_ids).where("data_hash like '%Part Number%'").where.not(id: listing_ids).select(:id).find_in_batches(batch_size: 50) do |listings|
  listings.each { |l| PopulateAmazonProductsJob.perform_async(l.id)}
end
Listing.where(scrape_id: scrape_ids).where("data_hash like '%Part Number%'").where.not(id: listing_ids).first
l = _
l.products
exit
scrape_ids = Scrape.where(source: Source.find_by(name: 'amazonus')).ids
listing_ids = ProductListing.uniq.pluck(:listing_id); nil
Listing.where(scrape_id: scrape_ids).where("data_hash like '%Part Number%'").where.not(id: listing_ids).select(:id).find_in_batches(batch_size: 50) do |listings|
  listings.each { |l| PopulateAmazonProductsJob.perform_async(l.id)}
end
l = Listing.where(scrape_id: scrape_ids).where("data_hash like '%Part Number%'").where.not(id: listing_ids).first
l.proudcts
l.products
l = Listing.where(scrape_id: scrape_ids).where("data_hash like '%Part Number%'").where.not(id: listing_ids).count
scrape_ids = Scrape.where(source: Source.find_by(name: 'amazonus')).ids
listing_ids = ProductListing.uniq.pluck(:listing_id); nil
l = Listing.where(scrape_id: scrape_ids).where("data_hash like '%Part Number%'").where.not(id: listing_ids).count
l = Listing.where(scrape_id: scrape_ids).where("data_hash like '%Part Number%'").where.not(id: listing_ids).first
l
listing_ids.count
scrape_ids = Scrape.where(source: Source.find_by(name: 'amazonus')).ids
listing_ids = ProductListing.uniq.pluck(:listing_id); nil
listing_ids.count
Listing.where(scrape_id: scrape_ids).where("data_hash like '%Part Number%'").where.not(id: listing_ids).select(:id).find_in_batches(batch_size: 50) do |listings|
  listings.each { |l| PopulateAmazonProductsJob.perform_async(l.id)}
end
scrape_ids = Scrape.where(source: Source.find_by(name: 'amazonus')).ids
listing_ids = ProductListing.uniq.pluck(:listing_id); nil
listing_ids.count
Listing.where(scrape_id: scrape_ids).where("data_hash like '%Part Number%'").where.not(id: listing_ids).first
Listing.where(scrape_id: scrape_ids).where("data_hash like '%Part Number%'").where.not(id: listing_ids).lst
Listing.where(scrape_id: scrape_ids).where("data_hash like '%Part Number%'").where.not(id: listing_ids).last
scrape_ids = Scrape.where(source: Source.find_by(name: 'amazonus')).ids
listing_ids = ProductListing.uniq.pluck(:listing_id); nil
Listing.where(scrape_id: scrape_ids).where("data_hash like '%Part Number%'").where.not(id: listing_ids).first
Listing.where(scrape_id: scrape_ids).where("data_hash like '%Part Number%'").where.not(id: listing_ids).last
Listing.where(scrape_id: scrape_ids).where("data_hash like '%Part Number%'").where.not(id: listing_ids).last 2
exit
Product.query_fields
Product.query_fields[:filter_fields]
Product.query_fields[:filter_fields].keys.each {|key| puts key }
Product.query_fields[:filter_fields].each {|key, _v| puts key }
params = {}
params[:keywords] = 'Solar Module'
keywords = [params[:keywords].to_s, params[:brands], params[:models]].join(' ')
params[:keywords].to_s
keywords = [params[:keywords], params[:brands], params[:models]].reject!(&:nil?).join(' ')
keywords = [params[:keywords], params[:brands], params[:models]].join(' ')
params[:brands] = 'Sony,LG'
params
terms = { keywords: params[:keywords] }
filters = {}
Product.query_fields[:filter_fields].each do |key, _v|
  if params[key].present?
    params[key].downcase!
    filters[key] = params[key].split(',').map! do |val|
      val.strip!
      val
    end
  end
end
params
filters
keywords = [params[:keywords], params[:brands], params[:models]].reject!(&:nil?).join(' '.freeze)
keywords = [params[:keywords], filters[:brands], filters[:models]].reject!(&:nil?).join(' '.freeze)
filters.delete_if { |k, _v| [:brands, :models].include?(k) }
filters
keywords
terms = { keywords: keywords }
operator = 'OR'
['str', ['1', '2'], '3'].join(' ')
['ssdf', nil, 0].compact
['ssdf', nil, 0].compact!
['ssdf', 0].compact!
arr = ['ssdf', nil, 0]
arr.compact!
arr
exit
str = 'hey'.freeze
str.frozen?
str << 'oo'
exit
GC.stat
exit
uri = URI("https://data.beta.ruvixx.com/api/v1/#{type}?#{param}")
uri = URI("https://data.beta.ruvixx.com/api/v1/products?keywords=hdmi")
req = Net::HTTP::Get.new(uri)
req.basic_auth 'rvx', '1Admin11'
res = Net::HTTP.start(uri.hostname, uri.port) {|http|
  http.request(req)
}
uri.hostname
uri.post
uri.port
uri = URI("https://data.beta.ruvixx.com/api/v1/#{type}?#{param}")
uri = URI("http://data.beta.ruvixx.com/api/v1/#{type}?#{param}")
uri = URI("http://data.beta.ruvixx.com/api/v1/products?keywords=hdmi")
req = Net::HTTP::Get.new(uri)
req.basic_auth 'rvx', '1Admin11'
res = Net::HTTP.start(uri.hostname, uri.port) {|http|
  http.request(req)
}
res = JSON.parse(res.body)
res['pagination']
res['pagination']['total']
res['pagination']['total'] / res['pagination']['per_page']
(res['pagination']['total'] / res['pagination']['per_page'].to_f).ceiling
(res['pagination']['total'] / res['pagination']['per_page'].to_f).ceil
pages = (res['pagination']['total'] / res['pagination']['per_page'].to_f).ceil
(2..pages).each do |i|
  puts i
end
edit
Redis.current
uri = URI("https://data.beta.ruvixx.com/api/v1/products/keywords=q54")
uri = URI("https://data.beta.ruvixx.com/api/v1/products/keywords=hdcp+2.2")
req = Net::HTTP::Get.new(uri)
req.basic_auth 'rvx', '1Admin11'
res = Net::HTTP.start(uri.hostname, uri.port) {|http|
  http.request(req)
}
uri = URI("https://data.ruvixx.com/api/v1/products/keywords=hdcp+2.2")
req = Net::HTTP::Get.new(uri)
req.basic_auth 'rvx', '1Admin11'
res = Net::HTTP.start(uri.hostname, uri.port) {|http|
  http.request(req)
}
uri = URI("http://data.ruvixx.com/api/v1/#{type}?#{param}")
type = products
type = 'products'
keywords = "keywords=hdmi"
uri = URI("http://data.ruvixx.com/api/v1/#{type}?#{param}")
param = keywords
uri = URI("http://data.ruvixx.com/api/v1/#{type}?#{param}")
req = Net::HTTP::Get.new(uri)
req.basic_auth 'rvx', '1Admin11'
res = Net::HTTP.start(uri.hostname, uri.port) {|http|
  http.request(req)
}
JSON.parse(res.body)
exit
edit
type = 'listings'
param = "keywords=led"
uri = URI("http://data.ruvixx.com/api/v1/#{type}?#{param}")
req = Net::HTTP::Get.new(uri)
req.basic_auth 'rvx', '1Admin11'
res = Net::HTTP.start(uri.hostname, uri.port) { |http|
  http.request(req)
}
res = JSON.parse(res.body)
pages = (res['pagination']['total'] / res['pagination']['per_page'].to_f).ceil
param = "keywords=usb3.0"
type = 'products'
uri = URI("http://data.ruvixx.com/api/v1/#{type}?#{param}")
req = Net::HTTP::Get.new(uri)
req.basic_auth 'rvx', '1Admin11'
res = Net::HTTP.start(uri.hostname, uri.port) { |http|
  http.request(req)
}
res = JSON.parse(res.body)
pages = (res['pagination']['total'] / res['pagination']['per_page'].to_f).ceil
23 * 200
edit
exit
cache = RdsApiCache.new
cache.keys('api_cache:*')
cache.keys('api_cache')
cache.keys
cache
redis = Redis.new
"#beta_#{ENV['REDIS_NAMESPACE']}:api_cache"
key = _
redis.keys("#{key}*")
redis.keys
redis.keys("#{key}")
key
key.gsub(/#/, '')
key.gsub!(/#/, '')
redis.keys("#{key}")
key
redis.keys("#{key}:*")
redis.keys("#{key}:*last_seen*")
keys = _
exit
Scrape.where(running: true)
Scrape.where(running: true).update_all(running: false)
s = Scrape.last
s.listings.count
s.products.count
s.listings.count
s.products.count
s.listings.count
l = s.listings.last
l.products
l.data_hash
l.screenshots.first
l.screenshots.first.attachment.url
l.reload
Listing.last
Listing.first
s.listings.first
s.listings.map {|l| l.data_hash[:General][:'Product Description'] }
exit
edit
html = <<-HTML
<html><head>
    <style type="text/css">
 h2 {
    color: #CC6600;
    font-size: medium;
    margin: 0 0 0.25em;
    /* for IE Quirks mode */* font-size: small;
  }
    
body {
  background-color: #FFFFFF;
  margin: 0px;
  height: 100%;
  width: 100%;
  color: #333333;
  word-wrap: break-word; 
  font-family: verdana,arial,helvetica,sans-serif;
  font-size: small;
}
td, th {
  font-family: verdana,arial,helvetica,sans-serif;
  font-size: small;
}
hr {
  border-top: 1px dashed #999999;
  height: 1px;
  color: #FFFFFF;
  margin: 3px 0px;
  border-style: dashed none none none;
  background-color: white
}
ul { 
  list-style-type: none;
  margin: 0px; 
  padding: 0px; 
}
ul li {
  margin: 0.5em 0em;
}
ul li ul {
  list-style-type: none;
  margin-left: 25px;
}
ul li ul li {
  margin: 0em;
}
div.bucket { padding: 5px 0em; }
div.bucket div.content { margin: 0.5em 0px 0em 25px; }
hr { margin-left: 0px; }
.bucket h3 {
  color: #000;
  font-size: 1em;
  font-weight: bold;
  margin: 0px 0px 0.25em 0px;
}
hr.bucketDivider { clearhr.bucketDivider { clearhrdinghr.bx hr.bucketDivider {
                      on                       #                      on                      CC  }
   in   in c   in   in c   in   in c   at   in   in c e;   in   in c   c   in   in c ;
   in   in c   in:   in   in c   in:   in lo   in   in c   in:   in   in c   in:   in lo   in#p   in   in c   in:h2.pr   in  sc   in   in c   in:   in   in c   in:   in lo   indu   in   in c   in:tyCle   {
   in   in c   in:   in   in c       in   in c px   in   in c   icr   in   in c   intD   in   in c   in:   in   in c       em   in   in c   in:   in ipti   i3.   in   in c   in:   ine    in   in c  gh   in   in c   olor   in 33   in   in c   in: 3em;   in   iin   in   in c   in:   in
                    prod            on             marg                    prod        -                    prod            on             maat                    pro-t           
              em              emft              em   tD              em            : 0               em              emft              em   tD              em      gi              em              emft              em   tD              em            : 0             .              em              emft              em   tD              em            : ;
              em              emft              em   tD              em            : 0               em              emft              em   tD              em      gi              em              emft    
edit
iframe = Nokogiri::HTML(html) { |config| config.noblanks }
desc_hash = {}
scrape_id = 1
listing = Listing.last
headers_arr = parse_values(iframe, 'h3.productDescriptionSource'.freeze)
contents_arr = parse_values(iframe, 'div.productDescriptionWrapper'.freeze)
headers_arr.length
contents_arr.length
if headers_arr.blank? && [headers_arr.length, contents_arr.length].max == 1
  desc_hash[:"Product Description"] = contents_arr[0]
else
  headers_arr.length.times do |idx|
    header = headers_arr.shift
    header = "Section #{idx + 1}" if header.blank?
    desc_hash[header.to_sym] = contents_arr.shift
  end
end
desc_hash
headers_arr
contents_arr
headers_arr = parse_values(iframe, 'h3.productDescriptionSource'.freeze)
contents_arr = parse_values(iframe, 'div.productDescriptionWrapper'.freeze)
desc_hash = {}
headers_arr
if headers_arr[0].blank? && [headers_arr.length, contents_arr.length].max == 1
  desc_hash[:"Product Description"] = contents_arr[0]
else
  headers_arr.length.times do |idx|
    header = headers_arr.shift
    header = "Section #{idx + 1}" if header.blank?
    desc_hash[header.to_sym] = contents_arr.shift
  end
end
headers_arr
desc_hash
image_srcs = iframe.xpath('//img/@src')
exit
edit -t
edit
exit
edit
measure(true) do
  array = []
  10_000_000.times do
    array <<  "a string"
  end
  return nil
edit
measure(true) do
  big_array
end
measure do
  big_array
end
edit
measure(true) do
  frozen_array
end
edit
GC.start
measure(true) do
  big_array
end
measure(true) { frozen_array }
edit
measure(true) { big_array }
edit
measure(true) { big_array }
measure(true) { frozen_array }
edit
exit
edit
GC.enable
100.times { GC.start }
GC.stat
big_array
GC.stat
10.times { GC.start
}
GC.stat
edit -t
edit
GC.start
arr = Array.new(10000) { ' x '}
edit
arr = Array.new(100000) { ' word '}
measure do
  arr = arr.map(&:strip);
end
arr = Array.new(1000000) { ' word '}
measure(true) do
  arr = arr.map(&:strip);
end
arr = Array.new(1000000) { ' WORD '}
GGe
exexit
exit
edit
GC.enable
20.times {GC.start}
arr = nil
20.times {GC.start}
edit
ObjectSpace.count_objects
GC.stat
ObjectSpace.count_objects
edit
exit
Product.first.sources
Product.first
source_ids = []
source_ids.present?
exit
Product.include_detail_info(4).where(id: 1)
exit
Product.include_detail_info(4).where(id: 1)
Product.joins(listings: [:source]).where('listings.scrape_id': Scrape.where(source_id: 4).ids)
Product.joins(listings: [:source]).where('listings.scrape_id': Scrape.where(source_id: 4).ids).where(id: 1)
p = _.first
p.listings
Product.joins(listings: [:source]).where('listings.scrape_id': Scrape.where(source_id: 4).ids).where(id: 1).distinct
exit
Product.include_detail_info(4).where(id: 1)
exit
Product.include_detail_info(4).where(id: 1)
Product.joins(listings: [:source]).where('listings.scrape_id': Scrape.where(source_id: 4).ids).
includes(:listings).where('listings.scrape_id': Scrape.where(source_id: 4).pluck(:ids)).where(id: 1)
includes(:listings).where('listings.scrape_id': Scrape.where(source_id: 4).pluck(:ids)).where(id: 1).references(:products)
Product.joins(listings: [:source]).where('listings.scrape_id': Scrape.where(source_id: 4).ids).
includes(:listings).where('listings.scrape_id': Scrape.where(source_id: 4).pluck(:ids)).where(id: 1).references(:products)
Product.joins(listings: [:source]).where('listings.scrape_id': Scrape.where(source_id: 4).ids)
Product.joins(listings: [:source]).where('listings.scrape_id': Scrape.where(source_id: 4).ids).to_sql
query = Product.joins(listings: [:source]).where('listings.scrape_id': Scrape.where(source_id: 4).ids)
query.includes(:listings).where('listings.scrape_id': Scrape.where(source_id: [4]).pluck(:ids))
query.includes(:listings).where('listings.scrape_id': Scrape.where(source_id: [4]).pluck(:id))
p = _.first
p.listings
exit
Product.include_detail_info(4).where(id: 1)
p = _
p = _.first
p.listings
p.listings[0]
p.listings.size
p.listings
exit
Product.include_detail_info(4).where(id: 1)
p = _.first
p.listings
Product.joins(:listings).where('listings.scrape_id': Scrape.where(source_id: 4))
p = _.first
p.listings
Product.includes(:listings).where('listings.scrape_id': Scrape.where(source_id: 4)).to_sql
Product.includes(:listings).where('listings.scrape_id': Scrape.where(source_id: 4))
p = _.first
p.listings
p.listings.size
exit
Product.include_detail_info(4).where(id: 1)
Product.include_detail_info(4).where(id: 1).to_sql
Product.include_detail_info(4).find(1)
exit
Product.include_detail_info(4).find(1)
p = Product.find 1
edit 
edit
p.listings
edit
p.listings
edit
Product.connection
edit
exit
edit
exit
edit
exit
edit
p.listings
p.listings.size
p.screenshots.size
p.listings.flat_map {|l| l.screenshots}.size
edit
source_ids = 4
edit
p.lisitngs.size
p.listings.size
p.listings
p.listings.map(&:source)
p.listings.map(&:source).uniq!
p.sources
p.listings.map(&:source)
sources = p.sources
sources.max_by(&:id)
sources
sources.sort_by(:id)
sources.sort_by(&:id)
sources.sort_by {|s| -s.id}
sources.sort_by(&:-id)
sources.scrapes
sources.map(&:scrapes)
exit
p = Product.include_detail_info.find(1)
p.listings
p.listings.map {|l| l.scrape.source }
exit
p = Product.include_detail_info.find(1)
p.listings.size
p.scrapes
exit
p = Product.include_detail_info.find(1)
p.sources
source = sources.first
source = p.sources.first
source.scrapes.size
source.scrapes.count
source.scrapes
edit
p = _.first
edit
queryyyy.first
query.first
p = _
p.listings.size
p.product_product_manuals.size
p.product_product_manuals.confirmed
p.product_product_manuals
ProductProductManual.first
ProductProductManual.where.not(status: nil)
ProductProductManual.where.not(status: nil).first
ProductProductManual.where(status: 1).first
ppm = _
ProductProductManual.status
ProductProductManual.statuses
ProductProductManual.statuses[:confirmed]
p
p.listings
p.listings.flat_map(&:categories)
cats = _
cats.uniq!
cats.as_json(only: [:id, :name])
exit
p = Product.include_detail_info.find 1
p.single_item_json
exit
p = Product.include_detail_info.find 1
exit
p = Product.include_detail_info.find 1
p.single_item_json
exit
p = Product.include_detail_info.find 1
reload!
p = Product.include_detail_info.find 1
exit
reload!
p = Product.include_detail_info.find 1
p = Product.merge(ProductManual.confirmed).find(1)
query = Product.includes(
  {
    listings: [
      :scrape_listing_sellers,
      { scrape: :source },
      :categories,
      :images,
      :screenshots,
      :scrape_listing_sellers,
      { sellers: :images }
    ]
  },
  { product_product_manuals: :manual }
).merge(ProductManual.confirmed)
relaod!
reload!
p = Product.include_detail_info.find 1
p.single_item_json
reload!
p = Product.include_detail_info.find 1
p.single_item_json
relaod!
reload!
p = Product.include_detail_info.find 1
p.single_item_json
exit
p = Product.include_detail_info.find 1
l = p.listings.first
l.scrape_listing_sellers.size
p.single_item_json
reload!
p = Product.include_detail_info.find 1
p.single_item_json
Redis
Redis.current
exit
Redis.current
Redis.current.flushall
exit
Product.include_detail_info(4).find(1).single_item_json
Oqxtei
qexit
exit
Product.include_detail_info(4).find(1).single_item_json
Product.where("id IN (?)", "1,2,3")
Product.where("id IN ?", "1,2,3")
exit
Product.include_api_info(4).size
Product.include_api_info
xit
exit
[][-1]
p = Product.find 1
sls_map = {}
listings = p.listings
scrape_listing_sellers = listings.flat_map { |l| l.scrape_listing_sellers }
scrape_listing_sellers.uniq!
edit
sls_map.values
sls_map.values.map do |sls|
  {
    id: sls.seller.try(:id),
    name: sls.seller.try(:name) || "UNIDENTIFIED SELLER",
    image_url: sls.seller.try(:image).try(:attachment).try(:url) || "NO SELLER IMAGE",
    min_price_cents: sls.exchanged_min,
    max_price_cents: sls.exchanged_max
  }
end
sls_map.class
sls_map
@arr = [1,2,3]
arr = @arr
arr.map! {|el| el + 10 }
arr
@arr
arr = @arr.dup
arr.map! {|el| el + 10 }
arr
@arr
reload!
products = Product.include_api_info.limit(100)
products.map {|r| ProductSerializer.new(r).as_json(root: false) }
arr
arr[-40..-1
arr[-40..-1]
arr.last(50)
arr.last(1)
reload!
products = Product.include_api_info.limit(100)
products.map {|r| ProductSerializer.new(r).as_json(root: false) }
p = Product.include_api_info.first
ProductSerializer.new(p).as_json(root: false)
p = Product.includes(listings: :scrape_listing_seller).first
p = Product.includes(listings: :scrape_listing_sellers).first
p.listings.size
l = p.listings.first
l.scrape_listing_sellers
products = Product.includes(listings: :scrape_listing_sellers).limit(10)
limited_listings = products.first.listings.to_a
limited_listings.flat_map {|l| l.scrape_listing_sellers }
scrape_listing_sellers.uniq!
sls_map = {}
scrape_listing_sellers.each do |sls|
  next if sls.seller.nil? && sls.min_price.nil?
  seller = sls.seller || Seller.new(name: "NO SELLER")
  sls_map[seller] = sls_map.key?(seller) ?
  [sls_map[seller], sls].max_by(&:updated_at)
  :
  sls
end
reload!
products = Product.includes(listings: :scrape_listing_sellers).limit(10)
products = Product.include_api_info.limit(100)
reload!
products = Product.include_api_info.limit(100)
products.map {|r| ProductSerializer.new(r).as_json(root: false) }
products = Product.include_api_info.limit(100)
p = products.first
limited_listings = p.listings
sls_map = {}
scrape_listing_sellers = limited_listings.flat_map { |l| l.scrape_listing_sellers }
j    scrape_listing_sellers.uniq!
scrape_listing_sellers.uniq!
scrape_listing_sellers.each do |sls|
  next if sls.seller.nil? && sls.min_price.nil?
  seller = sls.seller || Seller.new(name: "NO SELLER")
  sls_map[seller] = sls_map.key?(seller) ?
  [sls_map[seller], sls].max_by(&:updated_at)
  :
  sls
end
sls_map
sls = sls_map.values.first
sls.seller.present?
sls.seller
sls.seller(:id)
reload!
products = Product.include_api_info.limit(100)
products.map {|r| ProductSerializer.new(r).as_json(root: false) }
redis = Redis.current
redis.flushall
exit
params = Oj.load({\"^o\":\"Hashie::Mash\",\"self\":{\"source_ids\":\"4,8\",\"type\":\"products\"}})
params = Oj.load("{\"^o\":\"Hashie::Mash\",\"self\":{\"source_ids\":\"4,8\",\"type\":\"products\"}}")
params.each do |k, v|
  next if v.is_a? Numeric
  params[k] = v.split(',').reject(&:blank?).join(',') # for when commas are entered followed by nothing
  params[k].strip!; params[k].downcase!;
end
params
cache_params = params.slice(:brands, :category_ids, :keywords, :models, :scrape_ids, :source_ids).to_param
params
PER_PAGE ||= ENV['CACHE_PAGE_SIZE'].try(:to_i) || 200
API_CACHE ||= RdsApiCache.new
type = params[:type]
model = Object.const_get(type.classify)
serializer = Object.const_get("#{model}Serializer")
cache_params, last_page, prefix, scroll_id, search_params, total, version = nil
source_ids = params[:source_ids].try(:split, ',')
page = params[:page] || 1
scroll_id ||= params[:scroll_id]
edit
search_params ||= prepare_search_params(model, params)
search_response = model.search_by(*search_params, { size: PER_PAGE, scroll: '10m'.freeze })
total ||= search_response.results.total
ids = search_response.results.results.map! { |p| p.id }
ids.compact!
scroll_id = search_response.response._scroll_id
search_response = nil
cache_params ||= params.slice(:brands, :category_ids, :keywords, :models, :scrape_ids, :source_ids).to_param
prefix ||= "#{type}:#{cache_params}"
key = "#{prefix}:#{page}"
body = { type => [] }
model.include_api_info(source_ids).where(id: ids).find_in_batches(batch_size: 50) do |records|
  body[type] |= records.map! { |r| serializer.new(r).as_json(root: false) }
end
model.include_api_info(source_ids).where(id: ids).to_sql
model.include_api_info(source_ids).where(id: ids)
model.include_api_info(source_ids).where(id: ids).limit(50)
edit
ids
ids.count
body = { type => [] }
edit
exit
ids = [5, 3, 14, 2, 1, 8, 11, 19, 4, 7, 9, 6, 17, 22, 10, 13, 15, 23, 16, 24, 12, 20, 31, 18, 25, 21, 27, 37, 28, 26, 30, 35, 45, 39, 29, 32, 47, 36, 46, 40, 33, 50, 49, 41, 38, 53, 34, 66, 44, 56, 42, 43, 67, 48, 58, 59, 51, 71, 52, 61, 62, 54, 72, 60, 63, 65, 55, 73, 64, 70, 57, 77, 79, 68, 78, 80, 84, 69, 85, 81, 74, 89, 82, 93, 86, 75, 92, 87, 106, 95, 76, 98, 88, 109, 97, 94, 83, 99, 114, 107, 96, 90, 105, 118, 111, 91, 108, 100, 131, 112, 102, 110, 134, 115, 101, 122, 116, 121, 150, 133, 119, 103, 124, 159, 104, 141, 120, 125, 161, 113, 128, 148, 123, 162, 151, 163, 126, 138, 117, 152, 169, 129, 139, 127, 170, 153, 130, 140, 135, 175, 154, 132, 147, 136, 176, 156, 155, 143, 178, 137, 165, 158, 144, 180, 142, 179, 145, 146, 188, 167, 149, 198, 181, 157, 171, 199, 166, 160, 185, 202, 173, 164, 172, 203, 187, 174, 168, 189, 183, 207, 177, 191, 197, 190, 210, 200, 201, 182, 204, 211]
ids.count
body = { type => [] }
type = 'products'
source_ids = ['4', '8']
source_ids = ['4']
model = Prodcut
model = Product
serializer = ProductSerializer
edit
body
body['products']
body['products'][0]
body['products'][0]['source_urls']
body['products'][0]
source_ids
model.include_api_info(source_ids).where(id: ids).to_sql
Product.find_by_sql(_)
ids
ids.to_s
exit
GC.stat
`ps -o rss= -p #{Process.pid}`.to_i/1024
ids = _
ids = [5, 3, 14, 2, 1, 8, 11, 19, 4, 7, 9, 6, 17, 22, 10, 13, 15, 23, 16, 24, 12, 20, 31, 18, 25, 21, 27, 37, 28, 26, 30, 35, 45, 39, 29, 32, 47, 36, 46, 40, 33, 50, 49, 41, 38, 53, 34, 66, 44, 56, 42, 43, 67, 48, 58, 59, 51, 71, 52, 61, 62, 54, 72, 60, 63, 65, 55, 73, 64, 70, 57, 77, 79, 68, 78, 80, 84, 69, 85, 81, 74, 89, 82, 93, 86, 75, 92, 87, 106, 95, 76, 98, 88, 109, 97, 94, 83, 99, 114, 107, 96, 90, 105, 118, 111, 91, 108, 100, 131, 112, 102, 110, 134, 115, 101, 122, 116, 121, 150, 133, 119, 103, 124, 159, 104, 141, 120, 125, 161, 113, 128, 148, 123, 162, 151, 163, 126, 138, 117, 152, 169, 129, 139, 127, 170, 153, 130, 140, 135, 175, 154, 132, 147, 136, 176, 156, 155, 143, 178, 137, 165, 158, 144, 180, 142, 179, 145, 146, 188, 167, 149, 198, 181, 157, 171, 199, 166, 160, 185, 202, 173, 164, 172, 203, 187, 174, 168, 189, 183, 207, 177, 191, 197, 190, 210, 200, 201, 182, 204, 211]
body = { type => [] }
type = 'products'
model = Object.const_get(type.classify)
serializer = Object.const_get("#{model}Serializer")
body = { type => [] }
model.include_api_info.where(id: ids).find_in_batches(batch_size: 50) do |records|
  body[type] |= records.map! { |r| serializer.new(r).as_json(root: false) }
  records = nil
  memory_after = `ps -o rss= -p #{Process.pid}`.to_i/1024
  puts "MEMORY: #{memory_after}"
end
body
`ps -o rss= -p #{Process.pid}`.to_i/1024
body = nil
GC.start
`ps -o rss= -p #{Process.pid}`.to_i/1024
body = { type => [] }
ids.to_s
exit
ids = [5, 3, 14, 2, 1, 8, 11, 19, 4, 7, 9, 6, 17, 22, 10, 13, 15, 23, 16, 24, 12, 20, 31, 18, 25, 21, 27, 37, 28, 26, 30, 35, 45, 39, 29, 32, 47, 36, 46, 40, 33, 50, 49, 41, 38, 53, 34, 66, 44, 56, 42, 43, 67, 48, 58, 59, 51, 71, 52, 61, 62, 54, 72, 60, 63, 65, 55, 73, 64, 70, 57, 77, 79, 68, 78, 80, 84, 69, 85, 81, 74, 89, 82, 93, 86, 75, 92, 87, 106, 95, 76, 98, 88, 109, 97, 94, 83, 99, 114, 107, 96, 90, 105, 118, 111, 91, 108, 100, 131, 112, 102, 110, 134, 115, 101, 122, 116, 121, 150, 133, 119, 103, 124, 159, 104, 141, 120, 125, 161, 113, 128, 148, 123, 162, 151, 163, 126, 138, 117, 152, 169, 129, 139, 127, 170, 153, 130, 140, 135, 175, 154, 132, 147, 136, 176, 156, 155, 143, 178, 137, 165, 158, 144, 180, 142, 179, 145, 146, 188, 167, 149, 198, 181, 157, 171, 199, 166, 160, 185, 202, 173, 164, 172, 203, 187, 174, 168, 189, 183, 207, 177, 191, 197, 190, 210, 200, 201, 182, 204, 211]
type = 'products'
model = Object.const_get(type.classify)
serializer = Object.const_get("#{model}Serializer")
source_ids = ['4']
body = { type => [] }
model.include_api_info(source_ids).where(id: ids).find_in_batches(batch_size: 50) do |records|
  body[type] |= records.map! { |r| serializer.new(r).as_json(root: false) }
  # records = nil
  memory_after = `ps -o rss= -p #{Process.pid}`.to_i/1024
  puts "MEMORY: #{memory_after}"
end
memory_after = `ps -o rss= -p #{Process.pid}`.to_i/1024
GC.start
memory_after = `ps -o rss= -p #{Process.pid}`.to_i/1024
records = nil
GC.start
memory_after = `ps -o rss= -p #{Process.pid}`.to_i/1024
exit
memory_after = `ps -o rss= -p #{Process.pid}`.to_i/1024
source_ids = ['4']
Scrape.where(source_id: source_ids).uniq.pluck(:id)
source_ids = nil
scrape_ids = Scrape.where(source_id: source_ids)
source_ids = '4
source_ids = '4'
scrape_ids = Scrape.where(source_id: source_ids)
scrape_ids = Scrape.where(source_id: source_ids).uniq.pluck(:id)
scrape_ids = Scrape.where(source_id: nil).uniq.pluck(:id)
ids
ids = [5, 3, 14, 2, 1, 8, 11, 19, 4, 7, 9, 6, 17, 22, 10, 13, 15, 23, 16, 24, 12, 20, 31, 18, 25, 21, 27, 37, 28, 26, 30, 35, 45, 39, 29, 32, 47, 36, 46, 40, 33, 50, 49, 41, 38, 53, 34, 66, 44, 56, 42, 43, 67, 48, 58, 59, 51, 71, 52, 61, 62, 54, 72, 60, 63, 65, 55, 73, 64, 70, 57, 77, 79, 68, 78, 80, 84, 69, 85, 81, 74, 89, 82, 93, 86, 75, 92, 87, 106, 95, 76, 98, 88, 109, 97, 94, 83, 99, 114, 107, 96, 90, 105, 118, 111, 91, 108, 100, 131, 112, 102, 110, 134, 115, 101, 122, 116, 121, 150, 133, 119, 103, 124, 159, 104, 141, 120, 125, 161, 113, 128, 148, 123, 162, 151, 163, 126, 138, 117, 152, 169, 129, 139, 127, 170, 153, 130, 140, 135, 175, 154, 132, 147, 136, 176, 156, 155, 143, 178, 137, 165, 158, 144, 180, 142, 179, 145, 146, 188, 167, 149, 198, 181, 157, 171, 199, 166, 160, 185, 202, 173, 164, 172, 203, 187, 174, 168, 189, 183, 207, 177, 191, 197, 190, 210, 200, 201, 182, 204, 211]
type = '4'
type = 'products'
model = Object.const_get(type.classify)
serializer = Object.const_get("#{model}Serializer")
source_ids
scrape_ids = Scrape.where(source_id: source_ids).uniq.pluck(:id)
body = { type => [] }
exit
ids = [5, 3, 14, 2, 1, 8, 11, 19, 4, 7, 9, 6, 17, 22, 10, 13, 15, 23, 16, 24, 12, 20, 31, 18, 25, 21, 27, 37, 28, 26, 30, 35, 45, 39, 29, 32, 47, 36, 46, 40, 33, 50, 49, 41, 38, 53, 34, 66, 44, 56, 42, 43, 67, 48, 58, 59, 51, 71, 52, 61, 62, 54, 72, 60, 63, 65, 55, 73, 64, 70, 57, 77, 79, 68, 78, 80, 84, 69, 85, 81, 74, 89, 82, 93, 86, 75, 92, 87, 106, 95, 76, 98, 88, 109, 97, 94, 83, 99, 114, 107, 96, 90, 105, 118, 111, 91, 108, 100, 131, 112, 102, 110, 134, 115, 101, 122, 116, 121, 150, 133, 119, 103, 124, 159, 104, 141, 120, 125, 161, 113, 128, 148, 123, 162, 151, 163, 126, 138, 117, 152, 169, 129, 139, 127, 170, 153, 130, 140, 135, 175, 154, 132, 147, 136, 176, 156, 155, 143, 178, 137, 165, 158, 144, 180, 142, 179, 145, 146, 188, 167, 149, 198, 181, 157, 171, 199, 166, 160, 185, 202, 173, 164, 172, 203, 187, 174, 168, 189, 183, 207, 177, 191, 197, 190, 210, 200, 201, 182, 204, 211]
source_ids = '4'
type = 'products'
model = Object.const_get(type.classify)
serializer = Object.const_get("#{model}Serializer")
scrape_ids = Scrape.where(source_id: source_ids).uniq.pluck(:id)
body = { type => [] }
model.include_api_info(scrape_ids).where(id: ids).find_in_batches(batch_size: 50) do |records|
  body[type] |= records.map! { |r| serializer.new(r).as_json(root: false) }
  # records = nil
  memory_after = `ps -o rss= -p #{Process.pid}`.to_i/1024
  puts "MEMORY: #{memory_after}"
end
exit
ids = [5, 3, 14, 2, 1, 8, 11, 19, 4, 7, 9, 6, 17, 22, 10, 13, 15, 23, 16, 24, 12, 20, 31, 18, 25, 21, 27, 37, 28, 26, 30, 35, 45, 39, 29, 32, 47, 36, 46, 40, 33, 50, 49, 41, 38, 53, 34, 66, 44, 56, 42, 43, 67, 48, 58, 59, 51, 71, 52, 61, 62, 54, 72, 60, 63, 65, 55, 73, 64, 70, 57, 77, 79, 68, 78, 80, 84, 69, 85, 81, 74, 89, 82, 93, 86, 75, 92, 87, 106, 95, 76, 98, 88, 109, 97, 94, 83, 99, 114, 107, 96, 90, 105, 118, 111, 91, 108, 100, 131, 112, 102, 110, 134, 115, 101, 122, 116, 121, 150, 133, 119, 103, 124, 159, 104, 141, 120, 125, 161, 113, 128, 148, 123, 162, 151, 163, 126, 138, 117, 152, 169, 129, 139, 127, 170, 153, 130, 140, 135, 175, 154, 132, 147, 136, 176, 156, 155, 143, 178, 137, 165, 158, 144, 180, 142, 179, 145, 146, 188, 167, 149, 198, 181, 157, 171, 199, 166, 160, 185, 202, 173, 164, 172, 203, 187, 174, 168, 189, 183, 207, 177, 191, 197, 190, 210, 200, 201, 182, 204, 211]
edit
edit 
edit
exit
edit
model.include_api_info(scrape_ids).where(id: ids).to_sql
model.include_api_info([]).where(id: ids).to_sql
Product.connection.select_all(model.include_api_info(scrape_ids).where(id: ids.slice(50)).to_sql)
rows = _
rows.class
rows.first
Product.connection.select_rows(model.include_api_info(scrape_ids).where(id: ids.slice(50)).to_sql)
rows = _
rows.first
model.include_api_info(scrape_ids).where(id: ids.slice(50)).to_sql
sql = _
regex = %r{`\w+`\.`\w+`\sAS\st\d+_r\d+}
regex.match(sql)
regex.scan(sql)
sql.scan(regex)
Product.connection.select_all(model.include_api_info(scrape_ids).where(id: ids.slice(50)).to_sql)
recods = _
records = recods
recods = nil
records
records.first
sql.scan(regex)
mappings = sql.scan(regex)
rows.second
recods.second
records[1]
mappings
Product.first.listings.size
records[0]
sql = model.include_api_info(scrape_ids).where(id: 1).to_sql
Product.first.scrapes
scrape_ids = Scrape.where(source_id: 4).pluck(:id)
model = Product
sql = model.include_api_info(scrape_ids).where(id: 1).to_sql
regex = %r{`\w+`\.`\w+`\sAS\st\d+_r\d+}
mappings = sql.scan(regex)
mappings.inject({}) do |row, map|
  col, mapping = row.split(' AS ')
  map[col] = mapping
end
mappings.inject({}) do | map, row|
  col, mapping = row.split(' AS ')
  map[col] = mapping
end
mappings.first
mappings.first.split(' AS ')
map = {}
mappings.each do |row}
mappings.each do |row|
  col, mapping = row.split(' AS ')
  map[col] = mapping
end
map
map.each do |key, val|
  key = key.gsub(/`/, '')
end
map
edit
records
edit
exit
edit
p = _.first
p.listings.size
edit
edit -t
Product.preload(:listings).where('listings.scrape_id': scrape_ids)
Product.preload(:listings).where('listings.scrape_id': scrape_ids).references(:listings)
Product.eager_load(:listings).where('listings.scrape_id': scrape_ids).references(:listings)
p = _.first
p.listings
assoc = 'screenshots'
scrape_ids
edit -t
edit
ids
ids.count
ids = Product.limit(200).ids
ids.count
type = 'products'
edit
reload!
edit
exit
edit
{:TOTAL=>6640718, :FREE=>4108458, :T_OBJECT=>67621, :T_CLASS=>10154, :T_MODULE=>1460, :T_FLOAT=>10, :T_STRING=>1061626, :T_REGEXP=>3063, :T_ARRAY=>523574, :T_HASH=>111469, :T_STRUCT=>9916, :T_BIGNUM=>13, :T_FILE=>9, :T_DATA=>660609, :T_MATCH=>12523, :T_COMPLEX=>1, :T_RATIONAL=>903, :T_SYMBOL=>6449, :T_NODE=>60262, :T_ICLASS=>2598}
{:TOTAL=>5368164, :FREE=>4818945, :T_OBJECT=>28414, :T_CLASS=>9527, :T_MODULE=>1460, :T_FLOAT=>10, :T_STRING=>270881, :T_REGEXP=>2992, :T_ARRAY=>102410, :T_HASH=>24337, :T_STRUCT=>4557, :T_BIGNUM=>13, :T_FILE=>9, :T_DATA=>58216, :T_MATCH=>5, :T_COMPLEX=>1, :T_RATIONAL=>903, :T_SYMBOL=>6449, :T_NODE=>37064, :T_ICLASS=>1971}
{"2.2.5":{"gc":"enabled","time":40.55,"gc_count":51,"memory":"132 MB"}}
edit
records.first
records.class
p = products.first
p.listings.size
l = p.listings.first
products.class
edit
scrape_ids
model
serializer
edit
exit
edit
ids
edit
products = Product.joins(:listings).where('listings.scrape_id': scrape_ids)
ActiveRecord::Associations::Preloader.new.preload(
  products,
  {
    listings: [
      { scrape_listing_sellers: { seller: :images } },
      { scrape: :source },
      :images,
      :screenshots
    ]
  }
)
p = products.first
limited_listings = p.listings
screenshots = limited_listings.flat_map { |l| l.screenshots }.last(50)
images = limited_listings.flat_map { |l| l.images }.last(50)
edit
Product.joins(:listings).where('listings.scrape_id': scrape_ids)
Product.joins(:listings).where('listings.scrape_id': scrape_ids).to_sql
sql = _
sql = <<-SQL
SELECT `products`.* FROM `products`
INNER JOIN `product_listings` ON `product_listings`.`product_id` = `products`.`id`
INNER JOIN `listings` ON `listings`.`id` = `product_listings`.`listing_id`
WHERE `listings`.`scrape_id` IN (#{scrape_ids})
SQL
sql
Product.find_by_sql(sql)
sql = <<-SQL
SELECT `products`.* FROM `products`
INNER JOIN `product_listings` ON `product_listings`.`product_id` = `products`.`id`
INNER JOIN `listings` ON `listings`.`id` = `product_listings`.`listing_id`
WHERE `listings`.`scrape_id` IN (?)
SQL
Product.find_by_sql(sql, scrape_ids)
sql = <<-SQL
SELECT `products`.* FROM `products`
INNER JOIN `product_listings` ON `product_listings`.`product_id` = `products`.`id`
INNER JOIN `listings` ON `listings`.`id` = `product_listings`.`listing_id`
WHERE `listings`.`scrape_id` IN (?)
SQL
sql.gsub!(/\s+/, ' ')
sql
Product.find_by_sql(sql, scrape_ids)
Product.find_by_sql([sql, scrape_ids])
scrape_ids
products = Product.find_by_sql(sql, scrape_ids)
products = Product.find_by_sql([sql, scrape_ids])
ActiveRecord::Associations::Preloader.new.preload(products, :listings, Struct.new(:values, :bind_values).new({where: "listings.scrape_id"}, scrape_ids))
p = products.first
p.listings.size
products = Product.find_by_sql([sql, scrape_ids])
ActiveRecord::Associations::Preloader.new.preload(products, :listings, Struct.new(:values, :bind_values).new({where: "listings.scrape_id > (#{scrape_ids.join(',')})"}, []))
p = products.first
p.listings.size
"listings.scrape_id > (#{scrape_ids.join(',')})"
products = Product.find_by_sql([sql, scrape_ids])
ActiveRecord::Associations::Preloader.new.preload(products, :listings, Struct.new(:values, :bind_values).new({where: "listings.scrape_id IN (#{scrape_ids.join(',')})"}, []))
p = products.first
p.listings.size
exit
edit
require "json"
require "benchmark"
def measure(no_gc=false, &block)
  if no_gc
    GC.disable
  else
    # collect memory allocated during library loading
    # and our own code before the measurement
    GC.enable
    GC.start
  end
  memory_before = `ps -o rss= -p #{Process.pid}`.to_i/1024
  gc_stat_before = GC.stat
  time = Benchmark.realtime do
    yield
  end
  puts ObjectSpace.count_objects
  unless no_gc
    GC.start(full_mark: true, immediate_sweep: true, immediate_mark: true)
  end
  puts ObjectSpace.count_objects
  gc_stat_after = GC.stat
  memory_after = `ps -o rss= -p #{Process.pid}`.to_i/1024
  puts({
      RUBY_VERSION => {
        gc: no_gc ? 'disabled' : 'enabled',
        time: time.round(2),
        gc_count: gc_stat_after[:count] - gc_stat_before[:count],
        memory: "%d MB" % (memory_after - memory_before)
      }
  }.to_json)
end
edit
p = products.first
edit
p = products.first
edit
p.listings.size
p.listings.pluck(:url)
ProductSerializer.new(p).as_json(root: false)
json = _
Product.all.where(id: ids).to_sql
Product.all.class
exit
edit
body
body['products
body['products'].first[:source_urls]
r_ids = ids.first(50)
records = model.include_api_info(r_ids, scrape_ids)
require "json"
require "benchmark"
def measure(no_gc=false, &block)
  if no_gc
    GC.disable
  else
    # collect memory allocated during library loading
    # and our own code before the measurement
    GC.enable
    GC.start
  end
  memory_before = `ps -o rss= -p #{Process.pid}`.to_i/1024
  gc_stat_before = GC.stat
  time = Benchmark.realtime do
    yield
  end
  puts ObjectSpace.count_objects
  unless no_gc
    GC.start(full_mark: true, immediate_sweep: true, immediate_mark: true)
  end
  puts ObjectSpace.count_objects
  gc_stat_after = GC.stat
  memory_after = `ps -o rss= -p #{Process.pid}`.to_i/1024
  puts({
      RUBY_VERSION => {
        gc: no_gc ? 'disabled' : 'enabled',
        time: time.round(2),
        gc_count: gc_stat_after[:count] - gc_stat_before[:count],
        memory: "%d MB" % (memory_after - memory_before)
      }
  }.to_json)
end
edit
[].present?
edit
eit
edit
p = products.first
p.listings.size
l = p.listings.first
reload!
edit
exit
edit
body
body['products'].first[:source_urls]
body['products'].map{|p| p[:source_urls] }
l = Listing.find_by(url: 'http://www.ebay.com/itm/Broadcom-4311-Wireless-wifi-PCI-E-Card-For-HP-DELL-Laptop-Netowrk-Wlan-Wi-Fi-/180519002077')
l.scrape
exit
ids = Product.ids
ids = ids.shuffle
ids = ids.first(500)
ids
ids = ids.first(200)
type = 'products'
model = Object.const_get(type.classify)
serializer = Object.const_get("#{model}Serializer")
scrape_ids = Scrape.where(source_id: 4).ids
body = { type => []  }
model.include_api_info([], scrape_ids).where(id: ids).find_in_batches(batch_size: 50) do |records|
  body[type] |= records.map! { |r| serializer.new(r).as_json(root: false) }
  records = nil
end
model.include_api_info(scrape_ids).where(id: ids).find_in_batches(batch_size: 50) do |records|
  body[type] |= records.map! { |r| serializer.new(r).as_json(root: false) }
  records = nil
end
body['products'].length
ids.length
body['products'].map { |p| p[:source_urls] }
scrape_ids = Scrape.where(source_id: [4,8]).ids
body['products'].length
model.include_api_info(scrape_ids).where(id: ids).find_in_batches(batch_size: 50) do |records|
  body[type] |= records.map! { |r| serializer.new(r).as_json(root: false) }
  records = nil
end
body['products'].length
body['products'].map { |p| p[:source_urls] }
body['products'].last
model.where(id: ids)
model.where(id: ids).include_api_info(scrape_ids).find_in_batches(batch_size: 50) do |records|
  body[type] |= records.map! { |r| serializer.new(r).as_json(root: false) }
end
body['products'].length
redis = Redis.connection
redis = Redis.current
redis.flushall
exit
edit
reload!
exit
edit
body
edit
exit
edit
exit
edit
Listing.last
l = _
l.contact_name
Listing.where.not(contact_name: nil)
edit
SingleListingSerializer.new(Listing.include_detail_info.first).as_json(root: false)
edit
reload!
relaod!
edit
SingleListingSerializer.new(Listing.include_detail_info.first).as_json(root: false)
reload!
SingleListingSerializer.new(Listing.include_detail_info.first).as_json(root: false)
reload!
SingleListingSerializer.new(Listing.include_detail_info.first).as_json(root: false)
reload!
SingleListingSerializer.new(Listing.include_detail_info.first).as_json(root: false)
exit
Product.last
Product.first
exit
params = "{\"^o\":\"Hashie::Mash\",\"self\":{\"keywords\":\"sunpower solar\",\"source_ids\":\"1,4\"}}"
PER_PAGE ||= ENV['CACHE_PAGE_SIZE'].try(:to_i) || 200
API_CACHE ||= RdsApiCache.new
params = Hashie::Mash.new(Oj.load(params))
type = params[:type]
model = Object.const_get(type.classify)
type = 'products
type = 'products'
model = Object.const_get(type.classify)
serializer = Object.const_get("#{model}Serializer")
cache_params, last_page, prefix, scroll_id, search_params, total, version = nil
source_ids = params[:source_ids].try(:split, ',')
scrape_ids = Scrape.where(source_id: source_ids).uniq.pluck(:id)
page = params[:page] || 1
def prepare_search_params(model, params)
  operator = 'AND'
  terms = { keywords: params[:keywords] }
  filters = {}
  model.query_fields[:filter_fields].each do |key, _v|
    if params[key].present?
      params[key].downcase!
      filters[key] = params[key].split(',').map! do |val|
        val.strip!
        val
      end
    end
  end
  if params[:using] == 'any'
    keywords = [params[:keywords], filters[:brands], filters[:models]].reject!(&:nil?).join(' '.freeze)
    filters.delete_if { |k, _v| [:brands, :models].include?(k) }
    terms = { keywords: keywords }
    operator = 'OR'
  end
  return terms, operator, filters
end
search_params ||= prepare_search_params(model, params)
search_response = model.search_by(*search_params, { size: PER_PAGE, scroll: '10m'.freeze })
total ||= search_response.results.total
params
params.delete(:source_ids)
params
search_params ||= prepare_search_params(model, params)
search_params = prepare_search_params(model, params)
search_response = model.search_by(*search_params, { size: PER_PAGE, scroll: '10m'.freeze })
total ||= search_response.results.total
Listing.first
l = _
l.sources
l.source
l.id
exit
edit
all = gsus | alius | amazonus | dhgate | ebay | flipkart
all.uniq.sort
all.count
all
all.uniq.sort
all.map(&:downcase).uniq.sort
http://www.ebay.com/itm/like/172149694006
'http://www.ebay.com/itm/like/172149694006
'
gst
exit
page.url
current_url
page.current_url
exit
'goose'.pluralize
'human'.pluralize
'moose'.pluralize
'man'.pluralize
'woman'.pluralize
'one'.pluralize
'rail'.pluralize
'geese'.pluralize
'geese'.singularzie
'geese'.singularize
exit
ProductScreenshot.last
exit
ProductScreenshot.last
exit
ProductScreenshot.last
exit
ProductScreenshot.last
Product.count
exit
ProductScreenshot.last
exit
Seller.lsat
Seller.last
s = _
s.listing_seller_screenshots
exit
Seller.last
s.listing_seller_screenshots
s = Seller.last
s.listing_seller_screenshots
s.seller_screenshots
reload!
s = Seller.last
s.seller_screenshots
reload!
s = Seller.last
s.seller_screenshots
SellerScreenshot.last
l = Listing.last
l.screenshots
exot
exit
"listing_screenshot".constantize
"listing_screenshot".classify
model = 'listing'
"#{model}Screenshot".classify
"#{model}Screenshot"
"#{model}Screenshot".classify
Object.const_get("#{model}_screenshot".classify)
ListingScreenshot.count
Asset.uniq.pluck(:type)
exit
f
defined? f
f = 'hey
'
defined? f
exit
s = Scrape.where(running: true)
s = Scrape.where(running: true).update_all(running: true)
s = Scrape.where(running: true).update_all(running: false)
exit
s = Scrape.last
l = s.listings.first
l.screenshots
l = s.listings.last
l.screenshot
l.screenshots
s = Scrape.last
exit
s = Scrape.last
l = s.listings.last
l.screenshots
l.sellers.count
l.reload
l.sellers
s.scrape_listing_sellers
s.scrape_listing_sellers.count
seller = s.scrape_listing_sellers.last.seller
seller.seller_screenshots
SellerScreenshot.last
SellerScreenshot.last.seller
SellerScreenshot.last.sellers
reload!
SellerScreenshot.last.sellers
SellerScreenshot.last
SellerScreenshot.last.attachment.url
ss = SellerScreenshot.last
ss.attachment.url
seller = Seller.last
seller.seller_screenshots
seller.name
seller.seller_screenshots.first.attachment.url
s.reload
l = s.listings.last
l.sellers.count
l.reload
l.sellers.count
l.reload
l.sellers.count
l.reload
l.sellers.count
l.reload
l.sellers.count
l.reload
l.sellers.count
l
l = Listing.find 2665
l.sellers
l = Listing.find 2665
l.sellers
l = Listing.find 2665
l.sellers
l.url
l
l.name
l.reload
l.sellers
l.find 2665
l = Listing.find 2665
l.sellers
Seller.find_by(name: 'acopower')
sller = _
seller.seller_screenshots
seller.seller_screenshots.first.last.attachment.url
seller.seller_screenshots.last.attachment.url
ss = seller.seller_screenshots.last
ss.listing_seller_screenshot
ss.listing_seller_screenshots
l = Listing.find 2646
l.sellers
l = Listing.find 2665
l.sellers
l = Listing.last
l.sellers
l.reload
l.sellers
l.id
l.relaod
l.reload
l.sellers
l.seller_screenshots
l.seller_screenshots.count
l.seller_screenshots.pluck(:id)
l.seller_screenshots.pluck(:id).uniq
reload!
l.relaod
l.reload
l.seller_screenshots.pluck(:id)
l.seller_screenshots
exit
l = Listing.last
l.seller_screenshots
l.reload
l.seller_screenshots
l.reload
l.seller_screenshots
l.id
l.reload
l.seller_screenshots
s = Scrape.last
s.listings.count
s.listings.map {|l| l.sellers.count > 1 }
s.listings.select {|l| l.sellers.count > 1 }
l = s.listings.select {|l| l.sellers.count > 1 }.first
l.seller_screenshots
l.sellers
l.listing_seller_screenshots
seller = Seller.find 189
seller.seller_screenshots
seller.listing_seller_screenshots
Asset.where(type: 'ProductScreenshot')
Asset.where(type: 'ProductScreenshot').count
Asset.where(type: 'ProductScreenshot').first
SellerScreenshot.last
ListingScreenshot.last
Asset.where(type: 'ProductScreenshot').count
Asset.where(type: 'ProductScreenshot').pluck(:id).first
ListingScreenshot.find(18)
Asset.where(type: 'ProductScreenshot').update_all(type: 'ListingScreenshot')
ListingScreenshot.find(18)
Source.first
s.reload
s.listings.count
s.listings.uniq.pluck(:url)(
s.listings.uniq.pluck(:url).count
s.listings.pluck(:url).uniq.count
s.listings.count
exit
p = Product.first
p.screenshots
p.screenshots.map {|s| s.attachment.url }
p.last
p = Product.last
p.screenshots.map {|s| s.attachment.url }
p.sellers
p.listings
exit
s = Scrape.last
Seller.last
exit
l = Listing.last
l.selelrs
l.sellers
l.reload!
l.reload
l.sellers
l.seller_screenshots
l.seller_screenshots.first.attachment.url
s = Scrape.last
scrape_id = s.id
UpdateScrapeCounterJob.new.peform_async(scrape_id)
UpdateScrapeCounterJob.peform_async(scrape_id)
UpdateScrapeCounterJob.new.perform_async(scrape_id)
UpdateScrapeCounterJob.perform_async(scrape_id)
s.reload
s.listings.count
UpdateScrapeDurationJob.perform_async(scrape_id)
s.reload
reload!
s = Scrape.last
s.listings.count
s.products.count
s.sellers.count
seller = s.sellers.first
seller.listing_seller_screenshots
seller.seller_screenshots
seller.seller_screenshots.count
seller.seller_screenshots
seller.name
url = 'https://www.google.com/aclk?sa=l&ai=DChcSEwiT2PPJ6f3OAhVRWYYKHVvIAaEYABAL&sig=AOD64_1qXLlhkHDwOYECS_5KwzPwrouHPQ&ctype=5&q=&ved=0ahUKEwjz1PLJ6f3OAhWBbR4KHUheB4A4KBDYKQj1AjAA&adurl='
s.listings.where(original_url: url)
l = _.first
l.sellers
l.reload
l.scrape_listing_sellers
l.products
l.screenshots.first
s.listings.where(original_url: 'https://www.google.com/aclk?sa=l&ai=DChcSEwiT2PPJ6f3OAhVRWYYKHVvIAaEYABA4&sig=AOD64_1pm1QLx3MJ9KYOhmj3QxPsRibDOg&ctype=5&q=&ved=0ahUKEwjz1PLJ6f3OAhWBbR4KHUheB4A4KBDYKQj0AzAR&adurl=')
l = _.first
l.screenshots
l.reload
l.screenshots
seller = Seller.last
seller.seller_screenshots
l = s.listings.where(original_url: 'http://www.google.com/aclk?sa=L&ai=DChcSEwjF3paZ6v3OAhXMWoYKHVrqDZQYABAJ&sig=AOD64_3VA-T0qDfw5bXZ63CXrNSOKQ_fzA&ctype=5&rct=j&q=&ved=0ahUKEwjymJWZ6v3OAhVFPiYKHTPLBh84PBDYKQjXAjAE&adurl=').first
l.screenshots
l.screenshots.first.attachment.url
l
url = 'https://www.google.com/aclk?sa=l&ai=DChcSEwib3say6v3OAhVPa34KHUF-AmEYABAD&sig=AOD64_0cjtCDwH5XqPwKv7GYy_YsoqTa5g&ctype=5&q=&ved=0ahUKEwjI18Wy6v3OAhUQ92MKHUa3C-o4UBDYKQi8AjAB&adurl='
s.reload
l = s.listings.where(original_url: url).first
l.screenshots.first.attachment.url
l
s.listings.pluck(:url).uniq
s.listings.where('url like "%aclk%").count
s.listings.where('url like "%aclk%"').count
s.listings.where('url like "%aclk%"').map {|l| l.screenshots.flat_map {|s| s.attachment.url } }
s.listings.where('url like "%aclk%"').map {|l| l.screenshots.flat_map {|s| s.attachment.url } }.flatten.compact
listings = s.listings.where('url like "%aclk%"')
listings.first
GoogleImageDetailJob.new.perform(Scrape.first.id, 1, "https://www.google.com/aclk?sa=l&ai=DChcSEwjpgMu96f3OAhWTW34KHTnRAysYABAK&sig=AOD64_1YukgFIQo4K78TdQyHMifIb-Y5hQ&ctype=5&q=&ved=0ahUKEwiHtMi96f3OAhUN1GMKHU7mBTgQ2CkI4AIwAA&adurl=")
l = Listing.last
l.screenshots
l.screenshots.first.attachment.url
l.destroy
exit
GoogleImageDetailJob.new.perform(Scrape.first.id, 1, "https://www.google.com/aclk?sa=l&ai=DChcSEwjpgMu96f3OAhWTW34KHTnRAysYABAK&sig=AOD64_1YukgFIQo4K78TdQyHMifIb-Y5hQ&ctype=5&q=&ved=0ahUKEwiHtMi96f3OAhUN1GMKHU7mBTgQ2CkI4AIwAA&adurl=")
l = Listing.last
l.screenshots.first.attachment.url
l.destroy
reload!
exit
GoogleImageDetailJob.new.perform(Scrape.first.id, 1, "https://www.google.com/aclk?sa=l&ai=DChcSEwjpgMu96f3OAhWTW34KHTnRAysYABAK&sig=AOD64_1YukgFIQo4K78TdQyHMifIb-Y5hQ&ctype=5&q=&ved=0ahUKEwiHtMi96f3OAhUN1GMKHU7mBTgQ2CkI4AIwAA&adurl=")
l = Listing.last
l.screenshots.first.attachment.url
exit
Listing.last.destroy
ListingScreenshot.last
ListingScreenshot.last.destroy
GoogleImageDetailJob.new.perform(Scrape.first.id, 1, "https://www.google.com/aclk?sa=l&ai=DChcSEwjpgMu96f3OAhWTW34KHTnRAysYABAK&sig=AOD64_1YukgFIQo4K78TdQyHMifIb-Y5hQ&ctype=5&q=&ved=0ahUKEwiHtMi96f3OAhUN1GMKHU7mBTgQ2CkI4AIwAA&adurl=")
exit
GoogleImageDetailJob.new.perform(Scrape.first.id, 1, "https://www.google.com/aclk?sa=l&ai=DChcSEwjpgMu96f3OAhWTW34KHTnRAysYABAK&sig=AOD64_1YukgFIQo4K78TdQyHMifIb-Y5hQ&ctype=5&q=&ved=0ahUKEwiHtMi96f3OAhUN1GMKHU7mBTgQ2CkI4AIwAA&adurl=")
exit
url = 'https://www.google.com/aclk?sa=l&ai=DChcSEwib3say6v3OAhVPa34KHUF-AmEYABAD&sig=AOD64_0cjtCDwH5XqPwKv7GYy_YsoqTa5g&ctype=5&q=&ved=0ahUKEwjI18Wy6v3OAhUQ92MKHUa3C-o4UBDYKQi8AjAB&adurl='
Listing.where(url: url)
Listing.where(url: url).destroy
Listing.where(url: url).first.destroy
GoogleImageDetailJob.new.perform(Scrape.first.id, 1, url)
Listing.last.url == url
Listing.last.screenshots.first
Listing.last.screenshots.first.attachment.url
exit
ListingScreenshot.find(13295).destroy
Listing.last.destroy
url = 'https://www.google.com/aclk?sa=l&ai=DChcSEwib3say6v3OAhVPa34KHUF-AmEYABAD&sig=AOD64_0cjtCDwH5XqPwKv7GYy_YsoqTa5g&ctype=5&q=&ved=0ahUKEwjI18Wy6v3OAhUQ92MKHUa3C-o4UBDYKQi8AjAB&adurl='
GoogleImageDetailJob.new.perform(Scrape.first.id, 1, url)
ListingScreenshot.last
ListingScreenshot.last.url == url
ListingScreenshot.last.attachment.url
Listing.last.destroy
ListingScreenshot.last.destroy
exit
url = 'https://www.google.com/aclk?sa=l&ai=DChcSEwib3say6v3OAhVPa34KHUF-AmEYABAD&sig=AOD64_0cjtCDwH5XqPwKv7GYy_YsoqTa5g&ctype=5&q=&ved=0ahUKEwjI18Wy6v3OAhUQ92MKHUa3C-o4UBDYKQi8AjAB&adurl='
GoogleImageDetailJob.new.perform(Scrape.first.id, 1, url)
exit
url = 'https://www.google.com/aclk?sa=l&ai=DChcSEwib3say6v3OAhVPa34KHUF-AmEYABAD&sig=AOD64_0cjtCDwH5XqPwKv7GYy_YsoqTa5g&ctype=5&q=&ved=0ahUKEwjI18Wy6v3OAhUQ92MKHUa3C-o4UBDYKQi8AjAB&adurl='
GoogleImageDetailJob.new.perform(Scrape.first.id, 1, url)
Asset.find(13297)
ListingScreenshot.first
'https://www.googleadservices.com/pagead/aclk?sa=L&ai=C5jcMFlXQV5v7Cs_W-QPB_ImIBrjzhrpFz43XrZ0Bx5PP-_ACCAkQAiDezc8eKBRgyYaAgMgjkAEJoAHjjOL8A8gBB6oEJk_QLPwurva4W3uHGEQU-fqY9-IX2P3y2Y1M6hBOxGwCaeSACFwUwAUFoAYmgAe38aIskAcBqAemvhvYBwHgEtD7rqSTn8XP0wE&ctype=5&ved=0ahUKEwjI18Wy6v3OAhUQ92MKHUa3C-o4UBDYKQi8AjAB&dblrd=1&val=GghzzurAUbSmpCABKAAw1Mm1xP39tJNQOMfxlrsFQMfTwb4F&sig=AOD64_3UnflFHzHdlfhZN6oyyoDZuiQT_Q&adurl=http://clickserve.dartsearch.net/link/click%3Flid%3D92700010192235123%26ds_s_kwgid%3D58700000367275245%26ds_e_adid%3D42222138463%26ds_e_product_group_id%3D99043494343%26ds_e_product_id%3D9SIA4W31RE9750%26ds_e_product_merchant_id%3D102650385%26ds_e_product_country%3DUS%26ds_e_product_language%3Den%26ds_e_product_channel%3Donline%26ds_e_product_store_id%3D%26ds_e_ad_type%3Dpla%26ds_s_inventory_feed_id%3D97700000001004293%26ds_url_v%3D2%26ds_dest_url%3Dhttp://www.newegg.com/Product/Product.aspx%3FItem%3D9SIA4W31RE9750%26nm_mc%3DKNC-GoogleMKP-PC%26cm_mmc%3DKNC-GoogleMKP-PC-_-pla-_-EC%2B-%2BSensors%2B%2526%2BTransducers-_-9SIA4W31RE9750'
url = _
CGI.unescape(CGI.unescape('http' + url.split('http').last))
URI.unescape(CGI.unescape('http' + url.split('http').last))
url
exit
require 'rvx_rds/proxy'
detail_url = 'https://www.google.com/aclk?sa=l&ai=DChcSEwib3say6v3OAhVPa34KHUF-AmEYABAD&sig=AOD64_0cjtCDwH5XqPwKv7GYy_YsoqTa5g&ctype=5&q=&ved=0ahUKEwjI18Wy6v3OAhUQ92MKHUa3C-o4UBDYKQi8AjAB&adurl='
proxy = pop_proxy
load '/Users/jonathan/rvx-rds/lib/rvx_rds/proxy.rb'
proxy = pop_proxy
RvxRds.pop_proxy
pop_proxy
Sidekiq.redis do |redis|
  used_proxies = redis.smembers(:'proxies:used')
  all_proxies = redis.smembers(:'proxies:all')
  if used_proxies.length == all_proxies.length
    used_proxies = []
    redis.del :'proxies:used'
  end
  available_proxies = all_proxies - used_proxies
  proxy = available_proxies.sample
  redis.sadd :'proxies:used', proxy
  proxy
end
proxy = _
request = proxy_req(proxy, uri)
uri = URI.parse(detail_url)
request = proxy_req(proxy, uri)
require 'rvx_rds/proxy
require 'rvx_rds/proxy'
exit
pop_proxy
require 'rvx_rds/proxy'
include RvxRds::Proxy
detail_url = 'https://www.google.com/aclk?sa=l&ai=DChcSEwib3say6v3OAhVPa34KHUF-AmEYABAD&sig=AOD64_0cjtCDwH5XqPwKv7GYy_YsoqTa5g&ctype=5&q=&ved=0ahUKEwjI18Wy6v3OAhUQ92MKHUa3C-o4UBDYKQi8AjAB&adurl='
uri = URI.parse(detail_url)
proxy = pop_proxy
request = proxy_req(proxy, uri)
response = request.get(uri)
response.headers
response['headers']
response
response.header
response.header['Content-Type']
response.body
response.header['Location']
exit
url = 'https://www.google.com/aclk?sa=l&ai=DChcSEwib3say6v3OAhVPa34KHUF-AmEYABAD&sig=AOD64_0cjtCDwH5XqPwKv7GYy_YsoqTa5g&ctype=5&q=&ved=0ahUKEwjI18Wy6v3OAhUQ92MKHUa3C-o4UBDYKQi8AjAB&adurl='
GoogleImageDetailJob.new.perform(Scrape.first.id, 1, url)
exit
url = 'https://www.google.com/aclk?sa=l&ai=DChcSEwib3say6v3OAhVPa34KHUF-AmEYABAD&sig=AOD64_0cjtCDwH5XqPwKv7GYy_YsoqTa5g&ctype=5&q=&ved=0ahUKEwjI18Wy6v3OAhUQ92MKHUa3C-o4UBDYKQi8AjAB&adurl='
GoogleImageDetailJob.new.perform(Scrape.first.id, 1, url)
exit
url = 'https://www.google.com/aclk?sa=l&ai=DChcSEwib3say6v3OAhVPa34KHUF-AmEYABAD&sig=AOD64_0cjtCDwH5XqPwKv7GYy_YsoqTa5g&ctype=5&q=&ved=0ahUKEwjI18Wy6v3OAhUQ92MKHUa3C-o4UBDYKQi8AjAB&adurl='
GoogleImageDetailJob.new.perform(Scrape.first.id, 1, url)
exit
url = 'https://www.google.com/aclk?sa=l&ai=DChcSEwib3say6v3OAhVPa34KHUF-AmEYABAD&sig=AOD64_0cjtCDwH5XqPwKv7GYy_YsoqTa5g&ctype=5&q=&ved=0ahUKEwjI18Wy6v3OAhUQ92MKHUa3C-o4UBDYKQi8AjAB&adurl='
GoogleImageDetailJob.new.perform(Scrape.first.id, 1, url)
exit
url = 'https://www.google.com/aclk?sa=l&ai=DChcSEwib3say6v3OAhVPa34KHUF-AmEYABAD&sig=AOD64_0cjtCDwH5XqPwKv7GYy_YsoqTa5g&ctype=5&q=&ved=0ahUKEwjI18Wy6v3OAhUQ92MKHUa3C-o4UBDYKQi8AjAB&adurl='
GoogleImageDetailJob.new.perform(Scrape.first.id, 1, url)
l = Listing.last
l.screenshots
l.screenshots.first.attachment.url
Listing.last.destroy
ListingScreenshot.last.destroy
exit
url = 'https://www.google.com/aclk?sa=l&ai=DChcSEwib3say6v3OAhVPa34KHUF-AmEYABAD&sig=AOD64_0cjtCDwH5XqPwKv7GYy_YsoqTa5g&ctype=5&q=&ved=0ahUKEwjI18Wy6v3OAhUQ92MKHUa3C-o4UBDYKQi8AjAB&adurl='
GoogleImageDetailJob.new.perform(Scrape.first.id, 1, url)
Listing.last.destroy
ListingScreenshot.last.destroy
url
exit
url = 'https://www.google.com/aclk?sa=l&ai=DChcSEwib3say6v3OAhVPa34KHUF-AmEYABAD&sig=AOD64_0cjtCDwH5XqPwKv7GYy_YsoqTa5g&ctype=5&q=&ved=0ahUKEwjI18Wy6v3OAhUQ92MKHUa3C-o4UBDYKQi8AjAB&adurl='
GoogleImageDetailJob.new.perform(Scrape.first.id, 1, url)
exit
url = 'https://www.google.com/aclk?sa=l&ai=DChcSEwib3say6v3OAhVPa34KHUF-AmEYABAD&sig=AOD64_0cjtCDwH5XqPwKv7GYy_YsoqTa5g&ctype=5&q=&ved=0ahUKEwjI18Wy6v3OAhUQ92MKHUa3C-o4UBDYKQi8AjAB&adurl='
GoogleImageDetailJob.new.perform(Scrape.first.id, 1, url)
l = Listing.last
l.screenshots.last.attachment.url
exit
url = 'https://www.google.com/aclk?sa=l&ai=DChcSEwib3say6v3OAhVPa34KHUF-AmEYABAD&sig=AOD64_0cjtCDwH5XqPwKv7GYy_YsoqTa5g&ctype=5&q=&ved=0ahUKEwjI18Wy6v3OAhUQ92MKHUa3C-o4UBDYKQi8AjAB&adurl='
GoogleImageDetailJob.new.perform(Scrape.first.id, 1, url)
Listing.last.destroy
ListingScreenshot.last.destroy
url = 'https://www.google.com/aclk?sa=l&ai=DChcSEwib3say6v3OAhVPa34KHUF-AmEYABAD&sig=AOD64_0cjtCDwH5XqPwKv7GYy_YsoqTa5g&ctype=5&q=&ved=0ahUKEwjI18Wy6v3OAhUQ92MKHUa3C-o4UBDYKQi8AjAB&adurl='
GoogleImageDetailJob.new.perform(Scrape.first.id, 1, url)
l = Listing.last
l.screenshots.first.attachment.url
exit
l = Listing.where(original_url: 'https://www.google.com/aclk?sa=l&ai=DChcSEwiMx_LJ6v3OAhWDZn4KHRMJAL0YABAD&sig=AOD64_2FBXhQY9k0OZ0uCeUbxCiaKEneSQ&ctype=5&q=&ved=0ahUKEwikuPHJ6v3OAhUIxWMKHdOZA4k4ZBDYKQi2AjAB&adurl=')
l.screenshots.first.attachment.url
l.first.screenshots.first.attachment.url
url = 'http://www.google.com/aclk?sa=l&ai=DChcSEwiKjazR6v3OAhUPgmkKHUP_Cf4YABAN&sig=AOD64_1NGd7X4L2ouIibTRE6va0UhqbliQ&ctype=5&rct=j&q=&ved=0ahUKEwjbg6vR6v3OAhXr7IMKHWlfBuM4eBDYKQjFAjAD&adurl='
include RvxRds::Proxy
proxy = pop_proxy
proxy_host, proxy_pass, proxy_port, proxy_user = set_proxy(proxy)
def get_redirect_location(url)
  uri = URI.parse(url)
  proxy = pop_proxy
  request = proxy_req(proxy, uri)
  response = request.head(uri)
  response.header['Location']
end
url
get_redirect_location(url)
uri
uri = URI.parse(url)
uir.port
uri.port
uri.port = 443
uri
proxy = pop_proxy
request = proxy_req(proxy, uri)
response.header['Location']
response = request.head(uri)
response.header['Location']
response
response.body
uri
uri.port = 80
request = proxy_req(proxy, uri)
response = request.head(uri)
exit
url = 'http://www.google.com/aclk?sa=l&ai=DChcSEwiKjazR6v3OAhUPgmkKHUP_Cf4YABAN&sig=AOD64_1NGd7X4L2ouIibTRE6va0UhqbliQ&ctype=5&rct=j&q=&ved=0ahUKEwjbg6vR6v3OAhXr7IMKHWlfBuM4eBDYKQjFAjAD&adurl='
GoogleImageDetailJob.new.perform(Scrape.first.id, 1, url)
exit
url = 'http://www.google.com/aclk?sa=l&ai=DChcSEwiKjazR6v3OAhUPgmkKHUP_Cf4YABAN&sig=AOD64_1NGd7X4L2ouIibTRE6va0UhqbliQ&ctype=5&rct=j&q=&ved=0ahUKEwjbg6vR6v3OAhXr7IMKHWlfBuM4eBDYKQjFAjAD&adurl='
GoogleImageDetailJob.new.perform(Scrape.first.id, 1, url)
exit
GoogleImageDetailJob.new.perform(Scrape.first.id, 1, url)
url = 'http://www.google.com/aclk?sa=l&ai=DChcSEwiKjazR6v3OAhUPgmkKHUP_Cf4YABAN&sig=AOD64_1NGd7X4L2ouIibTRE6va0UhqbliQ&ctype=5&rct=j&q=&ved=0ahUKEwjbg6vR6v3OAhXr7IMKHWlfBuM4eBDYKQjFAjAD&adurl='
GoogleImageDetailJob.new.perform(Scrape.first.id, 1, url)
exit
url = 'http://www.google.com/aclk?sa=l&ai=DChcSEwiKjazR6v3OAhUPgmkKHUP_Cf4YABAN&sig=AOD64_1NGd7X4L2ouIibTRE6va0UhqbliQ&ctype=5&rct=j&q=&ved=0ahUKEwjbg6vR6v3OAhXr7IMKHWlfBuM4eBDYKQjFAjAD&adurl='
GoogleImageDetailJob.new.perform(Scrape.first.id, 1, url)
exit
url = 'http://www.google.com/aclk?sa=l&ai=DChcSEwiKjazR6v3OAhUPgmkKHUP_Cf4YABAN&sig=AOD64_1NGd7X4L2ouIibTRE6va0UhqbliQ&ctype=5&rct=j&q=&ved=0ahUKEwjbg6vR6v3OAhXr7IMKHWlfBuM4eBDYKQjFAjAD&adurl='
GoogleImageDetailJob.new.perform(Scrape.first.id, 1, url)
exit
url = 'http://www.google.com/aclk?sa=l&ai=DChcSEwiKjazR6v3OAhUPgmkKHUP_Cf4YABAN&sig=AOD64_1NGd7X4L2ouIibTRE6va0UhqbliQ&ctype=5&rct=j&q=&ved=0ahUKEwjbg6vR6v3OAhXr7IMKHWlfBuM4eBDYKQjFAjAD&adurl='
GoogleImageDetailJob.new.perform(Scrape.first.id, 1, url)
exit
url = 'http://www.google.com/aclk?sa=l&ai=DChcSEwiKjazR6v3OAhUPgmkKHUP_Cf4YABAN&sig=AOD64_1NGd7X4L2ouIibTRE6va0UhqbliQ&ctype=5&rct=j&q=&ved=0ahUKEwjbg6vR6v3OAhXr7IMKHWlfBuM4eBDYKQjFAjAD&adurl='
GoogleImageDetailJob.new.perform(Scrape.first.id, 1, url)
exit
url = 'http://www.google.com/aclk?sa=l&ai=DChcSEwiKjazR6v3OAhUPgmkKHUP_Cf4YABAN&sig=AOD64_1NGd7X4L2ouIibTRE6va0UhqbliQ&ctype=5&rct=j&q=&ved=0ahUKEwjbg6vR6v3OAhXr7IMKHWlfBuM4eBDYKQjFAjAD&adurl='
GoogleImageDetailJob.new.perform(Scrape.first.id, 1, url)
url
uri = URI.parse(url)
uri.port
url
Net::HTTP::SSL_IVNAMES
Net::HTTP::SSL_ATTRIBUTES
url
uri = URI.parse(url)
proxy = pop_proxy
include RvxRds::Proxy
proxy = pop_proxy
request = proxy_req(proxy, uri, false)
response = request.get(uri)
response.body
Listing.where('original_url like "%aclk%"').last
url2 = _.original_url
uri = URI.parse(url2)
request = proxy_req(proxy, uri, false)
response = request.get(uri)
request = proxy_req(proxy, uri, true)
response = request.get(uri)
url
url2
uri.scheme
response.body
response.header
response.header['Location']
url
uri = URI.parse(url)
use_ssl = uri.scheme == 'https' ? true : false
request = proxy_req(proxy, uri, use_ssl)
response = request.get(uri)
response.body
url
url2
exit
Listing.where(original_url: 'https://www.google.com/aclk?sa=l&ai=DChcSEwiD2-fd6v3OAhWHW34KHaeACV0YABAn&sig=AOD64_21-jUA_-rUAgb2CESt6NiiK_eVOg&ctype=5&q=&ved=0ahUKEwjA3Obd6v3OAhUY92MKHaB1ADE4jAEQ2CkIsAMwEw&adurl=').last
l = _
l.screenshots.last.attachment.url
url = l.original_url
include RvxRds::Proxy
uri = URI.parse(url)
proxy = pop_proxy
use_ssl = uri.scheme == 'https' ? true : false
request = proxy_req(proxy, uri, use_ssl)
response = request.get(uri)
response.header['Location']
url = _
detail_url = URI.unescape(CGI.unescape('http' + url.split('http').last))
l.original_url
url = _
s = Scrape.last
s.listings.destroy_all
reload!
s = Scrape.last
s.listings.destroy_all
url = 'https://www.google.com/aclk?sa=L&ai=DChcSEwijodba6_3OAhVEbH4KHTZdB6IYABAN&sig=AOD64_0mYONOrL4wtIbp3OFWw1qMb-SDPQ&ctype=5&q=&ved=0ahUKEwigl9Xa6_3OAhVX3WMKHVhYD8M4tAEQ2CkI3QIwBg&adurl='
def get_redirect_location(url)
  uri = URI.parse(url)
  proxy = pop_proxy
  use_ssl = uri.scheme == 'https' ? true : false
  request = proxy_req(proxy, uri, use_ssl)
  response = request.get(uri)
  if use_ssl
    # Redirect location is in header
    response.header['Location']
  else
    # Redirect location inside a script tag of the body
    response.body.match(%r{(?<=adurl=)http.+(?=\"\);\<\/script)})[0]
  end
end
url = get_redirect_location(url)
detail_url = URI.unescape(CGI.unescape('http' + url.split('http').last))
Listing.last
s = Scrape.last
s.listings.pluck(:id, :original_url)
l = Listing.find(3063)
l.screenshots.attachment.url
l.screenshots.first.attachment.url
s
s.reload
scrape_id = s.id
s.reload
UpdateScrapeCounterJob.new.perform_async(scrape_id)
UpdateScrapeCounterJob.perform_async(scrape_id)
UpdateScrapeDurationJob.perform_async(scrape_id)
s = Scrape.last
s.listings.pluck(:url)
exit
url = 'https://www.google.com/shopping/product/6336061729452959950?hl=en&output=search&q=audio+technica+noise+cancelling+headphones&oq=audio+technica+noi&prds=paur:ClkAsKraXwncqYo8iNLcsIMAIFWNgqk2P9MLkZkTvYQiSQ1ugbUeajuAhmwwM3Bq9_aMY63WpaaOj8q5nk0LU9JYux8NyruNPZTaKK_UnFe-9OnfPfr9vLjYIRIZAFPVH70RNlXrngsxSNKY_X4EqDa7yYZMEg&sa=X&ved=0ahUKEwjIj8fVx4DPAhUJFh4KHUE7AmkQ8wIImgcwAw'
url = 'https://www.google.com/aclk?sa=l&ai=DChcSEwi8m8nVx4DPAhXEH4YKHZeWCm4YABAJ&sig=AOD64_24-R7N2-slVxfJ8dJAALLOAxtq4Q&adurl=&ctype=5&q=&ved=0ahUKEwjIj8fVx4DPAhUJFh4KHUE7AmkQpysITw'
scrape_id = Scrape.last.id
Listing.last.categories
source_category_id = SourceCategory.where(source_id: 1, category_id: 703).id
source_category_id = SourceCategory.where(source_id: Source.find_by(name: 'gsus'), category_id: 703).first.id
GoogleImageDetailJob.new.peform(scrape_id, source_category_id, url)
GoogleImageDetailJob.new.perform(scrape_id, source_category_id, url)
@session
s
help
show-stack
c
next
detail_url
next
@session
@session.attributes
@session.mode
@session.app
@session.current_scope
@session.server
@session.driver
@session.driver[:options]
@session.driver.class
@session.driver.client
@session.driver.client[:phantomjs_options]
@session.driver.client.phantomjs_options
options = @session.driver.client.phantomjs_options
options.select {|opt| opt.include?('--proxy')
options.select {|opt| opt.include?('--proxy') }
options.select {|opt| opt.include?('--proxy=') }
options.select {|opt| opt.include?('--proxy=') }[0]
options.select {|opt| opt.include?('--proxy=') }[0].gsub('--proxy=', '')
continue
GoogleImageDetailJob.new.perform(scrape_id, source_category_id, url)
continue
reload!
GoogleImageDetailJob.new.perform(scrape_id, source_category_id, url)
reload!
GoogleImageDetailJob.new.perform(scrape_id, source_category_id, url)
Listing.last
GoogleImageDetailJob.new.perform(scrape_id, source_category_id, url)
exit
Listing.first
exit
include CapybaraJob
require 'jobs/capybara_job'
include CapybaraJob
visit_page('http://www.whoishostingthis.com/tools/user-agent/')
cd visit_page
ls visit_page
visit_page('http://www.whoishostingthis.com/tools/user-agent/')
phantomjs = CapybaraPhantomJs.new
proxy = CapybaraPhantomJs::pop_proxy
CapybaraPhantomJs::visit_page('http://www.whoishostingthis.com/tools/user-agent/')
exit
require '/Users/jonathan/rvx-rds/lib/scraping/capybara_phantomjs.rb'
CapybaraPhantomJs::visit_page('http://www.whoishostingthis.com/tools/user-agent/')
include CapybaraPhantomJs
proxy = CapybaraPhantomJs::pop_proxy
CapybaraPhantomJs::visit_page('http://www.whoishostingthis.com/tools/user-agent/')
puts @session.html
reload
reload!
require '/Users/jonathan/rvx-rds/lib/scraping/capybara_phantomjs.rb'
load '/Users/jonathan/rvx-rds/lib/scraping/capybara_phantomjs.rb'
include CapybaraPhantomJs
CapybaraPhantomJs::visit_page('http://www.whoishostingthis.com/tools/user-agent/')
@session.driver.header
exit
edit -t
agents.count
agents.uniq.count
agents.uniq!
agents.sort!
agents.uniq!
agents.count
agents
exit
Redis.current
Redis.keys
Redis.current.keys
Redis.current.flushall
exit
Redis.current.flushall
exit
def generate_categories(file)
  xlsx = open_spreadsheet(file)
  sources = {}
  source_names = Source.where(source_type_id: 1).pluck(:name)
  xlsx.sheets.each do |source_name|
    next unless source_names.include?(source_name)
    sheet = xlsx.sheet(source_name)
    columns = sheet.first.reject { |cell| cell.blank? }.map { |col| col.downcase.gsub(/\s+/, '_') }
    categories = []
    (xlsx.first_row + 1..xlsx.last_row).each do |i|
      cur_row = xlsx.row(i).map { |row| row.nil?  ? row : row.strip.gsub(/\s+/, ' ') }
      info = {}
      columns.each_with_index do |col, idx|
        info[col.to_sym] = cur_row[idx]
      end
      categories << info
    end
    sources[source_name.to_sym] = categories
  end
  File.open('db/seeds/categories.rb', 'w') do |file|
    file.puts "@source_categories = {"
    i = 1
    sources.each_pair do |source, categories|
      file.print "  #{source.to_sym}: " #4
      file.puts "["
      categories.each_with_index do |cat, idx|
        next if cat.values.compact.blank?
        file.puts "    #{cat.insp        file.puts "    #{cat.ingt        file."
                                                              '                                  .p                                                 .ni                                  pa                                                              '                                  .en
                  (f                  (f                  (f          fi                  (f                  (f                  (f          fi             Ra              ,                   (f                  (f  ai                  (f                  s.        en                  (f   
edit -t
path = File.join(Rails.root, 'db', 'seeds', 'rds_categories.xlsx')
file = path
xlsx = open_spreadsheet(file)
sources = {}
source_names = Source.where(source_type_id: 1).pluck(:name)
source_name = xlsx.sheets.first
source_names.include?(source_name)
source_name = xlsx.sheets[1]
sheet = xlsx.sheet(source_name)
columns = sheet.first.reject { |cell| cell.blank? }.map { |col| col.downcase.gsub(/\s+/, '_') }
categories = []
i = xlsx.first_row + 1
cur_row = xlsx.row(i).map { |row| row.nil?  ? row : row.strip.gsub(/\s+/, ' ') }
xlsx.row(i)
exit
Redis.current.flushall
exit
Scrape.last
Scrape.last.destroy
exit
sc = SourceCategory.find(-1)
sc = SourceCategory.find_by(md5hash: '30e83445de49b323f3ac80da4b56194b')
source_id, category_id, url, md5hash = -1, -1, 'https://www.google.com/searchbyimage?btnG=Search+by+image&hl=en&image_url=https%3A%2F%2Fs3.amazonaws.com%2Frvx-rds-public%2FBLUE%2BLOGO.PN', '30e83445de49b323f3ac80da4b56194b'
sc.url == url
sc.source_id == source_id
sc.category_id == category_id
edit -t
url
sc.url
exit
edit -t
source = Source.find_by(name: "gsimg")
config = SUNPOWER_LOGO_CONFIG.first
category = Category.find_or_create_by({ id: config[:id], name: config[:name] })
url = build_gimg_url(config[:url])
md5hash = Digest::MD5.hexdigest(
  {
    source_id: source.id,
    category_id: category.id,
    parent_id: nil,
    url: url
}.to_s)
SourceCategory.find_by(md5hash: md5hash)
sc = SourceCategory.find_by(source: source, category: category, url: url)
url
url.url
url.to_s
md5hash = Digest::MD5.hexdigest(
  {
    source_id: source.id,
    category_id: category.id,
    parent_id: nil,
    url: url.to_s
}.to_s)
SourceCategory.find_by(md5hash: _)
exit
Scrape.last
Scrape.last.destroy
exit
Redis.current.flushall
exot
exit
url = 'https://login.alibaba.com/?from=sm&return_url=http%3A%2F%2Fsec.alibaba.com%2Fquery.htm%3Faction%3DQueryAction%26event_submit_do_login%3Dok%26smApp%3Dmagellan%26smPolicy%3Dmagellan-product_category-anti_Spider-htmlrewrite-checklogin%26smCharset%3DUTF-8%26smTag%3DMzguMTIwLjE3LjIxNiwsYmRlNDA2NTM5NzZlNDVjODk2NjcyNWJhY2QxZDEwNmI%253D%26smReturn%3Dhttps%253A%252F%252Fwww.alibaba.com%252Fcatalogs%252Fproducts%252FCID52806------------------------------10-3984--------------------------------------------------------ATTR-10-3984%252F1%26smSign%3Df059RuJ77N%252FFu7CgKbIO%252Fw%253D%253D%26smLocale%3Den_US'
URI.decode(url)
URI.decode(URI.decode(url))
exit
require '/Users/jonathan/rvx-rds/lib/scraping/capybara_phantomjs.rb'
include CapybaraPhantomJs
url = 'https://www.alibaba.com/catalogs/products/CID52806----------------------------G--10-3984/1'
@session = nil
CapybaraPhantomJs::visit_page('https://www.alibaba.com/catalogs/products/CID52806----------------------------G--10-3984/1')
is_defined?(:logger)
defined? :logger
reload
reload!
require '/Users/jonathan/rvx-rds/lib/scraping/capybara_phantomjs.rb'
load '/Users/jonathan/rvx-rds/lib/scraping/capybara_phantomjs.rb'
CapybaraPhantomJs::visit_page('https://www.alibaba.com/catalogs/products/CID52806----------------------------G--10-3984/1')
@session
load '/Users/jonathan/rvx-rds/lib/scraping/capybara_phantomjs.rb'
include CapybaraPhantomJs
CapybaraPhantomJs::visit_page('https://www.alibaba.com/catalogs/products/CID52806----------------------------G--10-3984/1')
edit -t
blacklist.sort
exit
load '/Users/jonathan/rvx-rds/lib/scraping/capybara_phantomjs.rb'
include CapybaraPhantomJs
load '/Users/jonathan/rvx-rds/lib/scraping/capybara_phantomjs.rb'
include CapybaraPhantomJs
exit
require '/Users/jonathan/rvx-rds/lib/scraping/capybara_phantomjs.rb'
include CapybaraPhantomJs
@session = CapybaraPhantomJs::visit_page('https://www.alibaba.com/')
@session.current_url
@session.driver.browser.cookies
load '/Users/jonathan/rvx-rds/lib/scraping/capybara_phantomjs.rb'
include CapybaraPhantomJs
@session = CapybaraPhantomJs::visit('https://www.alibaba.com')
@session.visit('https://www.alibaba.com/catalogs/products/CID52806----------------------------G--10-3984')
@session
load '/Users/jonathan/rvx-rds/lib/scraping/capybara_phantomjs.rb'
exit
load '/Users/jonathan/rvx-rds/lib/scraping/capybara_phantomjs.rb'
include CapybaraPhantomJs
@session = CapybaraPhantomJs::visit_page('https://www.alibaba.com')
@session.driver.browswer.cookies
@session.driver.browser.cookies
@session.visit('https://www.alibaba.com/catalogs/products/CID52806----------------------------G--10-3984')
@session
@session.attributes
session
exit
include Capybara::DSL
include RvxRds::Proxy
require 'capybara/poltergeist'
require 'rvx_rds/proxy'
include RvxRds::Proxy
options = {
  load_images: true,
  proxy: (ENV['SOCKS_PROXY'].present? ? ENV['SOCKS_PROXY'] : pop_proxy)
}.merge!(opts)
opts={}
load_images: true,
proxy: (ENV['SOCKS_PROXY'].present? ? ENV['SOCKS_PROXY'] : pop_proxy)
}.merge!(opts)
options = {
  load_images: true,
  proxy: (ENV['SOCKS_PROXY'].present? ? ENV['SOCKS_PROXY'] : pop_proxy)
}.merge!(opts)
Capybara.register_driver :poltergeist do |app|
  # logger.info "Proxy: #{options[:proxy]}" if logger.present?
  # Ensure proxy is set before request
  fail 'Proxy is not available' if options[:proxy].nil?
  poltergeist_opts = {
    js_errors: false,
    timeout: 60,
    phantomjs_logger: FakePoltergeistLogger,
    phantomjs_options: [
      '--ignore-ssl-errors=yes',
      "--load-images=#{options[:load_images]}",
      '--web-security=false',
      "--proxy=38.80.29.0:60000",
      "--disk-cache=yes",
      "--max-disk-cache-size=200000",
      "--ssl-protocol=any"
    ]
  }
  if (ENV['SOCKS_PROXY'].blank?)
    poltergeist_opts[:phantomjs_options] << "--proxy-auth=#{ENV['PROXY_USERNAME']}:#{ENV['PROXY_PASSWORD']}"
  else
    poltergeist_opts[:phantomjs_options] << "--proxy-type=socks5"
  end
  Capybara::Poltergeist::Driver.new(app, poltergeist_opts)
end
Capybara.current_driver = :poltergeist
@session = Capybara::Session.new(:poltergeist, default_wait_time: 60, timeout: 60)
user_agent = pop_user_agent
@session.driver.headers = { 'User-Agent' => user_agent }
module FakePoltergeistLogger
  def self.puts(*)
  end
end
module FakePoltergeistLogger
  def self.puts(*)
  end
end
poltergeist_opts = {
  js_errors: false,
  timeout: 60,
  phantomjs_logger: FakePoltergeistLogger,
  phantomjs_options: [
    '--ignore-ssl-errors=yes',
    "--load-images=#{options[:load_images]}",
    '--web-security=false',
    "--proxy=38.80.29.0:60000",
    "--disk-cache=yes",
    "--max-disk-cache-size=200000",
    "--ssl-protocol=any"
  ]
}
Capybara.register_driver :poltergeist do |app|
  # logger.info "Proxy: #{options[:proxy]}" if logger.present?
  # Ensure proxy is set before request
  fail 'Proxy is not available' if options[:proxy].nil?
  poltergeist_opts = {
    js_errors: false,
    timeout: 60,
    phantomjs_logger: FakePoltergeistLogger,
    phantomjs_options: [
      '--ignore-ssl-errors=yes',
      "--load-images=#{options[:load_images]}",
      '--web-security=false',
      "--proxy=38.80.29.0:60000",
      "--disk-cache=yes",
      "--max-disk-cache-size=200000",
      "--ssl-protocol=any"
    ]
  }
  if (ENV['SOCKS_PROXY'].blank?)
    poltergeist_opts[:phantomjs_options] << "--proxy-auth=#{ENV['PROXY_USERNAME']}:#{ENV['PROXY_PASSWORD']}"
  else
    poltergeist_opts[:phantomjs_options] << "--proxy-type=socks5"
  end
  Capybara::Poltergeist::Driver.new(app, poltergeist_opts)
end
Capybara.current_driver = :poltergeist
@session = Capybara::Session.new(:poltergeist, default_wait_time: 60, timeout: 60)
@session.driver.headers = { 'User-Agent' => user_agent }
@session.driver.headers
edit -t
page
page.driver.headers
page.driver
page.driver.headers = { 'User-Agent' => user_agent }
page.driver
page.driver.browser
page.driver.browser.headers
page.driver.headers
exit
require 'capybara/poltergeist'
require 'rvx_rds/proxy'
module FakePoltergeistLogger
  def self.puts(*)
  end
end
include RvxRds::Proxy
opts={}
edit -t
session = Capybara::Session.new(:poltergeist, default_wait_time: 60, timeout: 60)
session
user_agent = pop_user_agent
session.driver.headers
session.driver.headers = { 'User-Agent' => user_agent }
session = Capybara.current_session
session.headers
session.driver.headers
session.driver.headers['User-Agent'] = user_agent
exit
edit -t
scraper = Scraper.new
scraper.visit_page('http://www.alibaba.com')
scraper.session
scraper.instance_variable_get(:@session)
scraper.instance_variable_get(:@session).driver
scraper.instance_variable_get(:@session).driver.browser.cookies
scraper.visit_page('www.ruvixx.com')
scraper.visit_page('http://www.ruvixx.com')
scraper.visit_page('https://www.ruvixx.com')
exit
options = {
  load_images: true,
  proxy: (ENV['SOCKS_PROXY'].present? ? ENV['SOCKS_PROXY'] : pop_proxy)
}
require 'capybara/poltergeist'
require 'rvx_rds/proxy'
include RvxRds::Proxy
options = {
  load_images: true,
  proxy: (ENV['SOCKS_PROXY'].present? ? ENV['SOCKS_PROXY'] : pop_proxy)
}
edit
browser = Capybara.current_session
user_agent = pop_user_agent
browser.headers
browser.driver
browser.driver.headers
browser.driver
browser.driver.headers
browser.driver.add_headers({ 'User-Agent' => user_agent })
exit
include CapybaraPhantomJs
require '/Users/jonathan/rvx-rds/lib/scraping/capybara_phantomjs.rb'
# include Capybara::DSL
require '/Users/jonathan/rvx-rds/lib/scraping/capybara_phantomjs.rb'
edit
session.visit('https://www.alibaba.com')
session.driver.browser.headesr
session.driver.browser.headers
session.headers
session.drivers.headers
session.driver.browser.headers
session.driver
session.driver.headers
page
session
session.driver
session.driver.reset
session.driver.clear_network_traffic
ENV
session.driver.cookies
exit
edit -t
Capybara.default_driver = :poltergeist
browser = Capybara.current_session
include RvxRds::Proxy
require 'rvx_rds/proxy'
include RvxRds::Proxy
user_agent = pop_user_agent
browser.driver.add_headers({ 'User-Agent' => user_agent })
browser.visit('https://www.alibaba.com')
browser.driver.cookies
browser.html
browser.visit('https://www.alibaba.com/catalogs/products/CID52806------------------------------10-3984--------------------------------------------------------ATTR-10-3984/1')
browser.html
browser.current_url
browser.save_and_open_screenshot
exit
Scrape.last
Scrape.last.destroy
Redis.current.flushall
exit
source = Source.find_by(name: 'alius')
SourceCategory.where(source: source)
SourceCategory.where(source: source).active.count
source = Source.find_by(name: 'alius').where(test_process: true)
source = Source.where(name: 'alius').where(test_process: true)
source = Source.find_by(name: 'alius')
SourceCategory.where(source: source).where(test_process: true)
SourceCategory.where(source: source).where(test_process: true).each do |sc|
  sc.url = sc.url.gsub(/\/1/, '')
  sc.save
end
exit
Scrape.last
Scrape.last.destroy
Redis.current.flushall
exit
edit -t
proxies = %w(38.80.29.0:60000
38.80.29.1:60000
38.80.29.2:60000
38.80.29.3:60000
38.80.29.4:60000
38.80.29.5:60000
38.80.29.6:60000
38.80.29.7:60000
38.80.29.8:60000
38.80.29.9:60000
38.80.29.10:60000
38.80.29.11:60000
38.80.29.12:60000
38.80.29.13:60000
38.80.29.14:60000
38.80.29.15:60000
38.80.29.16:60000
38.80.29.17:60000
38.80.29.18:60000
38.80.29.19:60000
38.80.29.20:60000
38.80.29.21:60000
38.80.29.22:60000
38.80.29.23:60000
38.80.29.24:60000
38.80.29.25:60000
38.80.29.26:60000
38.80.29.27:60000
38.80.29.28:60000
38.80.29.29:60000
38.80.29.30:60000
38.80.29.31:60000
38.80.29.32:60000
38.80.29.33:60000
38.80.29.34:60000
38.80.29.35:60000
38.80.29.36:60000
38.80.29.37:60000
38.80.29.38:60000
38.80.29.39:60000
38.80.29.40:60000
38.80.29.41:60000
38.80.29.42:60000
38.80.29.43:60000
38.80.29.44:60000
38.80.29.45:60000
38.80.29.46:60000
38.80.29.47:60000
38.80.29.48:60000
38.80.29.49:60000
38.80.29.50:60000
38.80.29.51:60000
38.80.29.52:60000
38.80.29.53:60000
38.80.29.54:60000
38.80.29.55:60000
38.80.29.56:60000
38.80.29.57:60000
38.80.238.80.238.80.230.38.80.238.80.238038.80.238.80.23.838.80.238.80.238.38.80.238.80.233838.80.238.80.23
33333333333333330
33333333333333300333333333333333003333333333333330003333333333333330003333333333333330003333333333333330003333333333333330003333333333333330003333333333333330003333333333333330000
38.80.29.75:600038.80.29.75:60000038.80.29.75:60000038.80.29.75:60000038.80.29.75:6000000
38.80.29.80:600038.80.29.80:60000038.80.29.80:60000038.80.29.80:60000038.80.29.80:60000038.80.29.80:60000038.80.29.80:6000000
38.80.29.87:600038.80.29.87:6000000
38.80.29.89:600038.80.29.89:60000038.8080.29.91:600038.80.29.89:60000038.80.29.89:60000038.80.29.89:60000038.80.29.89:60000038.80.29.89:60000038.80.29.89:60000038.80.29.89:60000038.80.29.89:60000038.80.29.89:60000000
38.80.29.101:6038.80.29.101:603:638.80.29.101:6033:38.80.29.101:6030438.80.29.101:6031038.80.29.101:603.138.80.29.101:6039.38.80.29.101:6032938.80.29.101:603.238.80.29.101:6030.38.80.29.101:6038038.80.29.101:603.838.80.29.101:6038.38.80.29.101:6033838.80.29.101:6038.80.29.101:603:638.80.29.101:6033:38.80.29.101:6030438.80.29.101:6031038.80.29.101:603.136038.80.29.101:603:638.80.29.101:6038.80.29.101:603:638.80.29.101:6033:38.80.29.101:6030438.80.29.101:6031038.80.29.101:603.138.80.29.101:6039.38.80.29.101:6032938.80.29.101:603.238.80.29.101:6030.3.838.80.29.101:6038.80.29.101:603:638.80.29.101:6033:38.80.29.101:6030438.80.29.101:6031030038.80.29.101:6038.80.29.101:603:638.80.29.101:6033:38.80.29.101:6030438.80.29.101:6031038.80.29.101:603.138.80.29.101:6039.31438.80.29.101:6038.80.29.101:603:639.38.80.29.101:6038.80.29.101:603:638.80.29.101:6033:38.80.29.101:6030438.80.29.101:6031038.80.29.101:603.138.80.29.101:6039.38.80.29.101:6032938.80.29.101:603.238.80.29.101:6030.38.80.29.101:6038038.80.29.101:603.83800
edit -t
proxies.first.split('.')[0..1]
proxies.first.split('.')[0..1].join('.')
subs1 = proxies.map {|proxy| proxies.split('.')[0..1].join('.') }
subs1.count
subs1.uniq!
subs1.count
subs1
subs1 = proxies.map {|proxy| proxy.split('.')[0..1].join('.') }
subs1.uniq!
smallset = []
subs1.each { |sub| smallset << proxies.first {|p| p.include?(sub) } }
smallset
proxies
subs1.each { |sub| smallset << proxies.first {|p| p.include?(sub) } }
smallset
subs1.each { |sub| smallset << proxies.find {|p| p.include?(sub) } }
smallset = []
subs1.each { |sub| smallset << proxies.find {|p| p.include?(sub) } }
smallset
Scrape.last
Scrape.last.destroy
Redis.current.flushall
exit
Scrape.last.destroy
Redis.current.flushall
exit
Scrape.last.destroy
Redis.current.flushall
exit
Scrape.last.destroy
Redis.current.flushall
require 'capybara/poltergeist'
require 'rvx_rds/proxy'
include RvxRds::Proxy
edit -t
edit
pop_proxy
pop_user_agent
RvxRds::Proxy.pop_proxy
exit
edit -t
require 'capybara/poltergeist'
Capybara.register_driver :poltergeist do |app|
  poltergeist_opts = {
    js_errors: false,
    timeout: 60,
    phantomjs_options: [
      '--ignore-ssl-errors=yes',
      "--load-images=true",
      '--web-security=false',
      "--proxy=38.80.29.0:60000",
      "--disk-cache=yes",
      "--max-disk-cache-size=200000",
      "--ssl-protocol=any"
    ]
  }
  poltergeist_opts[:phantomjs_options] << "--proxy-auth=#{ENV['PROXY_USERNAME']}:#{ENV['PROXY_PASSWORD']}"
  Capybara::Poltergeist::Driver.new(app, poltergeist_opts)
end
Capybara.default_driver = :poltergeist
browser = Capybara.current_session
browser.visit 'https://www.alibaba.com'
browser.driver.browser.cookies
browser.driver.browser.cookies.map {|c| "#{c.name}=#{c.value}"}
cookies = browser.driver.browser.cookies
cookies
cookies.class
cookies = browser.driver.browser.cookies.class
cookies = []
browser.driver.browser.cookies.each {|k, v| cookies << "#{v.name}=#{v.value}"}
cookies
browser.driver.cookies
browser.visit('https://www.alibaba.com/catalogs/products/CID52806----------------------------G--10-3984/2')
browser.save_and_open_screenshot
browser.visit('https://www.alibaba.com/catalogs/products/CID52806----------------------------G--10-3984/4')
browser.save_and_open_screenshot
browser.save_and_open_page
Redis.current.flushall
Scrape.last
exit
require 'capybara/poltergeist'
edit -t
cookies = []
browser.driver.browser.cookies.each {|k, v| cookies << {v.name => v.value} }
cookies
cookies.reduce({})
cookies.inject({})
cookies.inject(:merge)
cookies = browser.driver.browser.cookies
browser.driver.cookies = cookies
cookiecookies.first
cookie= cookies.first
cookies
cookies.each do |k, v|
v = cookies.first[0]
cookies.first
v = cookies.first[1]
v.attributes
v.to_h
v
v.as_json
v.hash
v
v.display
v.as_json
v.as_json['attributes
v.as_json['attributes']
v.as_json['attributes'].except('name', 'value')
v.as_json['attributes'].except('name', 'value').symbolize_keys
cookies = browser.driver.cookies
cookies = browser.driver.browser.cookies
cookies = browser.driver.cookies
cookies = browser.driver.browser.cookies
cookies
cookies.as_json
cookies.as_json.values
cookies cookies.as_json.values.map {|cookie| cookie['attributes'] }
cookies = cookies.as_json.values.map {|cookie| cookie['attributes'] }
cookies = cookies.as_json.symbolize_keys.values.map {|cookie| cookie['attributes'] }
cookies = browser.driver.browser.cookies
cookies = cookies.as_json.symbolize_keys.values.map {|cookie| cookie['attributes'] }
cookies
cookies = cookies.as_json.symbolize_keys.values.map {|cookie| cookie['attributes'].symbolize_keys }
cookies = browser.driver.browser.cookies
cookies = cookies.as_json.symbolize_keys.values.map {|cookie| cookie['attributes'].symbolize_keys }
cookie = cookies.first
name, value = cookie.delete(:name, :value)
name, value = cookie.delete(:name), cookie.delete(:value)
cookie
Scrape.last
exit
edit -t
cookie1
cookie1 = Oj.load(cookie1)
JSON.parse(cookie1)
edit -t
c1.each do |c|
c1.class
c1.first
c1.length
edit -t
c1.class
edit -t
c2.calss
c2.class
c1.keys.each do |key|
  if c1[key]['value'] == c2[key].try(:[], 'value')
    puts "#{c1[key]} == #{c2[key]}"
  else
    puts "#{c1[key]} != #{c2[key]}"
  end
end
c1.keys.each do |key|
  if c1[key]['value'] == c2[key].try(:[], 'value')
    puts "#{key} matches"
  else
    puts "#{key} doesn't match"
  end
end
c2.keys.each do |key|
  if c2[key]['value'] == c1[key].try(:[], 'value')
    puts "#{key} matches"
  else
    puts "#{key} doesn't match"
  end
end
c1.keys.sort
c2.keys.sort
c2.keys.sort - c1.keys.sort
c2
exit
require 'capybara/poltergeist'
require 'rvx_rds/proxy'
Capybara.register_driver :poltergeist do |app|
  poltergeist_opts = {
    js_errors: false,
    timeout: 60,
    phantomjs_options: [
      '--ignore-ssl-errors=yes',
      "--load-images=true",
      '--web-security=false',
      "--proxy=38.80.29.0:60000",
      "--disk-cache=yes",
      "--max-disk-cache-size=200000",
      "--ssl-protocol=any"
    ]
  }
  poltergeist_opts[:phantomjs_options] << "--proxy-auth=#{ENV['PROXY_USERNAME']}:#{ENV['PROXY_PASSWORD']}"
  Capybara::Poltergeist::Driver.new(app, poltergeist_opts)
end
Capybara.default_driver = :poltergeist
browser.visit 'https://www.alibaba.com'
browser = Capybara.current_session
browser = Capybara::Session.new(:poltergeist, default_max_wait_time: 60, timeout: 60)
browser.visit 'https://www.alibaba.com'
cookies = browser.driver.cookies.as_json.values.map {|cookie| cookie['attributes'].symbolize_keys }
cookies
browser.driver.clear_cookies
browser.driver.cookies
browser.driver.browser.cookies
cookies.each do |cookie|
  name, value = cookie.delete(:name), cookie.delete(:value)
  # @session.driver.set_cookie(name, value, cookie)
  browser.driver.set_cookie(name, value, cookie)
end
browser.cookies
browser.driver.browser.cookies
browser.driver.cookies
browser.visit('https://www.alibaba.com/catalogs/products/CID52806----------------------------G--10-3984/2')
browser.save_and_open_screenshot
browser.visit('https://www.alibaba.com/catalogs/products/CID52806----------------------------G--10-3984/1')
browser.save_and_open_screenshot
browser.visit('https://www.alibaba.com/catalogs/products/CID52806----------------------------G--10-3984/2')
Capybara.register_driver :poltergeist do |app|
  poltergeist_opts = {
    js_errors: false,
    timeout: 60,
    phantomjs_options: [
      '--ignore-ssl-errors=yes',
      "--load-images=true",
      '--web-security=false',
      "--proxy=160.20.10.0:60000",
      "--disk-cache=yes",
      "--max-disk-cache-size=200000",
      "--ssl-protocol=any"
    ]
  }
  poltergeist_opts[:phantomjs_options] << "--proxy-auth=#{ENV['PROXY_USERNAME']}:#{ENV['PROXY_PASSWORD']}"
  Capybara::Poltergeist::Driver.new(app, poltergeist_opts)
end
Capybara.default_driver = :poltergeist
cookies
browser = Capybara::Session.new(:poltergeist, default_max_wait_time: 60, timeout: 60)
browser.driver
browser.driver.clear_cookies
cookies.each do |cookie|
  name, value = cookie.delete(:name), cookie.delete(:value)
  # @session.driver.set_cookie(name, value, cookie)
  browser.driver.set_cookie(name, value, cookie)
end
browser.visit('https://www.alibaba.com/catalogs/products/CID52806----------------------------G--10-3984/3')
browser.save_and_open_screenshot
browser.driver.cookies
browser.visit('https://www.alibaba.com/catalogs/products/CID52806----------------------------G--10-3984/4')
browser.visit('https://www.alibaba.com/catalogs/products/CID52806----------------------------G--10-3984/6')
browser.driver.quit
exit
proxy = '38.80.29.0:60000
proxy = '38.80.29.0:60000'
require 'rvx_rds/proxy'
RvxRds::pop_proxy
RvxRds::Proxy.pop_proxy
RvxRds::Proxy::pop_proxy
RvxRds::Proxy
include RvxRds::Proxy
pop_proxy
exit
edit
browser = get_browser
browser.visit 'https://www.alibaba.com'
cookies = browser.driver.cookies.as_json.values.map {|cookie| cookie['attributes'].symbolize_keys }
browser.driver.clear_cookies
browser.driver.quit
browser = get_browser
browser.driver.clear_cookies
cookies.each do |cookie|
  name, value = cookie.delete(:name), cookie.delete(:value)
  browser.driver.set_cookie(name, value, cookie)
end
browser.visit('https://www.alibaba.com/catalogs/products/CID52806----------------------------G--10-3984/1')
browser.visit 'https://www.alibaba.com'
cookies = browser.driver.cookies.as_json.values.map {|cookie| cookie['attributes'].symbolize_keys }
browser.driver.clear_cookies
cookies.each do |cookie|
  name, value = cookie.delete(:name), cookie.delete(:value)
  browser.driver.set_cookie(name, value, cookie)
end
browser.visit('https://www.alibaba.com/catalogs/products/CID52806----------------------------G--10-3984/1')
browser.driver.headers
edit
user_agent = browser.driver.headers['User-Agent']
browser.driver.quit
browser = get_browser(user_agent)
browser.visit('https://www.alibaba.com/catalogs/products/CID52806----------------------------G--10-3984/2')
exit
edit
browser = get_browser
browser.visit 'https://www.alibaba.com'
cookies = browser.driver.cookies.as_json.values.map {|cookie| cookie['attributes'].symbolize_keys }
browser.driver.clear_cookies
cookies.each do |cookie|
  name, value = cookie.delete(:name), cookie.delete(:value)
  browser.driver.set_cookie(name, value, cookie)
end
user_agent = browser.driver.headers['User-Agent']
browser.driver.quit
browser = get_browser(user_agent)
user_agent = browser.driver.headers['User-Agent']
browser.driver.clear_cookies
cookies.each do |cookie|
  name, value = cookie.delete(:name), cookie.delete(:value)
  browser.driver.set_cookie(name, value, cookie)
end
browser.visit('https://www.alibaba.com/catalogs/products/CID52806----------------------------G--10-3984/1')
browser.save_and_open_screenshot
browser.visit 'https://www.alibaba.com'
browser.visit('https://www.alibaba.com/catalogs/products/CID52806----------------------------G--10-3984/1')
browser.driver.proxy
browser.driver.proxy?
browser.driver.browser.proxy
browser.driver.browser.methods
browser.driver.browser.instance_methods
browser.driver
browser.driver.cookies
cookies = browser.driver.cookies.as_json.values.map {|cookie| cookie['attributes'].symbolize_keys }
cookies
cookies.map(&:[], :value)
cookies.map(&:value)
cookies.map {|c| c[:value] }
browser.driver.quit
exit
edit
browser = get_browser
browser.visit 'https://www.alibaba.com'
browser = get_browser
browser.visit 'https://www.alibaba.com'
exit
require 'capybara/poltergeist'
require 'rvx_rds/proxy'
include RvxRds::Proxy
pop_proxy
proxy ||= pop_proxy
Capybara.register_driver :poltergeist do |app|
  poltergeist_opts = {
    js_errors: false,
    timeout: 60,
    phantomjs_options: [
      '--ignore-ssl-errors=yes',
      "--load-images=true",
      '--web-security=false',
      "--proxy=#{proxy}",
      "--disk-cache=yes",
      "--max-disk-cache-size=200000",
      "--ssl-protocol=any",
      "--proxy-auth=#{ENV['PROXY_USERNAME']}:#{ENV['PROXY_PASSWORD']}"
    ]
  }
  Capybara::Poltergeist::Driver.new(app, poltergeist_opts)
end
Capybara.current_driver = :poltergeist
browser = Capybara::Session.new(:poltergeist, default_max_wait_time: 60, timeout: 60)
user_agent ||= pop_user_agent
browser.driver.add_headers({ 'User-Agent' => user_agent })
browser.visit 'https://www.alibaba.com'
cookies = browser.driver.cookies.as_json.values.map {|cookie| cookie['attributes'].symbolize_keys }
browser.driver.clear_cookies
cookies.each do |cookie|
  name, value = cookie.delete(:name), cookie.delete(:value)
  browser.driver.set_cookie(name, value, cookie)
end
browser.visit('https://www.alibaba.com/catalogs/products/CID52806----------------------------G--10-3984/1')
browser.driver.clear_cookies
cookies.each do |cookie|
  name, value = cookie.delete(:name), cookie.delete(:value)
  browser.driver.set_cookie(name, value, cookie)
end
browser.visit('https://www.alibaba.com/catalogs/products/CID52806----------------------------G--10-3984/1')
user_agent = browser.driver.headers['User-Agent']
browser.driver
browser.driver.phantomjs_options
proxy = browser.driver.phantomjs_options[3].gsub(/.+proxy\=/, '')
browser.driver.quit
saved_proxy = proxy
proxy = pop_proxy
Capybara.register_driver :poltergeist do |app|
  poltergeist_opts = {
    js_errors: false,
    timeout: 60,
    phantomjs_options: [
      '--ignore-ssl-errors=yes',
      "--load-images=true",
      '--web-security=false',
      "--proxy=#{proxy}",
      "--disk-cache=yes",
      "--max-disk-cache-size=200000",
      "--ssl-protocol=any",
      "--proxy-auth=#{ENV['PROXY_USERNAME']}:#{ENV['PROXY_PASSWORD']}"
    ]
  }
  Capybara::Poltergeist::Driver.new(app, poltergeist_opts)
end
Capybara.current_driver = :poltergeist
browser = Capybara::Session.new(:poltergeist, default_max_wait_time: 60, timeout: 60)
user_agent ||= pop_user_agent
browser.driver.add_headers({ 'User-Agent' => user_agent })
browser.driver.clear_cookies
cookies.each do |cookie|
  name, value = cookie.delete(:name), cookie.delete(:value)
  browser.driver.set_cookie(name, value, cookie)
end
qjbrowser.visit('https://www.alibaba.com/catalogs/products/CID52806----------------------------G--10-3984/2')
browser.visit('https://www.alibaba.com/catalogs/products/CID52806----------------------------G--10-3984/2')
user_agent = browser.driver.headers['User-Agent']
saved_proxy2 = browser.driver.phantomjs_options[3].gsub(/.+proxy\=/, '')
saved_proxy
proxy
browser.driver.quit
edit 
browser.driver.clear_cookies
cookies.each do |cookie|
  name, value = cookie.delete(:name), cookie.delete(:value)
  browser.driver.set_cookie(name, value, cookie)
end
browser.visit('https://www.alibaba.com/catalogs/products/CID52806----------------------------G--10-3984/3')
proxy = nil
browser.driver.quit
proxy3 = proxy
proxy
edit
browser.driver.clear_cookies
cookies.each do |cookie|
  name, value = cookie.delete(:name), cookie.delete(:value)
  browser.driver.set_cookie(name, value, cookie)
end
browser.visit('https://www.alibaba.com/catalogs/products/CID52806----------------------------G--10-3984/5')
browser.driver.headers = { 'User-Agent' => user_agent }
browser.driver.clear_cookies
cookies.each do |cookie|
  name, value = cookie.delete(:name), cookie.delete(:value)
  browser.driver.set_cookie(name, value, cookie)
end
browser.visit('https://www.alibaba.com/catalogs/products/CID52806----------------------------G--10-3984/5')
cookies
browser.driver.quit
user_agent = nil
proxy = nil
edit
browser.driver.clear_cookies
cookies.each do |cookie|
  name, value = cookie.delete(:name), cookie.delete(:value)
  browser.driver.set_cookie(name, value, cookie)
end
browser.visit('https://www.alibaba.com/catalogs/products/CID52806----------------------------G--10-3984/6')
browser.driver.cookies
cookies = browser.driver.cookies.as_json.values.map {|cookie| cookie['attributes'].symbolize_keys }
cookies.map {|c| c[:value] }
proxy
cookies.map {|c| [c[:name], c[:value]] }
browser.response_headers
cookies == browser.driver.cookies.as_json.values.map {|cookie| cookie['attributes'].symbolize_keys }
cookies
exit
cache = RdsCache.new('api_cache')
cache.keys
exit
cache = RdsCache.new('api_cache')
cache.keys
cache = RedisCache.new
cache = RdsCache.new
cache.keys
cache.keys('proxies')
cache.keys('proxies:all')
cache.keys('proxies:all:*')
cache.fetch('proxies:all:*')
cache.fetch('proxies:all')
redis = Redis.new
redis.smembers('development_rvx_rds:proxies:all')
REDIS.call(:smembers, 'development_rvx_rds:proxies:all')
redis.call(:smembers, 'development_rvx_rds:proxies:all')
redis.flushall
exit
cache = RdsCache.new
cache.keys
cacke.smembers(_.first)
cache.smembers("development_rvx_rds:useragents:all")
cache.fakemethodjon("development_rvx_rds:useragents:all")
exit
cache = RdsCache.new
cache.keys
cache.smembers(_.first)
cache.smeasdfambers(_.first)
cache.command_map
NoMethodError
rase NoMethodError
raise NoMethodError
exit
cache = RdsCache.new
cache.keys
cache.adsflda
exit
cache = RdsCache.new
redis = Redis.current
redis.
redis.type
redist.type
redis = Redis.current
redis.type
redis.keys
redis.type("development_alius:proxies:all")
redis.close
redis.keys
redis.close
require 'capybara/poltergeist'
require 'rvx_rds/proxy'
include RvxRds::Proxy
proxy ||= pop_proxy
Capybara.register_driver :poltergeist do |app|
  poltergeist_opts = {
    js_errors: false,
    timeout: 60,
    phantomjs_options: [
      '--ignore-ssl-errors=yes',
      "--load-images=true",
      '--web-security=false',
      "--proxy=#{proxy}",
      "--disk-cache=yes",
      "--max-disk-cache-size=200000",
      "--ssl-protocol=any",
      "--proxy-auth=#{ENV['PROXY_USERNAME']}:#{ENV['PROXY_PASSWORD']}"
    ]
  }
  Capybara::Poltergeist::Driver.new(app, poltergeist_opts)
end
Capybara.current_driver = :poltergeist
browser = Capybara::Session.new(:poltergeist, default_max_wait_time: 60, timeout: 60)
user_agent ||= pop_user_agent
browser.driver.add_headers({ 'User-Agent' => user_agent })
browser
browser.visit 'https://www.alibaba.com'
browser.driver.headers
browser.driver.cookies
browser.response_headers
browser.driver.cookies
proxy = browser.driver.phantomjs_options[3].gsub(/.+proxy\=/, '')
user_agent =  browser.driver.headers['User-Agent']
cookies = browser.driver.browser.cookies.as_json.values.map {|cookie| cookie['attributes'] }
COOKIE_JAR ||= RdsCache.new('cookies')
COOKIE_JAR.set_value(proxy, {user_agent: user_agent, cookies: cookies})
cookies = COOKIE_JAR.fetch(proxy)
user_agent = cookies.delete('user_agent')
cookies
@session.driver.clear_cookies
browser.driver.clear_cookies
cookies['cookies'].each do |cookie|
  name, value = cookie.delete(:name), cookie.delete(:value)
  browser.driver.set_cookie(name, value, cookie)
end
browser.driver.cookies
cookies['cookies']
cookies['cookies'].each do |cookie|
  name, value = cookie.delete('name'), cookie.delete('value')
  browser.driver.set_cookie(name, value, cookie)
end
browser.driver.cookies
exit
Redis.current.flushall
exit
Redis.current.flushall
exit
edit
cookies1
cookies1.map! {|c| [c['name'], c['value'] }
cookies1.map! {|c| [c['name'], c['value']] }
cookies2.map! {|c| [c['name'], c['value']] }
cookies1.sort!
cookies2.sort!
edit
edit -t
edit
@session.visit('https://www.alibaba.com')
proxy = @session.driver.phantomjs_options[3].gsub(/.+proxy\=/, ''.freeze)
user_agent = @session.driver.headers['User-Agent']
cookies = @session.driver.browser.cookies.as_json.values.map { |cookie| cookie['attributes'] }
cookies.select {|c| %w(acs_usuc_t xman_t ali_apache_id xman_f)
cookies.select! {|c| %w(acs_usuc_t xman_t ali_apache_id xman_f).include?(c['name'])
cookies.select! {|c| %w(acs_usuc_t xman_t ali_apache_id xman_f).include?(c['name']) }
Redis.current.flushall
exit
Scrape.last
Scrape.last.destroy
Redis.current.flushall
edit
cookies1 = [{"domain"=>"login.alibaba.com", "expires"=>"Wed, 13 Sep 2017 22:27:32 GMT", "expiry"=>1505341652, "httponly"=>false, "name"=>"_umdata", "path"=>"/", "secure"=>false, "value"=>"F3DED6CFF81A8A1FF1A75D5095D0B9EEAEDFBA1CF787FB4D8C42A89F5F30A9F252290EF0378842F2C58B466574A24B59E13148C35B7A6BAD73DA9346B1C97CB726CCE41952FB59978F8E7428AF7A1A4705F5F38C5808116D07FCB49D23130DA8"}, {"domain"=>".alibaba.com", "expires"=>"Wed, 13 Sep 2017 22:27:32 GMT", "expiry"=>1505341652, "httponly"=>true, "name"=>"umdata_", "path"=>"/", "secure"=>true, "value"=>"F3DED6CFF81A8A1FF1A75D5095D0B9EEAEDFBA1CF787FB4D8C42A89F5F30A9F252290EF0378842F2C58B466574A24B59E13148C35B7A6BAD73DA9346B1C97CB726CCE41952FB59978F8E7428AF7A1A4705F5F38C5808116D07FCB49D23130DA8"}, {"domain"=>".alibaba.com", "httponly"=>false, "name"=>"_tb_token_", "path"=>"/", "secure"=>false, "value"=>"ee7eb5d3ea53e"}, {"domain"=>".alibaba.com", "expires"=>"Sat, 13 Jun 2026 23:07:31 GMT", "expiry"=>1781392051, "httponly"=>false, "name"=>"t", "path"=>"/", "secure"=>false, "value"=>"3a5a9c6a73f2490263d633015ec3b82c"}, {"domain"=>".alibaba.com", "httponly"=>true, "name"=>"cookie2", "path"=>"/", "secure"=>false, "value"=>"u99de6f129a25cadd939c00aa7d04121"}, {"domain"=>".alibaba.com", "httponly"=>false, "name"=>"v", "path"=>"/", "secure"=>false, "value"=>"0"}, {"domain"=>".alibaba.com", "expires"=>"Sun, 12 Mar 2017 22:27:31 GMT", "expiry"=>1489357651, "httponly"=>false, "name"=>"isg", "path"=>"/", "secure"=>false, "value"=>"ArCw7xpdk2sbnk9vJ6Cib4PtjXjFupRDpnlGX6oBfIveZVAPUglk0wbXP9f6"}, {"domain"=>".alibaba.com", "httponly"=>true, "name"=>"xman_t", "path"=>"/", "secure"=>false, "value"=>"s73LL7qoIIl1Xrt6DYBnjXMM8pddq9/GT0Jqp6UCdD3xAyOgRWsgOLJlXHYtWzF0RyrIp/HMBMU+hwhFXjVXpu1RWYviuoLL"}, {"domain"=>".alibaba.com", "httponly"=>false, "name"=>"ali_apache_tracktmp", "path"=>"/", "secure"=>false, "value"=>"\"\""}, {"domain"=>".alibaba.com", "expires"=>"Mon, 02 Oct 2084 01:41:38 GMT", "expiry"=>3621289298, "httponly"=>false, "name"=>"ali_apache_track", "path"=>"/", "secure"=>false, "value"=>"\"\""}, {"domain"=>".alibaba.com", "expires"=>"Fri, 01 Feb 2030 00:00:00 GMT", "expiry"=>1896134400, "httponly"=>false, "name"=>"l", "path"=>"/", "secure"=>false, "value"=>"AmNjVNr1/7oOCLvaK-h4KlRX8rwNWPea"}, {"domain"=>".alibaba.com", "expires"=>"Mon, 02 Oct 2084 01:41:38 GMT", "expiry"=>3621289298, "httponly"=>true, "name"=>"xman_f", "path"=>"/", "secure"=>false, "value"=>"D6pInumZ+uS4hGFmiTBr4xpWoLBfQhfWAiEQ4vEVyOGwOIQXVv2np6GZyDD0D/cPzQpYmaVnNHBU7Y79gpQkUOAcZRXYu9ZzPUkgRTXAWHXA3s879Uvegg=="}, {"domain"=>"login.alibaba.com", "httponly"=>false, "name"=>"JSESSIONID", "path"=>"/", "secure"=>false, "value"=>"y1adtq-SiqJVe5v+ZL1kEeh6"}, {"domain"=>".alibaba.com", "httponly"=>false, "name"=>"acs_rt", "path"=>"/", "secure"=>false, "value"=>"1942ed9bd278491990601583cc840a90"}, {"domain"=>".alibaba.com", "httponly"=>false, "name"=>"acs_usuc_t", "path"=>"/", "secure"=>false, "value"=>"acs_rt=1942ed9bd278491990601583cc840a90"}, {"domain"=>".alibaba.com", "expires"=>"Mon, 02 Oct 2084 01:41:23 GMT", "expiry"=>3621289283, "httponly"=>false, "name"=>"xman_us_f", "path"=>"/", "secure"=>false, "value"=>"x_l=0"}, {"domain"=>".alibaba.com", "expires"=>"Thu, 30 Nov 2084 01:01:01 GMT", "expiry"=>3626384461, "httponly"=>false, "name"=>"ali_apache_id", "path"=>"/", "secure"=>false, "value"=>"38.80.29.0.1473805636150.434263.5"}]
xt
xteqqexit
exit
edit -t
cookies1.map! {|c| [c['name'], c['value']] }
cookies2.map! {|c| [c['name'], c['value']] }
cookies3.map! {|c| [c['name'], c['value']] }
cookies1.sort!
cookies2.sort!
cookies3.sort!
cookies1 == cookies2
cookies1
cookies1.map {|i| i[0] }
cookies1.map! {|i| i[0] }
cookies2.map! {|i| i[0] }
cookies3.map! {|i| i[0] }
cookies1.sort!
cookies2.sort!
cookies3.sort!
cookies1
cookies2
cookies3
cookies1 - cookies2
cookies1 - cookies3
cookies2 - cookies3
cookies3 - cookies2
cookies3 - cookies1
Capybara.current_session
exit
Redis.current.flushall
exit
Scrape.last
exit
s = Scrape.last
l = Lisiting.first
l = Listing.first
s.listing_ids
s.listing_ids |= [l.id]
s.listings
exit
job = AlibabaProductListingJob.new
Redis.current.flushall
Scrape.last
Scrape.last.destroy
Scrape.last.listing_ids = []
Scrape.last.destroy
exit
job = AlibabaProductListingJob.new
cd job
category_url = 'https://www.alibaba.com/catalogs/products/CID52806----------------------------G--10-3984/1'
visit_page(URI.escape(category_url, '|'), load_images: false, cookie_url: @cookie_url)
@session.save_and_open_screenshot
@session.driver.cookies
COOKIE_JAR.fetch(@session.driver.phantomjs_options[3].gsub(/.+proxy\=/, ''.freeze))
user_agent = cookies.delete('user_agent'.freeze)
cookies = COOKIE_JAR.fetch(@session.driver.phantomjs_options[3].gsub(/.+proxy\=/, ''.freeze))
user_agent = cookies.delete('user_agent'.freeze)
@session.driver.clear_cookies
cookies['cookies'].each do |cookie|
  name, value = cookie.delete('name'.freeze), cookie.delete('value'.freeze)
  @session.driver.set_cookie(name, value, cookie)
end
cookies
@session.driver.cookies
@session.visit(category_url)
@session.driver.clear_cookies
@session.visit(@cookie_url)
@session.save_and_open_screenshot
visit_page(URI.escape(category_url, '|'), load_images: true, cookie_url: @cookie_url)
@session.save_and_open_screenshot
exit
Redis.current.flushall
Scrape.last
Scrape.last.destroy
exit
sc = SourceCategory.where(source: Source.find_by(name: 'alius', test_process: true))
sc = SourceCategory.where(source: Source.find_by(name: 'alius'),test_process: true))
sc = SourceCategory.where(source: Source.find_by(name: 'alius'),test_process: true)
SourceCategory.where(source: Source.find_by(name: 'alius'),test_process: true).each do |sc|
  sc.url << '/1'
  sc.save
end
exit
Redis.current
Redis.current.flushall
s = Scrape.last
s.listings.count
s.destroy
exit
redis = Redis::Namespace.new "#{Rails.env}_#{source_name}"
source_name = 'alius'
redis = Redis::Namespace.new "#{Rails.env}_#{source_name}"
proxies_file = "#{Rails.root}/proxy_list.txt"
f = File.open(proxies_file, 'r')
proxies = []
f.each_line do |line|
  l = line.chomp.strip
  proxies << l if l.present?
end
f.close
redis.del :'proxies:all'
num_added = redis.sadd :'proxies:all', proxies
redis.del :'proxies:used'
output_message = "#{num_added} proxies in list"
exit
Redis.current.flushall
Scrape.last.destroy
exit
Product.last
exit
s = Scrape.last
s.listings.count
s.listings.first
l = _
l.screenshots.count
l.screenshots.first.attachment.url
l.screenshots.first
l.images.count
l.images.first
l.images.first.save_attachment
l.images.pluck(:url)
l.images.pluck(:original_url)
exit
Asset.find 13622
exit
s = Scrape.last
l = s.listings.last
l.images
l.images.first.attachment.url
l.reload
l.images.first.attachment.url
l.screenshots.first.attachment.url
s = Scrape.last
s.listings.last
l = _)
l = s.listings.last
l.images.last.attachment.url
img = l.images.last
l.original_url
src = '//sc01.alicdn.com/kf/HTB12WJuKFXXXXX5XVXX760XFXXX6/High-Efficiency-A-grade-150W-Monocrystalline-Solar.png_50x50.png'
src.prepend('https:')
src
src.gsub(/\.(jpg|png)_\d+x\d+/, '')
Redis.current
s = Scrape.last
SourceCategories.count
SourceCategory.count
Category.count
Rails.env
Rails.env.production?
Rails.env.beta_or_production?
Rails.env.beta_or_production_or_development?
Rails.env.development?
exit
Product.index_name
Product.__elasticsearch__.type
Product.document_type
Product.client
exit
Product.last
p1, p2 = Product.last 2
p1.brand = 'Apple'
p2.brand = 'Apple Inc.'
p1.save
p2.save
Product.count
Product.reindex_alias
exit
Product.reindex_alias
Product.where(brand: 'Apple')
Product.where("brand in ('Apple', 'apple')")
Product.where("brand in ('apple')")
Product.where("brand in ('apple', 'apple inc')")
Product.where("brand in ('apple', 'apple inc.')")
exit
Cloud.count
Cloud.each
exit
params = "{\"^o\":\"Hashie::Mash\",\"self\":{\"brands\":\"apple\",\"type\":\"products\"}}"
PER_PAGE ||= ENV['CACHE_PAGE_SIZE'].try(:to_i) || 200
API_CACHE ||= RdsCache.new('api_cache')
params = Hashie::Mash.new(Oj.load(params))
type = params[:type]
model = Object.const_get(type.classify)
serializer = Object.const_get("#{model}Serializer")
cache_params, last_page, prefix, scroll_id, search_params, total, version = nil
source_ids = params[:source_ids].try(:split, ',')
scrape_ids = Scrape.where(source_id: source_ids).uniq.pluck(:id)
page = params[:page] || 1
scroll_id ||= params[:scroll_id]
search_params ||= model.prepare_search_params(model, params)
search_response = model.search_by(*search_params, { size: PER_PAGE, scroll: '10m'.freeze })
total ||= search_response.results.total
search_params ||= model.prepare_search_params(model, params)
search_params[0][:keywords] = ''
search_params
search_response = model.search_by(*search_params, { size: PER_PAGE, scroll: '10m'.freeze })
total ||= search_response.results.total
''.present
''.present?
Product.where('brand like "%apple%"')
Product.where('brand like "%apple%"').last(2).map {|p| p.listings.pluck(:data_hash) }
search_params[0][:keywords] = 'silicon'
search_response = model.search_by(*search_params, { size: PER_PAGE, scroll: '10m'.freeze })
total ||= search_response.results.total
fields = Product.query_fields
Product.where('brand like "%apple%"').last(2).each(&:index_document)
exit
Product.reindex_alias
Listing.reindex_alias
Redis.current.flushall
exit
Product.count
Listing.count
Product.first
Listing.count
Product.first
Product.count
Listing.count
Product.count
Product.pluck(:brand)
Product.pluck(:id, :brand)
p = Product.find 3
p.listings
p = Product.find 14
p.listings
Product.pluck(:id, :brand)
Product.where(id: 1,3,5).each {|p| p.update(brand: 'Renogy Inc') }
Product.where(id: [1,3,5]).each {|p| p.update(brand: 'Renogy Inc') }
Product.where(id: [1,3,5]).each {|p| p.index_document }
params = "{\"^o\":\"Hashie::Mash\",\"self\":{\"keywords\":\"solar panel\",\"brands\":\"renogy\",\"type\":\"products\"}}"
PER_PAGE ||= ENV['CACHE_PAGE_SIZE'].try(:to_i) || 200
API_CACHE ||= RdsCache.new('api_cache')
params = Hashie::Mash.new(Oj.load(params))
type = params[:type]
model = Object.const_get(type.classify)
serializer = Object.const_get("#{model}Serializer")
cache_params, last_page, prefix, scroll_id, search_params, total, version = nil
source_ids = params[:source_ids].try(:split, ',')
scrape_ids = Scrape.where(source_id: source_ids).uniq.pluck(:id)
page = params[:page] || 1
scroll_id ||= params[:scroll_id]
search_params ||= model.prepare_search_params(model, params)
search_response = model.search_by(*search_params, { size: PER_PAGE, scroll: '10m'.freeze })
total ||= search_response.results.total
search_params ||= model.prepare_search_params(model, params)
search_params[2] = {}
search_response = model.search_by(*search_params, { size: PER_PAGE, scroll: '10m'.freeze })
total ||= search_response.results.total
scroll_response['hits'.freeze]['total'.freeze]
search_response = model.search_by(*search_params, { size: PER_PAGE, scroll: '10m'.freeze })
search_response.results.total
Product.count
search_response.results.total
Product.reindex_alias
search_response = model.search_by(*search_params, { size: PER_PAGE, scroll: '10m'.freeze })
search_response.results.total
Product.count
params = Hashie::Mash.new(Oj.load(params))
params
type = params[:type]
search_params = model.prepare_search_params(model, params)
search_response = model.search_by(*search_params, { size: PER_PAGE, scroll: '10m'.freeze })
total = search_response.results.total
search_params
filters = _.last
filter_fields = Product.query_fields[:filter_fields]
definition[:query][:filtered][:filter][:bool][:must] ||= []
edit -t
edit
Product.__elasticsearch__.client.indices.delete index: Product.index_name
Product.__elasticsearch__.create_index! force: true
Product.bulk_index
Product.import
params
search_params = model.prepare_search_params(model, params)
search_response = model.search_by(*search_params, { size: PER_PAGE, scroll: '10m'.freeze })
total = search_response.results.total
exit
Product.delete_index!
Product.import
params = "{\"^o\":\"Hashie::Mash\",\"self\":{\"keywords\":\"solar panel\",\"brands\":\"renogy\",\"type\":\"products\"}}"
type = params[:type]
params
params.class
params = Hashie::Mash.new(Oj.load(params))
type = params[:type]
model = Object.const_get(type.classify)
serializer = Object.const_get("#{model}Serializer")
cache_params, last_page, prefix, scroll_id, search_params, total, version = nil
source_ids = params[:source_ids].try(:split, ',')
scrape_ids = Scrape.where(source_id: source_ids).uniq.pluck(:id)
page = params[:page] || 1
search_params = model.prepare_search_params(model, params)
search_response = model.search_by(*search_params, { size: PER_PAGE, scroll: '10m'.freeze })
PER_PAGE ||= ENV['CACHE_PAGE_SIZE'].try(:to_i) || 200
API_CACHE ||= RdsCache.new('api_cache')
search_params = model.prepare_search_params(model, params)
search_response = model.search_by(*search_params, { size: PER_PAGE, scroll: '10m'.freeze })
total = search_response.results.total
edit
Product.delete_index!
Listing.delete_index!
exit
Product.import
Listing.import
params = "{\"^o\":\"Hashie::Mash\",\"self\":{\"keywords\":\"solar panel\",\"brands\":\"renogy\",\"type\":\"products\"}}"
PER_PAGE ||= ENV['CACHE_PAGE_SIZE'].try(:to_i) || 200
API_CACHE ||= RdsCache.new('api_cache')
params = Hashie::Mash.new(Oj.load(params))
type = params[:type]
model = Object.const_get(type.classify)
serializer = Object.const_get("#{model}Serializer")
cache_params, last_page, prefix, scroll_id, search_params, total, version = nil
source_ids = params[:source_ids].try(:split, ',')
scrape_ids = Scrape.where(source_id: source_ids).uniq.pluck(:id)
scrape_ids
page = params[:page] || 1
scroll_id ||= params[:scroll_id]
search_params = model.prepare_search_params(model, params)
search_response = model.search_by(*search_params, { size: PER_PAGE, scroll: '10m'.freeze })
total = search_response.results.total
ids = search_response.results.results.map! { |p| p.id }
Product.where(id: ids).pluck(:brand)
p = Product.find(18)
p.brand << ' inc.'
p.save
p.update_document
search_params = model.prepare_search_params(model, params)
search_response = model.search_by(*search_params, { size: PER_PAGE, scroll: '10m'.freeze })
total = search_response.results.total
ids = search_response.results.results.map! { |p| p.id }
exit
params = "{\"^o\":\"Hashie::Mash\",\"self\":{\"keywords\":\"solar panel\",\"brands\":\"renogy\",\"type\":\"products\"}}"
PER_PAGE ||= ENV['CACHE_PAGE_SIZE'].try(:to_i) || 200
API_CACHE ||= RdsCache.new('api_cache')
params = Hashie::Mash.new(Oj.load(params))
params['brands'] = 'renogy inc'
params
type = params[:type]
model = Object.const_get(type.classify)
serializer = Object.const_get("#{model}Serializer")
# Keep variables that won't change in scope
cache_params, last_page, prefix, scroll_id, search_params, total, version = nil
source_ids = params[:source_ids].try(:split, ',')
scrape_ids = Scrape.where(source_id: source_ids).uniq.pluck(:id)
page = params[:page] || 1
scroll_id ||= params[:scroll_id]
search_params ||= model.prepare_search_params(model, params)
search_response = model.search_by(*search_params, { size: PER_PAGE, scroll: '10m'.freeze })
total ||= search_response.results.total
search_params
search_params[2][:brands] = ['renogy']
search_response = model.search_by(*search_params, { size: PER_PAGE, scroll: '10m'.freeze })
search_response.results.total
exit
Listing::BRAND_STOPWORDS
Listing.BRAND_STOPWORDS
Searchable::Base.BRAND_STOPWORDS
Searchable::Base::BRAND_STOPWORDS
eixt
exit
Listing::BRAND_STOPWORDS
Searchable::Base::BRAND_STOPWORDS
Listing::BRAND_STOPWORDS
Listing::BRAND_STOPWORDS.join('|')
'apple inc'.gsub(/(#{Listing::BRAND_STOPWORDS.join('|')})/i, '')
'apple inc.'.gsub(/(#{Listing::BRAND_STOPWORDS.join('|')})/i, '')
exit
params = "{\"^o\":\"Hashie::Mash\",\"self\":{\"keywords\":\"solar panel\",\"brands\":\"renogy\",\"type\":\"products\"}}"
params = "{\"^o\":\"Hashie::Mash\",\"self\":{\"keywords\":\"solar panel\",\"brands\":\"renogy inc.\",\"type\":\"products\"}}"
PER_PAGE ||= ENV['CACHE_PAGE_SIZE'].try(:to_i) || 200
API_CACHE ||= RdsCache.new('api_cache')
params = Hashie::Mash.new(Oj.load(params))
type = params[:type]
model = Object.const_get(type.classify)
serializer = Object.const_get("#{model}Serializer")
cache_params, last_page, prefix, scroll_id, search_params, total, version = nil
source_ids = params[:source_ids].try(:split, ',')
scrape_ids = Scrape.where(source_id: source_ids).uniq.pluck(:id)
page = params[:page] || 1
scroll_id ||= params[:scroll_id]
search_params ||= model.prepare_search_params(model, params)
search_response = model.search_by(*search_params, { size: PER_PAGE, scroll: '10m'.freeze })
total ||= search_response.results.total
terms, operator, filters, opts = search_params
terms
operator
file-mode
exit
edit
terms
operator
filters
opts
opts = {}
opts = { size: PER_PAGE, scroll: '10m'.freeze }
fields = Product.query_fields
definition = Product.generate_query_by_keywords(fields, terms, operator, filters)
reload!
definition = Product.generate_query_by_keywords(fields, terms, operator, filters)
reload!
definition = Product.generate_query_by_keywords(fields, terms, operator, filters)
reload!
definition = Product.generate_query_by_keywords(fields, terms, operator, filters)
reload!
definition = Product.generate_query_by_keywords(fields, terms, operator, filters)
reload!
definition = Product.generate_query_by_keywords(fields, terms, operator, filters)
reload!
definition = Product.generate_query_by_keywords(fields, terms, operator, filters)
relaod!
reload!
definition = Product.generate_query_by_keywords(fields, terms, operator, filters)
definition
definition = nil
definition = Product.generate_query_by_keywords(fields, terms, operator, filters)
Product.generate_query_by_keywords(fields, terms, operator, filters)
fields, terms, operator, filters
[fields, terms, operator, filters]
Product.query_fields
reload!
Product.generate_query_by_keywords(fields, terms, operator, filters)
Product.query_fields
[fields, terms, operator, filters]
Product.query_fields
search_fields
search_fields = fields[:search_fields]
fields[:search_fields]
fields[:search_fields].uniq!
filter_fields = fields[:filter_fields]
definition[:query][:filtered][:query][:bool][:filter][0][:multi_match][:fields]
reload!
Product.generate_query_by_keywords(fields, terms, operator, filters)
search_fields = fields[:search_fields].dup
fields[:search_fields] = fields[:search_fields][0..1]
relaod!
reload!
Product.generate_query_by_keywords(fields, terms, operator, filters)
reload!
Product.generate_query_by_keywords(fields, terms, operator, filters)
fields = Product.query_fields
search_fields = fields[:search_fields]
reload!
Product.generate_query_by_keywords(fields, terms, operator, filters)
exit
Redis.current.flushall
'.., .ad,af.'.gsub(/(\.|\,)/, '')
'.., .ad,af.'.gsub(/(#{self::BRAND_STOPWORDS.join('|')}|\.|\,)/i, '')
'.., .ad,af.'.gsub(/(#{Listing::BRAND_STOPWORDS.join('|')}|\.|\,)/i, '')
'.., .ad,af.'.gsub(/(#{self::BRAND_STOPWORDS.join('|')}|[^0-9a-z ])/i, '')
'.., .ad,af.'.gsub(/(#{Listing::BRAND_STOPWORDS.join('|')}|[^0-9a-z ])/i, '')
'Renogy inc.., .ad,af.'.gsub(/(#{Listing::BRAND_STOPWORDS.join('|')}|[^0-9a-z ])/i, '')
reload!
Product.generate_query_by_keywords(fields, terms, operator, filters)
exit
Product.delete_index!
Listing.delete_index!
Listing.index_name
Listing.__elasticsearch__.index_name
Listing.document_type
Listing.__elasticsearch__.document_type
exit
Listing.import
Product.import
Redis.current.flushall
exit
TargetListItem.joins(:messages)
exit
AlibabaProductDetailJob.new.perform(1, 1, 'https://reseller.alibaba.com/product-detail/1Chip-HOT-G1-Amlogic-S805-Android_60400988162.html'))
AlibabaProductDetailJob.new.perform(1, 1, 'https://reseller.alibaba.com/product-detail/1Chip-HOT-G1-Amlogic-S805-Android_60400988162.html')
l = Listing.last
exit
Listing.delete_index!
Listing.import
exit
Listing.delete_index!
Listing.import
exit
Listing.delete_index!
Listing.import
exit
Listing.delete_index!
Listing.import
reload!
exit
Listing.delete_index!
Listing.import
Listing.delete_index!
exit
next
continue
Listing.delete_index!
Product.delete_index!
exit
Listing.import
Listing.delete_index!
exit
Listing.import
Listing.first
l = Listing.first
Listing.delete_index!
Listing.create_index! force: true
Listing.reindex_alias
exit
Listing.__elasticsearch__.create_index! force: true
Listing.import
exit
Listing.delete_index!
Listing.__elasticsearch__.create_index! force: true
Listing.import
Listing.delete_index!
Listing.__elasticsearch__.create_index! force: true
Listing.import
exit
Listing.delete_index!
Listing.__elasticsearch__.create_index! force: true
Listing.import
exit
Listing.delete_index!
Listing.reindex_alias
Listing.delete_index!
Listing.reindex_alias
exit
Listing.delete_index!
Listing.reindex_alias
Listing.mapping
Listing.mapping.inspect
reload!
Listing.reindex_alias
Listing.mapping.inspect
Listing.delete_index!
Listing.reindex_alias
Listing.mapping.inspect
Listing.mapping
exit
Listing.reindex_alias
exit
Listing.delete_index!
Listing.__elasticsearch__.create_index! force: true
Listing.import
Listing.mapping.inspect
reload!
Listing.mapping.inspect
Listing.reindex_alias
fields = self.query_fields
cd Listing
fields = self.query_fields
fields
fields[:search_fields] = Set.new
exit
Listing.delete_index!
Listing.import
Listing.mapping
terms = {keywords: 'effiency'}
operator='AND'
filters = {}
opts = {}
cd Listing
terms
terms = {keywords: 'effiency'}
search_by(terms)
results = _
results.results
results.results.total
query_fields
Listing.delete_index!
Listing.__elasticsearch__.create_index! force: true
self.__elasticsearch__.index_name
first
first.as_indexed_json
delete_index!
create_index! force: true
exit
Listing.delete_Ind
Listing.delete_index!
Listing.create_index! force: true
Listing.import
Listing.mapping
exit
Listing.reindex_alias
client = Listing.__elasticsearch__.client
client.indices.put_mapping index: Listing.index, type: 'listings', body: {
  listings: {
    properties: {
      data_hash: { type: 'string', analyzer: 'snowball', fields: { raw: { type: 'string', index: 'not_analyzed' } } }
    }
  }
}
client.indices.put_mapping index: Listing.index_name, type: 'listings', body: {
  listings: {
    properties: {
      data_hash: { type: 'string', analyzer: 'snowball', fields: { raw: { type: 'string', index: 'not_analyzed' } } }
    }
  }
}
reload!
Listing.delete_index!
Listing.create_index! force: true
Listing.mapping
client.indices.put_mapping index: Listing.index_name, type: 'listings', body: {
  listings: {
    properties: {
      data_hash: { type: 'string', analyzer: 'snowball', fields: { raw: { type: 'string', index: 'not_analyzed' } } }
    }
  }
}
Listing.mapping.inspect
reload!
Listing.mapping.inspect
reload!
Listing.import
exit
Listing.delete_index!
Listing.create_index! force: true
Listing.import
Listing.first.data_hash
Listing.delete_index!
Listing.create_index! force: true
Listing.import
exit
Listing.delete_index!
Listing.create_index! force: true
Listing.import
exit
Listing.delete_index!
Listing.create_index! force: true
Listing.import
exit
Listing.reindex_alias
exit
Listing.reindex_alias
reload!
Listing.reindex_alias
Listing.delete_index!
reload!
Listing.reindex_alias
reload!
Listing.import
Listing.first.data_hash
exit
Listing.first.as_indexed_json
Listing.delete_index!
Listing.create_index! force: true
Listing.import
exit
reload!
Listing.delete_index!
Listing.create_index! force: true
reload!
Listing.create_index! force: true
Listing.import
reload!
Listing.delete_index!
Listing.create_index! force: true
Listing.import
exit
Listing.delete_index!
Listing.create_index! force: true
reload!
Listing.create_index! force: true
exit
Listing.delete_index!
exit
Listing.create_index! force: true
reload!
exit
relaod!
reload!
Listing.reindex_alias
Product.reindex_alias
Listing.reindex_alias
reload!
Listing.reindex_alias
exit
Listing.delete_index!
Listing.reindex_alias
relaod!
reload!
Listing.reindex_alias
Listing.delete_index!
Listing.create_index! force: true
Listing.import
relaod!
reload!
Listing.delete_index!
Listing.create_index! force: true
Listing.import
reload!
Listing.create_index! force: true
Listing.delete_index!
Listing.create_index! force: true
Listing.import
reload!
Listing.delete_index!
Listing.create_index! force: true
Listing.import
reload!
Listing.delete_index!
Listing.create_index! force: true
Listing.import
exit
Listing.delete_index!
Listing.create_index! force: true
Listing.import
reload!
Listing.delete_index!
reload!
exit
Listing.delete_index!
Listing.create_index! force: true
Listing.import
Listing.delete_index!
reload!
Listing.delete_index!
reload!
exit
reload!
Listing.delete_index!
Listing.create_index! force: true
Listing.import
reload!
Listing.delete_index!
Listing.create_index! force: true
Listing.import
Listing.delete_index!
Listing.create_index! force: true
Listing.import
Listing.delete_index!
reload!
Listing.delete_index!
Listing.import
Listing.delete_index!
Listing.create_index! force: true
Listing.import
relaod!
reload!
Listing.create_index! force: true
Listing.import
reload!
Listing.delete_index!
Listing.create_index! force: true
Listing.import
Listing.delete_index!
reload!
Listing.create_index! force: true
Listing.import
Listing.delete_index!
reload!
Listing.create_index! force: true
Listing.reindex_alias
reload!
Listing.reindex_alias
Product.first.model
reload!
Listing.reindex_alias
Product.reindex_alias
exit
redis = Redis.current
redis.flushall
exit
Listing.first
l = Listing.first
l.data_hash << ' local dimming'
l.data_hash[:General][:'Product Description'] << ' local dimming'
l.save
l = Listing.last
l.data_hash[:General][:'Product Description'] << ' local something dimming'
l.save
Listing.delete_index!
reload!
Listing.create_index! force: true
Listing.import
Redis.current.flushall
fields = Listing.query_fields
terms = {keywords: "local dimming"}
operator='AND', filters = {}, opts = {}
definition = Listing.generate_query_by_keywords(fields, terms, operator, filters)
definition
Listing.__elasticsearch__.explain
definition.as_json
definition.to_json
definition
definition.as_json
query = {
  "query": {
    "filtered": {
      "query": {
        "bool": {
          "filter": [
            {
              "multi_match": {
                "query": "local dimming",
                "type": "phrase",
                "slop": 0,
                "operator": "AND",
                "fields": [
                  "name",
                  "data_hash"
                ]
              }
            }
          ]
        }
      }
    }
  }
}
query
query.to_json
Listing.first.as_indexed_json
Listing.first.as_indexed_json.to_json
Listing.first.as_indexed_json
Listing.first.as_indexed_json.to_json
Listing.delete_index!
reload!
Listing.create_index! force: true
Listing.import
exit
set = Set.new['field1', 'field2']
set = Set.new(['field1', 'field2'])
set.delete('field1')
set
set.delete('*1')
set = Set.new(['field1', 'field2'])
set.delete_if {|f| f.match(/2/)
}
set = Set.new(['field', 'field.raw'])
set.delete_if {|f| f.match(/raw/) }
edit
exit
data_hash = {}
def do_something(data_hash={})
  data_hash[:General] ||= {}
  data_hash[:General][:Array] = [1,2,3,4,5]
end
data_hash
do_something(data_hash)
data_hash
text = "Built for newborns earmuffs. Suitable for ages 0-2 years. These funky earmuffs look great and more importantly, with their category 4 rating they will protect children's ears from potentially harmful noise. Weighing only 190 grams, Banz earmuffs are small enough to fit in the palm of your hand, making them easy to store or carry with you. The soft cushion cups will also ensure that your child will be comfortable when wearing Banz Earmuffs.
text
edit
text
text.strip
text
text.squish
text.prepend('   ')
text
text.prepend('     some text   ')
text.squish
text.squish!
text
text.squish!
name = 'Shipping Weight: '
name.squish!
name
name.gsub!(/:$/, '')
name.remove!(/:$/)
name
%r(#{name}
%r(#{name})
%r(#{name}:)
%r(\A\s+?#{name}:)
%r(\A\s*?#{name}:)
%r(\A\s*?#{name}:\s*)
detail = "Product Dimensions: 6.3 x 6.3 x 2.4 inches"
text = _
name = text.try(:squish!).try(:remove!, /:$/)
name = "Product Dimensions"
value = detail.text.remove(%r(\A\s*?#{name}:\s*?))
value = text.remove(%r(\A\s*?#{name}:\s*?))
text
text.remove(%r(\A\s+?#{name}:\s+?))
name
text
value = text.remove(%r(\A\s+?#{name}:\s+?))
value
value = text.remove(%r(\A(\s+)?#{name}:\s+?))
text
text.remove(%r(\A(\s+)?#{name}:(\s+)?))
exit
url = 'https://www.amazon.com/Baby-BanzearBanZ-Infant-Hearing-Protection/dp/B007BEIKGO/ref=sr_1_32/158-3071333-9803742?s=aht&ie=UTF8&qid=1474327871&sr=1-32'
job = AmazonProductDetailJob.new
cd job
scrape_id = 1
source_category_id = 1
url = 'https://www.amazon.com/Baby-BanzearBanZ-Infant-Hearing-Protection/dp/B007BEIKGO/ref=sr_1_32/158-3071333-9803742?s=aht&ie=UTF8&qid=1474327871&sr=1-32'
exit
url = 'https://www.amazon.com/Baby-BanzearBanZ-Infant-Hearing-Protection/dp/B007BEIKGO/ref=sr_1_32/158-3071333-9803742?s=aht&ie=UTF8&qid=1474327871&sr=1-32'
scrape_id = 1
source_category_id = 1
AmazonProductDetailJob.new.perform(scrape_id, source_category_id, url)
exit
l = Listing.last
url = 'https://www.amazon.com/Baby-BanzearBanZ-Infant-Hearing-Protection/dp/B007BEIKGO/ref=sr_1_32/158-3071333-9803742?s=aht&ie=UTF8&qid=1474327871&sr=1-32'
source_category_id = 1
scrape_id = 1
AmazonProductDetailJob.new.perform(scrape_id, source_category_id, url)
l = Listing.last
l.destroy
exit
url = 'https://www.amazon.com/Baby-BanzearBanZ-Infant-Hearing-Protection/dp/B007BEIKGO/ref=sr_1_32/158-3071333-9803742?s=aht&ie=UTF8&qid=1474327871&sr=1-32'
AmazonProductDetailJob.new.perform(1,1, url)
l = Listing.last
l.screenshots.first
l.screenshots.first.attachment.url
l = Listing.last
l.destroy
xit
exit
url = 'https://www.amazon.com/Baby-BanzearBanZ-Infant-Hearing-Protection/dp/B007BEIKGO/ref=sr_1_32/158-3071333-9803742?s=aht&ie=UTF8&qid=1474327871&sr=1-32'
AmazonProductDetailJob.new.perform(1,1, url)
l = Listing.last
l.destroy
exit
url = 'https://www.amazon.com/Baby-BanzearBanZ-Infant-Hearing-Protection/dp/B007BEIKGO/ref=sr_1_32/158-3071333-9803742?s=aht&ie=UTF8&qid=1474327871&sr=1-32'
AmazonProductDetailJob.new.perform(1,1, url)
l = Listing.last
l.destroy
exit
url = 'https://www.amazon.com/Baby-BanzearBanZ-Infant-Hearing-Protection/dp/B007BEIKGO/ref=sr_1_32/158-3071333-9803742?s=aht&ie=UTF8&qid=1474327871&sr=1-32'
AmazonProductDetailJob.new.perform(1,1, url)
l = Listing.last
l.data_hash
l.data_hash[:General]
l.data_hash[:Description]
l.destroy
exit
url = 'https://www.amazon.com/Baby-BanzearBanZ-Infant-Hearing-Protection/dp/B007BEIKGO/ref=sr_1_32/158-3071333-9803742?s=aht&ie=UTF8&qid=1474327871&sr=1-32'
AmazonProductDetailJob.new.perform(1,1, url)
l = Listing.last
l.destroy
exit
url = 'https://www.amazon.com/Baby-BanzearBanZ-Infant-Hearing-Protection/dp/B007BEIKGO/ref=sr_1_32/158-3071333-9803742?s=aht&ie=UTF8&qid=1474327871&sr=1-32'
AmazonProductDetailJob.new.perform(1,1, url)
job = AmazonProductDetailJob.new
cd job
url = 'https://www.amazon.com/Baby-BanzearBanZ-Infant-Hearing-Protection/dp/B007BEIKGO/ref=sr_1_32/158-3071333-9803742?s=aht&ie=UTF8&qid=1474327871&sr=1-32'
detail_url = get_detail_url(url)
visit_page detail_url, cookie_url: @cookie_url
product_description = @session.first('#descriptionAndDetails'.freeze)
data_hash = {}
data_hash[:'Product Description'] ||= {}
data_hash
product_description.first('#productDescription'.freeze)
product_description.first('#productDescription'.freeze).text.squish!
product_description.first('#detailBullets li span.a-list-item'.freeze)
detail = _
detail.text.blank?
detail.text
name = detail.first('.a-text-bold'.freeze).text.try(:squish!).try(:remove!, /:$/)
value = detail.text.remove!(%r(\A(\s+)?#{name}:(\s+)?))
data_hash[:'Product Description'][name.to_sym] = value unless (name.blank? || value.blank?)
data_hash
product_description.all('#detailBullets li span.a-list-item'.freeze) do |detail|
  puts '~~~~~~~~~' * 80 if detail.text.blank?
  next if detail.text.blank?
  name = detail.first('.a-text-bold'.freeze).text.try(:squish!).try(:remove!, /:$/)
  value = detail.text.remove!(%r(\A(\s+)?#{name}:(\s+)?))
  data_hash[:'Product Description'][name.to_sym] = value unless (name.blank? || value.blank?)
end
data_hash
product_description.all('#detailBullets li span.a-list-item'.freeze)
product_description.all('#detailBullets li span.a-list-item'.freeze).count
product_description.all('#detailBullets li span.a-list-item'.freeze).each do |detail|
  next if detail.text.blank?
  name = detail.first('.a-text-bold'.freeze).text.try(:squish!).try(:remove!, /:$/)
  value = detail.text.remove!(%r(\A(\s+)?#{name}:(\s+)?))
  data_hash[:'Product Description'][name.to_sym] = value unless (name.blank? || value.blank?)
end
data_hash
exit
l = Listing.last
l.destroy
url = 'https://www.amazon.com/Baby-BanzearBanZ-Infant-Hearing-Protection/dp/B007BEIKGO/ref=sr_1_32/158-3071333-9803742?s=aht&ie=UTF8&qid=1474327871&sr=1-32'
AmazonProductDetailJob.new.perform(1,1, url)
l = Listing.last
exit
job = AmazonProductDetailJob.new
cd job
exit
fields = Listing.query_fields
search_fields = fields[:search_fields]
search_fields.select {|f| f.include?('.raw') }
Listing.delete_index!
Product.delete_index!
exit
Listing.delete_index!
Product.delete_index!
Listing.create_index force: true
Listing.create_index! force: true
Product.create_index! force: true
Listing.reindex_alias
Product.reindex_alias
exit
Listing.reindex_alias
Product.reindex_alias
exit
Product.first
exit
Listing.reindex_alias
Product.reindex_alias
Redis.current.flushall
exit
Listing.reindex_alias
Product.reindex_alias
Listing.create_index! force: true
Product.create_index! force: true
Listing.import
Product.import
Listing.delete_index!
Product.delete_index!
exit
Listing.delete_index!
Product.delete_index!
Listing.create_index! force: true
Product.create_index! force: true
Listing.import
Product.import
exit
Listing.delete_index!
Product.delete_index!
Listing.create_index! force: true
Product.create_index! force: true
fields = ['products.model', 'listings.name', 'listings.data_hash', 'listing.data_hash.raw']
fields.select {|f| f.match(/(model|\.raw)/)
fields.select {|f| f.match(/(model|\.raw)/) }
Product.first
Products.import
Product.import
exit
Listing.delete_index!
Product.delete_index!
Listing.create_index! force: true
Product.create_index! force: true
Listing.import
Product.import
Product.first.model
exit
Listing.reindex_alias
Product.reindex_alias
exit
Listing.reindex_alias
Product.reindex_alias
exit
Listing.reindex_alias
Product.reindex_alias
exit
Product.reindex_alias
Listing.reindex_alias
exit
Listing.reindex_alias
Product.reindex_alias
exit
Listing.reindex_alias
Product.reindex_alias
fields = ['brand']
fields << ['model', 'model.raw']
fields = ['brand']
fields.concat(['model', 'model.raw'])
fields.concat('brand.raw')
exit
Listing.reindex_alias
Product.reindex_alias
Redis.current.flushall
cd Listing
terms = {}
operator = 'AND'
filters: {brands: 'RNG'}
filters= {brands: 'RNG'}
opts = {}
search_by(terms, operator, filters, opts)
filters= {brands: ['RNG']}
search_by(terms, operator, filters, opts)
results = _
results.results
results.results.size
filters= {brands: ['renogy']}
results = search_by(terms, operator, filters, opts)
results.results.size
filters= {brands: ['Renogy']}
results = search_by(terms, operator, filters, opts)
results.results.size
exit
cd Product
terms = {}
filters = { brands: ['renogy' }
filters = { brands: ['renogy'] }
operator = 'AND'
opts = {}
fields = self.query_fields
definition = generate_query_by_keywords(fields, terms, operator, filters)
filter_fields = fields[:filter_fields]
filters
filters.keys
filters.keys.first
key = _
filter_fields[key].is_a? Array
filter_fields[key].class
filter_fields[key].class.class
filter_fields[key].is_a? Array
filter_fields[key]
filter_fields[:models]
filter_fields[:models].map |field|
{ terms: { field => 'renogy' } }
filter_fields[:models].map |es_field|
{ terms: { es_field => 'renogy' } }
filter_fields[:models].map do |field|
  { terms: { field => 'renogy' } }
end
key
{
  or: filter_fields[key].map { |field| { terms: { field => filters[key] } } }
}
key = :models
{
  or: filter_fields[key].map { |field| { terms: { field => filters[key] } } }
}
filters
exit
Listing.reindex_alias
Product.reindex_alias
Redis.current.flushall
Product.first
cd Product
terms = {}
filters = {brands: ['renogy'], models: ['rng100d']}
operator = 'AND'
opts = {}
fields = self.query_fields
search_fields = fields[:search_fields]
definition = search_definition(search_fields, terms, operator)
filter_fields = fields[:filter_fields]
definition[:query][:filtered][:filter][:bool][:must] ||= []
definition
filters.keys
key = :models
filter_fields[key].is_a? Array
definition[:query][:filtered][:filter][:bool][:must] << {
  or: filter_fields[key].map { |field| { terms: { field => filters[key] } } }
}
definition
self.search(definition, opts)
results = _
results.results
results.results.size
definition = generate_query_by_keywords(fields, terms, operator, filters)
self.search(definition, opts)
results = _
results.results.size
filters
filters[:models] = ['rng-100d']
filters
definition = generate_query_by_keywords(fields, terms, operator, filters)
self.search(definition, opts)
_.results.size
exit
Product.reindex_alias
Listing.reindex_alias
Redis.current.flushalll
Redis.current.flushall
cd Product
terms = {}, operator='AND', filters = {brands: ["Renogy Inc."], models: ['rng-100d'] }, opts = {}
fields = self.query_fields
definition = generate_query_by_keywords(fields, terms, operator, filters)
terms
operator
filters
opts
terms = {}
definition = generate_query_by_keywords(fields, terms, operator, filters)
exit
cd Product
terms = {}; operator='AND'; filters = {brands: ["Renogy Inc."], models: ['rng-100d'] }; opts = {}
terms
operator
fields = self.query_fields
search_fields = fields[:search_fields]
definition = search_definition(search_fields, terms, operator)
filter_fields = fields[:filter_fields]
filters
definition[:query][:bool][:must] = []
definition
key = :models
if filters[key].present?
if filter_fields[key].is_a? Array
  definition[:query][:bool][:must] << {
    or: filter_fields[key].map { |field| { terms: { field => filters[key] } } }
  }
end
definition
self.search(definition, opts)
results = _
results.results.size
definition
definition[:query][:bool].delete(:should)
definition
self.search(definition, opts)
_.results.size
Product.connection
self.search(definition, opts)
_.results.size
definition[:query][:bool].delete(:minimum_should_match)
results = self.search(definition, opts)
results.results.size
filter
filters
exit
Listing.reindex_alias
Product.reindex_alias
fields = Product.query_fields
filter_fields = fields[:filter_fields]
exit
Product.reindex_alias
Listing.reindex_alias
cd Product
terms = {}; operator='AND'; filters = {brands: ["Renogy Inc."], models: ['rng-100d'] }; opts = {}
fields = self.query_fields
definition = generate_query_by_keywords(fields, terms, operator, filters)
reload!
exit
cd Product
terms = {}; operator='AND'; filters = {brands: ["Renogy Inc."], models: ['rng-100d'] }; opts = {}
fields = self.query_fields
definition = generate_query_by_keywords(fields, terms, operator, filters)
exit
reload!
cd Product
terms = {}; operator='AND'; filters = {brands: ["Renogy Inc."], models: ['rng-100d'] }; opts = {}
definition = generate_query_by_keywords(fields, terms, operator, filters)
fields = self.query_fields
definition = generate_query_by_keywords(fields, terms, operator, filters)
exit
reload!
cd Product
terms = {}; operator='AND'; filters = {brands: ["Renogy Inc."], models: ['rng-100d'] }; opts = {}
fields = self.query_fields
definition = generate_query_by_keywords(fields, terms, operator, filters)
exit
reload!
cd Product
terms = {}; operator='AND'; filters = {brands: ["Renogy Inc."], models: ['rng-100d'] }; opts = {}
fields = self.query_fields
definition = generate_query_by_keywords(fields, terms, operator, filters)
exit
reload!
cd Product
terms = {}; operator='AND'; filters = {brands: ["Renogy Inc."], models: ['rng-100d'] }; opts = {}
fields = self.query_fields
definition = generate_query_by_keywords(fields, terms, operator, filters)
definition[:query][:bool][:filter]
definition[:query][:bool].delete(:filter)
definition
self.search(definition, opts)
_.results.size
definition
terms = {}; operator='AND'; filters = {brands: ["Renogy Inc."], models: ['rng100d'] }; opts = {}
definition = generate_query_by_keywords(fields, terms, operator, filters)
self.search(definition, opts)
_.results.size
terms = {}; operator='AND'; filters = {brands: ["Renogy Inc."], models: ['rng'] }; opts = {}
definition = generate_query_by_keywords(fields, terms, operator, filters)
self.search(definition, opts)
_.results.size
terms = {}; operator='AND'; filters = {brands: ["Renogy Inc."], models: ['100d'] }; opts = {}
definition = generate_query_by_keywords(fields, terms, operator, filters)
self.search(definition, opts)
_.results.size
terms = {}; operator='AND'; filters = {brands: ["Renogy Inc."], models: ['rng', '100d'] }; opts = {}
definition = generate_query_by_keywords(fields, terms, operator, filters)
definition = nil
definition = generate_query_by_keywords(fields, terms, operator, filters)
terms = {}; operator='AND'; filters = {brands: ["Renogy Inc."], models: ['rng', '100d'] }; opts = {}
fields = self.query_fields
definition = nil
search_fields = fields[:search_fields]
definition = search_definition(search_fields, terms, operator)
filter_fields = fields[:filter_fields]
filters.keys.each do |key|
filters.keys
key = :models
if filters[key].present?
  filters[key].present?
filters[key].present?
filter_fields[key].is_a? Array
filter_query = {
  bool: { should: [], minimum_should_match: 1 }
}
filters[key]
filter = filters[key].first
filter_query[:bool][:should] << {
  multi_match: {
    query: filter,
    type: 'phrase',
    slop: 0,
    minimum_should_match: 1,
    operator: 'and',
    fields: filter_fields[key]
  }
}
filter_query
definition[:query][:bool][:must] << filter_query
definition
filter = filters[key].last
exit
relaod!
reload!
terms = {}; operator='AND'; filters = {brands: ["Renogy Inc."], models: ['rng', '100d'] }; opts = {}
cd Product
terms = {}; operator='AND'; filters = {brands: ["Renogy Inc."], models: ['rng', '100d'] }; opts = {}
fields = self.query_fields
definition = generate_query_by_keywords(fields, terms, operator, filters)
exit
reload!
terms = {}; operator='AND'; filters = {brands: ["Renogy Inc."], models: ['rng', '100d'] }; opts = {}
fields = self.query_fields
cd Product
terms = {}; operator='AND'; filters = {brands: ["Renogy Inc."], models: ['rng', '100d'] }; opts = {}
fields = self.query_fields
definition = generate_query_by_keywords(fields, terms, operator, filters)
self.search(definition, opts)
_.results.size
exit 
reload!
cd Product
terms = {}; operator='AND'; filters = {brands: ["Renogy Inc."], models: ['rng', '100d'] }; opts = {}
fields = self.query_fields
definition = generate_query_by_keywords(fields, terms, operator, filters)
exit
reload!
cd Product
fields = self.query_fields
definition = generate_query_by_keywords(fields, terms, operator, filters)
terms = {}; operator='AND'; filters = {brands: ["Renogy Inc."], models: ['rng', '100d'] }; opts = {}
definition = generate_query_by_keywords(fields, terms, operator, filters)
self.search(definition, opts)
_.results.size
exit
Redis.current.flushall
exit
Redis.current.flushall
exit
Redis.current.flushall
terms = {keywords: 'monocrystalline'}; operator='AND'; filters = {brands: ["Renogy Inc."] }; opts = {}
cd Product
terms = {keywords: 'monocrystalline'}; operator='AND'; filters = {brands: ["Renogy Inc."] }; opts = {}
fields = self.query_fields
search_fields = fields[:search_fields]
definition = search_definition(search_fields, terms, operator)
exit
Redis.current.flushall
exit
Redis.current.flushall
terms = {keywords: 'monocrystalline'}; operator='AND'; filters = {brands: ["Renogy Inc."] }; opts = {}
cd Product
terms = {keywords: 'monocrystalline'}; operator='AND'; filters = {brands: ["Renogy Inc."] }; opts = {}
fields = self.query_fields
search_fields = fields[:search_fields]
definition = search_definition(search_fields, terms, operator)
search_fields
filter_fields[:models]
filter_fields
filter_fields = fields[:filter_fields]
filter_fields[:models]
search_fields
search_fields + filter_fields[:models]
search
search_fields
search_fields |= filter_fields[:models]
search_fields
search_fields.pop
search_fields.delet_if? {|f| f.include? 'model' }
search_fields.delete_if? {|f| f.include? 'model' }
search_fields.delete_if {|f| f.include? 'model' }
search_fields
search_fields.merge(filter_fields[:models])
search_fields
exit
Redis.current.flushall
exit
Redis.current.flushall
exit
Listing.sample
Listing.connection
Listing.samp.e
Listing.sample
Listing.count
Listing.all.sample
Product.query_fields
fields = Product.query_fields[:search_fields]
search_fields = _
search_fields.select { |f| f.include?('.raw') }
_.class
search_fields
search_fields.keep_if { |f| f.include?('.raw') }
exit
params = "{\"^o\":\"Hashie::Mash\",\"self\":{\"keywords\":\"mc4 adaptor\",\"exact\":\"true\",\"type\":\"products\"}}"
PER_PAGE ||= ENV['CACHE_PAGE_SIZE'].try(:to_i) || 200
API_CACHE ||= RdsCache.new('api_cache')
params = Hashie::Mash.new(Oj.load(params))
type = params[:type]
model = Object.const_get(type.classify)
serializer = Object.const_get("#{model}Serializer")
cache_params, last_page, prefix, scroll_id, search_params, total, version = nil
source_ids = params[:source_ids].try(:split, ',')
scrape_ids = Scrape.where(source_id: source_ids).uniq.pluck(:id)
page = params[:page] || 1
search_params ||= model.prepare_search_params(model, params)
terms, operator, filters, opts = {:keywords=>"mc4 adaptor", :exact=>"true"}, "AND", {}, {}
terms
cd Product
terms, operator, filters, opts = {:keywords=>"mc4 adaptor", :exact=>"true"}, "AND", {}, {}
fields = self.query_fields
definition = generate_query_by_keywords(fields, terms, operator, filters)
self.search(definition, opts)
_.results.size
Listing.first.data_hash
Listing.first.as_indexed_json
Listing.first.as_indexed_json.to_json
definition
exit
Redis.current.flushall
reload!
definition
terms, operator, filters, opts = {:keywords=>"mc4 adaptor", :exact=>"true"}, "AND", {}, {}
cd Product
terms, operator, filters, opts = {:keywords=>"mc4 adaptor", :exact=>"true"}, "AND", {}, {}
definition = generate_query_by_keywords(fields, terms, operator, filters)
fields = self.query_fields
definition = generate_query_by_keywords(fields, terms, operator, filters)
self.search(definition, opts)
_.results.size
exit
cd Listing
terms, operator, filters, opts = {:keywords=>"mc4 adaptor", :exact=>"true"}, "AND", {}, {}
fields = self.query_fields
definition = generate_query_by_keywords(fields, terms, operator, filters)
self.search(definition, opts)
_.results.size
results = self.search(definition, opts)
ids = results.results.results.map! { |p| p.id }
Listing.where(id: ids).map(&:product_ids).uniq
Product.pluck(:brand)
params = "{\"^o\":\"Hashie::Mash\",\"self\":{\"brands\":\"aleko,renogy\",\"type\":\"products\"}}"
params = Hashie::Mash.new(Oj.load(params))
Product.prepare_search_params(Product, params)
terms, operator, filters, opts = _ << {}
terms
operator
fields = self.query_fields
definition = generate_query_by_keywords(fields, terms, operator, filters)
arr = [{:keywords=>nil, :exact=>nil}, "AND", {:brands=>["aleko", "renogy"]}, {}]
terms, operator, filters, opts = [{:keywords=>nil, :exact=>nil}, "AND", {:brands=>["aleko", "renogy"]}, {}]
definition
Array(['field1'])
Array('field1')
eixt
exit
reload!
cd Product
terms, operator, filters, opts = [{:keywords=>nil, :exact=>nil}, "AND", {:brands=>["aleko", "renogy"]}, {}]
fields = self.query_fields
definition = generate_query_by_keywords(fields, terms, operator, filters)
exit
Redis.current.flushall
exit
Redis.current.flushall
exit
Redis.current.flushall
esit
exit
cache = RdsCache.new('api_cache')
keys = cache.keys
TIMESTAMP_FMT = '%Y%m%d%H%M%S'.freeze unless const_defined?(:TIMESTAMP_FMT)
TIMESTAMP_FMT = '%Y%m%d%H%M%S'
type_params = Hash.new { |h, k| h[k] = [] }
pairs.each do |pair|
  type_params[pair[0]] << pair[1]
end
pairs = keys.map { |k| k.split(':')[2..3] }.uniq
pairs.each do |pair|
  type_params[pair[0]] << pair[1]
end
type_params
type_params.first
type, params_list = _
type
params_list
params = CGI.parse(params)
params = params_list.first
params
params = Hashie::Mash.new(params.map { |k, str| [k, str.first] }.to_h.sort.to_h.symbolize_keys)
params = CGI.parse(params)
params = Hashie::Mash.new(params.map { |k, str| [k, str.first] }.to_h.sort.to_h.symbolize_keys)
key = "#{type}:#{params.to_param}"
params = params_list.first
exit
Scrape.count
Scrape.first
s = Scrape.last
s.listings.coutn
s.listings.count
s.products.count
s.listings.count
s.listings.first
s.listings.last
s.listings
l = s.listings.first
l.screenshots.first
l.screenshots.count
l.screenshots.map {|s| s.attachment.url }
l = s.listings.last
l.screenshots.map {|s| s.attachment.url }
s.listings.last
l = s.listings.last
l.screenshots.map {|s| s.attachment.url }
l = Listing.where(url: 'https://www.google.com/aclk?sa=l&ai=DChcSEwjQ5Zmbz7DPAhVMW4YKHRk6A38YABAR&sig=AOD64_1vvdXxkSdvuX6w8xOgQYcqPypo1g&ctype=5&q=&ved=0ahUKEwjgqpebz7DPAhUH2B4KHbV5Bs8Q2CkI8wIwCA&adurl=')
l.screenshots.first.attachment.url
l = l.first
l.screenshots.first.attachment.url
l.url
exit
s = Scrape.last
s.listings.count
s.listings.last
s.listings.where("url like '%ack%').count
s.listings.where("url like '%ack%'").count
s.listings.first
s.listings.where("url like '%aclk%'").count
s.listings.where("url like '%aclk%'")
s.listings.where("url like '%aclk%'").destroy_all
Scrape.pluck(:source_id)
Product.reindex_alias
Listing.reindex_alias
exit
s = Scrape.last
l = s.listings.where(url: "https://www.google.com/aclk?sa=L&ai=DChcSEwja45LC17DPAhVLJIYKHUc6D1QYABAZ&sig=AOD64_06HRCNEOTmQwioCJ8yKQ2WHuqMDQ&ctype=5&q=&ved=0ahUKEwj1-JDC17DPAhWLXB4KHZIkBn0Q2CkIpgMwCA&adurl=").last
l = s.listings.where(original_url: "https://www.google.com/aclk?sa=L&ai=DChcSEwja45LC17DPAhVLJIYKHUc6D1QYABAZ&sig=AOD64_06HRCNEOTmQwioCJ8yKQ2WHuqMDQ&ctype=5&q=&ved=0ahUKEwj1-JDC17DPAhWLXB4KHZIkBn0Q2CkIpgMwCA&adurl=").last
l.screenshots.first.attachment.url
s = Scrape.last
s.listings.count
s.products
s.products.first.update(brand: 'Renogy')
s.source_id
Product.reindex_alias
Listing.reindex_alias
Redis.current.flushall
exit
Scrape.first.source_id
s.listings.first
Listing.first
hash = Listing.first.data_hash
hash[:General]
hash[:General][:
hash[:General][:"Product Description"] << " AAC+"
Listing.first.update(data_hash: hash)
exit
Listing.reindex_alias
Product.reindex_alias
exit
cd Product
terms = {keywords: 'corel', exact: true }; operator='AND'; filters = { }; opts = {}
terms
fields = self.query_fields
definition = generate_query_by_keywords(fields, terms, operator, filters)
terms = {keywords: 'corel' }; operator='AND'; filters = { }; opts = {}
fields = self.query_fields
definition = generate_query_by_keywords(fields, terms, operator, filters)
terms = {keywords: 'corel', exact: true }; operator='AND'; filters = { }; opts = {}
fields = self.query_fields
definition = generate_query_by_keywords(fields, terms, operator, filters)
exit
exsit
exit
edit -t
exit
Product.count
lenobo_models = Product.where(brand: 'Lenovo').pluck(:model)
collection = lenobo_models
fields = [:model]
lenobo = Product.where(brand: 'Lenovo')
lenovo = Product.where(brand: 'Lenovo')
collection = lenovo
fuzzy_cache = collection.map {|item| item.slice(fields) }
fuzzy_cache = collection.map {|item| item.slice(*fields) }
edit -t
FuzzyMatch.new(fuzzy_cache, read: :model, threshold: 0.80)
matcher = _
fields
matchers
matchers = matcher
results = matchers.flat_map do |name, matcher|
  matcher.find_all_with_score(fields[name])[0..10]
end
matchers = Hash[fields.map do |field|
    [
      field,
      FuzzyMatch.new(fuzzy_cache,
        read: field,
        threshold: 0.85,
        must_match_at_least_one_word: false
      )
    ]
end]
results = matchers.flat_map do |name, matcher|
  matcher.find_all_with_score(fields[name])[0..10]
end
matchers
fields
edit -t
FuzzyMatch.new(lenovo, :model)
edit -t
FuzzyMatch.new(lenovo, :model)
exit
edit -t
lenovo = Product.where(brand: 'lenovo')
FuzzyMatch.new(lenovo, :model)
FuzzyMatcher.new(lenovo, :model)
matcher = FuzzyMatcher.new(lenovo, :model)
matcher.search(model: lenovo.first.brand)
matcher.search(model: lenovo.sample.model)
model = lenovo.sample.model
matcher.search(model: model)
model = lenovo.sample.model
edit -t
matcher = FuzzyMatcher.new(lenovo, :model)
model = lenovo.sample.model
matcher.search(model: model)
model = lenovo.sample.model
matcher.search(model: model)
edit -t
matcher = FuzzyMatcher.new(lenovo, :model)
matcher.search(model: model)
model = lenovo.sample.model
matcher.search(model: model)
edit -t
matcher = FuzzyMatcher.new(lenovo, :model)
multimatches = []
matcher.search(model: model)
lenovo.map(&:model).each do |model|
  model.downcase!
  matches = matcher.search(model: model)
  multimatches << model if matches.count > 1
end
multimatches
multimatches.count
lenovo.count
model = multimatches.sampe
model = multimatches.sample
matcher.search(model: model)
model = multimatches.sample
matcher.search(model: model)
model = multimatches.sample
matcher.search(model: model)
model = multimatches.sample
matcher.search(model: model)
model = multimatches.sample
matcher.search(model: model)
matcher.search(model: 0)
matcher.search(model: '0')
exit
p = Product.find 67300
p.listings
p.listings.pluck(:url)
matcher = FuzzyMatcher.new(Product.where(brand: 'Samsung'), :model)
edit -t
matcher = FuzzyMatcher.new(Product.where(brand: 'Samsung'), :model)
matcher.search(model: require 'fuzzy_match')
require 'amatch'
FuzzyMatch.engine = :amatch
class FuzzyMatcher
  # Generic fuzzy matcher with caching.
  #
  # If matching on multiple fields, it uses "OR" matching.
  #
  # Usage:
  #
  #  fuzzy_product = FuzzyMatcher.new(Product.all, :brand, :model)
  #  fuzzy_product.search(
  #    brand: "papsi", model: "diiet"
  #  ).first(3)
  #  # => [{ brand: "Pepsi", model: "Diet" }]
  #
  # @param collection - ActiveRecord collection to fuzzy match
  #   the :id field is required and will be used for grouping
  # @param fields - fields of collection items to use in fuzzy matching
  #
  # @returns Array of hashes with indifferent access.
  def initialize(collection, *fields)
    @fuzzy_cache = fuzzy_cache(collection, fields)
    @matchers = Hash[fields.map do |field|
        [
          field,
          FuzzyMatch.new(@fuzzy_cache,
            read: field,
            threshold: 0.45,
            must_match_at_least_one_word: true
          )
        ]
    end]
  end
  def search(fields)
matcher.search(model: 'UN55JU6500FXZA')
edit -t
matcher = FuzzyMatcher.new(Product.where(brand: 'Samsung'), :model)
matcher.search(model: "UN40H5203")
matcher.search(model: "UN55JU6500FXZA"")
matcher.search(model: "UN55JU6500FXZA")
edit -t
matcher = FuzzyMatcher.new(Product.where(brand: 'Samsung'), :model)
matcher.search(model: "UN55JU6500FXZA")
matcher.search(model: "UN55JU6500")
matcher.search(model: "DVD")
p = Product.find(_.first['id'])
matcher.search(model: "UN55JU6500".downcase)
edit -t
matcher = FuzzyMatcher.new(Product.where(brand: 'Samsung'), :model)
matcher.search(model: "UN55JU6500".downcase)
matcher.search(model: "dvd".downcase)
matcher.search(model: "s".downcase)
edit -t
matcher = FuzzyMatcher.new(Product.where(brand: 'Samsung'), :model)
matcher.search(model: "dvd".downcase)
matcher
edit -t
matcher = FuzzyMatcher.new(Product.where(brand: 'Samsung'), :model)
matcher.search(model: "dvd".downcase)
resutls = _
matcher = FuzzyMatcher.new(Product.where(brand: 'Samsung'), :model)
matcher.first.class
matcher.instance_variable_get(:@matchers)
matcher.instance_variable_get(:@matchers)[:model]
model_matcher = _
model_matcher.find_with_score('un40H52')
model_matcher.find_with_score('un40')
model_matcher.find_all_with_score('un40')
model_matcher.find_all_with_score('un40j*')
model_matcher.find_all_with_score('un40j~')
edit -t
matcher.search(model: "un40j")
matcher.search(model: "un40j~")
matcher.search(model: "un40j").      group_by { |r| r[0][:id] }.
map { |k, v| [v[0][0], v.map { |v| v[1] }.reduce(:+)] }.
sort_by { |r| -r[1] }.
map { |r| r[0].with_indifferent_access }
p = Product.where(brand: 'Samsung', model: ["UN40J6200", "UN40J5500"]).flat_map {|p| p.listings.pluck(:url) }.uniq
Product.find_by(model: 'UN40J6200')
matcher.search(model: "UN40J6200")
p = Product.find 316034
p.listings.pluck(:url).uniq
results = matcher.search(model: "UN40J6200")
results.group_by {|r| r[0][:id] }
results.group_by {|r| r[0][:id] }.first
v = results.group_by {|r| r[0][:id] }.first[0]
v = results.group_by {|r| r[0][:id] }.first[1]
v
scores = v
scores[0][0]
scroes
scores
scores.first
scores.first[1]
scores = results.group_by {|r| r[0][:id] }.last[1]
scores = results.group_by {|r| r[0][:id] }[566787][1]
scores = results.group_by {|r| r[0][:id] }
scores.class
scores.keys
key = scores.keys.last
scores[key]
scores = scores[key]
scores.first
scores.first[1]
scores.first[1..-1
scores.first[1..-1]
scores.first[1..-1].reduce(:+)
results
results.
group_by { |r| r[0][:id] }
group_by { |r| r[0][:id] }    results.
group_by { |r| r[0][:id] }.
map { |_k, v| [v[0][0], v.map { |v| v[1..-1] }.reduce(:+)] }.
sort_by { |r| -r[1] }.
map { |r| r[0].with_indifferent_access }
results.
group_by { |r| r[0][:id] }.
map { |_k, v| [v[0][0], v.map { |v| v[1..-1] }.reduce(:+)] }.
sort_by { |r| -r[1] }.
map { |r| r[0].with_indifferent_access }
results.    results.
group_by { |r| r[0][:id] }.
map { |_k, v| [v[0][0], v.map { |v| v[1..-1] }.reduce(:+)] }.
sort_by { |r| -r[1] }.
map { |r| r[0].with_indifferent_access }
results
results.
group_by { |r| r[0][:id] }.
map { |_k, v| [v[0][0], v.map { |v| v[1..-1] }.reduce(:+)] }.
sort_by { |r| -r[1] }.
map { |r| r[0].with_indifferent_access }
results.
group_by { |r| r[0][:id] }.
map { |_k, v| [v[0][0], v.flat_map { |v| v[1..-1] }.reduce(:+)] }.
sort_by { |r| -r[1] }.
map { |r| r[0].with_indifferent_access }
results.
group_by { |r| r[0][:id] }.
map { |_k, v| [v[0][0], v.flat_map { |v| v[1..-1] }.reduce(:+), v.flat_map {|v| v[1..-1]}] }.
sort_by { |r| -r[1] }.
map { |r| r[0].with_indifferent_access }
results.
group_by { |r| r[0][:id] }.
map { |_k, v| [v[0][0], v.flat_map { |v| v[1..-1] }.reduce(:+)] }.
sort_by { |r| -r[1] }.
map { |r| r[0].with_indifferent_access.merge(scores: r[1]) }
results.
group_by { |r| r[0][:id] }.
map { |_k, v| [v[0][0], v.flat_map { |v| v[1..-1] }.reduce(:+)] }.
sort_by { |r| -r[1] }.
map { |r| r[0].with_indifferent_access.merge(score: r[1]) }
map { |r| r[0].with_indifferent_access.merge(score: r[1]) }.count
results.
group_by { |r| r[0][:id] }.
map { |_k, v| [v[0][0], v.flat_map { |v| v[1..-1] }.reduce(:+)] }.
sort_by { |r| -r[1] }.
map { |r| r[0].with_indifferent_access.merge(score: r[1]) }
_.length
edit -t
matcher = FuzzyMatcher.new(Product.where(brand: 'Samsung'), :model)
results = matcher.search(model: "UN40J6200")
edit -t
matcher = FuzzyMatcher.new(Product.where(brand: 'Samsung'), :model)
results = matcher.search(model: "UN40J6200")
exit
'products'.constantize
'products'.classify.constantize
'products'.singularize'
'products'.singularize
exit
Scrape.first
Scrape.where(source_id: [1,2,3,4,5,6]).each do |scrape|
  puts "Scrape: #{scrape.id}"
edit -t
edit
exit
edit
exit
scrape = Scrape.where(source_id: [1,2,3,4,5,6]).first
scrape.products.include_indexed_fields.to_sql
scrape.products.include_indexed_fields
class AddMissingIndexes < ActiveRecord::Migration
  def change
    add_index :scrapes, :source_id unless index_exists? :scrapes, :source_id
      add_index :product_listings, :product_id unless index_exists? :product_listings, :product_id
        add_index :product_listings, :listing_id unless index_exists? :product_listings, :listing_id
          add_index :sellers, :name unless index_exists? :sellers, :name
          end
        end
AddMissingIndexes.change
AddMissingIndexes.new.change
migration = AddMissingIndexes
migration = AddMissingIndexes.new
migration.connection
migration
migration.connection
migration.index_exists? :product_listings, :product_id
exit
edit
terms = {keywords: 'solar' }; operator='AND'; filters = { }; opts = {}
terms
cd Listing
fields = self.query_fields
definition = generate_query_by_keywords(fields, terms, operator, filters)
terms = {keywords: 'solar' }; operator='AND'; filters = { }; opts = {}
definition = generate_query_by_keywords(fields, terms, operator, filters)
results =       self.search(definition, opts)
results.results.results
exit
cd Listing
terms = {keywords: 'solar' }; operator='AND'; filters = { }; opts = {}
fields = self.query_fields
search_response = self.search(definition, opts)
definition = generate_query_by_keywords(fields, terms, operator, filters)
search_response = self.search(definition, opts)
search_response.results.results
r = search_response.results.results.first
r.id
r.products.first.id
r = search_response.results.results.flat_map {|r| r.products.map {|p| p.id } }
r = search_response.results
r = search_response.results.hits
r = search_response.results.response
r = search_response.results.response.first
r = search_response.results.response.first.products
r = search_response.results.response.first.products.map {|p| p.id}
r = search_response.results.response
API_CACHE ||= RdsCache.new('api_cache')
Listing.count
Product.count
search_response.results.count
r = search_response.results.response.map {|l| l.products.map {|p| p.id } } 
ids = search_response.results.response.map {|l| l.products.map {|p| p.id } } 
ids.flatten!
ids = search_response.results.results.map! { |l| l.products.map! { |p| p.id } }
search_response.results
search_response.results.resultts
search_response.results.results
API_CACHE
API_CACHE.prefix
API_CACHE.smembers(API_CACHE.prefix)
API_CACHE.sadd("#{API_CACHE.prefix}:ids")
ids
ids[1] << nil
ids
ids.flatten!
ids.compact
ids.compact!
API_CACHE.sadd("#{API_CACHE.prefix}:ids", ids)
API_CACHE.members("#{API_CACHE.prefix}:ids")
API_CACHE.smembers("#{API_CACHE.prefix}:ids")
API_CACHE.spop("#{API_CACHE.prefix}:ids", 3)
PER_PAGE ||= ENV['CACHE_PAGE_SIZE'].try(:to_i) || 200
API_CACHE ||= RdsCache.new('api_cache')
MODELS ||= { 'products' => Product, 'listings' => Listing }
Listing == Listing
model.is_a? Listing
Listing.is_a? Listing
Listing.is_a? Class
listing_block = { |l| l.id |
listing_block = { |l| l.id }
listing_block = -> { |l| l.id }
listing_block = Proc.new { |l| l.id }
search_response.results.results.map!(listing_block)
search_response.results.results.map!(&:listing_block)
search_response.results.results.map!(:listing_block)
search_response.results.results.map(:listing_block)
search_response.results.results.map(listing_block)
listing_block = proc { |l| l.id }
search_response.results.results.map { listing_block.call(r) }
search_response.results.results
search_response.results.results.map { listing_block.call(r) }
search_response.results.results { listing_block.call(r) }
exit
reload!
cd Listing
terms = {keywords: 'solar', exact: true }; operator='AND'; filters = { }; opts = {type: 'products'}
fields = self.query_fields
type = opt.delete(:type)
type = opts.delete(:type)
opts
definition = generate_query_by_keywords(fields, terms, operator, filters, type)
search_response = self.search(definition, opts)
search_response.results.results
reload!
exit
reload!
cd Listing
terms = {keywords: 'solar', exact: true }; operator='AND'; filters = { }; opts = {type: 'products'}
fields = self.query_fields
type = opts.delete(:type)
definition = generate_query_by_keywords(fields, terms, operator, filters, type)
self.search(definition, opts)
response = _
response.results
response.results.results
exit
reload!
cd Listing
terms = {keywords: 'solar' }; operator='AND'; filters = { }; opts = {type: 'products'}
fields = self.query_fields
type = opts.delete(:type)
definition = generate_query_by_keywords(fields, terms, operator, filters, type)
definition
sub_query = {
  constant_score: {
    filter: {
      exists: { field: 'listings.products.id' }
    }
  }
}
definition[:query][:bool][:must] << sub_query
response =       self.search(definition, opts)
response.results.results
exit
reload!
terms
self
cd Listing
self
cd Listing
terms = {keywords: 'solar' }; operator='AND'; filters = { }; opts = {type: 'products'}
fields = self.query_fields
type = opts.delete(:type)
definition = generate_query_by_keywords(fields, terms, operator, filters, type)
response =       self.search(definition, opts)
response.results.results
definition[:query]
definition[:query][:bool][:must].pop
response =       self.search(definition, opts)
definition
ls
qqadeexit
exit
reload!
terms = {keywords: 'solar' }; operator='AND'; filters = { }; opts = {type: 'products'}
fields = self.query_fields
type = opts.delete(:type)
definition = generate_query_by_keywords(fields, terms, operator, filters, type)
cd Listing
terms = {keywords: 'solar' }; operator='AND'; filters = { }; opts = {type: 'products'}
fields = self.query_fields
type = opts.delete(:type)
definition = generate_query_by_keywords(fields, terms, operator, filters, type)
response =       self.search(definition, opts)
response.results.results
response
my_proc = proc {|l| l.id}
ids = response.results.results.map my_proc
ids = response.results.results.map {my_proc}
ids = response.results.results.map {my_proc.call(p)}
ids = search_response.results.results.map { |l| l.id }
search_response = response
ids = search_response.results.results.map { |l| l.id }
ids = search_response.results.results.class
ids = search_response.results.results
l_proc = proc { |l| l['_id'] }
search_response.results.results.map l_proc
search_response.results.results.map l_proc.call(p)
search_response.results.results.map {l_proc.call(p)}
search_response.results.results.map { l_proc.call(p) }
search_response.results.results.map(&:l_proc.call(p))
search_response.results.results.map(&:l_proc)
response =       self.search(definition, opts)
response.results.results.map { l_proc.call(r) }
response.results.results.map { l_proc }
response.results.results.first
response.results.results.map { l_proc }
response.results.results.map(&:l_proc)
response.results.results.map(:l_proc)
response.results.results.map(l_proc)
response.results.results.map { |r| l_proc.call(r) }
type = 'products'
opts
opts = { type: type, scroll: '5m' }
type
opts.delete(:type)
id_proc = if type == 'listings'.freeze
  proc { |l| l['_id'] }
else
  proc { |l| l['_source']['products'].map { |p| p['id'] } }
end
kj        search_response = Listing.search_by(*search_params, { size: PER_PAGE, scroll: '3m'.freeze, type: type })
search_response = Listing.search_by(*search_params, { size: PER_PAGE, scroll: '3m'.freeze, type: type })
search_response = Listing.search_by(terms, operator, filters, opts)
total ||= search_response.results.total
j          ids = search_response.results.results.map! { |l| l.products.map! { |p| p.id } }
ids = search_response.results.results.map! { |l| l.products.map! { |p| p.id } }
ids = search_response.results.results.map! { |l| id_proc.call(l) }
ids.flatten!
ids.compact!
ids
scroll_id = search_response.response._scroll_id
scroll_response = Listing.__elasticsearch__.client.scroll(scroll_id: scroll_id, scroll: '3m'.freeze)
ids = scroll_response['hits'.freeze]['hits'.freeze].map! { |l| id_proc.call(l) }
ids.flatten
search_response = Listing.search_by(terms, operator, filters, opts)
searc_response['hits']['total']
search_response['hits']['total']
search_response.results['hits']['total']
search_response
search_response.response
search_response = Listing.search_by(*search_params, { size: PER_PAGE, scroll: '3m'.freeze, type: type }).response
search_response = Listing.search_by(terms, operator, filters, opts).response
search_response['hits']['total']
ids = search_response['hits']['hits'].map! { |l| id_proc.call(l) }
search_response
search_response.scroll_id
search_response['_scroll_id']
search_response.class
search_response._scroll_id
search_response
scroll_response
search['hits']['hits'].count
search_response['hits']['hits'].count
REDIS
API_CACHE ||= RdsCache.new('api_cache')
API_CACHE.smembers("#{API_CACHE.prefix}:ids")
API_CACHE.scard("#{API_CACHE.prefix}:ids")
1203 / 200
200 * 6
1203 / 200.0
(1203 / 200.0).ciel
num = (1203 / 200.0)
num.ceil
200.to_f
num.ceil.times do |i| puts i end
API_CACHE.scard("#{API_CACHE.prefix}:ids").class
(1..7).each do |page|
  puts page
end
API_CACHE.scard("#{API_CACHE.prefix}:ids")
API_CACHE.spop("#{API_CACHE.prefix}:ids", 3)
API_CACHE.scard("#{API_CACHE.prefix}:ids")
API_CACHE.spop("#{API_CACHE.prefix}:ids", 10)
exit
Redis.current.flushall
exit
Product.reindex_alias
exit
Product.reindex_alias
Listing.reindex_alias
Product.reindex_alias
exit
params = "{\"^o\":\"Hashie::Mash\",\"self\":{\"keywords\":\"solar\",\"type\":\"products\"}}"
search_params = "keywords=solar"
params = Hashie::Mash.new(Oj.load(params))
type = params[:type]
model = MODELS[type]
PER_PAGE ||= ENV['CACHE_PAGE_SIZE'].try(:to_i) || 200
API_CACHE ||= RdsCache.new('api_cache')
MODELS ||= { 'products' => Product, 'listings' => Listing }
model = MODELS[type]
raise "Unrecognized type: #{type}" if model.nil?
serializer = Object.const_get("#{model}Serializer")
page_count, scroll_id, search_params, search_response, total = nil
ids = []
id_proc = if type == 'listings'.freeze
  proc { |l| l['_id'] }
else
  proc { |l| l['_source']['products'].map { |p| p['id'] } }
end
prefix = "#{type}:#{cache_params}"
cache_params = "keywords=solar"
prefix = "#{type}:#{cache_params}"
search_params = nil
search_params ||= model.prepare_search_params(params)
search_response = Listing.search_by(*search_params, { size: PER_PAGE, scroll: '3m'.freeze, type: type }).response
scroll_id = search_response['_scroll_id']
total ||= search_response['hits']['total']
ids = search_response['hits']['hits'].map! { |l| id_proc.call(l) }
page_count = search_response['hits']['hits'].count
search_response = nil
API_CACHE.sadd("#{API_CACHE.prefix}:#{prefix}:ids", ids)
"#{API_CACHE.prefix}:#{prefix}:ids"
exit
Redis.current.flushall
exit
if page_count && page_count > 1
  puts hey
if page_count && page_count > 1
  puts '1'
else 
  puts '0'
end
Redis.current.flushall
exit
Product.count
p = Product.find 20
p.listings
exit
params = "{\"^o\":\"Hashie::Mash\",\"self\":{\"keywords\":\"solar\",\"type\":\"products\"}}"
search_params = "keywords=solar"
cache_params = "keywords=solar"
PER_PAGE ||= ENV['CACHE_PAGE_SIZE'].try(:to_i) || 200
API_CACHE ||= RdsCache.new('api_cache')
MODELS ||= { 'products' => Product, 'listings' => Listing }
job = PopulateApiCacheJob.new
cd job
params = Hashie::Mash.new(Oj.load(params))
params
params = "{\"^o\":\"Hashie::Mash\",\"self\":{\"keywords\":\"solar\",\"type\":\"products\"}}"
cache_params = "keywords=solar"
params = Hashie::Mash.new(Oj.load(params))
type = params[:type]
model = MODELS[type]
prefix = "#{type}:#{cache_params}"
page_count, scroll_id, search_params, search_response, total = nil
id_proc = if type == 'listings'.freeze
  proc { |l| l['_id'] }
elsif type == 'products'.freeze
  proc { |l| l['_source']['products'].map { |p| p['id'] } }
end
search_params ||= Listing.prepare_search_params(params)
opts = { size: PER_PAGE, scroll: '3m'.freeze, type: type }
terms
fields = Listing.query_fields
type = opts.delete(:type)
definition = Listing.generate_query_by_keywords(*search_params, type)
terms, operator, filters = search_params
terms
operator
filters
definition = Listing.generate_query_by_keywords(fields, terms, operator, filters, type)
p definition
exit
Product.index_name
exit
params = "{\"^o\":\"Hashie::Mash\",\"self\":{\"keywords\":\"solar\",\"type\":\"products\"}}"
l = Listing.last
l.created_at.strftime('%F')
exit
Redis.current.flushall
cache_params = "category_ids=1%2C2%2C3%2C4&keywords=cyberlink+powerdvd"
type = 'products'
in_progress = API_CACHE.fetch("#{type}:#{cache_params}:in_progress")
API_CACHE ||= RdsCache.new('api_cache')
in_progress = API_CACHE.fetch("#{type}:#{cache_params}:in_progress")
API_CACHE.fetch("#{type}:#{cache_params}:last_seen")
Redis.current.flushall
API_CACHE ||= RdsCache.new('api_cache')
MODELS ||= { 'products' => Product, 'listings' => Listing}
TIMESTAMP_FMT = '%Y%m%d%H%M%S'.freeze unless const_defined?(:TIMESTAMP_FMT)
TIMESTAMP_FMT = '%Y%m%d%H%M%S'.freeze
page = params[:page] || 1
page = 1
in_progress = API_CACHE.fetch("#{type}:#{cache_params}:in_progress")
API_CACHE.fetch("#{type}:#{cache_params}:last_seen")
API_CACHE.set_value("#{type}:#{cache_params}:last_seen", DateTime.now.strftime(TIMESTAMP_FMT))
in_progress = API_CACHE.fetch("#{type}:#{cache_params}:in_progress")
key = "#{type}:#{cache_params}:#{page}"
cached_body = API_CACHE.fetch(key)
API_CACHE.set_value("#{type}:#{cache_params}:in_progress", true)
PopulateApiCacheJob.perform_async Oj.dump(params.merge!({ type: type })), cache_params
params = Hashie::Mash.new({keywords: 'Cyberlink PowerDVD', category_ids=[1,2,3,4]})
params = Hashie::Mash.new({keywords: 'Cyberlink PowerDVD', category_ids: [1,2,3,4]})
params = clean_params(params)
def clean_params(params)
  params.each do |k, v|
    next if v.is_a? Numeric
    params[k] = v.split(',').reject(&:blank?) # for when commas are entered followed by nothing
    params[k].sort!; # ensure params with multiple values are in order for creating cache key
    params[k] = params[k].join(',')
    params[k].strip!; params[k].downcase!;
  end
end
par    page = params[:page] || 1
page = params[:page] || 1
params = clean_params(params)
cache_params = params.slice(:brands, :category_ids, :exact, :keywords, :models, :scrape_ids, :source_ids).to_param
PopulateApiCacheJob.perform_async Oj.dump(params.merge!({ type: type })), cache_params
key = "#{type}:#{cache_params}:#{page}"
params
params = Hashie::Mash.new(Oj.load(params))
params, cache_params = Oj.dump(params.merge!({ type: type })), cache_params
PER_PAGE ||= ENV['CACHE_PAGE_SIZE'].try(:to_i) || 200
API_CACHE ||= RdsCache.new('api_cache')
MODELS ||= { 'products' => Product, 'listings' => Listing }
params = Hashie::Mash.new(Oj.load(params))
type = params[:type]
model = MODELS[type]
prefix = "#{type}:#{cache_params}"
page_count, scroll_id, search_params, search_response, total = nil
id_proc = if type == 'listings'.freeze
  proc { |l| l['_id'] }
elsif type == 'products'.freeze
  proc { |l| l['_source']['products'].map { |p| p['id'] } }
end
search_params ||= Listing.prepare_search_params(params)
search_response = Listing.search_by(*search_params, { size: PER_PAGE, scroll: '3m'.freeze, type: type }).response
scroll_id = search_response['_scroll_id']
total ||= search_response['hits']['total']
page_count = search_response['hits']['hits'].count
ids = search_response['hits']['hits'].map! { |l| id_proc.call(l) }
ids.flatten!
ids.compact!
search_response = nil
API_CACHE.sadd("#{API_CACHE.prefix}:#{prefix}:ids", ids) unless ids.empty?
ids = nil
page_count && page_count < PER_PAGE
Listing.clear_scroll! scroll_id
serializer = Object.const_get("#{model}Serializer")
source_ids = params[:source_ids].try(:split, ',')
scrape_ids = Scrape.where(source_id: source_ids).uniq.pluck(:id)
total = API_CACHE.scard("#{API_CACHE.prefix}:#{prefix}:ids")
pages = (total / PER_PAGE.to_f).ceil
(1..pages).each do |page|
  # Using logger.warn to print progress every 10 pages since logger level set to :warn
  logger.warn "PopulateApiCacheJob: #{type}/#{cache_params}/#{page}" if page % 10 == 0
  key = "#{prefix}:#{page}"
  body = { type => [] }
  # AWS doesn't support redis 3.2, so can't use COUNT with SPOP
  ids = []
  PER_PAGE.times {
    id = API_CACHE.spop("#{API_CACHE.prefix}:#{prefix}:ids")
    break if id.nil?
    ids << id
  }
  model.where(id: ids).include_api_info(scrape_ids).find_in_batches(batch_size: 50) do |records|
    body[type] |= records.map! { |r| serializer.new(r).as_json(root: false) }
  end
  # NOTE: version is only being set to not break platform sig generators,
  #       it can be removed once sig generators are updated to not check for version
  meta_data = { pagination: { page: page, per_page: PER_PAGE, total: total }, version: 1 }
  API_CACHE.set_value(key, body.merge!(meta_data))
  body = nil
end
(1..pages).each do |page|
  logger.warn "PopulateApiCacheJob: #{type}/#{cache_params}/#{page}" if page % 10 == 0
  key = "#{prefix}:#{page}"
  page = 1
page = 1
key = "#{prefix}:#{page}"
body = { type => [] }
ids = []
PER_PAGE
API_CACHE.spop("#{API_CACHE.prefix}:#{prefix}:ids")
PER_PAGE.times {
  id = API_CACHE.spop("#{API_CACHE.prefix}:#{prefix}:ids")
  break if id.nil?
  ids << id
}
ids
model.where(id: ids).include_api_info(scrape_ids).find_in_batches(batch_size: 50) do |records|
  body[type] |= records.map! { |r| serializer.new(r).as_json(root: false) }
end
body
meta_data = { pagination: { page: page, per_page: PER_PAGE, total: total }, version: 1 }
API_CACHE.set_value(key, body.merge!(meta_data))
body = nil
API_CACHE.clear_cache!("#{prefix}:in_progress")
API_CACHE.clear_cache!("#{prefix}:ids")
API_CACHE.prefix
params
cache_params = "keywords=Cyberlink%20PowerDVD&using=ALL&type=products&no_cache=true"
cache_params = "keywords=Cyberlink%20PowerDVD"
keyword
params
params.delete(:type)
params
params.to_param
exit
p = Product.first
product = Product.first
p = nil
GC.start
ids = [1]
products_hash = {}
edit
products_hash
exit
edit -t
edit
exit
ids = Product.ids
edit
Product.write_to_excel(products_hash)
products_hash
products_hash.inject([]) { |memo, p| memo |= p.keys.map(&:to_s) }
[products_hash].inject([]) { |memo, p| memo |= p.keys.map(&:to_s) }
products_hash.keys
reload!
measure
ids = Product.ids
products_hash = Product.dump_all_product_info(ids)
Product.write_to_excel(products_hash)
reload!
Product.write_to_excel(products_hash)
file
file ||= "tmp/#{SecureRandom.uuid}.xlsx"
workbook = RubyXL::Workbook.new
sheet = workbook[0]
sheet.sheet_name = "products_hash"
header_row_index = 0
sheet.count
headers = products_hash.keys
headers.map!(&:to_s)
def insert_row_data(sheet, row_index, *data)
  row = sheet[row_index] || sheet.add_row(row_index)
  col_index = row.cells.length
  data.each do |val|
    cell = row.cells.find { |c| c.raw_value == val }
    next if cell.present?
    set_cell_value sheet, row_index, col_index, val
    col_index += 1
  end
end
include RubyXL::ExtraExcel
insert_row_data
insert_row_data(sheet, header_row_index, *headers)
row_index = sheet.count
products_hash
reload!
ids = Product.ids
products_hash = Product.dump_all_product_info(ids)
products_hash
reload!
products_hash = Product.dump_all_product_info(ids)
Product.write_to_excel(products_hash)
edit
reload!
edit
exit
ids = Product.ids
measure(true) do
  products_hash = Product.dump_all_product_info(ids)
  Product.write_to_excel(products_hash)
end
products_hash = Product.dump_all_product_info(ids)
Product.write_to_excel(products_hash)
reload!
products_hash = Product.dump_all_product_info(ids)
Product.write_to_excel(products_hash)
exit
workbook = RubyXL::Workbook.new
workbook.
workbook.count
workbook
exit
ListingScreenshot.last
ls = 
s = ListingScreenshot.last
s.attachment.url
current_url = _
s.original_url = current_url
s.original_url = current_url.gsub(/screenshots/, 'assets')
s.original_url
s.save_attachment
file =  File.open(s.original_url)
include RvxRds::Proxy
url = s.original_url
original_url = url
file = download_via_proxy(original_url)
s.attachment = file
s.save
s.attachment.url
s = ListingScreenshot.last(2).first
s.attachment.url
current_url = _
current_url.match(/screenshots(?=\/uploads)/)
current_url =~ (/screenshots(?=\/uploads)/)
current_url.gsub!(/screenshots(?=\/uploads)/, 'assets'.freeze)
current_url
s
s.atttachment.url
s.attachment.url
current_url
s.attachment = current_url
s.save
s.attachment.url
s = ListingScreenshot.last(3).first
current_url
url = s.attachment.url
current_url
s.attachment = current_url
s.attachment.url
s = ListingScreenshot.last(3).first
s.attachment.url
exit
ProductImage.type
ProductImage.TYPE
ProductImage::TYPE
Asset::TYPE
ProductScreenshot::TYPE
ListingScreenshot::TYPE
gst
exit
Listing.delete_index!
Listing.reindex_alias
exit
Listing.reindex_alias
exit
Listing.delete_index!
Listing.reindex_alias
xit
exit
cache_params = 'keywords=RGBW&category_ids=95%2C96%2C98%2C100%2C104&source_ids=1%2C3'
CGI.unescape(cache_params)
CGI.parse(cache_params
CGI.parse(cache_params)
params = _
params.each {|k, v| v = v.map!{|s| s.split(',') }).flatten }
params.each {|k, v| v = v.map!{|s| s.split(',') }.flatten }
params
params.each {|k,v| v.flatten! }
params
params1 = _
params1 = params.dup
PER_PAGE ||= ENV['CACHE_PAGE_SIZE'].try(:to_i) || 200
API_CACHE ||= RdsCache.new('api_cache')
MODELS ||= { 'products' => Product, 'listings' => Listing }
params = Hashie::Mash.new(Oj.load(params))
params.to_param
params = Hashie::Mash.new(Oj.load(params.to_param))
params = Hashie::Mash.new(Oj.load(Oj.dump(params)))
type = params[:type]
type = 'products'
model = MODELS[type]
prefix = "#{type}:#{cache_params}"
page_count, scroll_id, search_params, search_response, total = nil
id_proc = if type == 'listings'.freeze
  proc { |l| l['_id'] }
elsif type == 'products'.freeze
  proc { |l| l['_source']['products'].map { |p| p['id'] } }
end
search_params ||= Listing.prepare_search_params(params)
params
params.each {|k,v| v = v.is_a?(Array) ? v.join(',') : v }
params
params['category_ids'] = params['category_ids'].join(',')
params
params['source_ids'] = params['source_ids'].join(',')
params
params['keywords'] = 'RGBW'
cache_params
search_params ||= Listing.prepare_search_params(params)
fields = Listing.query_fields
fields, terms, operator = search_params
fields
fields = Listing.query_fields
terms, operator, filters = search_params
terms
operator
filters
search_fields = fields[:search_fields]
definition = search_definition(search_fields, terms, operator)
edit -t
definition = search_definition(search_fields, terms, operator)
filter_fields = fields[:filter_fields]
terms[:keywords].present?
if filters[:brands].nil?
filters[:brands].nil?
filters
definition[:query][:bool][:must][0][:multi_match][:fields] << filter_fields[:brands]
definition[:query][:bool][:must][0][:multi_match][:fields].merge(filter_fields[:models])
filter_fields.present?
filters.keys
filters[key].present?
key = :category_ids
filters[key].present?
sub_query = {
  bool: { should: [], minimum_should_match: 1 }
}
filters[key].each do |filter|
  sub_query[:bool][:should] << {
    multi_match: {
      query: filter,
      type: 'phrase',
      slop: 0,
      minimum_should_match: 1,
      operator: operator,
      fields: Array(filter_fields[key])
    }
  }
  definition[:query][:bool][:must] << sub_query
kk            sub_query = {
  bool: { should: [], minimum_should_match: 1 }
}
sub_query = {
  bool: { should: [], minimum_should_match: 1 }
}
filters[key].each do |filter|
  sub_query[:bool][:should] << {
    multi_match: {
      query: filter,
      type: 'phrase',
      slop: 0,
      minimum_should_match: 1,
      operator: operator,
      fields: Array(filter_fields[key])
    }
  }
  definition[:query][:bool][:must] << sub_query
end
definition
definition = nil
edit -t
definition = generate_query_by_keywords(fields, terms, operator, filters, type)
type
type = 'listings'
terms
filters
filters.keys.first
key = filters.keys.first
key.include?('ids')
key.to_s.include?('ids')
key.to_s.end_with?('ids')
filter
filters
filters[key]
filter_fields[key]
edit -t
definition
definition = {}
fields
type
terms
filters
definition = generate_query_by_keywords(fields, terms, operator, filters, type)
reload!
definition = Listing.generate_query_by_keywords(fields, terms, operator, filters, type)
reload!
definition = Listing.generate_query_by_keywords(fields, terms, operator, filters, type)
filters
search_fields = fields[:search_fields]
definition = search_definition(search_fields, terms, operator)
filter_fields = fields[:filter_fields]
filters
if filters[:brands].nil?
  definition[:query][:bool][:must][0][:multi_match][:fields] << filter_fields[:brands]
end
if filters[:models].nil?
  # Model is multifield so need to merge sets
  definition[:query][:bool][:must][0][:multi_match][:fields].merge(filter_fields[:models])
end
definition
definition = nil
search_fields = fields[:search_fields]
definition = search_definition(search_fields, terms, operator)
search_fields
Listing.query_fields
fields = Listing.query_fields
search_fields = fields[:search_fields]
definition = search_definition(search_fields, terms, operator)
filter_fields = fields[:filter_fields]
if filters[:brands].nil?
  definition[:query][:bool][:must][0][:multi_match][:fields] << filter_fields[:brands]
end
if filters[:models].nil?
  # Model is multifield so need to merge sets
  definition[:query][:bool][:must][0][:multi_match][:fields].merge(filter_fields[:models])
end
terms
filter_fields.present? && filters.present?
filters.keys
key = filters.keys.first
filters[key].present?
key == :brands
key.to_s.end_with?('ids')
sub_query = nil
if key.to_s.end_with?('ids')
  sub_query = {
    filter: {
      bool: {
        must: [
          terms: { filter_fields[key] => filters[key] }
        ]
      }
    }
  }
else
  sub_query = {
    bool: { should: [], minimum_should_match: 1 }
  }
  filters[key].each do |filter|
    sub_query[:bool][:should] << {
      multi_match: {
        query: filter,
        type: 'phrase',
        slop: 0,
        minimum_should_match: 1,
        operator: operator,
        fields: Array(filter_fields[key])
      }
    }
  end
  sub_query
sub_query
edit -t
sub_query
definition
definition[:query][:bool][:must] << sub_query
terms
fields
fields = Listing.query_fields`
fields = Listing.query_fields
Listing.reload!
Listing.reload
reload!
definition = Listing.generate_query_by_keywords(fields, terms, operator, filters, type)
search_results = Listing.search(definition, {})
search_response = _.response
definition
reload!
definition = Listing.generate_query_by_keywords(fields, terms, operator, filters, type)
search_response = Listing.search(definition, {}).response
filters
filters[:brands] = ['renogy']
definition = Listing.generate_query_by_keywords(fields, terms, operator, filters, type)
filters
filters[:brands] << 'sunpower'
Product.pluck(:brand, :model)
filters[:models] = ['RNG-100D']
filters
p = Product.find_by(model: 'RNG-100D')
p.categories.first.id
p.categories.ids
p.sources
filters[:source_ids] << '4'
filters[:category_ids] << '4'
reload!
definition = nil
fields = Listing.query_fields
definition = Listing.generate_query_by_keywords(fields, terms, operator, filters, type)
search_response = Listing.search(definition, {}).response
search_response.hits
p.listings.first
reload!
definition = Listing.generate_query_by_keywords(fields, terms, operator, filters, type)
search_response = Listing.search(definition, {}).response
search_response.hits
terms
terms[:keywords] = 'solar'
defintion = nil
definition = Listing.generate_query_by_keywords(fields, terms, operator, filters, type)
search_response = Listing.search(definition, {}).response
search_response.hits
id_proc = if type == 'listings'.freeze
  proc { |l| l['_id'] }
elsif type == 'products'.freeze
  proc { |l| l['_source']['products'].map { |p| p['id'] } }
end
ids = search_response['hits']['hits'].map! { |l| id_proc.call(l) }
definition
reload!
defintion = nil
definition = Listing.generate_query_by_keywords(fields, terms, operator, filters, type)
search_response = Listing.search(definition, {}).response
filters
Product.pluck(:brand, :model)
filters[:brands] << 'Sunforce'; filters[:models] << '38006'
filters
definition = nil
fields = Listing.query_fields
definition = Listing.generate_query_by_keywords(fields, terms, operator, filters, type)
search_response = Listing.search(definition, {}).response
response.hits
terms
Product.find_by(model: '38006')
Product.find_by(model: '38006').listings.count
Product.find_by(model: '38006').listings.first
Product.find_by(model: '38006').listings.last
definitino
definition
definition.to_json
defintion.as_json
Listing.reindex_alias
p
p.listings
definition
search_response = Listing.search(definition, {}).response
Listing.first
Listing.first.name
difinition
definition
definition[:query][:bool].first[:multi_match][:fields].pop
definition[:query][:bool].first[:multi_match][:fields]
definition[:query][:bool].first[:multi_match]
definition[:query][:bool].first
definition[:query][:bool]
definition[:query][:bool][:must].first[:multi_match][:fields].pop
definition[:query][:bool][:must].first[:multi_match][:fields]
definition[:query][:bool][:must].first[:multi_match][:fields].delete_if { |v| v.include?('data_hash') }
definition
search_response = Listing.search(definition, {}).response
definition = inl
definition = nil
definition = Listing.generate_query_by_keywords(fields, terms, operator, filters, type)
reload!
definition = Listing.generate_query_by_keywords(fields, terms, operator, filters, type)
search_response = Listing.search(definition, {}).response
definition
definition = nil
definition = Listing.generate_query_by_keywords(fields, terms, operator, filters, type)
fields = Listing.query_fields
definition = nil
definition = Listing.generate_query_by_keywords(fields, terms, operator, filters, type)
Product.find_by(model: '38006').listings.map(&:source_id)
Product.find_by(model: '38006').listings.map(&:source)
filters
filters[:source_ids] << '2'
reload!
definition = nil
definition = Listing.generate_query_by_keywords(fields, terms, operator, filters, type)
search_response = Listing.search(definition, {}).response
Product.find_by(model: '38006').listings.map(&:categories)
filters[:categories_ids] << '3'
filters[:category_ids] << '3'
definition = Listing.generate_query_by_keywords(fields, terms, operator, filters, type)
search_response = Listing.search(definition, {}).response
ids = search_response['hits']['hits'].map! { |l| id_proc.call(l) }
Product.where(id: ids).pluck(:brand, :model)
ids
Product.where(id: ids)
id_proc = proc { |l| l['_source']['products'].map { |p| p['id'] } }
ids = search_response['hits']['hits'].map! { |l| id_proc.call(l) }
search_response = Listing.search(definition, {}).response
ids = search_response['hits']['hits'].map! { |l| id_proc.call(l) }
ids.flatten!
ids.compact!
ids
ids.uniq!
ids
Product.where(id: ids).pluck(:brand, :model)
exit
params = Oj.load("{\"^o\":\"Hashie::Mash\",\"self\":{\"keywords\":\"RGBA\",\"category_ids\":\"95,96,98,100,104\",\"source_ids\":\"1,3\"}}")
type = "listings
params
params = Oj.load(params)
params = Oj.load("{\"^o\":\"Hashie::Mash\",\"self\":{\"keywords\":\"rgba\",\"category_ids\":\"100,104,95,96,98\",\"source_ids\":\"1,3\"}}")
cache_params = params.slice(:brands, :category_ids, :exact, :keywords, :models, :scrape_ids, :source_ids).to_param
exit
p = Product.first
p.sources
Listing.first
Scrape.first
l = Listing.last.dup
Scrape.find 3
p = Product.first
p.listings << Listing.last
edit -t
perform(1, 'hey')
exit
exit
str = "兼容平台: ANDROID MIUI iOS"
str.split(':')
str.split(':').map(&:strip)
str2 = str.split(':').last
str2[0]
str2[0].strip
str2[0].gsub(' ', '')
str2[0].gsub.gsub("\u00A0", "")
str2[0].gsub("\u00A0", "")
str2[0].gsub("\u00A0", " ")
str2[0].gsub("\u00A0", " ").strip
str = '      el.gsub!(/¥/, '')
str = '¥ 1234'
str.gsub(/¥/, '')
el = '¥ 1234'
el.gsub!(/\u00A0/, ''.freeze)
el
el.gsub!(/\s+/, ''.freeze)
el.gsub!(/¥/, '')
el.strip!
el
price = '¥730.00'
price.gsub!(/\u00A0/, ''.freeze)
price.gsub!(/\s+/, ''.freeze)
price.gsub!(/¥/, ''.freeze)
price.strip!
price
price.to_f
prices = [123.02, 153.0, 100]
min, max prices.minmax
min, max = prices.minmax
min
max
img = 'https://img.alicdn.com/bao/uploaded/i2/TB1GvXQNXXXXXaAXFXXXXXXXXXX_!!0-item_pic.jpg_60x60q90.jpg'
img.gsub(/(?<=\.jpg)/, '')
img.gsub(/(?<=\.jpg).*/, '')
text = '机身颜色: 白色 金色'
text.match(/\A.+:/)
text.match(/\A.+(?=:)/)
text.match(/\A.+(?=:)/)[0]
' asdf     asdf     '.squish
_.remove(' ')
string = ' asdf     asdf   ¥  '
string.remove(/(¥|\s+|\u00A0)/)
price = '¥ 1234.25'
price.remove!(/(\u00A0|\s+|¥)/)
price
price = '¥ 1234.25'
price.remove(/[^\d\.]/)
price
price.remove(/[^:digit:\.]/)
price.remove(/[^\d\.]/)
price = '11.80-12.80'
price.match(/-/)
price.match(/#/)
min_max = [1,2]
prices = [ ] 
prices + min_max
prices
prices.concat(min_max)
prices
prices.concat(1)
nil.to_f
''.to_f
prices
prices = ['1', '2', '', nil]
prices.compact!
prices.delete_if(&:blank)
prices.delete_if(&:blank?)
prices
exit
TaoBaoListingDetailJob
TaobaoListingDetailJob
TaobaoListingDetailJob.new.perform(1, 1, 
url = 'https://item.taobao.com/item.htm?id=537701385028'
TaobaoListingDetailJob.new.perform(1, 1, url)
exit
url = 'https://item.taobao.com/item.htm?id=537701385028'
TaobaoListingDetailJob.new.perform(1, 1, url)
str 'sometthign: asdf'
str = 'sometthign: asdf'
str.try(:text).try(:match, /\A.+(?=:)/).try(:[], 0)
str.try(:match, /\A.+(?=:)/).try(:[], 0)
exit
Listing.last
url = 'https://item.taobao.com/item.htm?id=537701385028'
TaobaoListingDetailJob.new.perform(1, 1, url)
Listing.last
exit
url = 'https://item.taobao.com/item.htm?id=537701385028'
TaobaoListingDetailJob.new.perform(1, 1, url)
url = 'https://item.taobao.com/world/item.htm?id=538238071240'
TaobaoListingDetailJob.new.perform(1, 1, url)
Listing.last
exit
url = 'https://item.taobao.com/world/item.htm?id=538238071240'
TaobaoListingDetailJob.new.perform(1, 1, url)
Listing.last
l = _
l.images
exit
url = 'https://item.taobao.com/world/item.htm?id=538238071240'
TaobaoListingDetailJob.new.perform(2, 1, url)
url = 'https://world.tmall.com/item/536636364831.htm?'
TaobaoListingDetailJob.new.perform(2, 1, url)
l = Listing.last
l.images
l.images.map {|i| i.attachment.url
l.images.map {|i| i.attachment.url }
l.images.map {|i| i.save_attachment }
l.images.map {|i| i.attachment.url
l.images.map {|i| i.attachment.url }
l
l.screenshots
l.screenshots.first.attachment.url
exit
url = 'https://world.taobao.com/item/539333500982.htm'
TaobaoListingDetailJob.new.perform(2, 1, url)
l = Listing.last
exit
url = 'https://world.taobao.com/item/539333500982.htm'
l = Listing.last
l.destroy
url = 'https://world.taobao.com/item/539333500982.htm'
exit
url = 'https://world.taobao.com/item/539333500982.htm'
TaobaoListingDetailJob.new.perform(2, 1, url)
l = Listing.last
l.images.count
l = Listing.last(2).first
l.images
l.images.count
Listing.last.destroy
exit
url = 'https://world.taobao.com/item/539333500982.htm'
TaobaoListingDetailJob.new.perform(2, 1, url)
l = Listing.last
l.images.count
l.destroy
exit
url = 'https://world.taobao.com/item/539333500982.htm'
TaobaoListingDetailJob.new.perform(2, 1, url)
l = Listing.last
l.images.count
params
Redis.current.keys
exit
edit -t
edit
uri1 = URI.parse(url1)
uri1.params
CGI.parse(URI.parse(url1).query)
params1, params2, params3 = nil
[url1, url2, url3].each do |url|
[url1, url2, url3].each_with_index do |url, idx|
params = {}
[url1, url2, url3].each_with_index do |url, idx|
  params["url#{idx}
[url1, url2, url3].each_with_index do |url, idx|
  params["url#{idx}"] = CGI.parse(URI.parse(url).query)
end
params
url4 = 'https://world.taobao.com/search/search.htm?cat=54336001&_ksTS=1476736856914_41&spm=a21bp.7806943.banner_XX_cat.784.TS9ZvX&_input_charset=utf-8&navigator=all&json=on&callback=__jsonp_cb&cna=PByMEOlMEksCARgFRqIAe843&abtest=_AB-LR517-LR854-LR895-PR517-PR854-PR895'
[url1, url2, url3, url4].each_with_index do |url, idx|
  params["url#{idx}"] = CGI.parse(URI.parse(url).query)
end
params
params.keys
params.keys.each do |key|
params.keys.each do |url|
non_matching = []
params.keys.each do |key|
  params.keys.each do |url|
params.keys.each do |url|
urls = params.keys
keys = []
urls.each do |url|
  keys |= params[url].keys
end
keys
mismatches = []
keys.each do |key|
  urls.each do |url1|
    urls.each do |url2|
      next if url1 == url2
      if params[url1][key] == params[url2][key]
        mismatches << key
      end
    end
  end
end
mismatches
params
params['url0']['abtest'] == params['url1']['abtest']
mismatches = []
keys.each do |key|
  urls.each do |url1|
    urls.each do |url2|
      next if url1 == url2
      unless params[url1][key] == params[url2][key]
        mismatches << key
      end
    end
  end
end
mismatches
mismatches.uniq!
params
exit
TaobaoListingDetailJob.new.perform(2, 1, url)
url = 
url = 'https://world.tmall.com/item/531051015996.htm'
TaobaoListingDetailJob.new.perform(2, 1, url)
Monetize.new("CNY 5999.00")
Monetize("CNY 5999.00")
Monetize.parse("CNY 5999.00")
Monetize.parse("CNY 5999.00").exchange_to(:USD)
Monetize.parse("CNY 5999.00").exchange_to(:USD).to_f
exit
url = 'https://item.taobao.com/world/item.htm?ft=t&toSite=main&id=527300437215'
exit
url = 'https://item.taobao.com/world/item.htm?ft=t&toSite=main&id=527300437215'
TaobaoListingDetailJob.new.perform(2, 1, url)
l = Listing.last
l.screenshots.count
l.screenshots.first.attachment.url
l.images
l.images.count
l.images.map {|i| i.attachment.url }
l.images.map {|i| [i.id, i.original_url]}
exit
arr = [1,2,3,4,]
arr
arr.length
edit -t
edit
test
edit
test
exit
str = '掌柜：
str = '掌柜：shuanggun'
str.gsub(/掌柜:/, '')
name.gsub!(/\u00A0/, ' '.freeze)
str.gsub!(/\u00A0/, '!'.freeze)
str
str.gsub(/掌柜:/i, '')
str.force_encoding("BINARY")
':'.force_encoding("BINARY")
str = '：'
str.force_encoding("BINARY")
str.length
str.strip
str
str = '：'
str.strip
str.gsub(/\u00A0/, '!'.freeze)
str.force_encoding("UNICODE")
str.gsub(/\W/,"")
str = '掌柜：shuanggun'
str.gsub(/\W/,"")
str = '：'
str.class
str.count
str.length
str.strip
str
str.include?(': ')
str
str.strip
str.force_encoding("BINARY")
'a'.force_encoding("BINARY")
str.gsub(/掌柜\xEF\xBC\x9A/, '')
str
str = '掌柜：shuanggun'
str.gsub(/掌柜\xEF\xBC\x9A/, '')
str
str.encoding
str.force_encoding("ASCII")
str = '掌柜：shuanggun'
str.gsub(/\A.*(\xEF\xBC\x9A|:)/, '').strip
str
str = '掌柜：shuanggun'
'掌柜:shuanggun'
str = _
str.gsub(/:/, '')
exit
url = 'https://item.taobao.com/item.htm?spm=a219r.lm872.14.3.LqzPY1&id=537828694124&ns=1&abbucket=13'
TaobaoListingDetailJob.new.perform(2, 1, url)
exit
Seller.last
s = _
ScrapeListingSeller.where(seller: s)
ScrapeListingSeller.where(seller: s).pluck(:seller_url)
ScrapeListingSeller.where(seller: s).pluck(:seller_url).reject(&:blank?)
str
str = '掌柜
str = '掌柜shuanggun'
str.gsub(/\A掌柜.*(\xEF\xBC\x9A|:)?/, ''.freeze)
str.gsub(/\A掌柜(\xEF\xBC\x9A|:)?/, ''.freeze)
str2 = '掌柜：shuanggun'
str2.gsub(/\A掌柜(\xEF\xBC\x9A|:)?/, ''.freeze)
exit
url = 'https://item.taobao.com/item.htm?spm=a219r.lm872.14.3.LqzPY1&id=537828694124&ns=1&abbucket=13'
TaobaoListingDetailJob.new.perform(2, 1, url)
l = Listing.last
l.sellers
l.scrape_listing_sellers.pluck(:seller_url)
l.images.count
l.scrape_listing_seller
l.scrape_listing_sellers
sls = l.scrape_listing_sellers.first
sls.exchanged_min.to_f
l.destroy
exit
url = 'https://item.taobao.com/item.htm?spm=a219r.lm872.14.3.LqzPY1&id=537828694124&ns=1&abbucket=13'
TaobaoListingDetailJob.new.perform(2, 1, url)
l = Listing.last
sls = l.scrape_listing_sellers.first
url = 'https://item.taobao.com/item.htm?spm=a219r.lm872.14.3.LqzPY1&id=537828694124&ns=1&abbucket=13'
url.gsub(/?.*$/, ''.freeze)
url.gsub(/?.+$/, ''.freeze)
url.gsub(/\?.*$/, ''.freeze)
CGI.parse(url)
cgi = _
uri = URI.parse(url)
uri.path
uri.query
params = CGI.parse(uri.query)
params[:id]
params['id']
uri.query
params.keep_if {|k,v| k == 'id' }
params
params.to_param
params.to_query
params
url
url.gsub(/\?.*$/, '')
url.gsub(/\?.*$/, '') << "?#{params['id']}"
url.gsub(/\?.*$/, '') << "?#{params['id'].first}"
url.gsub(/\?.*$/, '') << "?id=#{params['id'].first}"
exit
Listing.last.destroy
url = 'http://stackoverflow.com/questions/10993080/how-to-replace-multibyte-characters-in-ruby-using-gsub'
url = 'https://item.taobao.com/item.htm?spm=a219r.lm872.14.3.LqzPY1&id=537828694124&ns=1&abbucket=13'
TaobaoListingDetailJob.new.perform(2, 1, url)
l = Listing.last
job = TaobaoListingDetailJob
cd job
url = 'https://item.taobao.com/item.htm?spm=a219r.lm872.14.3.LqzPY1&id=537828694124&ns=1&abbucket=13'
detail_url = get_detail_url(url)
cd
job = TaobaoListingDetailJob.new
url = 'https://item.taobao.com/item.htm?spm=a219r.lm872.14.3.LqzPY1&id=537828694124&ns=1&abbucket=13'
detail_url = get_detail_url(url)
cd job
url = 'https://item.taobao.com/item.htm?spm=a219r.lm872.14.3.LqzPY1&id=537828694124&ns=1&abbucket=13'
detail_url = get_detail_url(url)
visit_page detail_url, cookie_url: @cookie_url
@session.current_url
url
detail_url = @session.current_url
url_params = CGI.parse(URI.parse(url).query)
url = url.gsub!(/\?.*$/, '')
url << "?id=#{url_params['id'].first}" if url_params['id']
detail_url = @session.current_url
detail_params = CGI.parse(URI.parse(detail_url).query)
detail_url = detail_url.gsub!(/\?.*$/, '')
detail_url << "?id=#{detail_params['id'].first}" if detail_params['id']
detail_params
exit
Listing.last
Listing.last.destory
Listing.last.destroy
url = 'https://item.taobao.com/item.htm?spm=a219r.lm872.14.3.LqzPY1&id=537828694124&ns=1&abbucket=13
url = 'https://item.taobao.com/item.htm?spm=a219r.lm872.14.3.LqzPY1&id=537828694124&ns=1&abbucket=13'
TaobaoListingDetailJob.new.perform(2, 1, url)
l = Listing.last
url
url = 'https://item.taobao.com/item.htm?spm=a219r.lm872.14.3.LqzPY1&id=537828694124&ns=1&abbucket=13'
Listing.last.destroy
Listing.last
Listing.last.destroy
Listing.last
Listing.last.destroy
Listing.last
Listing.last.destroy
Listing.last
Listing.last.destroy
Listing.last
exdit
exit
url = 'https://item.taobao.com/item.htm?spm=a219r.lm872.14.3.LqzPY1&id=537828694124&ns=1&abbucket=13'
TaobaoListingDetailJob.new.perform(2, 1, url)
l = Listing.last
l.listing_url
url = 'https://world.taobao.com/item/537828694124.htm?fromSite=main'
url.match(/(?=item\/)\d+/)
url.match(/(?<=item\/)\d+/)
url.match(/(?<=item\/)\d+/)[0]
params = CGI.parse(URI.parse(url).query)
params['id'].present? ? params['id'].first : url.match(/(?<=item\/)\d+/).try(:[], 0)
url = 'https://item.taobao.com/item.htm?spm=a219r.lm872.14.3.LqzPY1&id=537828694124&ns=1&abbucket=13'
params = CGI.parse(URI.parse(url).query)
params['id'].present? ? params['id'].first : url.match(/(?<=item\/)\d+/).try(:[], 0)
exit
url
url = 'https://item.taobao.com/item.htm?spm=a219r.lm872.14.3.LqzPY1&id=537828694124&ns=1&abbucket=13'
Listing.last
Listing.last.destroy
Listing.last
TaobaoListingDetailJob.new.perform(2, 1, url)
l = Listing.last
l.screenshots.first.attachment.url
url = 'https://item.taobao.com/item.htm?spm=a217h.1099669.1998016581-3.1.cpCqvJ&id=540064379659'
TaobaoListingDetailJob.new.perform(2, 1, url)
l = Listing.last
l.listing_url
Listing.last.url
url
Listing.last.original_url
Listing.last.destroy
exit
url = 'https://item.taobao.com/item.htm?spm=a217h.1099669.1998016581-3.1.cpCqvJ&id=540064379659'
TaobaoListingDetailJob.new.perform(2, 1, url)
l = Listing.last
l.listing_url
Source.find(7)
Source.find(8)
Source.find(9)
Source.find_by(name: 'taobao')
exit
Source.find_by(name: 'taobao')
source = _
source.source_categories
(1.0).is_a? Number
(1.0).is_a? Numeric
(1).is_a? Numeric
path = File.join(Rails.root, 'db', 'seeds', 'rds_categories.xlsx')
file = path
xlsx = open_spreadsheet(file)
def open_spreadsheet(file_path)
  case File.extname(file_path)
  when '.csv' then
    Roo::CSV.new(file_path)
  when '.xls' then
    Roo::Excel.new(file_path)
  when '.xlsx' then
    Roo::Excelx.new(file_path)
  else
    fail "Unknown file type: #{file_path}"
  end
end
xlsx = open_spreadsheet(file)
sources = {}
source_names = Source.where(source_type_id: 1).pluck(:name)
xlsx.sheets('amazonus')
xlsx.sheets
xlsx.sheets['amazonus']
xlsx.sheets[6]
sheet = xlsx.sheet('amazonus')
columns = sheet.first.reject { |cell| cell.blank? }.map { |col| col.downcase.gsub(/\s+/, '_') }
categories = []
xlsx.first_row
i = 2
cur_row = xlsx.row(i).map do |row|
  if row.nil? then
    row
  else
    if row.is_a? Numeric
      puts row
    else
      row.strip.gsub(/\s+/, ' ')
    end
  end
end
edit -t
xlsx.row(i)
row = _
1.0.to_i
0 > 0.0
0.0 > 0
source
source.reload!
source.reload
source.source_categories
source.source_categories.active
source.source_categories.active.pluck(:url)
SourceCategory.where(url: 'https://world.taobao.com/search/search.htm?cat=110203&_ksTS=1472956627173_388&spm=a21bp.7806943.banner_XX_cat.724.bW8CDc&json=on&cna=8fxNEF3dpVYCARgFRqKIxD76&ppath=1626691%3A123369&_input_charset=utf-8&navigator=all&s=0&callback=__jsonp_cb&abtest=_AB-LR517-LR854-LR895-PR517-PR854-PR895')
SourceCategory.where(url: 'https://world.taobao.com/search/search.htm?cat=110203&_ksTS=1472956627173_388&spm=a21bp.7806943.banner_XX_cat.724.bW8CDc&json=on&cna=8fxNEF3dpVYCARgFRqKIxD76&ppath=1626691%3A123369&_input_charset=utf-8&navigator=all&s=0&callback=__jsonp_cb&abtest=_AB-LR517-LR854-LR895-PR517-PR854-PR895').first
SourceCategory.where(url: 'https://world.taobao.com/search/search.htm?cat=110203&_ksTS=1472956627173_388&spm=a21bp.7806943.banner_XX_cat.724.bW8CDc&json=on&cna=8fxNEF3dpVYCARgFRqKIxD76&ppath=1626691%3A123369&_input_charset=utf-8&navigator=all&s=0&callback=__jsonp_cb&abtest=_AB-LR517-LR854-LR895-PR517-PR854-PR895').first.category.name
url = 'https://world.tmall.com/item/#{product_id}.htm'
url = "https://world.tmall.com/item/#{product_id}.htm"
product_id = 1251461235
url = "https://world.tmall.com/item/#{product_id}.htm"
exit
s = Source.last
s.source_categories.active.first
source_category_id = _.id
category_url = "https://world.taobao.com/search/search.htm?cat=50040834&_ksTS=1472955712432_388&spm=a21bp.7806943.banner_XX_cat.723.bW8CDc&json=on&cna=8fxNEF3dpVYCARgFRqKIxD76&ppath=33387%3A14396781&_input_charset=utf-8&navigator=all&s=0&callback=__jsonp_cb&abtest=_AB-LR517-LR854-LR895-PR517-PR854-PR895"
scrape_id = 1
TaobaoListingJob.new.perform(scrape_id, source_category_id, category_url)
TaobaoListingJob
exit
TaobaoListingJob
AlibabaProductListingJob
exit
TaobaoListingJob
s.source_categories.active.first
s = Source.last
sc = s.source_categories.first
TaobaoListingJob.new.perform(1, sc.id, sc.url)
reload!
TaobaoListingJob.new.perform(1, sc.id, sc.url)
exit
sc = Source.last.source_categories.first
TaobaoListingJob.new.perform(1, sc.id, sc.url)
sc.parse_url
sc.url
sc = Source.last.source_categories.first
sc = Source.last.source_categories.actice.first
sc = Source.last.source_categories.active.first
TaobaoListingJob.new.perform(1, sc.id, sc.url)
exit
sc = Source.last.source_categories.actice.first
sc = Source.last.source_categories.active.first
TaobaoListingJob.new.perform(1, sc.id, sc.url)
exit
sc = Source.last.source_categories.active.first
TaobaoListingJob.new.perform(1, sc.id, sc.url)
current_url = 'https://world.taobao.com/search/search.htm?cat=50040834&_ksTS=1476923033383_2431&spm=a21bp.7806943.banner_XX_cat.723.bW8CDc&json=on&cna=1e1dEB%20XZFACARgFRqLkyLQo&ppath=33387%3A14396781&module=page&_input_charset=utf-8&navigator=all&s=0&callback=__jsonp_cb&abtest=_AB-LR517-LR854-LR895-PR517-PR854-PR895'
uri = URI.parse(current_url)
params = uri.query
params = CGI.parse(uri.query)
params['s'].first += 60
params['s'].first
params['s'].first.to_i + 60
params
params['s'].first.to_i += 60
params['s'] = [params['s'].first.to_i += 60]
params['s'] = [params['s'].first.to_i + 60]
params
params['s'] = [params['s'].first.to_i + 60].to_s
params
uri
uri.query_values = params
params.to_query
CGI.unescape(_)
params.map {|key,values| [key.to_sym, values[0]||true]}]
params.map {|key,values| [key.to_sym, values[0]||true]}
Hash[params.map {|key,values| [key.to_sym, values[0]||true]}]
params['s'] = [params['s'].first.to_i + 60].to_s
params['s'] = [params['s'].first.to_i + 60.to_s]
params['s'] = [(params['s'].first.to_i + 60).to_s]
Hash[params.map {|key,values| [key.to_sym, values[0]||true]}]
uri
params = CGI.parse(uri.query)
params['s'] = [(params['s'].first.to_i + 60).to_s]    params['s'] = [(params['s'].first.to_i + 60).to_s]
params['s'] = [(params['s'].first.to_i + 60).to_s]
params
params = Hash[params.map { |key, values| [key.to_sym, values[0]||true] }]
params.to_query
params = params.to_query
params = CGI.unescape(params)
"#{scheme}://#{host}"
"#{uri.scheme}://#{uri.host}"
"#{uri.scheme}://#{uri.host}#{uri.path}"
uri.to_s
params
uri.query = params
params
params = "_input_charset=utf-8&_ksTS=1476923033383_2431&abtest=_AB-LR517-LR854-LR895-PR517-PR854-PR895&callback=__jsonp_cb&cat=50040834&cna=1e1dEB XZFACARgFRqLkyLQo&json=on&module=page&navigator=all&ppath=33387:14396781&s=120&spm=a21bp.7806943.banner_XX_cat.723.bW8CDc"
params
uri.to_s
uri.query = params
uri.to_s
uri =     uri = URI.parse(current_url)
uri = URI.parse(current_url)
params = CGI.parse(uri.query)
params = Hash[params.map { |key, values| [key.to_sym, values[0]||true] }]
params[:s] = params[:s] + 60
params[:s] = params[:s].to_i + 60
params = params.to_query
params = CGI.unescape(params)
uri.query = params
uri.to_s
l = Listing.last
l.images.count
(1 > 2)..(2 < 2)
[1,2,3,4,5].each do |i|
  if (i > 2)..(i < 5)
    puts i
  end
end
[1,2,3,4,5,6,7].each do |i|
  if (i > 2)..(i < 5)
    puts i
  end
end
if (i == 2)..(i == 5)\
[1,2,3,4,5,6,7].each do |i|
  if (i == 2)..(i == 5)
    puts i
  end
end
nums = []
[1,2,3,4,5,6,7].each do |i|
  if (i == 2)..(i == 5)
    nums << i
  end
end
nums
nums = []
[1,2,3,4,5,6,7].each do |i|
  if (i == 2)...(i == 5)
    nums << i
  end
end
nums
edit
nums
edit
nums
edit
nums
exit
money = Monetize.parse('KRW 28,400')
money.amount
money.as_us_dollar
money.as_us_dollar.to_f
s = Source.last
s.source_categories.test
reload!
s = Source.last
s.source_categories.test
s.source_categories.test.pluck(:id)
reload!
s.source_categories_for_testing
gst
exit
s = Source.last
cat = s.source_categories.test.first
c = s.source_categories.test.first
c.url
URI.parse(c.url)
URI.encode(c.url)
s.source_categories.where.not(url: nil)
s.source_categories.where.not(url: nil).each do |sc|
  sc.update(url: URI.encode(sc.url)
    end
s.source_categories.where.not(url: nil).each do |sc|
  sc.update(url: URI.encode(sc.url))
end
s.reload
s.source_categories.pluck(:url)
s.source_categories.active.pluck(:url)
exit
file_path = '/Users/jonathan/Downloads/clorox_hand_scrape_20161022'
scrape_name = 'clorox Oct 22, 2016'
job = ImportScrapeDataJob.new(scrape_name, file_path)
job = ImportScrapeDataJob
job = ImportScrapeDataJob.new
cd job
file_path = '/Users/jonathan/Downloads/clorox_hand_scrape_20161022'
scrape_name = 'clorox Oct 22, 2016'
start_time = DateTime.now
file_hash = Digest::MD5.hexdigest(DateTime.now.to_s)
source = Source.find_by(file_hash: file_hash)
source = Source.create name: scrape_name,
# upload_url: url,
upload_url: "import/#{DateTime.now.strftime('%Y%m%d%H%M%S')}",
source_type_id: SourceType::IMPORT,
file_hash: file_hash
tmp_folder_path = file_path
data_file_paths = Dir["#{tmp_folder_path}/*.csv", "#{tmp_folder_path}/*.xlsx", "#{tmp_folder_path}/*.xls"]
scrape = Scrape.create(
  source: source,
  name: scrape_name,
  running: true,
  created_at: start_time
)
xlsx = open_spreadsheet data_file_paths.first
xlsx.sheets
xlsx.sheets.first
sheet_name = xlsx.sheets.first
sheet = xlsx.sheet(sheet_name)
first = sheet.first_row
key_mapping = generate_key_mapping sheet.row(first)
data_hash_keys = key_mapping.keys - @keys
xlsx.close
xlsx.open?
xlsx = open_spreadsheet data_file_paths.first
sheet_name = xlsx.sheets.first
sheet = xlsx.sheet(sheet_name)
first = sheet.first_row
key_mapping = generate_key_mapping sheet.row(first)
data_hash_keys = key_mapping.keys - @keys
exit
job = ImportScrapeDataJob.new
cd job
file_path = '/Users/jonathan/Downloads/clorox_hand_scrape_20161022'
scrape_name = 'clorox Oct 22, 2016'
edit
xlsx = open_spreadsheet data_file_paths.first
sheet_name = xlsx.sheets.first
sheet = xlsx.sheet(sheet_name)
first = sheet.first_row
key_mapping = generate_key_mapping sheet.row(first)
data_hash_keys = key_mapping.keys - @keys
xlsx = open_spreadsheet data_file_paths.first
sheet = xlsx.sheet(sheet_name)
first = sheet.first_row
key_mapping = generate_key_mapping sheet.row(first)
data_hash_keys = key_mapping.keys - @keys
row = sheet.row(i)
i = 2
row = sheet.row(i)
data = {}
@keys.each do |key|
  value = key_mapping[key]
  data[key] = fix_values(row[value]) unless value.nil?
end
data_hash_keys.each do |key|
  value = fix_values(row[key_mapping[key]])
  next if value.blank?
  data_hash_category = :General
  data[:data_hash] ||= {}
  data[:data_hash][data_hash_category] ||= {}
  data[:data_hash][data_hash_category][key] = value
end
listing = import_listing scrape, data
l = Listing.last
l.screenshots
if key_mapping[:product_images].present? && row[key_mapping[:product_images]].present?
  img_srcs = clean_urls(row[key_mapping[:product_images]])
  save_images(scrape.id, listing, img_srcs)
end
data[:product_images]
l.reload
l.images
img_srcs = clean_urls(row[key_mapping[:product_images]])
ey_mapping[:product_images]
key_mapping[:product_images]
row[key_mapping[:product_images]]
url_csv = _
urls = url_csv.encode(Encoding.find('ASCII'), ENCODING_OPTIONS).gsub(/\s+/, '').split(/,(?=(https?:|\/\/))/)
urls.map do |url|
  if url.starts_with?('//')
    if url.scan(/(?<!src=)(\/\/)/).size > 1
      url.split(/(?<!src=)(\/\/)/).map { |l| l.gsub(/\/\//, '').gsub(/http:?/, '').strip }.reject(&:blank?).map { |l| "http://#{l}" }
    else
      "http:#{url}"
    end
  else
    if url.scan(/(?<!src=)(https?)?(\/\/)/).size > 1
      url.split(/(?<!src=)(http:?)?\/\//).map(&:strip).reject(&:blank?).map { |l| "http://#{l}" }
    else
      url
    end
  end
end
urls = _
url = urls.first
url.match(/(jpg|gif|jpeg|png)/)
url.match(/newmant/)
urls.flatten.select { |url| url.present? && url =~ /(jpg|gif|jpeg|png)/ }
exit
file_path = '/Users/jonathan/Downloads/clorox_hand_scrape_20161022'
scrape_name = 'clorox Oct 22, 2016'
job = ImportScrapeDataJob.new
cd job
file_path = '/Users/jonathan/Downloads/clorox_hand_scrape_20161022'
scrape_name = 'clorox Oct 22, 2016'
edit -t
xlsx = open_spreadsheet data_file_paths.first
sheet = xlsx.sheet(xlsx.sheets.first)
first = sheet.first_row
key_mapping = generate_key_mapping sheet.row(first)
data_hash_keys = key_mapping.keys - @keys
i = 2
row = sheet.row(i)
data = {}
@keys.each do |key|
  value = key_mapping[key]
  data[key] = fix_values(row[value]) unless value.nil?
end
data
data_hash_keys.each do |key|
  value = fix_values(row[key_mapping[key]])
  next if value.blank?
  data_hash_category = :General
  data[:data_hash] ||= {}
  data[:data_hash][data_hash_category] ||= {}
  data[:data_hash][data_hash_category][key] = value
end
data
listing = import_listing scrape, data
l = Listing.last
l.categories
exit
file_path = '/Users/jonathan/Downloads/clorox_hand_scrape_20161022'
scrape_name = 'clorox Oct 22, 2016'
job = ImportScrapeDataJob.new
cd job
file_path = '/Users/jonathan/Downloads/clorox_hand_scrape_20161022'
scrape_name = 'clorox Oct 22, 2016'
edit -t
xlsx = open_spreadsheet data_file_paths.first
sheet = xlsx.sheet(xlsx.sheets.first)
first = sheet.first_row
key_mapping = generate_key_mapping sheet.row(first)
data_hash_keys = key_mapping.keys - @keys
i = 2
row = sheet.row(i)
data = {}
@keys.each do |key|
  value = key_mapping[key]
  data[key] = fix_values(row[value]) unless value.nil?
end
data_hash_keys.each do |key|
  value = fix_values(row[key_mapping[key]])
  next if value.blank?
  data_hash_category = :General
  data[:data_hash] ||= {}
  data[:data_hash][data_hash_category] ||= {}
  data[:data_hash][data_hash_category][key] = value
end
listing = import_listing scrape, data
exit
cd job
file_path = '/Users/jonathan/Downloads/clorox_hand_scrape_20161022'
scrape_name = 'clorox Oct 22, 2016'
edit -t
job = ImportScrapeDataJob.new
cd job
file_path = '/Users/jonathan/Downloads/clorox_hand_scrape_20161022'
scrape_name = 'clorox Oct 22, 2016'
edit -t
xlsx = open_spreadsheet data_file_paths.first
sheet = xlsx.sheet(xlsx.sheets.first)
edit -t
i = 2
row = sheet.row(i)
data = {}
@keys.each do |key|
  value = key_mapping[key]
  data[key] = fix_values(row[value]) unless value.nil?
end
edit -t
listing = import_listing scrape, data
listing.images
listing.categories
listing.destroy
listing = import_listing scrape, data
reload!
exit
relaod!
exit
job = ImportScrapeDataJob.new
cd job
file_path = '/Users/jonathan/Downloads/clorox_hand_scrape_20161022'
scrape_name = 'clorox Oct 22, 2016'
start_time = DateTime.now
file_hash = Digest::MD5.hexdigest(DateTime.now.to_s)
@source = Source.find_by(file_hash: file_hash)
if @source.nil?
  @source = Source.create name: scrape_name,
  # upload_url: url,
  upload_url: "import/#{DateTime.now.strftime('%Y%m%d%H%M%S')}",
  source_type_id: SourceType::IMPORT,
  file_hash: file_hash
end
tmp_folder_path = file_path
data_file_paths = Dir["#{tmp_folder_path}/*.csv", "#{tmp_folder_path}/*.xlsx", "#{tmp_folder_path}/*.xls"]
fail 'The data file couldn\'t found.' if data_file_paths.empty?
logger.info 'Importing scrape data'
scrape = Scrape.create(
  source: @source,
  name: scrape_name,
  running: true,
  created_at: start_time
)
xlsx = open_spreadsheet data_file_paths.first
sheet = xlsx.sheet(xlsx.sheets.first)
first = sheet.first_row
key_mapping = generate_key_mapping sheet.row(first)
data_hash_keys = key_mapping.keys - @keys
i = 2
row = sheet.row(i)
data = {}
@keys.each do |key|
  value = key_mapping[key]
  data[key] = fix_values(row[value]) unless value.nil?
end
data_hash_keys.each do |key|
  value = fix_values(row[key_mapping[key]])
  next if value.blank?
  data_hash_category = :General
  data[:data_hash] ||= {}
  data[:data_hash][data_hash_category] ||= {}
  data[:data_hash][data_hash_category][key] = value
end
listing = import_listing scrape, data
listing.categories
cat_name = 'Health & Beauty > Lotion'
cats = cat_name.split('>')
cats.map!(&:strip)
if key_mapping[:product_images].present? && row[key_mapping[:product_images]].present?
  img_srcs = clean_urls(row[key_mapping[:product_images]])
  save_images(scrape.id, listing, img_srcs)
end
listing.images
manual_filename = key_mapping[:manual_filename].present? ? row[key_mapping[:manual_filename]] : nil
screenshot_filename = key_mapping[:screenshot_filename].present? ? row[key_mapping[:screenshot_filename]] : nil
save_manuals(listing, tmp_folder_path, manual_filename) unless listing.nil?
save_screenshots(listing, tmp_folder_path, screenshot_filename) unless listing.nil?
listing.name
listing.screenshots.first.attachment.url
exit
Scrape.last.destroy
Scrape.last
Scrape.last.listings.destroy_all
Scrape.last.destroy
Scrape.last
Scrape.last.listings.destroy_all
Scrape.last.destroy
Scrape.last
Scrape.last.destroy
Scrape.last.listings.destroy_all
Scrape.last
Scrape.last.destroy
Scrape.last
Scrape.last.destroy
Scrape.last.listings.destroy_all
Scrape.last
Scrape.last.destroy
Scrape.last
Scrape.last.destroy
Scrape.last.listings.destroy_all
Scrape.last.destroy
Scrape.last
Scrape.last.listings.destroy_all
Scrape.last.destroy
Scrape.last
file_path = '/Users/jonathan/Downloads/clorox_hand_scrape_20161022'
scrape_name = 'clorox Oct 22, 2016'
ImportScrapeDataJob.new.perform(scrape_name, file_path)
s = Scrape.last
s.listings.count
s.listings.sample
s = Scrape.last
s.products.count
s.products
exit
l = s.listings.last
s
s.listings.pluck(:url)
s.listings.last(2).map(&:destroy)
l = s.listings.last
l.as_indexed_json
l.name
xit
exit
Listing.delete_index!
Listing.reindex_alias
Listing.delete_index!
Product.delete_index!
Listing.create_index! force: true
Listing.reindex_alias
Listing.last.id
keywords = '%EB%B2%84%ED%8A%B8%EC%9D%98+%EA%BF%80%EB%B2%8C'
URI.decode(keywords)
'버트의 꿀벌' == _
keywords = "keywords=" << keywords
CGI.parse(keywords)
params = _
params['keywords'].first
keywords = '%EB%B2%84%ED%8A%B8%EC%9D%98+%EA%BF%80%EB%B2%8C'
CGI.parse(keywords)
keywords = '%EB%B2%84%ED%8A%B8%EC%9D%98+%EA%BF%80%EB%B2%8C'
CGI.unescape(keywords)
model = Listing
exit
model = Listing
type = 'listings'
params = Hashie::Mash.new({'keywords': ['    id_proc = if type == 'listings'.freeze
                proc { |l| l['_id'] }
elsif type == 'products'.freeze
proc { |l| l['_source']['products'].map { |p| p['id'] } }
end
keywords = '버트의 꿀벌'
operator = 'AND'
terms = { keywords: params[:keywords], exact: nil }
terms = { keywords: keywords, exact: nil }
Listing.query_fields[:filter_fields].each do |key, _v|
  if params[key].present?
    params[key].downcase!
    filters[key] = params[key].split(',').map! do |val|
      val.strip!
      val
    end
  end
end
edit
params = {}
Listing.query_fields[:filter_fields].each do |key, _v|
  if params[key].present?
    params[key].downcase!
    filters[key] = params[key].split(',').map! do |val|
      val.strip!
      val
    end
  end
end
terms
operators
operator
filters
filters = {}
Listing.query_fields[:filter_fields].each do |key, _v|
  if params[key].present?
    params[key].downcase!
    filters[key] = params[key].split(',').map! do |val|
      val.strip!
      val
    end
  end
end
filters
params
terms
type
per_page = 200
fields = Listing.query_fields
definition = Listing.generate_query_by_keywords(fields, terms, operator, filters, type)
response = Listing.search(definition, {})
response.page(1)
response.page(1).results
response.page(1).results.total
s = Scrape.last
s.listings.where("data_hash like '%버트의 꿀벌%'")
s.listings.where("name like '%버트의 꿀벌%'")
s.listings.pluck(:name)
keywords
s.listings.pluck(:url)
xit
sexit
exit
ImportScrapeDataJob.new.perform(scrape_name, file_path)
scrape_name = 'test'
file_path = '/Users/jonathan/Downloads/clorox_hand_scrape_20161022
'
ImportScrapeDataJob.new.perform(scrape_name, file_path)
file_path = '/Users/jonathan/Downloads/clorox_hand_scrape_20161022'
ImportScrapeDataJob.new.perform(scrape_name, file_path)
s = Scrape.last
s.listings.count
file_path = '/Users/jonathan/Downloads/clorox_hand_scrape_20161022'
s.listings.count
exit
exit
Scrape.last
s = _
s.listings.count
ScrapeListingSeller.includes(:currency).where(scrape_id: 11).each do |sls|
  currency_code = sls.currency.iso_code
  min, max = [sls.min_price, sls.max_price].minmax
  exchanged_min = Monetize.parse("#{sls.currency.iso_code} #{min}").exchange_to(:USD)
  exchanged_max = Monetize.parse("#{sls.currency.iso_code} #{max}").exchange_to(:USD)
  sls.update(exchanged_min: exchanged_min, exchanged_max: exchanged_max)
end
sls = ScrapeListingSeller.find(1108)
sls = ScrapeListingSeller.find(1109)
s.scrape_listing_seller.where(currency: nil)
s.scrape_listing_sellers.where(currency: nil)
currency_code = sls.try(:currency).try(:iso_code)
min, max = [sls.min_price, sls.max_price].minmax
exchanged_min = Monetize.parse("#{sls.currency.iso_code} #{min}").exchange_to(:USD)
exchanged_max = Monetize.parse("#{sls.currency.iso_code} #{max}").exchange_to(:USD)
sls.update(exchanged_min: exchanged_min, exchanged_max: exchanged_max)
ScrapeListingSeller.includes(:currency).where(scrape_id: 11).each do |sls|
  currency_code = sls.try(:currency).try(:iso_code)
  min, max = [sls.min_price, sls.max_price].minmax
  exchanged_min = Monetize.parse("#{sls.currency.iso_code} #{min}").exchange_to(:USD)
  exchanged_max = Monetize.parse("#{sls.currency.iso_code} #{max}").exchange_to(:USD)
  sls.update(exchanged_min: exchanged_min, exchanged_max: exchanged_max)
end
ScrapeListingSeller.includes(:currency).where(scrape_id: 11).each do |sls|
  currency_code = sls.try(:currency).try(:iso_code)
  min, max = [sls.min_price, sls.max_price].minmax
  exchanged_min = Monetize.parse("#{currency_code} #{min}").exchange_to(:USD)
  exchanged_max = Monetize.parse("#{currency_code} #{max}").exchange_to(:USD)
  sls.update(exchanged_min: exchanged_min, exchanged_max: exchanged_max)
end
exit
s = Scrape.last
s.listings.count
s.listings.first
reload!
source = Source.find('taobao'); soucre.source_categories.where(test_process: true)
source = Source.find_by(name: 'taobao'); soucre.source_categories.where(test_process: true)
source = Source.find_by(name: 'taobao'); source.source_categories.where(test_process: true)
sc = _.first
sc.url
URI.encode(sc.url)
source.source_categories.each do |sc|
  sc.update(url: URI.encode(sc.url))
end
source.source_categories.each do |sc|
  sc.update(url: URI.encode(sc.url)) unless sc.url.blank?
end
s = Scrape.last
s.listings.count
l = s.listings.first
l.images
l.images.map {|i| i.attachment.url }
l.reload
l.images.map {|i| i.attachment.url }
s.listings.count
s.products.count
s.products.first
p = _
p.listings
l = _
l.images
l = l.first
l.images
l.images.map {|i| i.attachment.url }
l.reload
l.images.map {|i| i.attachment.url }
l.reload
l.images.map {|i| i.attachment.url }
l.data_hash
s.listings.count
s.products.count
s.listings
l = s.listings.first
l.prodcuts
l.products
DateTime.now.to_i
img = Asset.find 2261
l = s.listings.first
l.sellers
ScrapeListingSeller.last
sls = _
sls.seller
l
l.sellers
l.scrape_listing_sellers
l.url
s.reload
s.listings.count
s.scrape_listing_sellers.where(seller: nil).count
l = Listing.first
l.sellers
l = s.listings.first
l.sellers
s.listings
s.listings.where(url: 'https://detail.tmall.com/item.htm?id=44527077350')
l = _.first
l.sellers
l.scrape_listing_sellers
l_ids = s.scrape_listing_sellers.pluck(:listing_id)
l_ids.uniq
l_ids.count
s.listings.where.not(id: l_ids).count
s.listings.where.not(id: l_ids).pluck(:url)
s = Scrape.last
scrape_id = s.id
UpdateScrapeCounterJob.perform_async scrape_id
scrape = Scrape.find(scrape_id)
scrape.listings.include_indexed_fields.find_in_batches(batch_size: 350) do |records|
  Listing.bulk_index(records)
end
UpdateScrapeDurationJob.perform_async scrape_id, restart
UpdateScrapeDurationJob.perform_async scrape_id
Listing.reindex_alias
l = Listing.find_by(url: 'https://world.taobao.com/item/530470070337.htm')
s.listings.count
exit
source = Source.find_by(name: 'taobao')
source.source_categories.active
source.source_categories.active.sample
source.source_categories.map(&:category_name)
source.source_categories.includes(:category).map(&:category_name)
source.source_categories.includes(:category).pluck(:id, :category_name)
source.source_categories.includes(:category).pluck(:id, :name)
source.source_categories.includes(:category).map{|sc| [sc.id, sc.category_name]}
SourceCategory.find(1707)
urls = ['https://world.taobao.com/search/search.htm?cat=11&_ksTS=1477343624410_450&spm=a21bp.7806943.banner_XX_cat.724.l9ruV6&json=on&cna=1e1dEB%20XZFACARgFRqLkyLQo&ppath=413%3A1001889&_input_charset=utf-8&s=0&navigator=all&callback=__jsonp_cb&abtest=_AB-LR517-LR854-LR895-PR517-PR854-PR895',
'https://world.taobao.com/search/search.htm?cat=11&_ksTS=1472956989812_996&spm=a21bp.7806943.banner_XX_cat.724.bW8CDc&json=on&cna=8fxNEF3dpVYCARgFRqKIxD76&ppath=413%25253A1001889&_input_charset=utf-8&s=0&navigator=all&callback=__jsonp_cb&abtest=_AB-LR517-LR854-LR895-PR517-PR854-PR895']
url = urls.first
uri = URI.parse(url)
uri.query
CGI.parse(_
)
urls.map{|url| CGI.parse(URI.parse(url).path) }
urls.map{|url| CGI.parse(URI.parse(url).query) }
source = Source.find_by(name: 'taobao')
source.source_categories.includes(:category).map{|sc| [sc.id, sc.category_name]}
SourceCategory.where(id: 2218, 2219)
SourceCategory.where(id: [2218, 2219])
urls = SourceCategory.where(id: [2218, 2219]).pluck(:url)
urls.map{|url| CGI.parse(URI.parse(url).query) }
urls
url = urls.last
URI.decode(url)
urls.first
SourceCategory.destroy_all
urls.map{|url| CGI.parse(URI.parse(url).query) }
source.source_categories.includes(:category).map{|sc| [sc.id, sc.category_name]}
SourceCategory.find(171)
s = Scrape.last
s.listings.count
s.reload
s.listings.count
s = Scrape.last
s.listings.count
s.listings.first
l = _
l.images.count
exit
s = Scrape.last
s.listings.first
s.listings.last
Listing.find_by(original_url: 'https://item.taobao.com/item.htm?id=539670033632')
Listing.count
Listing.destroy_all
Scrape.destroy_all
exit
Redis.current.flushall
price = '400.00 - 590.00'
price.match(/-/)
min_max = price.split('-')
min_max.map! { |p| p.remove(/[^\d\.]/) }
prices = []
prices.concat(min_max)
prices
prices.delete_if(&:blank?)
min, max = prices.minmax
s = Scrape.alst
s = Scrape.last
s.listings.count
exit
s = Scrape.last
s.listings.count
ScrapeListingSeller.count
ScrapeListingSeller.map(&:seller_name)
s_ids = ScrapeListingSeller.pluck(:seller_id)
ScrapeListingSeller.pluck(:exchanged_min)
ScrapeListingSeller.pluck(:exchanged_min).map(&:exchange_to, :USD)
ScrapeListingSeller.pluck(:exchanged_min).map{|m} m.exchange_to(:USD) }
ScrapeListingSeller.pluck(:exchanged_min).map{|m| m.exchange_to(:USD) }
ScrapeListingSeller.pluck(:exchanged_min).map(&:to_f)
s = Scrape.last
s.listings.count
s.products.count
Time.now - s.created_at
(Time.now - s.created_at).to_minutes
(Time.now - s.created_at).to_min
(Time.now - s.created_at) / 60
s.listings.count
45 / 29
45 / 29.to_f
s.listings.count
exit
s = Scrape.last
s.listings.count
Seller.pluck(:name).count
Seller.pluck(:name).uniq.count
ScrapeListingSeller.count
ScrapeListingSeller.group(:seller)
ScrapeListingSeller.group(:seller_id)
ScrapeListingSeller.count(:seller_id)
ScrapeListingSeller.group_by(:seller_id)
ScrapeListingSeller.group_by(&:seller_id)
ScrapeListingSeller.all.group_by(&:seller_id)
s.listings.sample
l = _
l.images.count
l.images.map {|i| i.attachment.url }
exit
CREDENTIALS_FILE = Rails.root.join('google_api_credentials.json')
credentials_storage = ::Google::APIClient::FileStorage.new(CREDENTIALS_FILE)
require 'google/api_client'
require 'google/api_client/auth/file_storage'
require 'google/api_client/auth/installed_app'
require 'google/apis/drive_v3'
require 'googleauth'
Dir.home
OOB_URI = 'urn:ietf:wg:oauth:2.0:oob'
exit
require 'google/apis/drive_v3'
require 'googleauth'
require 'googleauth/stores/file_token_store'
require 'fileutils'
OOB_URI = 'urn:ietf:wg:oauth:2.0:oob'
APPLICATION_NAME = 'Drive API'
CLIENT_SECRETS_PATH = 'google_api_credentials.json'
CREDENTIALS_PATH = File.join(Dir.home, '.credentials', 'drive.yaml')
SCOPE = Google::Apis::DriveV3::AUTH_DRIVE_METADATA_READONLY
FileUtils.mkdir_p(File.dirname(CREDENTIALS_PATH))
client_id = Google::Auth::ClientId.from_file(CLIENT_SECRETS_PATH)
token_store = Google::Auth::Stores::FileTokenStore.new(file: CREDENTIALS_PATH)
authorizer = Google::Auth::UserAuthorizer.new(client_id, SCOPE, token_store)
user_id = 'default'
credentials = authorizer.get_credentials(user_id)
url = authorizer.get_authorization_url(
base_url: OOB_URI)
code = '4/QXwuDf-D84sqx6BfDSFO5kHxYfabzQch7KAPt4Z3IBY'
credentials = authorizer.get_and_store_credentials_from_code(
user_id: user_id, code: code, base_url: OOB_URI)
credentials
service = Google::Apis::DriveV3::DriveService.new
service.client_options.application_name = APPLICATION_NAME
service.authorization = credentials || authorize
response = service.list_files(page_size: 10,
fields: 'nextPageToken, files(id, name)')
file_id = "1FDKGOmcOZzO-mvliBnOC9UK7lH9of4shEyYKn_gAoKw"
download_dest = Rails.root.join('db', 'seeds', 'rds_categories.rb')
file = service.get_file(file_id, download_dest: download_dest)
file = service.get_file(file_id)
file.class
file.open?
ProduPPexitxei
exit
Product.count.to_sql
require 'google/apis/drive_v3'
require 'googleauth'
require 'googleauth/stores/file_token_store'
require 'fileutils'
OOB_URI = 'urn:ietf:wg:oauth:2.0:oob'
APPLICATION_NAME = 'Drive API'
CLIENT_SECRETS_PATH = Rails.root.join('google_api_credentials.json')
CREDENTIALS_PATH = File.join(Dir.home, '.credentials', 'drive.yaml')
SCOPE = Google::Apis::DriveV3::AUTH_DRIVE_METADATA_READONLY
FileUtils.mkdir_p(File.dirname(CREDENTIALS_PATH))
client_id = Google::Auth::ClientId.from_file(CLIENT_SECRETS_PATH)
token_store = Google::Auth::Stores::FileTokenStore.new(file: CREDENTIALS_PATH)
authorizer = Google::Auth::UserAuthorizer.new(client_id, SCOPE, token_store)
user_id = 'default'
credentials = authorizer.get_credentials(user_id)
gets
credentials
service = Google::Apis::DriveV3::DriveService.new
service.client_options.application_name = APPLICATION_NAME
service.authorization = credentials || authorize
file_id = "1FDKGOmcOZzO-mvliBnOC9UK7lH9of4shEyYKn_gAoKw"
response = service.list_files(page_size: 10,
fields: 'nextPageToken, files(id, name)')
file_id
response.files.count
reponse.files.first
reponse.files.each do |f|
  puts f.class
end
response.files.first
response.files.first.class
file = response.files.first
file.to_h
EXCEL_MIME = 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
file = service.export_file(file_id, EXCEL_MIME)
OOB_URI = 'urn:ietf:wg:oauth:2.0:oob'
APPLICATION_NAME = 'Drive API'
CLIENT_SECRETS_PATH = Rails.root.join('google_api_credentials.json'
  CREDENTIALS_PATH = File.join(Dir.home, '.credentials', 'drive.yaml')
SCOPE = Google::Apis::DriveV3::AUTH_DRIVE_FILE
service.authorization = authorize
def authorize
  FileUtils.mkdir_p(File.dirname(CREDENTIALS_PATH))
  client_id = Google::Auth::ClientId.from_file(CLIENT_SECRETS_PATH)
  token_store = Google::Auth::Stores::FileTokenStore.new(file: CREDENTIALS_PATH)
  authorizer = Google::Auth::UserAuthorizer.new(client_id, SCOPE, token_store)
  user_id = 'default'
  credentials = authorizer.get_credentials(user_id)
  if credentials.nil?
    url = authorizer.get_authorization_url(
    base_url: OOB_URI)
    puts "Open the following URL in the browser and enter the " +
    "resulting code after authorization"
    puts url
    code = gets
    code = '4/QXwuDf-D84sqx6BfDSFO5kHxYfabzQch7KAPt4Z3IBY'
    credentials = authorizer.get_and_store_credentials_from_code(
    user_id: user_id, code: code, base_url: OOB_URI)
  end
  credentials
end
service.authorization = authorize
def authorize
  FileUtils.mkdir_p(File.dirname(CREDENTIALS_PATH))
  client_id = Google::Auth::ClientId.from_file(CLIENT_SECRETS_PATH)
  token_store = Google::Auth::Stores::FileTokenStore.new(file: CREDENTIALS_PATH)
  authorizer = Google::Auth::UserAuthorizer.new(client_id, SCOPE, token_store)
  user_id = 'default'
  credentials = authorizer.get_credentials(user_id)
  if credentials.nil?
    url = authorizer.get_authorization_url(
    base_url: OOB_URI)
    puts "Open the following URL in the browser and enter the " +
    "resulting code after authorization"
    puts url
    code = gets
    # code = '4/QXwuDf-D84sqx6BfDSFO5kHxYfabzQch7KAPt4Z3IBY'
    credentials = authorizer.get_and_store_credentials_from_code(
    user_id: user_id, code: code, base_url: OOB_URI)
  end
  credentials
end
service.authorization = authorize
file = service.export_file(file_id, EXCEL_MIME)
response = service.list_files(page_size: 10,
fields: 'nextPageToken, files(id, name)')
Google::Apis::DriveV3::AUTH_DRIVE
SCOPE = Google::Apis::DriveV3::AUTH_DRIVE_READONLY
service.authorization = authorize
response = service.list_files(page_size: 10,
fields: 'nextPageToken, files(id, name)')
file_id
file = service.get_file(file_id, StringIO.new)
file = service.get_file(file_id, download_dest: StringIO.new)
file = service.get_file(file_id)
file.viewers_can_copy_content?
file.viewers_can_copy_content = true
file
DateTime.new.strftime('%Y')
DateTime.new.strftime('Y')
DateTime.new.strftime('%Y')
DateTime.new.strftime('%YY')
DateTime.new.strftime('%Y%m%d%S%L')
DateTime.now.strftime('%Y%m%d%S%L')
metadata = Drive::File.new(title: "rds_categories_#{DateTime.now.strftime('%Y%m%d%S%L')}")
Google::Apis::DriveV3::File.new(title: "rds_categories_#{DateTime.now.strftime('%Y%m%d%S%L')}")
EXCEL_MIME = 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
metadata = service.insert_file(metadata, upload_source: "rds_categories_#{DateTime.now.strftime('%Y%m%d%S%L')}", content_type: EXCEL_MIME)
metadata = service.create_file(metadata, upload_source: "rds_categories_#{DateTime.now.strftime('%Y%m%d%S%L')}", content_type: EXCEL_MIME)
metadata = service.create_file(metadata, upload_source: nil, content_type: EXCEL_MIME)
SCOPE = Google::Apis::DriveV3::AUTH_DRIVE_FILE
service.authorization = authorize
metadata
metadata = Drive::File.new(title: "rds_categories_#{DateTime.now.strftime('%Y%m%d%S%L')}")
metadata = Google::Apis::DriveV3::File.new(title: "rds_categories_#{DateTime.now.strftime('%Y%m%d%S%L')}")
metadata = service.create_file(metadata, content_type: EXCEL_MIME)
metadata
service.create_file(metadata)
metadata
service.delete_file('0B8EfLUwqdS5qQUEwbHp1STBNOFU')
metadata = Google::Apis::DriveV3::File.new(title: "rds_categories_#{DateTime.now.strftime('%Y%m%d%S%L')}")
metadata
metadata = Google::Apis::DriveV3::File.new(
  {
  original_filename: "rds_categories_#{DateTime.now.strftime('%Y%m%d%S%L')}",
  mime_type: 'application/vnd.ms-excel',
  file_extension: 'xlsx'
  }
)
metadata
service.create_file(metadata)
service.insert_file(metadata, upload_source: Rails.root.join('db', 'seeds', 'rds_categories.xlsx'))
service.create_file(metadata, upload_source: Rails.root.join('db', 'seeds', 'rds_categories.xlsx'))
Rails.root.join('db', 'seeds', 'rds_categories.xlsx')
path = Rails.root.join('db', 'seeds', 'rds_categories.xlsx').to_s
service.create_file(metadata, upload_source: path)
service.create_file(upload_source: path)
file = _
file.class
file.update!(name: 'rds_categories.xlsx')
file.id
file.name
file.update!
path = Rails.root.join('db', 'seeds', 'rds_categories.xlsx')
service.delete_file(file.id)
filename = "rds_categories_#{DateTime.now.strftime('%Y%m%d%S%L')}"
path = Rails.root.join('db', 'seeds', 'rds_categories.xlsx').to_s
metadata = Google::Apis::DriveV3::File.new(
  {
    name: filename
  }
)
file = service.create_file(metadata, upload_source: path)
service.delete_file(file.id)
metadata = Google::Apis::DriveV3::File.new(
  {
    mime_type: 'application/vnd.google-apps.spreadsheet',
    name: filename
  }
)
file = service.create_file(metadata)
eixt
exit
require "google_drive"
session = GoogleDrive::Session
session.from_credentials('google_api_credentials.json')
session.files
session = GoogleDrive::Session.from_config('google_api_credentials.json')
session.files
file = session.file_by_title('RDS Categories')
file.rows
file.worksheets
ws = file.worksheet_by_title('gsus')
ws.rows.first
ws.rows[0]
ws.rows[1..-1].first
path = File.join(Rails.root, 'db', 'seeds', 'rds_categories.xlsx')
File.extname(file_path)
File.extname(path)
exit
require 'hexapdf'
exit
HexaPDF::Document
path = '/Users/jonathan/Desktop/Verizon Wireless _ Order Confirmation.pdf'
pdf = HexaPDF::Document.open(path)
pdf.class
pdf.pages
page = pdf.pages.first
page = pdf.pages.page(0)
page.contents
text = HexaPDF::Content::Parser.parse(page.contents)
page.contents.class
processor = HexaPDF::Content::Processor.new
text = HexaPDF::Content::Parser.parse(page.contents, processor)
processor.decode_text(page)
processor.decode_text(page.contents)
processor.calss
processor.class
pdf.show_text
eixt
exit
bulk_cats = %w(
Consumer Electronics > Audio
Consumer Electronics > Computers
Consumer Electronics > Consumer Electronics > ConsicConsumer Electronics > ConstrConsumer Electronics > ConsElConsumer Electronics > ConsmeConsumer Electronics > ConsonConsumer Electronics > ConsrsConsumer Electronics > ConspuConsumer Electronics > Cons CConsumer Electronics > ConscsConsumer Electronics > ConsroConsumer Electronics > ConsleConsumer Electronics > ConserConsumer Electronics > ConsnsConsumer Electronics > Conss
Consumer Electronics > ConsutConsumer Electronics > ConsCoConsumer Electronics > Conss Consumer Electronics > ConsonConsumer Electronics > ConsecConsumer Electronics > Consr Consumer Electronics > ConssuConsumer Electronics > Cons
CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC
CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCnsCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCumCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCr CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCleCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCtrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCniCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCs CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCmeCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCnsCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCumCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCr CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCleCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCtrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCniCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCs CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCmeCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCnsCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCumCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCr CCCCCCCCCCCCCCCCCCCCCCCCCCCCCumCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCnsCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCumCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCr CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCleCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCtrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCniCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCleCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCoCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCs er Electronics > Audio
Consumer Electronics > Audio
Consumer Electronics > Activity Monitors
Consumer Electronics > Solar
Consumer Electronics > Communications
Consumer Electronics > Television
Consumer Electronics > Lighting
Consumer Electronics > Televisions
Consumer Electronics > TelevConsumer Electronics roConsumer Electronics > Tele Electronics >Consumer Electronics > TelevConsumer ElevConsumer Electronics > TelevConsumer ElnsConsumer Electronics > TelevConsumer Elsumer Consumer Electronics > TeleonConsumer Electronics > TeleioConsumer Electronics > TeleeleConsumer Electronics > TelevConsumerisConsumer Electronics > TelevConsumeronConsumer Electronics > TelevConsumermeConsumer Electronics > TelevConsumerroConsumer Electronics > TelevConsumerecConsumer Electronics > TelevConsumer EConsumer Electronics > TelevConsumermeConsumer Electronics > TelevConsumernsConsumer Electronics > TelevConsumer
edit
edit -t
bulk_cats.count
bulk_cats.uniq.count
bulk_cats.uniq!
bulk_cats.sort
bulk_cats.sort!
exit
CLIENT_SECRETS_PATH = Rails.root.join('google_api_credentials.json'
CLIENT_SECRETS_PATH = Rails.root.join('google_api_credentials.json')
require "google_drive"
session = GoogleDrive::Session.from_config(CLIENT_SECRETS_PATH)
session = GoogleDrive::Session.from_config(CLIENT_SECRETS_PATH.to_s)
spreadsheet = session.file_by_title('RDS Categories')
spreadsheet.worksheets
spreadsheet.worksheet_by_title('gsus')
ws = spreadsheet.worksheet_by_title('gsus')
headers = worksheet.rows[0]
worksheet = ws
headers = worksheet.rows[0]
headers = worksheet.rows[1]
worksheet.rows[1..-1].last
rows = worksheet.rows[1..-1].select {|r| r.first.present? }
rows.last
SourceCategory
SourceCategory.connection
SourceCategory
migration = ActiveRecord::Migration.new
migration = ActiveRecord::Migration
exit
Rails.root.join('google_api_credentials.json')
path = _
path.class
path.realpath
path.display
path.to_path
require "google_drive"
CLIENT_SECRETS_PATH = Rails.root.join('google_api_credentials.json')
session = GoogleDrive::Session.from_config(CLIENT_SECRETS_PATH.to_s)
spreadsheet = session.file_by_title('RDS Categories')
bcs = spreadsheet.worksheet_by_title('Bulk Categories')
bcs.rows[1..-1]
bulk_categories = bcs.rows[1..-1].map { |row| row[0] }
bulk_categories
bulk_categories.map {|bc| bc.split('>') }
bulk_categories.map {|bc| bc.bulk_categories.map! {|bc| bc.split('>') }split('>') }
bulk_categories.map! {|bc| bc.split('>') }
bulk_categories
bulk_categories.flatten!
bulk_categories
bulk_categories.uniq!
bulk_categories.map!(&:squish!)
bulk_categories
bulk_categories = bcs.rows[1..-1].map { |row| row[0] }
bc = bulk_categories[1]
name = bc.split('>').first
name.squish!
bulk_categories = bcs.rows[1..-1].map { |row| row[0] }
bulk_categories.map! {|bc| bc.split('>') }
bulk_categories.flatten!
bulk_categories.uniq!
bulk_categories.map!(&:squish!)
bulk_categories
bulk_categories.each do |name|
  BulkCategory.find_or_create_by(name: name)
end
BulkCategory.count
source_names = Source.where(source_type_id: 1).pluck(:name)
name = 'gsimg'
source_name = _
worksheet = spreadsheet.worksheet_by_title(source_name)
spreadsheet.worksheets
spreadsheet.
spreadsheet.list
spreadsheet.worksheets.map(&:name)
spreadsheet.worksheets.map(&:title)
sheet_names = spreadsheet.worksheets.map(&:title)
source_names = Source.where(source_type_id: 1).pluck(:name)
sheet_names.include?('gsimg')
source_name = 'gsus'
worksheet = spreadsheet.worksheet_by_title(source_name)
headers = worksheet.rows[0]
rows = worksheet.rows.select {|r| r[0].present? }
headers = rows[0]
headers = rows[0].each_with_index
headers = rows[0]
headers.map {|h| h.camelize }
headers.map {|h| h.capitalize.split(' ').join('').camelize }
headers.map {|h| h.split(' ').join('').camelize }
headers.map {|h| h.split(' ').join('').underscoreize }
headers.map {|h| h.split(' ').join('').underscore }
headers.map! {|h| h.split(' ').join('').underscore }
headers = headers.map {|h| h.split(' ').join('').underscore }
headers.index_of('bulk_category')
headers.index('bulk_category')
mapping = {}
headers.each_with_index {|h, idx| mapping[h] = idx }
mapping
headers = {}
rows[0].each_with_index {|h, idx| headers[h] = idx }
headers
rows[0].map {|h| h.split(' ').join('').underscore }.each_with_index {|h, idx| headers[h] = idx }}
rows[0].map {|h| h.split(' ').join('').underscore }.each_with_index {|h, idx| headers[h] = idx }
headers
headers = {}
rows[0].map {|h| h.split(' ').join('').underscore }.each_with_index {|h, idx| headers[h] = idx }
headers
row = rows[1]
bulk_category = row[headers['bulk_category']]
bcs = spreadsheet.worksheet_by_title('Bulk Categories')
bulk_categories = bcs.rows[1..-1].map { |row| row[0] }
bulk_names = bulk_categories.map {|bc| bc.split('>') }
bulk_names.flatten!
bulk_names.uniq!
bulk_names.map!(&:squish!)
bulk_names.each do |name|
  BulkCategory.find_or_create_by(name: name)
end
bulk_categor
bulk_categories
"Invalid Bulk Category: #{bulk_category} -- Sheet: #{source_name}"
raise "Invalid Bulk Category: '#{bulk_category}' -- Sheet: #{source_name}"
raise "Invalid Bulk Category: '#{bulk_category}' -- Sheet: #{source_name}" unless bulk_categories.include?(bulk_category)
columns = rows.first.reject { |cell| cell.blank? }.map { |col| col.downcase.gsub(/\s+/, '_') }
bulk_category = row[0]
raise "Invalid Bulk Category: '#{bulk_category}' -- Sheet: #{source_name}" unless bulk_categories.include?(bulk_category)
info = {}
headers.each_with_index do |header, idx|
  info[header.to_sym] = row[idx]
  categories << info
headers.each_with_index do |header, idx|
  info[header.to_sym] = row[idx]
end
categories << info
categories = []
headers.each_with_index do |header, idx|
  info[header.to_sym] = row[idx]
end
categories << info
categories
headers.each_with_index do |header, idx|
headers
headers = rows.first.reject { |cell| cell.blank? }.map { |col| col.downcase.gsub(/\s+/, '_') }
headers
headers.each_with_index do |header, idx|
  info[header.to_sym] = row[idx]
end
categories << info
categories
categories = []
rows[1..-1].each do |row|
  # check bulk category
  bulk_category = row[0]
  raise "Invalid Bulk Category: '#{bulk_category}' -- Sheet: #{source_name}" unless bulk_categories.include?(bulk_category)
  info = {}
  headers.each_with_index do |header, idx|
    info[header.to_sym] = row[idx]
  end
  categories << info
end
categories
info
sheet_names = spreadsheet.worksheets.map(&:title)
attrs = ['bulk_category', 'active', 'source_category', 'url']
source_names = Source.where(source_type_id: 1).pluck(:name)
sources = {}
source_names.each do |source_name|
  next unless sheet_names.include?(source_name)
  worksheet = spreadsheet.worksheet_by_title(source_name)
  rows = worksheet.rows.select { |r| r[0].present? }
  headers = rows.first.reject { |cell| cell.blank? }.map { |col| col.downcase.gsub(/\s+/, '_') }
  categories = []
  rows[1..-1].each do |row|
    # check bulk category
    bulk_category = row[0]
    raise "Invalid Bulk Category: '#{bulk_category}' -- Sheet: #{source_name}" unless bulk_categories.include?(bulk_category)
    info = {}
    headers.each_with_index do |header, idx|
      next unless attrs.include?(header)
      info[header.to_sym] = row[idx]
    end
    categories << info
  end
  sources[source_name.to_sym] = categories
end
edit
categories
sources
edit -t
exit
ProductImage.where.not(url: nil).first
config = Hashie::Mash.new({
    # "refresh_token": "1/NPXT85psYqaIwrYEygkYw6gdF75iundbuXPmbwuznoI"
    "client_id": ENV['GOOGLE_CLIENT_ID'],
    "client_secret": ENV['GOOGLE_CLIENT_SECRET'],
    "scope": [
      "https://www.googleapis.com/auth/drive",
      "https://spreadsheets.google.com/feeds/"
    ],
})
config
exit
edit -t
config
session = GoogleDrive::Session.from_config(config)
session.file_by_title('RDS Categories')
exit
path = '/Users/jonathan/Downloads/rvx-rds-099f64ad9c5f.json
path = '/Users/jonathan/Downloads/rvx-rds-099f64ad9c5f.json'
session = GoogleDrive::Session.from_service_account_key(path)
session.files
session.file_by_title('RDS Categories')
path
path = '/Users/jonathan/Downloads/rvx-rds-e59849269d94.json'
session = GoogleDrive::Session.from_service_account_key(path)
session.files
exit
edit -t
session = GoogleDrive::Session.from_service_account_key(key_config)
key_config
key_config.as_json
key_config.to_s
session = GoogleDrive::Session.from_service_account_key(key_config.to_s)
StringIO.new(key_config.to_s)
io = _
session = GoogleDrive::Session.from_service_account_key(io)
key_config.to_s
key_config.to_json
io.open?
io
io.close
io = StringIO.new(key_config.to_json)
session = GoogleDrive::Session.from_service_account_key(io)
session.files
io.closed?
io.close
exit
edit -t
session = get_session
session.files
spreadsheet = session.file_by_title('RDS Categories')
bcs = spreadsheet.worksheet_by_title('Bulk Categories')
bcs.rows[1]
exit
bulk_categories = BulkCategory.where(name: [:bulk_category].split('>').map(&:strip))
ProductImage.select(:id).find_in_batches do |images|
  images.each_slice(100).map(&:id).minmax
end
c = {:bulk_category=>"Consumer Electronics > Audio", :active=>"TRUE", :source_category=>"Electronics > Audio > Audio Components > Speakers", :url=>"https://www.google.com/webhp?hl=en#q=audio+speakers&hl=en&tbm=shop"}
cats = c[:source_category].split('>').map(&:strip)
bulk_categories = BulkCategory.where(name: c[:bulk_category].split('>').map(&:strip))
SourceCategory.where(url: c[:url])
sc = SourceCategory.where(url: c[:url]).first
bc = bulk_categories.first
bc.source_categories << sc
BulkCategorySourceCategory.first
bc.source_categories << sc
bc.source_categories << sc rescue ActiveRecord::RecordNotUnique
rake db:seed
exit
SourceCategory
SourceCategory.last
SourceCategory.first
SourceCategory.active
BulkCategory.first
bc = _
bc.source_categories
BulkCategory.last
bc = BulkCategory.last
bc.source_categories
exit
bc = BulkCategory.last
BulkCategory.second
bc = BulkCategory.second
bc.source_categories
load File.join(Rails.root, 'db', 'seeds', "categories.rb")
@source_categories
@source_categories.first
source, categories = @source_categories.first
source
categories
source = Source.find_by(name: source.to_s)
new_ids, old_ids = [], SourceCategory.where(source_id: source).ids
c = categories.first
cats = c[:source_category].split('>').map(&:strip)
bulk_categories = BulkCategory.where(name: c[:bulk_category].split('>').map(&:strip))
c[:url].split('\,').map(&:strip).each do |url|
url = c[:url].split('\,').map(&:strip).first
parent = nil
cats
cat_name = cats.first
idx = 0
category = Category.find_or_create_by name: cat_name
parent = SourceCategory.find_or_create_by(
  source: source,
  category: category,
  parent_id: parent.try(:id)
)
bc = bulk_categories.first
bc << parent
exit
bc = BulkCategory.sample
bc = BulkCategory.all.sample
bc.source_categories
bc = BulkCategory.all.sample
bc.source_categories
BulkCategory.count
BulkCategory.all
bc = BulkCategory.second
bc.source_categories
BulkCategorySourceCategory.count
bc = BulkCategory.first
SourceCategory.where(category: Category.find_by(name: "Blank Media"))
sc = SourceCategory.where(category: Category.find_by(name: "Blank Media")).first
bc
bc.source_categories
bc.source_categories << sc
exit
BulkCategorySourceCategory.count
SourceCategory.count
bc = BulkCategory.first
bc.source_categories
BulkCategory.all.each do |bc|
  puts "#{bc.name} - #{bc.source_categories.map(&:url)}"
end
puts "#{bc.name} - #{bc.source_categories.count}"
BulkCategory.all.each do |bc|
  puts "#{bc.name} - #{bc.source_categories.count}"
end
bc
parent
parent = bc.source_categories.first
BulkCategorySourceCategory.find_or_create_by(bulk_category_id: bc.id, source_category_id: parent.id)
exit
SourceCategory.delete_all
Category.delete_all
Category.destroy_all
exit
Source.count
exit
SourceCategory.count
SourceCategory.all
SourceCategory.count
BulkCategory.count
reload!
bc = BulkCategory.all.sample
bc.source_categories
bc.source_categories.count
bc.categories.count
bc.categories
bc.categories.count
BulkCategory.first
bc = _
bc.categories
bc.source_categories
bc = BulkCategory.last
bc = BulkCategory.last(2).first
bc.categories
bc.categories.count
bc.source_categories.count
bc.source_categories
bc.source_categories.where(url: nil)
bc.source_categories.where.not(url: nil)
sc = _.first
child = sc
def long_name(source_cat)
  unless source_cat.parent.nil?
    "#{long_name(source_cat.parent)} > #{source_cat.name}"
  else
    "#{source_cat.name}"
  end
end
long_name(child)
child.name
child.category.name
SourceCategory
reload!
child.reload
child.name
exit
child.category_name
bc.source_categories.where.not(url: nil)
SourceCategory.where.not(url: nil)
child = _.first
child.category_name
sc = SourceCategory.where.not(url: nil).last
sc.category_name
def long_name(source_cat)
  unless source_cat.parent.nil?
    "#{long_name(source_cat.parent)} > #{source_cat.name}"
  else
    "#{source_cat.name}"
  end
end
long_name(sc)
def long_name(source_cat)
  unless source_cat.parent.nil?
    "#{long_name(source_cat.parent)} > #{source_cat.category_name}"
  else
    "#{source_cat.category_name}"
  end
end
long_name(sc)
sc.source
sc.bulk_categories
sc.bulk_categories.pluck(:name).join(' > ')
sc.active
sc.active.to_s
long_name.sc
long_name(sc)
sc.url
categories = {}
sources = Source.where(source_type_id: 1)
Source.first
s = _
s.source_categor
s.source_categories
sources = Source.where(source_type_id: 1)
sources = Source.where(source_type_id: 1).where.not(name: 'gsimg')
edit -t
categories
categories['gsus']
sc = SourceCategory.first
gsus = Source.find_by(name: 'gsus')
sc = gsus.source_categories.first
sc.category_name
sc.active
source_category = sc
source_category.test_process && !source_category.active
categories = {}
sources = Source.where(source_type_id: 1).where.not(name: 'gsimg')
qexit
exit
edit
categories
BulkCategory.first
BulkCategory.second
edit -t
categories
DateTime.now.strftime('%Y')
DateTime.now.strftime('%Y%M')
DateTime.now.strftime('%Y%m')
DateTime.now.strftime('%Y%m%d')
DateTime.now.strftime('%Y%m%d%t')
DateTime.now.strftime('%Y%m%d%T')
DateTime.now.strftime('%Y%m%d%H%M%S')
"rds_categories_#{DateTime.now.strftime('%Y%m%d%H%M%S')}"
data = categories
data.keys
source = _.first
data[source].count
data[source].first
data[source].first.keys.count
data[source]
data[source].map(&:values)
data[source].first
data[source].first.keys
data[source].map {|sc| sc[:bulk_category] }
data[source].map {|sc| [sc[:bulk_category], sc[:active], sc[:source_category], sc[:url]] }
data[source].first
data[source].map {|sc| [sc[:bulk_category], sc[:active], sc[:source_category], sc[:url]] }
edit -t
title = "rds_categories_#{DateTime.now.strftime('%Y%m%d%H%M%S')}"
session = get_session
spreadsheet = session.create_spreadsheet(title)
sources = data.keys
headers = ['Bulk Category', 'Active', 'Source Category', 'Url']
edit -t
session.list
session.files
session.files.first
file = _
file.url
file.human_url
ws
spreadsheet
spreadsheet.num_rows
spreadsheet.worksheets
ws = spreadsheet.worksheets.first
edit
file
edit
file
file.dele
file.delete
data
title
title = "rds_categories_#{DateTime.now.strftime('%Y%m%d%H%M%S')}"
edit
spreadsheet.delete
edit
spreadsheet.sheets
spreadsheet.worksheets
spreadsheet.worksheet_by_title('Sheet1').delete
SourceCategory.count
sc = SourceCategory.where(url: 'https://www.google.com/webhp?hl=en#hl=en&tbm=shop&q=blank+media')
sc = sc.last
sc.active
sc.active.to_s.upcase
session.files
session.files[0..3]
session.files[0..3].each(&:delete)
session.files
session.
file = session.files.first
file = session.files[0]
file
file.
file
file.delete
session.files
SourceCategory.where(active: nil)
sc = SourceCategory.where(active: nil).first
sc.active.to_s.upcase
sc.active ? 'TRUE' : 'FALSE'
Rails.env
bc = BulkCategory.first
bc.categories
bc.categories.pluck(:ids
bc.categories.pluck(:ids)
bc.categories.pluck(:id)
BulkCategory.all.map {|bc| [bc.name, bc.categories.pluck(:id).join(', ')]}
BulkCategory.all.map {|bc| [bc.name, bc.categories.pluck(:id).uniq.join(', ')]}
rows = [['Bulk Category', 'Category Ids']]
cats = BulkCategory.all.map {|bc| [bc.name, bc.categories.pluck(:id).uniq.join(', ')]}
cats
rows.concat(cats)
Category.find(9)
bc
bc.source_categories.pluck(:category_id)
Category.find(bc.source_categories.pluck(:category_id))
Category.find(bc.source_categories.where.not(url: nil).pluck(:category_id))
BulkCategory.all.map { |bc| [bc.name, bc.source_categories.where.not(url:nil).pluck(:category_id).uniq.join(', ')] }
"78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 90, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 301, 299, 399, 454, 162, 455, 456, 457, 458, 459, 444, 445, 172, 461, 462, 463, 464, 465, 178, 174, 180, 466, 467, 468, 469, 470, 471, 489, 106, 490, 491, 626, 627, 628, 629, 630, 631, 632, 13, 8, 633, 634, 767, 812, 813, 814, 815, 816, 780, 781, 850, 837, 941, 942, 943, 944, 945, 1142, 778, 89"
ids = _.split(',').map(&:strip)
Category.find(ids).pluck(:name)
Category.find(ids).map(&:name)
session.files
session.files[0..1]
session.files[0..1].each(&:delete)
Category.find 8
exit
ENV['GOOGLE_PRIVATE_KEY_ID']
ENV['GOOGLE_PRIVATE_KEY']
exit
s = Scrape.last
s.listings.count
l = s.listings.first
l.images
s.listings.count
l.images.count
exit
ProductImage.where(id: ids).each { |image|
  path = image.attachment? ? image.attachment.url : image.original_url
  begin
    image.update(dhash: DHasher.hash_from_path(path))
edit -t
ProductImage.first.dhash
exit
ProductImage.all.each do |image|
exit
img = ProductImage.first
image = _
path = image.attachment? ? image.attachment.url : image.original_url
DHasher.hash_from_path(path, 7)
dhashes = {}
"dhash#{0+1}"
dhashes = {}
DHasher.hash_from_path(path, 7).each_with_index do |hash, idx|
  dhashes["dhash#{idx+1}"] = hash
end
dhashes
image.update(dhashes)
ProductImage.all.each { |image|
  path = image.attachment? ? image.attachment.url : image.original_url
  begin
    dhashes = {}
    DHasher.hash_from_path(path, 7).each_with_index do |hash, idx|
      dhashes["dhash#{idx+1}"] = hash
    end
    image.update(dhashes)
  rescue Magick::ImageMagickError => e
    Rails.logger.error("Image dhash generation failed for ProductImage id : #{image.id}")
    Rails.logger.error(e.message)
  end
}
ProductImage.all
image.attachment.url
img.original_url
ProductImage.all.sample.original_url
img = ProductImage.where(original_url: _)
image = img.first
ProductImage.find 26
edit -t
edit
image = ProductImage.first
exit
image1 = ProductImage.first
image1.slice(:dhash1, :dhash2, :dhash3, :dhash4)
image1.slice(:dhash1, :dhash2, :dhash3, :dhash4).values
exit
require 'd_hasher'
DHasher.similar?(ProductImage.first, ProductImage.second)
exit
DHasher.similar?(ProductImage.first, ProductImage.second)
require 'd_hasher'
DHasher.similar?(ProductImage.first, ProductImage.second)
DHasher.hash_from_path(ProductImage.all.sample.original_url)
exit
path = ProductImaage.first.attachment.url
path = ProductImage.first.attachment.url
image = Magick::Image.read(path).first
image.base_rows
image.rows
image.base_columns
image.columns
image = Magick::Image.read(path).first
size = 16
new_rows = 0
new_columns = 0
if image.base_rows > image.base_columns
  new_columns = size + 1
  new_rows = (image.base_rows.to_f / image.base_columns) * new_columns
else
  new_rows = size + 1
  new_columns = (image.base_columns.to_f / image.base_rows) * new_rows
end
new_columns
new_rows
image.resize!(new_columns, new_rows)
width = image.columns / 2
height = image.rows / 2
image = Magick::Image.read(path).first
image.resize!(16, 16)
width = image.columns / 2
height = image.rows / 2
image.base_columns
image.columns
top_left = image.excerpt(0, 0, width, height)
top_right = image.excerpt(width + 1, 0, width - 1, height)
bot_left = image.excerpt(0, height + 1, width, height - 1)
bot_right = image.excerpt(width + 1, height + 1, width - 1, height - 1)
slices = [top_left, top_right, bot_left, bot_right]
slices.map { |slice| DHasher.hash_from_image(slice, hash_size) }
require 'd_hasher'
top_left = image.excerpt(0, 0, width, height)
top_right = image.excerpt(width + 1, 0, width - 1, height)
bot_left = image.excerpt(0, height + 1, width, height - 1)
bot_right = image.excerpt(width + 1, height + 1, width - 1, height - 1)
slices = [top_left, top_right, bot_left, bot_right]
slices.map { |slice| DHasher.hash_from_image(slice, hash_size) }
hash_size = 7
top_left = image.excerpt(0, 0, width, height)
top_right = image.excerpt(width + 1, 0, width - 1, height)
bot_left = image.excerpt(0, height + 1, width, height - 1)
bot_right = image.excerpt(width + 1, height + 1, width - 1, height - 1)
slices = [top_left, top_right, bot_left, bot_right]
slices.map { |slice| DHasher.hash_from_image(slice, hash_size) }
ProductImage.first
image
path
image = Magick::Image.read(path).first
resize(image, 16)
image.resize!(16, 16)
top_left = image.excerpt(0, 0, 8, 8)
top_right = image.excerpt(8, 0, 8, 8)
bot_left = image.excerpt(0, 8, 8, 8)
bot_right = image.excerpt(8, 8 , 8 , 8)
slices = [top_left, top_right, bot_left, bot_right]
slices.map { |slice| DHasher.hash_from_image(slice, hash_size) }
bot_left.base_rows
bot_left.rows
exit
require 'd_hasher'
image = ProductImage.first
path = image.attachment.url
DHasher.hash_from_path(path)
hashes = _
hashes.map(&:bit_length)
1<<8
image.base_rows > image.base_columns
image
image = Magick::Image.read(path).first
image.base_rows > image.base_columns
new_rows = 0
new_columns = 0
size = 8
new_columns = (image.base_columns.to_f / image.base_rows) * new_rows
new_columns
new_rows = size
new_columns = (image.base_columns.to_f / image.base_rows) * new_rows
image.base_columns.to_f
image.base_rows
500.0 / 286
def resize(image, size = 8)
  new_rows = 0
  new_columns = 0
  if image.base_rows > image.base_columns
    new_columns = size
    new_rows = (image.base_rows.to_f / image.base_columns) * new_columns
  else
    new_rows = size
    new_columns = (image.base_columns.to_f / image.base_rows) * new_rows
  end
  image.resize!(new_columns, new_rows)
end
img = resize(image, 16)
img.base_rows
img.rows
img.columns
img.columns / 2
path
image = Magick::Image.read(path).first
image.resize!(16, 16)
top_left = image.excerpt(0, 0, 8, 8)
top_right = image.excerpt(8, 0, 8, 8)
bot_left = image.excerpt(0, 8, 8, 8)
bot_right = image.excerpt(8, 8 , 8 , 8)
slices = [top_left, top_right, bot_left, bot_right]
slices.map { |slice| DHasher.hash_from_image(slice, hash_size) }
hash_size = 8
slices.map { |slice| DHasher.hash_from_image(slice, hash_size) }
hashes = _
hashes.map(&:bit_length)
gst
exit
ProductImage.all.each do |image|
  path = image.attachment? ? image.attachment.url : image.original_url
  dhashes = {}
  DHasher.hash_from_path(path).each_with_index do |hash, idx|
    dhashes["dhash#{idx+1}"] = hash
  end
  image.update(dhashes)
end
rerequire 'd_hasher'
require 'd_hasher'
edit -t
ProductImage.first
path
path = ProductImage.first.attachment.url
image = Magick::Image.read(path).first
new_size = 9
image.resize!(new_size * 2, new_size * 2)
top_left = image.excerpt(0, 0, new_size, new_size)
top_right = image.excerpt(new_size, 0, new_size, new_size)
top_left
top_right
image
image = ProductImage.last
exit
image.save_attachment
image = ProductImage.last
image.save_attachment
reload!
exit
image = ProductImage.last
image.save_attachment
include RvxRds::Proxy
file = download_via_proxy(image.original_url)
file.path
DHasher.hash_from_path(file.path)
image
image.attachment = file
image.save
dhashes = {}
DHasher.hash_from_path(file.path).each_with_index do |hash, idx|
  dhashes["dhash#{idx+1}"] = hash
end
dhashes
dhashes.each_pair do {|k,v| image.send("#{k}=".to_sym, 1) }
dhashes.each_pair {|k,v| image.send("#{k}=".to_sym, 1) }
image
dhashes.each_pair {|k,v| image.send("#{k}=".to_sym, v) }
image
exit
image = ProductImage.first
image.attachment = nil
image.save
exit
image = ProductImage.first
image.attachment.url
image.clear_dhashes
image
image.save_attachment
image
image.attachment.url
image.type
exit
Redis.current.flushall
exit
ProductImage.all.each(&:clear_dhashes)
exit
ProductImage.first
exit
Rails.env.beta?
Rails.env.development?
Rails.env.beta? || Rails.env.development?
path = 'http://s3.amazonaws.com/rvx-rds-dev/images/uploads/007/575/306/original/open-uri20160707-29932-17rjp5x?1467863200'
DHasher.hash_from_path(path)
require 'd_hasher'
DHasher.hash_from_path(path)
path2 = 'https://images-na.ssl-images-amazon.com/images/G/01/aplusautomation/vendorimages/fb19f46e-3b84-4048-98b8-3a569c83e6e5.jpg._CB289531031__SR300,300_.jpg'
DHasher.hash_from_path(path2)
exit
cd Containers
ls
cd PriorityQueue
ls
ls push
cd push
define_method push
exit
BulkCategory.destroy_all
exit
c =     {:bulk_category=>"Consumer Electronics > Audio", :active=>"TRUE", :source_category=>"Electronics > Audio > Audio Components > Speakers", :url=>"https://www.google.com/webhp?hl=en#q=audio+speakers&hl=en&tbm=shop"},
c =     {:bulk_category=>"Consumer Electronics > Audio", :active=>"TRUE", :source_category=>"Electronics > Audio > Audio Components > Speakers", :url=>"https://www.google.com/webhp?hl=en#q=audio+speakers&hl=en&tbm=shop"}
bulk_categories = []
bulk_categories = c[:bulk_category].split('>').map(&:squish).each do |name|
  bulk_categories |= BulkCategory.find_or_create_by(name: name)
end
bulk_categories = []
bulk_categories = c[:bulk_category].split('>').map(&:squish).each do |name|
  bulk_categories << BulkCategory.find_or_create_by(name: name)
end
exit
c =     {:bulk_category=>"Consumer Electronics > Audio", :active=>"TRUE", :source_category=>"Electronics > Audio > Audio Components > Speakers", :url=>"https://www.google.com/webhp?hl=en#q=audio+speakers&hl=en&tbm=shop"}
bulk_categories = []
bulk_categories = c[:bulk_category].split('>').map(&:squish).each do |name|
  bulk_categories << BulkCategory.find_or_create_by(name: name)
end
bulk_categories
namae = 'Audio'
name = _
BulkCategory.find_or_create_by(name: name)
bulk_categories = []
c[:bulk_category].split('>').map(&:squish).each do |name|
  bulk_categories << BulkCategory.find_or_create_by(name: name)
end
bulk_categories
exit
BulkCategory.count
BulkCategory.last
bc = _
bc.categories.count
bc.categories.pluck(:name)
bc.categories.pluck(:id).uniq
exit
url = 'http://s3.amazonaws.com/rvx-rds-dev/images/uploads/009/844/461/original/open-uri20160714-3388-1cae12n?1468526705'
url.gsub('rvx-rds-dev', 'rvx-rds')
exit
node = VpNode.create
exit
:w
exit
node = VpNode.create
node.parent
node.children
vp.node.parent
node.parent.to_sql
node.parent
node.parent_id
exit
node = VpNode.first
ProductImage.count
ProductImage.where(dhash1: nil)
xit
exit
ProductImage.find_each do |image|
  VpNode.create(ref_image: image)
end
VpNode.first
node = VpNode.first
node.left
node.left_sibling
VpNode.all.map(&:left)
VpNode.roots
exit
image = ProductImage.first
image.ref_node
node = _
node.left
node.children
image.vp_node
exit
image = ProductImage.first
image.ref_node
image.leaf_node
image.leaf_node.to_sql
image.leaf_node
image.leaf_node = VpNode.first
exit
image = ProductImage.first
image.leaf_node = VpNode.first
image.save
image = Image.first
image = ProductImage.first
image.leaf_node
image.ref_node
image.leaf_node = nil
image.save
node1, node2, node3 = VpNode.first(3)
node4 = VpNode.find 4
node2.move_to_child_of(node1)
node2
node3.move_to_child_of(node1)
node4.move_to_child_of(node1)
node1.children
exit
offeset = rand(VpNode.count)
root = VpNode.offset(offset).first
root = VpNode.offset(offeset).first
root
root.ref_image
root_node = VpNode.offset(rand(VpNode.count)).first
node.is_a?(Integer)
root_node.is_a?(Integer)
node = root_node
node.ref_image
ProductImage.all.class
records=ProductImage::ActiveRecord_Relation.new([])
records=ProductImage::ActiveRecord_Relation.new
ProductImage.where(id: 102412).empty?
query = <<-SQL.gsub(/\A\s+/, '')
      SELECT
        scores.id
      FROM (
        SELECT
        id,
        HAMMINGDISTANCE(
        18446319897753230138, 2604263736710662420, 15198935003868057102, 10988925484007592588,
        dhash1, dhash2, dhash3, dhash4
        ) AS hd,
        original_url
        FROM assets
        WHERE dhash1 IS NOT NULL
        ) AS scores
      ORDER BY scores.hd ASC
      LIMIT 14, 1
    SQL
query
query = <<-SQL.gsub(/^\s+/, '')
      SELECT
        scores.id
      FROM (
        SELECT
        id,
        HAMMINGDISTANCE(
        18446319897753230138, 2604263736710662420, 15198935003868057102, 10988925484007592588,
        dhash1, dhash2, dhash3, dhash4
        ) AS hd,
        original_url
        FROM assets
        WHERE dhash1 IS NOT NULL
        ) AS scores
      ORDER BY scores.hd ASC
      LIMIT 14, 1
    SQL
sql
query
ProductImage.select(    query = <<-SQL.gsub(/^\s+/, '')
      SELECT
        scores.id
      FROM (
        SELECT
        id,
        HAMMINGDISTANCE(
        18446319897753230138, 2604263736710662420, 15198935003868057102, 10988925484007592588,
        dhash1, dhash2, dhash3, dhash4
        ) AS hd,
        original_url
        FROM assets
        WHERE dhash1 IS NOT NULL
        ) AS scores
      ORDER BY scores.hd ASC
      LIMIT 14, 1
    SQL
)
images = ProductImage.execute(query)
images = ProductImage.select(query)
images.count
edit
ProductImage.select(query)
ProductImage.find_by_sql(query)
ProductImage.none
ProductImage.none.claass
ProductImage.none.class
ProductImage.none.pluck(:id)
ProductImage.find_by_sql('Select * from assets where id in ?', ProductImage.none.pluck(:id))
ProductImage.find_by_sql('Select * from assets where id in (?)', ProductImage.none.pluck(:id))
ProductImage.where(id: ProductImage.none).to_sql
records = []
node
img = node.ref_image
edit -t
middle_count = records.empty? ? (ProductImage.count / 2) : (records.count / 2)
mid_image = ProductImage.find_by_sql(query, [img.dash1, img.dash2, img.dash3, img.dash4, ]).where(id: records).first
mid_image = ProductImage.find_by_sql(query, [img.dhash1, img.dhash2, img.dhash3, img.dhash4, ]).where(id: records).first
mid_image = ProductImage.find_by_sql(query, [img.dhash1, img.dhash2, img.dhash3, img.dhash4, middle_count]).where(id: records).first
edit -t
mid_image = ProductImage.find_by_sql(query, [img.dhash1, img.dhash2, img.dhash3, img.dhash4, middle_count]).where(id: records).first
[img.dhash1, img.dhash2, img.dhash3, img.dhash4, middle_count]
edit -t
middle_count = records.empty? ? (ProductImage.count / 2) : (records.count / 2)
mid_image = ProductImage.find_by_sql(query, [img.dhash1, img.dhash2, img.dhash3, img.dhash4, middle_count]).where(id: records).first
mid_image = ProductImage.find_by_sql(query, [img.dhash1, img.dhash2, img.dhash3, img.dhash4, middle_count])
kj    mid_image = ProductImage.find_by_sql(query, [img.dhash1, img.dhash2, img.dhash3, img.dhash4, middle_count])
mid_image = ProductImage.find_by_sql(query, [img.dhash1, img.dhash2, img.dhash3, img.dhash4, middle_count])
mid_image = ProductImage.find_by_sql([query, img.dhash1, img.dhash2, img.dhash3, img.dhash4, middle_count])
mid_image = ProductImage.find_by_sql(query, img.dhash1, img.dhash2, img.dhash3, img.dhash4, middle_count)
middle_count
mid_image = ProductImage.find_by_sql([query, img.dhash1, img.dhash2, img.dhash3, img.dhash4, middle_count])
edit -dt
edit -t
mid_image = ProductImage.find_by_sql([query, img.dhash1, img.dhash2, img.dhash3, img.dhash4, middle_count])
edit -t
middle_count = records.empty? ? (ProductImage.count / 2) : (records.count / 2)
mid_image = ProductImage.find_by_sql([query, img.dhash1, img.dhash2, img.dhash3, img.dhash4, middle_count])
edit -t
mid_image = ProductImage.find_by_sql([query, img.dhash1, img.dhash2, img.dhash3, img.dhash4, middle_count])
img
mid_image = ProductImage.find_by_sql([query, img.dhash1, img.dhash2, img.dhash3, img.dhash4, middle_count])
mid_image = ProductImage.select([query, img.dhash1, img.dhash2, img.dhash3, img.dhash4, middle_count])
mid_image = ProductImage.select(query, img.dhash1, img.dhash2, img.dhash3, img.dhash4, middle_count)
mid_image = ProductImage.select(query, [img.dhash1, img.dhash2, img.dhash3, img.dhash4, middle_count])
mid_image = ProductImage.find_by_sql([query, img.dhash1, img.dhash2, img.dhash3, img.dhash4, middle_count]).select(:id, :hd)
mid_image = ProductImage.select(:id, :hd).find_by_sql([query, img.dhash1, img.dhash2, img.dhash3, img.dhash4, middle_count])
mid_image = ProductImage.select(:id, 'scores.hd').find_by_sql([query, img.dhash1, img.dhash2, img.dhash3, img.dhash4, middle_count])
mid_image = ProductImage.select('scores.id', 'scores.hd').find_by_sql([query, img.dhash1, img.dhash2, img.dhash3, img.dhash4, middle_count])
mid_image = ProductImage.select('scores.id', 'scores.hd').find_by_sql([query, img.dhash1, img.dhash2, img.dhash3, img.dhash4, middle_count]).first
mid_image.hd
records
records.class
records = ProductImage.none
mid_image = ProductImage.find_by_sql([query, img.dhash1, img.dhash2, img.dhash3, img.dhash4, middle_count]).where(id: records)
mid_image = ProductImage.where(id: records).find_by_sql([query, img.dhash1, img.dhash2, img.dhash3, img.dhash4, middle_count])
edit -t
records
middle_count = records.empty? ? (ProductImage.count / 2) : (records.count / 2)
img
mid_image = ProductImage.connection.execute(median_query, [img.dhash1, img.dhash2, img.dhash3, img.dhash4, records.pluck(:id), middle_count]).first
mid_image = ProductImage.where(median_query, [img.dhash1, img.dhash2, img.dhash3, img.dhash4, records.pluck(:id), middle_count]).first
mid_image = ProductImage.where(median_query, img.dhash1, img.dhash2, img.dhash3, img.dhash4, records.pluck(:id), middle_count).first
mid_image = ProductImage.where(id: records).where(median_query, img.dhash1, img.dhash2, img.dhash3, img.dhash4, middle_count).first
median_query = 'SELECT
                 scores.id,
                 scores.hd
             FROM (
                 SELECT
                   id,
                   HAMMINGDISTANCE(
                     ?, ?, ?, ?, dhash1, dhash2, dhash3, dhash4
                   ) AS hd
                 FROM assets
                 WHERE dhash1 IS NOT NULL
                 ) AS scores
             ORDER BY scores.hd ASC
             LIMIT ?, 1;'
middle_count = records.empty? ? (ProductImage.count / 2) : (records.count / 2)
mid_image = ProductImage.where(id: records).where(median_query, img.dhash1, img.dhash2, img.dhash3, img.dhash4, middle_count).first
median_query = 'SELECT
                 scores.id,
                 scores.hd
             FROM (
                 SELECT
                   id,
                   HAMMINGDISTANCE(
                     ?, ?, ?, ?, dhash1, dhash2, dhash3, dhash4
                   ) AS hd
                 FROM assets
                 WHERE dhash1 IS NOT NULL
                 ) AS scores
             ORDER BY scores.hd ASC
             LIMIT ?, 1'
edit -t
median_query
mid_image = ProductImage.where(id: records).where(median_query, img.dhash1, img.dhash2, img.dhash3, img.dhash4, middle_count).first
mid_image = ProductImage.connection.execute(median_query, [img.dhash1, img.dhash2, img.dhash3, img.dhash4, records.pluck(:id), middle_count]).first
median_query = 'SELECT
                 scores.id,
                 scores.hd
             FROM (
                 SELECT
                   id,
                   HAMMINGDISTANCE(
                     ?, ?, ?, ?, dhash1, dhash2, dhash3, dhash4
                   ) AS hd
                 FROM assets
                 WHERE dhash1 IS NOT NULL
                 ) AS scores
             ORDER BY scores.hd ASC
             LIMIT ?, 1;'
middle_count = records.empty? ? (ProductImage.count / 2) : (records.count / 2)
mid_image = ProductImage.where(id: records).find_by_sql([median_query, img.dhash1, img.dhash2, img.dhash3, img.dhash4, middle_count]).first
img.id
mid_image
mid_image.hd
median_score = mid_image.hd
mid_id = mid_image.id
left_query = 'SELECT
                 scores.id
             FROM (
                 SELECT
                   id,
                   HAMMINGDISTANCE(
                     ?, ?, ?, ?, dhash1, dhash2, dhash3, dhash4
                   ) AS hd
                 FROM assets
                 WHERE dhash1 IS NOT NULL
                 ) AS scores
             WHERE scores.hd <= ?
             ORDER BY scores.hd ASC;'
left_records = ProductImage.where(id: records).connection.find_by_sql([left_query, img.dhash1, img.dhash2, img.dhash3, img.dhash4, median_score]).count
left_records = ProductImage.where(id: records).find_by_sql([left_query, img.dhash1, img.dhash2, img.dhash3, img.dhash4, median_score]).count
kk    left_query = 'SELECT
                 scores.id
             FROM (
                 SELECT
                   id,
                   HAMMINGDISTANCE(
                     ?, ?, ?, ?, dhash1, dhash2, dhash3, dhash4
                   ) AS hd
                 FROM assets
                 WHERE dhash1 IS NOT NULL
                 ) AS scores
             WHERE scores.hd < ?
             ORDER BY scores.hd ASC;'
left_query = 'SELECT
                 scores.id
             FROM (
                 SELECT
                   id,
                   HAMMINGDISTANCE(
                     ?, ?, ?, ?, dhash1, dhash2, dhash3, dhash4
                   ) AS hd
                 FROM assets
                 WHERE dhash1 IS NOT NULL
                 ) AS scores
             WHERE scores.hd < ?
             ORDER BY scores.hd ASC;'
left_records = ProductImage.where(id: records).find_by_sql([left_query, img.dhash1, img.dhash2, img.dhash3, img.dhash4, median_score]).count
lt_query = 'SELECT
                 scores.id
             FROM (
                 SELECT
                   id,
                   HAMMINGDISTANCE(
                     ?, ?, ?, ?, dhash1, dhash2, dhash3, dhash4
                   ) AS hd
                 FROM assets
                 WHERE dhash1 IS NOT NULL
                 ) AS scores
             WHERE scores.hd < ?
             ORDER BY scores.hd ASC;'
left_records = ProductImage.where(id: records).find_by_sql([lt_query, img.dhash1, img.dhash2, img.dhash3, img.dhash4, median_score]).count
rt_query = 'SELECT
                 scores.id
             FROM (
                 SELECT
                   id,
                   HAMMINGDISTANCE(
                     ?, ?, ?, ?, dhash1, dhash2, dhash3, dhash4
                   ) AS hd
                 FROM assets
                 WHERE dhash1 IS NOT NULL
                 ) AS scores
             WHERE scores.hd >= ?
             ORDER BY scores.hd ASC;'
rt_records = ProductImage.where(id: records).find_by_sql([lt_query, img.dhash1, img.dhash2, img.dhash3, img.dhash4, median_score]).count
left_records = ProductImage.where(id: records).find_by_sql([lt_query, img.dhash1, img.dhash2, img.dhash3, img.dhash4, median_score])
rt_records = ProductImage.where(id: records).find_by_sql([lt_query, img.dhash1, img.dhash2, img.dhash3, img.dhash4, median_score])
rt_records = ProductImage.where(id: records).find_by_sql([rt_query, img.dhash1, img.dhash2, img.dhash3, img.dhash4, median_score])
lt_records = ProductImage.where(id: records).find_by_sql([lt_query, img.dhash1, img.dhash2, img.dhash3, img.dhash4, median_score])
l_ids = lt_records.map(&:id)
r_ids = rt_records.map(&:id)
r_ids | l_ids
r_ids & l_ids
ProductImage.where(id: l_ids).connection.select_all('SELECT *')
ProductImage.where(id: l_ids).connection.select_all('SELECT * FROM assets where type="ProductImage"')
raw_images = _
raw_images.first
l_ids
lt_query
where_clause = 'scores.hd < ? AND type = "ProductImage"'
where_clause = "scores.hd < ? AND type = 'ProductImage'"
records
node
node.images
ProductImage.where.nil(vp_node_id: nil)(
ProductImage.where.nil(vp_node_id: nil)
ProductImage.where.not(vp_node_id: nil)
ProductImage.first.vp_node = node
ProductImage.first.leaf_node = node
image = _
image = ProductImage.first
image.leaf_node = node
image.save
ProductImage.where(vp_node: node)
ProductImage.where(vp_node_id: node.id)
median_query = 'SELECT
                 scores.id,
                 scores.hd
             FROM (
                 SELECT
                   id,
                   HAMMINGDISTANCE(
                     ?, ?, ?, ?, dhash1, dhash2, dhash3, dhash4
                   ) AS hd
                 FROM assets
                 WHERE dhash1 IS NOT NULL
                 ) AS scores
             ORDER BY scores.hd ASC
             LIMIT ?, 1;'
middle_count = records.empty? ? (ProductImage.count / 2) : (records.count / 2)
records = ProductImage.where(vp_node_id: node.id)
middle_count = records.empty? ? (ProductImage.count / 2) : (records.count / 2)
middle_count = records.empty? ? (ProductImage.count / 2) : [(records.count / 2), 1].max
img = node.ref_image
middle_count = records.empty? ? (ProductImage.count / 2) : [(records.count / 2), 1].max
median_query = "SELECT
                 scores.id,
                 scores.hd
             FROM (
                 SELECT
                   id,
                   HAMMINGDISTANCE(
                     #{[img.dhash1, img.dhash2, img.dhash3, img.dhash4].join(', ')}, dhash1, dhash2, dhash3, dhash4
                   ) AS hd
                 FROM assets
                 WHERE dhash1 IS NOT NULL
                 ) AS scores
             ORDER BY scores.hd ASC
             LIMIT ?, 1;"
mid_image = ProductImage.where(id: records).find_by_sql([median_query, img.dhash1, img.dhash2, img.dhash3, img.dhash4, middle_count]).first
median_query = "SELECT
                 scores.id,
                 scores.hd
             FROM (
                 SELECT
                   id,
                   HAMMINGDISTANCE(
                     #{[img.dhash1, img.dhash2, img.dhash3, img.dhash4].join(', ')}, dhash1, dhash2, dhash3, dhash4
                   ) AS hd
                 FROM assets
                 WHERE dhash1 IS NOT NULL
                 ) AS scores
             ORDER BY scores.hd ASC
             LIMIT #{middle_count}, 1;"
mid_image = ProductImage.where(id: records).select_all(median_query).first
where_clause = "dhash1 IS NOT NULL"
where_clause << " AND id in (#{records.pluck(:id)}" unless records.empty?
where_clause = "dhash1 IS NOT NULL"
where_clause << " AND id in (#{records.pluck(:id).join(',')})" unless records.empty?
where_clause << " AND id IN (#{records.pluck(:id).join(',')})" unless records.empty?
median_query = "SELECT
                 scores.id,
                 scores.hd
             FROM (
                 SELECT
                   id,
                   HAMMINGDISTANCE(
                     #{[img.dhash1, img.dhash2, img.dhash3, img.dhash4].join(', ')}, dhash1, dhash2, dhash3, dhash4
                   ) AS hd
                 FROM assets
                 WHERE #{where_clause}
                 ) AS scores
             ORDER BY scores.hd ASC
             LIMIT #{middle_count}, 1;"
where_clause = "dhash1 IS NOT NULL"
where_clause << " AND id IN (#{records.pluck(:id).join(',')})" unless records.empty?
where_clause
median_query = "SELECT
                 scores.id,
                 scores.hd
             FROM (
                 SELECT
                   id,
                   HAMMINGDISTANCE(
                     #{[img.dhash1, img.dhash2, img.dhash3, img.dhash4].join(', ')}, dhash1, dhash2, dhash3, dhash4
                   ) AS hd
                 FROM assets
                 WHERE #{where_clause}
                 ) AS scores
             ORDER BY scores.hd ASC
             LIMIT #{middle_count}, 1;"
mid_image = ProductImage.select_all(median_query).first
mid_image = ProductImage.connection.select_all(median_query).first
mid_image
mid_image = ProductImage.connection.select_all(median_query)
middle_count = records.empty? ? (ProductImage.count / 2) : (records.count / 2)
where_clause = "dhash1 IS NOT NULL"
where_clause << " AND id IN (#{records.pluck(:id).join(',')})" unless records.empty?
median_query = "SELECT
                 scores.id,
                 scores.hd
             FROM (
                 SELECT
                   id,
                   HAMMINGDISTANCE(
                     #{[img.dhash1, img.dhash2, img.dhash3, img.dhash4].join(', ')}, dhash1, dhash2, dhash3, dhash4
                   ) AS hd
                 FROM assets
                 WHERE #{where_clause}
                 ) AS scores
             ORDER BY scores.hd ASC
             LIMIT #{middle_count}, 1;"
mid_image = ProductImage.connection.select_all(median_query)
mid_image.first
mid_image = ProductImage.connection.select_all(median_query).first
mid_id, median_score = mid_image['id'], mid_image['hd']
mid_id
where_clause = "dhash1 IS NOT NULL AND id != #{img.id}"
where_clause << " AND id IN (#{records.pluck(:id).join(',')})" unless records.empty?
median_query = "SELECT
                 scores.id,
                 scores.hd
             FROM (
                 SELECT
                   id,
                   HAMMINGDISTANCE(
                     #{[img.dhash1, img.dhash2, img.dhash3, img.dhash4].join(', ')}, dhash1, dhash2, dhash3, dhash4
                   ) AS hd
                 FROM assets
                 WHERE #{where_clause}
                 ) AS scores
             ORDER BY scores.hd ASC
             LIMIT #{middle_count}, 1;"
mid_image = ProductImage.connection.select_all(median_query).first
mid_id, median_score = mid_image['id'], mid_image['hd']
ProductImage.where(vp_node_id: node.id)
[] || 'hey'
image = ProductImage.where.not(dhash1: nil).offset(rand(ProductImage.where.not(dhash1: nil).count)).first
node.update(ref_image: image)
reload!
exit
root_node = VpNode.offset(rand(VpNode.count)).first
exit
root_node = VpNode.offset(rand(VpNode.count)).first
node = _
records = ProductImage.where(vp_node_id: node.id)
return unless records.count > VpNode::MAX_IMAGES
records.count > VpNode::MAX_IMAGES
node.ref_image.nil?
img = node.ref_image
middle_count = records.empty? ? (ProductImage.count / 2) : (records.count / 2)
where_clause = "dhash1 IS NOT NULL AND id != #{img.id}"
where_clause << " AND id IN (#{records.pluck(:id).join(',')})" unless records.empty?
median_query = "SELECT
                 scores.id,
                 scores.hd
             FROM (
                 SELECT
                   id,
                   HAMMINGDISTANCE(
                     #{[img.dhash1, img.dhash2, img.dhash3, img.dhash4].join(', ')}, dhash1, dhash2, dhash3, dhash4
                   ) AS hd
                 FROM assets
                 WHERE #{where_clause}
                 ) AS scores
             ORDER BY scores.hd ASC
             LIMIT #{middle_count}, 1;"
mid_image = ProductImage.connection.select_all(median_query).first
mid_id, median_score = mid_image['id'], mid_image['hd']
node.update(tau: median_score)
where_clause = "scores.hd < ? AND type = 'ProductImage'"
where_clause << " #{records.pluck(:id)}" unless records.empty?
records
where_clause = "scores.hd < #{median_score} AND type = 'ProductImage'"
where_clause << " #{records.pluck(:id)}" unless records.empty?
lt_query = "SELECT
                 scores.id
             FROM (
                 SELECT
                   id,
                   HAMMINGDISTANCE(
                     #{[img.dhash1, img.dhash2, img.dhash3, img.dhash4].join(', ')}, dhash1, dhash2, dhash3, dhash4
                   ) AS hd
                 FROM assets
                 WHERE dhash1 IS NOT NULL
                 ) AS scores
             WHERE #{where_clause}
             ORDER BY scores.hd ASC;"
lt_records = ProductImage.connection.select_all(lt_query)
lt_query
lt_query.squish!
lt_records = ProductImage.connection.select_all(lt_query)
where_clause = "scores.hd < #{median_score}"
where_clause << " #{records.pluck(:id)}" unless records.empty?
lt_query = "SELECT
                 scores.id
             FROM (
                 SELECT
                   id,
                   HAMMINGDISTANCE(
                     #{[img.dhash1, img.dhash2, img.dhash3, img.dhash4].join(', ')}, dhash1, dhash2, dhash3, dhash4
                   ) AS hd
                 FROM assets
                 WHERE dhash1 IS NOT NULL
                 ) AS scores
             WHERE #{where_clause}
             ORDER BY scores.hd ASC;"
lt_query = "SELECT
                 scores.id
             FROM (
                 SELECT
                   id,
                   HAMMINGDISTANCE(
                     #{[img.dhash1, img.dhash2, img.dhash3, img.dhash4].join(', ')}, dhash1, dhash2, dhash3, dhash4
                   ) AS hd
                 FROM assets
                 WHERE dhash1 IS NOT NULL
                 ) AS scores
             WHERE #{where_clause}
             ORDER BY scores.hd ASC;".squish!
lt_records = ProductImage.connection.select_all(lt_query)
lt_records.map {|record| record['id']}
lt_records.each_slice(4) do |records|
  records.map {|r| r['id']}
end
lt_records
lt_records.all
lt_records.hash_rows.each_slice(4) do |records|
  records.map {|r| r['id']}
end
lt_records.to_hash
lt_records.hash_rows
lt_records.call(:hash_rows)
lt_records.send(:hash_rows)
lt_records.send(:hash_rows).class
lt_records.bit_length
lt_records.map {|r| r['id'] }
lt_img_ids = lt_records.map {|r| r['id'] }
"UPDATE assets set vp_node_id=#{lt_node.id} WHERE assets.id in (#{lt_img_ids.join(',')})"
lt_node = VpNode.create(parent_id: node.id)
exit
root_node = VpNode.offset(rand(VpNode.count)).first
root_node = VpNode.offset(rand(VpNode.count)).first || VpNode.create
exit
root_node = VpNode.offset(rand(VpNode.count)).first || VpNode.create
node = _
records = ProductImage.where(vp_node_id: node.id)
return unless records.count > VpNode::MAX_IMAGES
unless records.count > VpNode::MAX_IMAGES
node.ref_image.nil?
image = ProductImage.where.not(dhash1: nil).offset(rand(ProductImage.where.not(dhash1: nil).count)).first
node.update(ref_image: image)
img = node.ref_image
middle_count = records.empty? ? (ProductImage.count / 2) : (records.count / 2)
where_clause = "dhash1 IS NOT NULL AND id != #{img.id}"
where_clause << " AND id IN (#{records.pluck(:id).join(',')})" unless records.empty?
median_query = "SELECT
                 scores.id,
                 scores.hd
             FROM (
                 SELECT
                   id,
                   HAMMINGDISTANCE(
                     #{[img.dhash1, img.dhash2, img.dhash3, img.dhash4].join(', ')}, dhash1, dhash2, dhash3, dhash4
                   ) AS hd
                 FROM assets
                 WHERE #{where_clause}
                 ) AS scores
             ORDER BY scores.hd ASC
             LIMIT #{middle_count}, 1;"
where_clause = "dhash1 IS NOT NULL AND id != #{img.id}"
where_clause << " AND id IN (#{records.pluck(:id).join(',')})" unless records.empty?
median_query = "SELECT
                 scores.id,
                 scores.hd
             FROM (
                 SELECT
                   id,
                   HAMMINGDISTANCE(
                     #{[img.dhash1, img.dhash2, img.dhash3, img.dhash4].join(', ')}, dhash1, dhash2, dhash3, dhash4
                   ) AS hd
                 FROM assets
                 WHERE #{where_clause}
                 ) AS scores
             ORDER BY scores.hd ASC
             LIMIT #{middle_count}, 1;".squish!
mid_image = ProductImage.connection.select_all(median_query).first
mid_id, median_score = mid_image['id'], mid_image['hd']
node.update(tau: median_score)
where_clause = "scores.hd < #{median_score}"
where_clause << " #{records.pluck(:id)}" unless records.empty?
lt_query = "SELECT
                 scores.id
             FROM (
                 SELECT
                   id,
                   HAMMINGDISTANCE(
                     #{[img.dhash1, img.dhash2, img.dhash3, img.dhash4].join(', ')}, dhash1, dhash2, dhash3, dhash4
                   ) AS hd
                 FROM assets
                 WHERE dhash1 IS NOT NULL
                 ) AS scores
             WHERE #{where_clause}
             ORDER BY scores.hd ASC;".squish!
lt_records = ProductImage.connection.select_all(lt_query)
lt_img_ids = lt_records.map {|r| r['id'] }
lt_node = VpNode.create(parent_id: node.id)
ActiveRecord::Base.connection.execute("UPDATE assets set vp_node_id=#{lt_node.id} WHERE assets.id in (#{lt_img_ids.join(',')})")
lt_node.product_images
lt_records.count
where_clause = "scores.hd >= #{median_score}"
where_clause << " #{records.pluck(:id)}" unless records.empty?
rt_query = "SELECT
                 scores.id
             FROM (
                 SELECT
                   id,
                   HAMMINGDISTANCE(
                     #{[img.dhash1, img.dhash2, img.dhash3, img.dhash4].join(', ')}, dhash1, dhash2, dhash3, dhash4
                   ) AS hd
                 FROM assets
                 WHERE dhash1 IS NOT NULL
                 ) AS scores
             WHERE #{where_clause}
             ORDER BY scores.hd ASC;".squish!
rt_records = ProductImage.connection.select_all(rt_query)
lt_records
where_clause = "scores.hd >= #{median_score}"
where_clause << " #{records.pluck(:id)}" unless records.empty?
rt_query = "SELECT
                 scores.id
             FROM (
                 SELECT
                   id,
                   HAMMINGDISTANCE(
                     #{[img.dhash1, img.dhash2, img.dhash3, img.dhash4].join(', ')}, dhash1, dhash2, dhash3, dhash4
                   ) AS hd
                 FROM assets
                 WHERE dhash1 IS NOT NULL
                 ) AS scores
             WHERE #{where_clause}
             ORDER BY scores.hd ASC;".squish!
rt_records = ProductImage.connection.select_all(rt_query)
rt_img_ids = rt_records.map {|r| r['id'] }
lt_img_ids
lt_img_ids & rt_img_ids
(lt_img_ids & rt_img_ids).count
(lt_img_ids | rt_img_ids).count
rt_img_ids.count
rt_node = VpNode.create(parent_id: node.id)
exit
VpNode.destroy_all
root_node = VpNode.offset(rand(VpNode.count)).first || VpNode.create
node = _
records = ProductImage.where(vp_node_id: node.id)
image = ProductImage.where.not(dhash1: nil).offset(rand(ProductImage.where.not(dhash1: nil).count)).first
node.update(ref_image: image)
middle_count = records.empty? ? (ProductImage.count / 2) : (records.count / 2)
where_clause = "dhash1 IS NOT NULL AND id != #{img.id}"
img = node.ref_image
middle_count = records.empty? ? (ProductImage.count / 2) : (records.count / 2)
where_clause = "dhash1 IS NOT NULL AND id != #{img.id}"
where_clause << " AND id IN (#{records.pluck(:id).join(',')})" unless records.empty?
median_query = "SELECT
                 scores.id,
                 scores.hd
             FROM (
                 SELECT
                   id,
                   HAMMINGDISTANCE(
                     #{[img.dhash1, img.dhash2, img.dhash3, img.dhash4].join(', ')}, dhash1, dhash2, dhash3, dhash4
                   ) AS hd
                 FROM assets
                 WHERE #{where_clause}
                 ) AS scores
             ORDER BY scores.hd ASC
             LIMIT #{middle_count}, 1;".squish!
mid_image = ProductImage.connection.select_all(median_query).first
mid_id, median_score = mid_image['id'], mid_image['hd']
node.update(tau: median_score)
where_clause = "scores.hd < #{median_score}"
where_clause << " #{records.pluck(:id)}" unless records.empty?
lt_query = "SELECT
                 scores.id
             FROM (
                 SELECT
                   id,
                   HAMMINGDISTANCE(
                     #{[img.dhash1, img.dhash2, img.dhash3, img.dhash4].join(', ')}, dhash1, dhash2, dhash3, dhash4
                   ) AS hd
                 FROM assets
                 WHERE dhash1 IS NOT NULL AND id != #{img.id}
                 ) AS scores
             WHERE #{where_clause}
             ORDER BY scores.hd ASC;".squish!
lt_records = ProductImage.connection.select_all(lt_query)
lt_records.count
lt_img_ids = lt_records.map {|r| r['id'] }
lt_node = VpNode.create(parent_id: node.id)
ActiveRecord::Base.connection.execute("UPDATE assets set vp_node_id=#{lt_node.id} WHERE assets.id in (#{lt_img_ids.join(',')})")
where_clause = "scores.hd >= #{median_score}"
where_clause << " #{records.pluck(:id)}" unless records.empty?
rt_query = "SELECT
                 scores.id
             FROM (
                 SELECT
                   id,
                   HAMMINGDISTANCE(
                     #{[img.dhash1, img.dhash2, img.dhash3, img.dhash4].join(', ')}, dhash1, dhash2, dhash3, dhash4
                   ) AS hd
                 FROM assets
                 WHERE dhash1 IS NOT NULL AND id != #{img.id}
                 ) AS scores
             WHERE #{where_clause}
             ORDER BY scores.hd ASC;".squish!
rt_records = ProductImage.connection.select_all(rt_query)
rt_records.count
rt_img_ids = rt_records.map {|r| r['id'] }
lt_img_ids & rt_img_ids
img.id
(rt_img_ids | lt_img_ids).include?(img.id)
rt_node = VpNode.create(parent_id: node.id)
ActiveRecord::Base.connection.execute("UPDATE assets set vp_node_id=#{rt_node.id} WHERE assets.id in (#{rt_img_ids.join(',')})")
exit
nodeVpNode.first
node = VpNode.first
node.children
node.children.each(&:rebalance)
exit
root = VpNode.first
root.children.each(&:rebalance)
reload!
root = VpNode.first
root.children.each(&:rebalance)
reload!
root = VpNode.first
root.children.each(&:rebalance)
VpNode.destroy_all
root_node = VpNode.offset(rand(VpNode.count)).first || VpNode.create
root_node.children.each(&:rebalance)
root_node.rebalance
root_node
root_node.children
reload!
root_node.reload
root_node.rebalance
eixt
root_node = VpNode.first
root_node.rebalance
root_node
exit
node = VpNode.first
cd node
images = ProductImage.where(vp_node_id: self.id)
ref_image.present? && images.count > 2
rebalance
exit
VpNode.destroy_all
node = VpNode.first
node.rebalance
root_node = VpNode.offset(rand(VpNode.count)).first || VpNode.create
root_node.rebalance
reload!
root_node = VpNode.offset(rand(VpNode.count)).first || VpNode.create
VpNode.count
root_node.rebalance
exit
root_node.rebalance
VpNode.destroy_all
root_node = VpNode.offset(rand(VpNode.count)).first || VpNode.create
root_node.rebalance
VpNode.destroy_all
exit
root_node = VpNode.offset(rand(VpNode.count)).first || VpNode.create
root_node.rebalance
cd root_node
images = ProductImage.where(vp_node_id: self.id)
exit
root_node = VpNode.first
root_node.rebalance
root_node
reload!
root_node = VpNode.first
root_node.rebalance
VpNode.pluck(:tau).sort
VpNode.pluck(:tau)
VpNode.pluck(:parent_id)
VpNode.pluck(:parent_id).sort
VpNode.pluck(:parent_id)
VpNode.pluck(:parent_id).count
ProductImage.pluck(:vp_node_id)
ProductImage.pluck(:vp_node_id).sort
node.first
VpNode.first
node = _
node.children
exit
relaod!
reload!
VpNode.show_tree(VpNode.first)
exit
VpNode.show_tree(VpNode.first)
node = VpNode.first
str = nil
node.is_child?
node.children
node.children.first.is_child?
if node.is_child?
  str = "--#{node.id}
  |"
else
  str = "#{node.id}
  |"
end
str
puts str
exit
reload!
node = VpNode.first
VpNode.show_tree(node)
puts _
reload!
VpNode.show_tree(node)
puts _
relaod!
reload!
VpNode.show_tree(node)
puts _
reload!
node = VpNode.first
node.count_children
VpNode.count
ProductImage.where(vp_node_id: nil).count
exit
ProductImage.create(original_url: 'http://assets.inhabitat.com/wp-content/blogs.dir/1/files/2015/10/Panasonic-HIT-module-lead.jpg')
image = _
Redis.current.flushall
image.save_attachment
VpNode    VpNode.where(parent_id: nil)
VpNode.where(parent_id: nil)
VpNode.where(parent_id: nil).count
VpNode.update_all(parent_id: nil, ref_image_id: nil)
ProductImage.update_all(vp_node_id: nil)
VpNode.destroy_all
ActiveRecord::Base.reset_primary_key
VpNode.reset_primary_key
VpNode.create
VpNode.quoted_table_name
cd VpNode
update_all(parent_id: nil, ref_image_id: nil)
ProductImage.update_all(vp_node_id: nil)
destroy_all
connection.execute("truncate table #{quoted_table_name}")
root_node = offset(rand(count)).first || create
root_node.rebalance
VpNode.count
VpNode.pluck(:parent_id)
node = VpNode.find(15)
node.children
c = Node.find(16)
n = Node.find(16)
exit
n = VpNode.find(16)
n.children
n = VpNode.find(17)
n.children
n.ref_image
n.image
n.images
n.product_image
n.product_images
n.ref_image
n.ref_image.id
n.images.id
n.product_images.id
n.product_images
n.ref_image
image = _
n
n.ref_image
n
n.id
image = n.ref_image
image.vp_node
image.ref_node.id
n
n.ref_image_id
image.id
image.ref_node.id
n.id
images = n.product_images
image.class
images.class
images.offset(images.count).first
images.offset(rand(images.count)).first
reload!
VpNode.rebuild_tree
relaod!
reload!
VpNode.rebuild_tree
root = _
root.children
root.ref_image
image = _
image.ref_node
image.vp_node
img.leaf_node
img
image
ProductImage.pluck(:vp_node_id)
ProductImage.pluck(:id, :vp_node_id)
root.ref_image.id
ProductImage.pluck(:vp_node_id)
ProductImage.pluck(:vp_node_id).select {|n| n.to_i > 2}
ProductImage.pluck(:vp_node_id).select {|n| n.to_i > 2}.count
ProductImage.pluck(:vp_node_id).select {|n| n.to_i <= 2}.count
reload!
VpNode.rebuild_tree
ProductImage.pluck(:vp_node_id).select {|n| n.to_i <= 2}.count
ProductImage.pluck(:vp_node_id)
relaod!
reload!
VpNode.rebuild_tree
ProductImage.pluck(:vp_node_id)
ProductImage.pluck(:id, :vp_node_id)
VpNode.where.not(ref_image_id: nil).pluck(:ref_image_id)
VpNode.last
VpNode.last.product_images
VpNode.last.product_images.count
VpNode.count
VpNode.last(4).map {|n| [n.id, n.product_images.count] }
VpNode.last(4).map {|n| [n.id, n.ref_image_id, n.product_images.count] }
root = VpNode.first
root.product_images
VpNode.last
root
VpNode.all
ref_images = VpNode.all.map(&:ref_image_id)
p_images = VpNode.all.flat_map {|n| n.product_images.pluck(:id) }
p_images.count + ref_images.count
ProductImage.count
VpNode.last(4).map {|n| [n.id, n.ref_image_id, n.product_images.count] }
VpNode.last(4).map {|n| [n.id, n.parent_id, n.ref_image_id, n.product_images.count] }
VpNode.all
VpNode.last(4).map {|n| [n.id, n.parent_id, n.ref_image_id, n.product_images.count] }
VpNode.order(parent_id: :asc).map {|n| [n.id, n.parent_id, n.ref_image_id, n.product_images.count] }
VpNode.order(parent_id: :asc).map {|n| n.id }
VpNode.root_node
root = _
ProductImage.create(original_url: 'http://rockeyafrica.com/wp-content/uploads/2014/09/Evolution-Solar-Energy-Los-Angeles-County.png')
image = _
image.save_attachment
root
DHasher.similar?(root.ref_image, image)
root
root =     VpNode.where(parent_id: nil).first
DHasher.similar?(root.ref_image, image)
edit -t
d
root.tau
root
root.tau
VpNode.map(&:tau)
VpNode.all.map(&:tau)
root
root.ref_image
iamge
image
reload!
DHasher.distance(image, root.ref_image) < root.tau
require 'd_hasher'
exit
image = ProductImage.last
root = VpNode.root_node
DHasher.distance(image, root.ref_image)
DHasher.distance(image, root.ref_image) < root.tau
rt = root.children.last
DHasher.distance(image, node.ref_image) < node.tau
node = rt
DHasher.distance(image, node.ref_image) < node.tau
DHasher.distance(image, node.ref_image)
node.tau
node.children
node.children.first
node = _
node.images
node.product_image
node.product_images
.count
node.product_images.count
closest = nil
distance = 1000
node.product_images.each do |image|
node.product_images.each do |image2|
  if DHasher.distance(image, image2) < distance
    closest = image2
  end
end
closest
DHasher.distance(image, closest)
image.original_url
closest.original_url
root
root.ref_image
root.children.first.ref_image
VpNode.rebuild_tree
Redis.current.flushall
Product.count
Product.all
exit
node = VpNode.root_node
reload!
node = VpNode.root_node
exit
node = VpNode.root_node
node.self_and_descendents
node = VpNode.find 2
node.self_and_descendents
exit
node = VpNode.find 2
node.descendent_pictures
root_node
node
node.descendents
exit
VpNode.rebuild_tree
node = _
exit
root = VpNode.rebuild_tree
root.descendent
root.descendents
VpNode.last
exit
root = VpNode.rebuild_tree
root.descendents
exit
root = VpNode.rebuild_tree
root.descendents
VpNode.last
root.self_and_descendents
self.descendent_pictures
root.descendent_pictures
node.self_and_descendents.class
root.self_and_descendents.class
root.descendents.class
root.descendents.merge(root)
ProductImage.where(vp_node_id: [root.id] | root.descendents.pluck(:id)).count
ProductImage.where(vp_node_id: [root.id] | root.descendents.pluck(:id))
ProductImage.count
32 - 25
VpNode.count
root.descendents.pluck(:id, :ref_image_id)
exit
root = VpNode.first
root.descendent_pictures
root.descendent_pictures.count
ProductImage
root.descendents.pluck(:id, :ref_image_id)
node_ids = [root.id]
image_ids = [root.ref_image_id]
exit
root = VpNode.first
root.descendents
root.descendent_images
root.descendent_pictures
root
root.descendents.pluck(:ref_image_id)
exit
root = VpNode.first
root.descendent_pictures
root.descendent_pictures.count
root.descendents.class
root.descendents.shift(root)
root.descendents << [1
exit
node = VpNode.first
node.descendents
node.self_and_descendents
node.descendents.pluck(:id)
node.descendents.pluck(:id).shift(id)
node.descendents.pluck(:id).shift(root.id)
node.descendents.pluck(:id).shift(node.id)
exit
node = VpNode.first
node.self_and_descendents
node.self_and_descendents.class
exit
root = VpNode.first
root.self_and_descendents
root.self_and_descendents.pluck(:id, :ref_image_id)
arr = _
arr.unzip
arr.zip
arr
arr.flatten
arr.map {|pair| *pair }
arr.to_h
h = arr.to_h
h.keys
h.values
exit
root = VpNode.first
root.self_and_descendent_images
root.self_and_descendent_images.count
ProductImage.where('assets.id IN (?) || assets.vp_node_id IN (?)', root.self_and_descendents.select(:ref_image_id), root.self_and_descendents.ids)
ProductImage.where('assets.id IN (?) || assets.vp_node_id IN (?)', root.self_and_descendents.select(:ref_image_id), root.self_and_descendents.select(:id)
ProductImage.where('assets.id IN (?) || assets.vp_node_id IN (?)', root.self_and_descendents.select(:ref_image_id), root.self_and_descendents.select(:id))
exit
root = VpNode.first
root.self_and_descendent_images
exit
root = VpNode.first
root.self_and_descendent_images
exit
root = VpNode.first
root.descendent_images.count
infinity > 0
url = 'http://rockeyafrica.com/wp-content/uploads/2014/09/Evolution-Solar-Energy-Los-Angeles-County.png'
hashes = DHasher.hash_from_path(url)
hashes.to_h
hashes
image_obj = Hashie::Mash.new()
hashes.length.times do |i|
  image_obj["dhash#{i}"] = hashes.shift
end
image_obj.slice(:dhash1)
ProductImage.all.sample
image = _
DHasher.distance(image_obj, image)
root.mu
DHasher.distance(image_obj, root.ref_image)
root.descendents
root.ancestry
VpNode.where("ancestry like '#{ancestry}#{id}/'")
VpNode.where("ancestry like '#{root.ancestry}#{root.id}/'")
exit
root = VpNode.first
reload!
root = VpNode.first
root.children
node = root.children.first
node.children
arr
root
nodes_to_visit = [root]
nodes_to_visit.append(root.children)
exit
VpNode.rebuild_tree
VpNode.direction
Identifier.id_type[:asin]
Identifier.id_types[:asin]
exit
VpNode.rebuild_tree
root = VpNode.first
root.left_child
exit
root.left_child
root = VpNode.first
root.left_child
exit
root = VpNode.first
root.left_child
exit
root = VpNode.first
root.left_child
VpNode.last.product_images.count
exit
# url = 'http://rockeyafrica.com/wp-content/uploads/2014/09/Evolution-Solar-Energy-Los-Angeles-County.png'
url = 'http://rockeyafrica.com/wp-content/uploads/2014/09/Evolution-Solar-Energy-Los-Angeles-County.png'
ProductImage.where(original_url: url)
exit
VpNode.rebuild_tree
url = 'http://rockeyafrica.com/wp-content/uploads/2014/09/Evolution-Solar-Energy-Los-Angeles-County.png'
VpNode.find_nearest_neighbors(url, nil)
exit
url = 'http://rockeyafrica.com/wp-content/uploads/2014/09/Evolution-Solar-Energy-Los-Angeles-County.png'
VpNode.find_nearest_neighbors(url, nil)
exit
url = 'http://rockeyafrica.com/wp-content/uploads/2014/09/Evolution-Solar-Energy-Los-Angeles-County.png'
VpNode.find_nearest_neighbors(url, nil)
exit
url = 'http://rockeyafrica.com/wp-content/uploads/2014/09/Evolution-Solar-Energy-Los-Angeles-County.png'
root = VpNode.root_node
hashes = DHasher.hash_from_path(url)
image_obj = Hashie::Mash.new()
hashes.length.times do |i|
  image_obj["dhash#{i}"] = hashes.shift
end
image_obj
image_obj.dhash1
tau = 45
nodes_to_visit = [root]
matches = []
nodes_to_visit.length > 0
node = nodes_to_visit.shift
distance = nil
node.is_leaf?
distance = DHasher.distance(node.ref_image, image_obj)
node.ref_image
image2 = image_obj
exit
url = 'http://rockeyafrica.com/wp-content/uploads/2014/09/Evolution-Solar-Energy-Los-Angeles-County.png'
VpNode.find_nearest_neighbors(url, nil)
exit
url = 'http://rockeyafrica.com/wp-content/uploads/2014/09/Evolution-Solar-Energy-Los-Angeles-County.png'
VpNode.find_nearest_neighbors(url, nil)
ProductImage.find(14)
url = 'http://rockeyafrica.com/wp-content/uploads/2014/09/Evolution-Solar-Energy-Los-Angeles-County.png'
ProductImage.where(original_url: url)
image1 = _
image2 = ProductImage.find 14
require 'd_hasher'
DHasher.distance(image1, image2)
image1
image1 = image1.first
DHasher.distance(image1, image2)
image2.ref_node
node = _
node.children
node.product_image_ids
VpNode.where(ref_image_id: nil)
exit
url = 'http://rockeyafrica.com/wp-content/uploads/2014/09/Evolution-Solar-Energy-Los-Angeles-County.png'
VpNode.find_nearest_neighbors(url, nil)
ProductImage.find 35
Time.now
start = Time.now
Time.now - start
exit
url = 'http://rockeyafrica.com/wp-content/uploads/2014/09/Evolution-Solar-Energy-Los-Angeles-County.png'
VpNode.find_nearest_neighbors(url, nil)
exit
url = 'http://rockeyafrica.com/wp-content/uploads/2014/09/Evolution-Solar-Energy-Los-Angeles-County.png'
VpNode.find_nearest_neighbors(url, nil)
Dhasher.hash_from_path(      distance = nil
DHasher.hash_from_path(url)
exit
url = 'http://rockeyafrica.com/wp-content/uploads/2014/09/Evolution-Solar-Energy-Los-Angeles-County.png'
VpNode.find_nearest_neighbors(url, nil)
exit
url = 'http://rockeyafrica.com/wp-content/uploads/2014/09/Evolution-Solar-Energy-Los-Angeles-County.png'
VpNode.find_nearest_neighbors(url, nil)
exit
VpNode.rebuild_tree
url = 'http://rockeyafrica.com/wp-content/uploads/2014/09/Evolution-Solar-Energy-Los-Angeles-County.png'
VpNode.find_nearest_neighbors(url, nil)
reload!
url = 'http://rockeyafrica.com/wp-content/uploads/2014/09/Evolution-Solar-Energy-Los-Angeles-County.png'
VpNode.find_nearest_neighbors(url, nil)
exit
VpNode.rebuild_tree
url = 'http://rockeyafrica.com/wp-content/uploads/2014/09/Evolution-Solar-Energy-Los-Angeles-County.png'
VpNode.find_nearest_neighbors(url, nil)
exit
url = 'http://rockeyafrica.com/wp-content/uploads/2014/09/Evolution-Solar-Energy-Los-Angeles-County.png'
VpNode.rebuild_tree
VpNode.find_nearest_neighbors(url, nil)
existing_image = ProductImage.find_by(original_url: url)
existing_image.slice(:dhash1, :dhash2, :dhash3, :dhash4)
existing_image.slice(:dhash1, :dhash2, :dhash3, :dhash4).values
existing_image.slice(:dhash1, :dhash2, :dhash3, :dhash4).class
existing_image.slice(:dhash1, :dhash2, :dhash3, :dhash4).dhash1
image_obj = existing_image.slice(:dhash1, :dhash2, :dhash3, :dhash4)
image_obj.dhash1
hashes = DHasher.hash_from_path(url)
image_obj1 = hashes.each_with_object(Hashie::Mash.new) do |hash, obj|
  obj["dhash#{i+1}"] = hash
end
i = 0
image_obj1 = hashes.each_with_object(Hashie::Mash.new) do |hash, obj|
  obj["dhash#{i+=1}"] = hash
end
image_obj1
image_obj1.dhash1
exit
url = 'http://rockeyafrica.com/wp-content/uploads/2014/09/Evolution-Solar-Energy-Los-Angeles-County.png'
VpNode.find_nearest_neighbors(url, nil)
existing_image = ProductImage.find_by(original_url: url)
image_obj = if existing_image.present?
  Hashie::Mash.new(existing_image.slice(:dhash1, :dhash2, :dhash3, :dhash4))
else
  hashes = DHasher.hash_from_path(url)
  i = 0
  hashes.each_with_object(Hashie::Mash.new) do |hash, obj|
    obj["dhash#{i+=1}"] = hash
  end
end
exit
OpenURI::Buffer.send :remove_const, 'StringMax' if OpenURI::Buffer.const_defined?('StringMax')
OpenURI::Buffer.const_set 'StringMax', 0
url = 'http://rockeyafrica.com/wp-content/uploads/2014/09/Evolution-Solar-Energy-Los-Angeles-County.png'
VpNode.find_nearest_neighbors(url, nil)
exit
VpNode.rebuild_tree
url = 'http://rockeyafrica.com/wp-content/uploads/2014/09/Evolution-Solar-Energy-Los-Angeles-County.png'
VpNode.find_nearest_neighbors(url, nil)
exit
url = 'http://rockeyafrica.com/wp-content/uploads/2014/09/Evolution-Solar-Energy-Los-Angeles-County.png'
VpNode.find_nearest_neighbors(url, nil)
VpNode.find_nearest_neighbors(url)
exit
VpNode.find_nearest_neighbors(url, nil)
url = 'http://rockeyafrica.com/wp-content/uploads/2014/09/Evolution-Solar-Energy-Los-Angeles-County.png'
VpNode.find_nearest_neighbors(url)
VpNode.count
exit
OpenURI::StringMax
OpenURI.StringMax
OpenURI::Buffer.StringMax
OpenURI::Buffer::StringMax
exit
ProductImage.update_all(dhash1: nil, dhash2: nil, dhash3: nil, dhash4: nil)
exit
ProductImage.where(dhash1: nil)
exit
include RvxRds::Proxy
require 'proxy'
require 'rvx_rds/proxy'
include RvxRds::Proxy
url = 'http://rockeyafrica.com/wp-content/uploads/2014/09/Evolution-Solar-Energy-Los-Angeles-County.png'
file = OpenURI.open_uri(url)
file.class
require 'd_hasher'
DHasher.hash_from_path(file)
DHasher.hash_from_path(file.path)
ProductImage.all.sample
image = _
image.id
ImageGenerateDhashFixerJob
ImageGenerateDhashFixerJob.new.perform(image.id)
reload!
ImageGenerateDhashFixerJob.new.perform(image.id)
reload!
ImageGenerateDhashFixerJob.new.perform(image.id)
reload
reload!
ImageGenerateDhashFixerJob.new.perform(image.id)
image
reload!
ImageGenerateDhashFixerJob.new.perform(image.id)
GC.start
exit
image = ProductImage.all.sample
image.clear_dhashes
path = image.attachment.url
Magick::Image.read(path) do |image|
  new_size = hash_size + 1
  image.resize!(new_size * 2, new_size * 2)
  top_left = image.excerpt(0, 0, new_size, new_size)
  top_right = image.excerpt(new_size, 0, new_size, new_size)
  bot_left = image.excerpt(0, new_size, new_size, new_size)
  bot_right = image.excerpt(new_size, new_size, new_size, new_size)
  slices = [top_left, top_right, bot_left, bot_right]
  slices.map { |slice| DHasher.hash_from_image(slice, hash_size) }
end
hash_size = 8
Magick::Image.read(path) do |image|
  new_size = hash_size + 1
  image.resize!(new_size * 2, new_size * 2)
  top_left = image.excerpt(0, 0, new_size, new_size)
  top_right = image.excerpt(new_size, 0, new_size, new_size)
  bot_left = image.excerpt(0, new_size, new_size, new_size)
  bot_right = image.excerpt(new_size, new_size, new_size, new_size)
  slices = [top_left, top_right, bot_left, bot_right]
  slices.map { |slice| DHasher.hash_from_image(slice, hash_size) }
end
Magick::Image.read(path) do |image|
  puts image
  p image
end
image = Magick::Image.read(path).first
new_size = hash_size + 1
image.resize!(new_size * 2, new_size * 2)
top_left = image.excerpt(0, 0, new_size, new_size)
top_right = image.excerpt(new_size, 0, new_size, new_size)
bot_left = image.excerpt(0, new_size, new_size, new_size)
bot_right = image.excerpt(new_size, new_size, new_size, new_size)
slices = [top_left, top_right, bot_left, bot_right]
slices
slices.concat(image)
slices << image
slices.each(&:destroy!)
slices
exit
ProductImage.where(dhash1: nil)
id = 2
ImageGenerateDhashFixerJob.new.perform(2)
image
image = ProductImage.find 2
exit
Sidekiq::Client
ProductImage.all.sample
node = VpNode.find 4
ActiveRecord::Base.connection.execute("UPDATE assets set vp_node_id=#{lt_node.id} WHERE assets.id in (#{lt_img_ids.join(',')})")
ActiveRecord::Base.connection.execute("UPDATE assets set vp_node_id=? WHERE assets.id in (?)", node.id, )
ids = [1,2,3,4,5,7,8,9,10]
ActiveRecord::Base.connection.execute("UPDATE assets set vp_node_id=? WHERE assets.id in (?)", node.id, ids)
ActiveRecord::Base.connection.execute("UPDATE assets set vp_node_id=? WHERE assets.id in (?)", [node.id, ids])
ActiveRecord::Base.connection.execute(["UPDATE assets set vp_node_id=? WHERE assets.id in (?)", node.id, ids])
ActiveRecord::Base.connection.exec_query("UPDATE assets set vp_node_id=? WHERE assets.id in (?)", [node.id, ids])
ActiveRecord::Base.connection.exec_query("UPDATE assets set vp_node_id=? WHERE assets.id in (?)", 'myql', [node.id, ids])
ActiveRecord::Base.connection.exec_query("UPDATE assets set vp_node_id=? WHERE assets.id in (?)", 'myql', [node.id, *ids])
ActiveRecord::Base.connection.exec_query("UPDATE assets set vp_node_id=? WHERE assets.id in (?)", 'myql', [node.id, ids.join(','])
ActiveRecord::Base.connection.exec_query("UPDATE assets set vp_node_id=? WHERE assets.id in (?)", 'myql', [node.id, ids.join(',')])
ActiveRecord::Base.connection.exec_query("UPDATE assets set vp_node_id = ? WHERE assets.id in (?)", 'myql', [node.id, ids.join(',')])
node.id
ids
ActiveRecord::Base.connection.exec_query("UPDATE assets set vp_node_id = ? WHERE assets.id in (?)", "SQL", [node.id, ids.join(',')])
ActiveRecord::Base.connection.exec_query('UPDATE assets set vp_node_id = ? WHERE assets.id in (?)', "SQL", [node.id, ids.join(',')])
ActiveRecord::Base.connection.exec_query("UPDATE assets set vp_node_id=#{node.id} WHERE assets.id in (#{ids})")
ActiveRecord::Base.connection.exec_query("UPDATE assets set vp_node_id=#{node.id} WHERE assets.id in (#{ids.join(',')})")
ActiveRecord::Base.connection.execute("UPDATE assets set vp_node_id=#{node.id} WHERE assets.id in (#{ids.join(',')})")
VpNode.destroy_all
ProductImage.all.sample
ProductImage.update_all(vp_node_id: nil)
[*1..10]
[*1..1000000]
arr = _
arr.size
[*1..10000000]
ids = [*1..10000000]
ids.length
ids.each_slice(100000) do |i|
  puts i.count
  puts i.first
  puts i.last
  puts '*'
end
ProductImage.create(original_url: 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/009/732/815/original/open-uri20161104-22544-rw49ca?1478285995')
ProductImage.create(original_url: 'https://s3.amazonaws.com/rvx-rds/images/uploads/010/891/820/original/open-uri20160721-11976-9zrjj2?1478414850')
Digest::MD5.file(.path).hexdigest
image1, image2 = ProductImage.last(2)
Digest::MD5.file(image1.attachment.url).hexdigest
Digest::MD5.file(image1.attachment.path).hexdigest
Digest::MD5.file(image1.image.path).hexdigest
image1path = OpenURI.open_uri(image1.attachment.url)
image1.save_attachment
image1.original_url = image1.attachment.url.gsub('rvx-rds-dev', 'rvx-rds')
image1.original_url = image1.original_url.gsub('rvx-rds-dev', 'rvx-rds')
image1.save
image1.save_attachment
image.original_url
image1.original_url
image1.attachment = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/009/732/815/original/open-uri20161104-22544-rw49ca?1478285995'
image1.attachment = 'https://s3.amazonaws.com/rvx-rds/images/uploads/009/732/815/original/open-uri20161104-22544-rw49ca?1478285995'
image1.attachment = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/009/732/815/original/open-uri20161104-22544-rw49ca?1478285995'
image.original_url = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/009/732/815/original/open-uri20161104-22544-rw49ca?1478285995'
image1.original_url = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/009/732/815/original/open-uri20161104-22544-rw49ca?1478285995'
image1.save_attachment
file1 = OpenURI.open_uri("http://s3.amazonaws.com/rvx-rds-dev/images/uploads/010/873/446/original/open-uri20160721-15544-11w2m24?1478412162")
file2 = OpenUri.open_uri("http://s3.amazonaws.com/rvx-rds-dev/images/uploads/010/873/397/original/open-uri20160721-15574-wxt9fm?1478412001")
file2 = OpenURI.open_uri("http://s3.amazonaws.com/rvx-rds-dev/images/uploads/010/873/397/original/open-uri20160721-15574-wxt9fm?1478412001")
exit
Redis.current.flushall
VpNode.rebuild_tree
Redis.current.flushall
Redis.current
exit
VpNode.rebuild_tree
exit
VpNode.rebuild_tree
VpNode.MAX_IMAGES
VpNode::MAX_IMAGES
root = VpNode.first
root.descendents
root.children.first.descendents
exit
VpNode.rebuild_tree
ProductImage.count
ListingAsset.count
exit
['jon', 233, 'hello'].to_param
exit
VpNode.rebuild_tree
VpNode.root_node
root = VpNode.root_node
root.self_and_descendents
node = _.last
path = node.ancestry.splic('/').map(&:to_i)
path = node.ancestry.split('/').map(&:to_i)
path = node.ancestry.split('/').compact!.map(&:to_i)
path = node.ancestry.split('/').compact.map(&:to_i)
path = node.ancestry.split('/')
path = node.ancestry.split('/').reject(&:blank?)
path = node.ancestry.split('/').reject(&:blank?).map(&:to_i)
include ActiveRecord::Serialization
serialize
include ActiveRecord::AttributeMethods::Serialization
serialize
ActiveRecord::AttributeMethods::Serialization.serialize
ProductImage.serialize
ProductImage.serialize(path)
path
VpNode.where(id: path)
VpNode.includes.where(id: path)
distances = []
VpNode.includes(:ref_image).where(id: path).each do 
require 'DHasher'
require 'd_hasher'
VpNode.includes(:ref_image).where(id: path).each do |vp|
  distances << DHasher.distance(node.ref_image, vp.ref_image)
end
distances
node.ref_image.attachment.url
VpNode.find_nearest_neighbors(node.ref_image.attachment.url)
node
Listing.reindex_alias
exit
Listing.count
Product.count
ProductImage.count
BulkCatgory.all
BulkCategory.pluck(:id, :name)
Category.where(id: SourceCategory.where(id: BulkCategorySourceCategory
    .where(bulk_category_id: [1, 29])
  .select(:source_category_id))
.uniq.select(:category_id))
edit -t
edit
Category.select("SELECT DISTINCT `categories`.* FROM `categories` WHERE `categories`.`id` IN (SELECT `source_categories`.`category_id` FROM `source_categories` WHERE `source_categories`.`id` IN (SELECT `bulk_category_source_categories`.`source_category_id` FROM `bulk_category_source_categories` WHERE `bulk_category_source_categories`.`bulk_category_id` IN (?)))", ids)
Category.connection.exec_sql("SELECT DISTINCT `categories`.* FROM `categories` WHERE `categories`.`id` IN (SELECT `source_categories`.`category_id` FROM `source_categories` WHERE `source_categories`.`id` IN (SELECT `bulk_category_source_categories`.`source_category_id` FROM `bulk_category_source_categories` WHERE `bulk_category_source_categories`.`bulk_category_id` IN (?)))", ids)
Category.find_by_sql("SELECT DISTINCT `categories`.* FROM `categories` WHERE `categories`.`id` IN (SELECT `source_categories`.`category_id` FROM `source_categories` WHERE `source_categories`.`id` IN (SELECT `bulk_category_source_categories`.`source_category_id` FROM `bulk_category_source_categories` WHERE `bulk_category_source_categories`.`bulk_category_id` IN (?)))", ids)
Category.connection.exec_query("SELECT DISTINCT `categories`.* FROM `categories` WHERE `categories`.`id` IN (SELECT `source_categories`.`category_id` FROM `source_categories` WHERE `source_categories`.`id` IN (SELECT `bulk_category_source_categories`.`source_category_id` FROM `bulk_category_source_categories` WHERE `bulk_category_source_categories`.`bulk_category_id` IN (?)))", ids)
Category.connection.select_all("SELECT DISTINCT `categories`.* FROM `categories` WHERE `categories`.`id` IN (SELECT `source_categories`.`category_id` FROM `source_categories` WHERE `source_categories`.`id` IN (SELECT `bulk_category_source_categories`.`source_category_id` FROM `bulk_category_source_categories` WHERE `bulk_category_source_categories`.`bulk_category_id` IN (?)))", ids)
reload!
BulkCategory.categories_by_bulk_ids([1,29])
reload!
BulkCategory.categories_by_bulk_ids([1,29])
reload!
BulkCategory.categories_by_bulk_ids([1,29])
',' << BulkCategory.categories_by_bulk_ids([29]).ids.join(',')
BulkCategory.pluck(:id, :name)
exit
type = 'listings'
cache_params = 'keywords=solar'
in_progress = API_CACHE.fetch("#{type}:#{cache_params}:in_progress")
APIrequire 'rds_cache'
require 'rds_cache'
A  TIMESTAMP_FMT = '%Y%m%d%H%M%S'.freeze unless const_defined?(:TIMESTAMP_FMT)
API_CACHE ||= RdsCache.new('api_cache')
MODELS ||= { 'products' => Product, 'listings' => Listing }
TIMESTAMP_FMT = '%Y%m%d%H%M%S'.freeze unless const_defined?(:TIMESTAMP_FMT)
API_CACHE ||= RdsCache.new('api_cache')
MODELS ||= { 'products' => Product, 'listings' => Listing }
in_progress = API_CACHE.fetch("#{type}:#{cache_params}:in_progress")
in_progress = API_CACHE.fetch("#{type}:#{cache_params}:1")
TIMESTAMP_FMT = '%Y%m%d%H%M%S'.freeze unless const_defined?(:TIMESTAMP_FMT)
TIMESTAMP_FMT = '%Y%m%d%H%M%S'
cache = RdsCache.new('api_cache')
keys = cache.keys
pairs = keys.map { |k| k.split(':')[2..3] }.uniq
type_params = Hash.new { |h, k| h[k] = [] }
pairs.each do |pair|
  type_params[pair[0]] << pair[1]
end
type, params_list = type_params.first
cache_params = params_list.first
params = CGI.parse(cache_params)
params = Hashie::Mash.new(params.map { |k, str| [k, str.first] }.to_h.sort.to_h.symbolize_keys)
key = "#{type}:#{params.to_param}"
last_seen_at = cache.fetch("#{key}:last_seen")
params = Oj.dump(params.merge!({ type: type }))
cache_params
PER_PAGE ||= ENV['CACHE_PAGE_SIZE'].try(:to_i) || 200
API_CACHE ||= RdsCache.new('api_cache')
MODELS ||= { 'products' => Product, 'listings' => Listing }
params = Hashie::Mash.new(Oj.load(params))
type = params[:type]
model = MODELS[type]
prefix = "#{type}:#{cache_params}"
page_count, scroll_id, search_params, search_response, total = nil
id_proc = if type == 'listings'.freeze
  proc { |l| l['_id'] }
elsif type == 'products'.freeze
  proc { |l| l['_source']['products'].map { |p| p['id'] } }
end
bulk_ids = BulkCategory.categories_by_bulk_ids(params['bulk_category_ids'].split(',').reject(&:blank?)).ids.join(',')
params
page = params[:page] || 1
per_page = params[:per_page] || 10
page_count, scroll_id, search_params, search_response, total = nil
search_params ||= Listing.prepare_search_params(params)
search_response = Listing.search_by(*search_params, { size: PER_PAGE, scroll: '3m'.freeze, type: type }).response
scroll_id = search_response['_scroll_id']
total ||= search_response['hits']['total']
page_count = search_response['hits']['hits'].count
ids = search_response['hits']['hits'].map! { |l| id_proc.call(l) }
ids.flatten!
ids.compact!
search_response = nil
API_CACHE.sadd("#{API_CACHE.prefix}:#{prefix}:ids", ids) unless ids.empty?
PER_PAGE ||= ENV['CACHE_PAGE_SIZE'].try(:to_i) || 200
API_CACHE ||= RdsCache.new('api_cache')
MODELS ||= { 'products' => Product, 'listings' => Listing }
page_count && page_count < PER_PAGE
Listing.clear_scroll! scroll_id
[cache_params, model, params, prefix, type]
serializer = Object.const_get("#{model}Serializer")
source_ids = params[:source_ids].try(:split, ',')
scrape_ids = Scrape.where(source_id: source_ids).uniq.pluck(:id)
total = API_CACHE.scard("#{API_CACHE.prefix}:#{prefix}:ids")
pages = (total / PER_PAGE.to_f).ceil
page = 1
logger.warn "PopulateApiCacheJob: #{type}/#{cache_params}/#{page}" if page % 10 == 0
key = "#{prefix}:#{page}"
body = { type => [] }
ids = API_CACHE.spop("#{API_CACHE.prefix}:#{prefix}:ids", PER_PAGE)
model.where(id: ids).include_api_info(scrape_ids).find_in_batches(batch_size: 50) do |records|
  body[type] |= records.map! { |r| serializer.new(r).as_json(root: false) }
end
body
meta_data = { pagination: { page: page, per_page: PER_PAGE, total: total }, version: 1 }
API_CACHE.set_value(key, body.merge!(meta_data))
body = nil
API_CACHE.clear_cache!("#{prefix}:in_progress")
API_CACHE.clear_cache!("#{prefix}:ids")
exit
BulkCategory.find 1
BulkCategory.categories_by_bulk_ids(1)
BulkCategory.categories_by_bulk_ids([1])
ids = 1
BulkCategory.categories_by_bulk_ids(*1)
BulkCategory.categories_by_bulk_ids(1)
BulkCategory.categories_by_bulk_ids(ids)
BulkCategory.categories_by_bulk_ids(*ids)
BulkCategory.categories_by_bulk_ids([1])
BulkCategory.first
key = :bulk_category_ids
model = key.to_s.gsub('_id', '')
model = key.to_s.gsub('_ids', '')
model.classify
model = key.to_s.gsub('_id', '').classify.constantize
key = :jonahtan_ids
model = key.to_s.gsub('_id', '').classify.constantize
params
params = {}
params[key] = '1,2,29'
key = :bulk_category_id
key = :bulk_category_ids
model.where(id: params[key]).count
model.where(id: ids).count
ids
ids = params[key].split(',').uniq
key
params[key] = '1,2,29'
ids = params[key].split(',').uniq
model
model.where(id: ids).count
ids.length
ids.to_s
"#{ids}"
ids
ids.to_param
"#{key}:#{params[key]}"
errors = []
[:bulk_category_ids, :category_ids, :scrape_ids, :source_ids].each do |key|
  params
params
params.delete(:jonahtan)
params
params.delete(:jonahtan_ids)
params
errors = []
[:bulk_category_ids, :category_ids, :scrape_ids, :source_ids].each do |key|
  model = key.to_s.gsub('_id', '').classify.constantize
  ids = params[key].split(',').uniq
  unless model.where(id: ids).count == ids.length
    errors << "#{key}:#{params[key]}"
  end
end
[:bulk_category_ids, :category_ids, :scrape_ids, :source_ids].each do |key|
  next unless params[key].present?
  model = key.to_s.gsub('_id', '').classify.constantize
  ids = params[key].split(',').uniq
  unless model.where(id: ids).count == ids.length
    errors << "#{key}:#{params[key]}"
  end
end
error_msg = "Invalid ids present for: #{errors.join(' -- ')}"
errors = []
[:bulk_category_ids, :category_ids, :scrape_ids, :source_ids].each do |key|
  next unless params[key].present?
  model = key.to_s.gsub('_id', '').classify.constantize
  ids = params[key].split(',').uniq
  unless model.where(id: ids).count == ids.length
    errors << "#{key}:#{params[key]}"
  end
end
error_msg = "Invalid ids present for: #{errors.join(' -- ')}"
error_msg = "Invalid ids present (#{errors.join(') (')})"
params = {}
key = :bulk_category_ids
params[key] = '1,2,29'
errors = []
model = key.to_s.gsub('_id', '').classify.constantize
ids = params[key].split(',').uniq
record_ids = model.where(id: ids).ids
record_ids.length == ids.length
ids.map!(&:to_i)
ids - record_ids
"#{key}:#{record_ids}"
"#{key}:#{ids - record_ids}"
errors = []
[:bulk_category_ids, :category_ids, :scrape_ids, :source_ids].each do |key|
  next unless params[key].present?
  model = key.to_s.gsub('_id', '').classify.constantize
  ids = params[key].split(',').uniq
  record_ids = model.where(id: ids).ids
  unless  record_ids.length == ids.length
    ids.map!(&:to_i)
    errors << "#{key}:#{ids - record_ids}"
  end
end
error_msg = "Some : (#{errors.join(') (')})"
error_msg = "Invalid IDs: (#{errors.join(') (')})"
params[:category_ids] = '1000,284532'
errors = []
[:bulk_category_ids, :category_ids, :scrape_ids, :source_ids].each do |key|
  next unless params[key].present?
  model = key.to_s.gsub('_id', '').classify.constantize
  ids = params[key].split(',').uniq
  record_ids = model.where(id: ids).ids
  unless  record_ids.length == ids.length
    ids.map!(&:to_i)
    errors << "#{key}:#{ids - record_ids}"
  end
end
error_msg = "Invalid IDs: (#{errors.join(') (')})"
raise 404, error_msg if errors.present?
def validate_params(params)
  errors = []
  [:bulk_category_ids, :category_ids, :scrape_ids, :source_ids].each do |key|
    next unless params[key].present?
    model = key.to_s.gsub('_id', '').classify.constantize
    ids = params[key].split(',').uniq
    record_ids = model.where(id: ids).ids
    unless  record_ids.length == ids.length
      ids.map!(&:to_i)
      errors << "#{key}:#{ids - record_ids}"
    end
  end
  "Invalid IDs: (#{errors.join(') (')})" if errors.present?
end
validate_params(params)
parmas = { bulk_category_ids: '29' }
params = { bulk_category_ids: '29' }
validate_params(params)
def validate_params(params)
  errors = []
  [:bulk_category_ids, :category_ids, :scrape_ids, :source_ids].each do |key|
    next unless params[key].present?
    model = key.to_s.gsub('_id', '').classify.constantize
    ids = params[key].split(',').uniq
    record_ids = model.where(id: ids).ids
    unless  record_ids.length == ids.length
      ids.map!(&:to_i)
      errors << "#{key}:#{ids - record_ids}"
    end
  end
  error! "Invalid IDs: (#{errors.join(') (')})", 404 if errors.present?
end
validate_params(params)
key
params[key] << '1'
validate_params(params)
exit
node_id = 1
VpNode.destroy_all
cd VpNode
connection.execute("truncate table #{quoted_table_name}")
root_node = offset(rand(count)).first || create(ancestry: '/')
node_id = 1
exit
node = VpNode.find(node_id)
images = ProductImage.where(vp_node_id: node_id)
image = node.ref_image
ProductImage.where.not(dhash1: nil).offset(rand(ProductImage.where.not(dhash1: nil).count)).first
image = _
node.update(ref_image: image)
image.update(vp_node_id: nil)
middle_count = images.empty? ? (ProductImage.where.not(dhash1: nil).count / 2) : (images.count / 2)
job = BalanceVpNode.new
reload!
job = BalanceVpNode.new
exit
job = BalanceVpNode.new
job = BalanceVpNodeJob.new
cd job
node_id = 1
node = VpNode.find(node_id)
images = ProductImage.where(vp_node_id: node_id)
image = node.ref_image
middle_count = images.empty? ? (ProductImage.where.not(dhash1: nil).count / 2) : (images.count / 2)
median_query = build_median_query(image, node_id, middle_count); nil
median_query
root = true
median_query = build_median_query(image, node_id, middle_count, true); nil
median_query
median_score = ProductImage.connection.select_all(median_query).first['hd']
node.update(mu: median_score)
ancestry = node.ancestry.blank? ? "#{node.id}" : "#{node.ancestry}#{node.id}/"
lt_query = build_query(node_id, image, median_score, :left, root)
remove_duplicates(image, lt_query) # reassign listings images that are identical and delete duplicates
query = lt_query
dir = :left
lt_node = VpNode.create(parent_id: node.id, ancestry: ancestry, direction: VpNode.directions[dir])
query
ActiveRecord::Base.connection.execute(
  "UPDATE assets set vp_node_id=#{lt_node.id} WHERE assets.id in (#{query})"
)
query
relaod!
reload!
relaod
reload!
exit
eixt
exit
cd VpNode
update_all(parent_id: nil, ref_image_id: nil)
ProductImage.update_all(vp_node_id: nil)
destroy_all
connection.execute("truncate table #{quoted_table_name}")
root_node = offset(rand(count)).first || create(ancestry: '/')
cd
cd BalanceVpNodeJob
exit
job = BalanceVpNodeJob.new
cd job
node_id = 1
node = VpNode.find(node_id)
images = ProductImage.where(vp_node_id: node_id)
image = node.ref_image
if node.ref_image.nil?
  image = if images.count.zero?
    ProductImage.where.not(dhash1: nil).offset(rand(ProductImage.where.not(dhash1: nil).count)).first
  else
    images.offset(rand(images.count)).first
  end
  node.update(ref_image: image)
  image.update(vp_node_id: nil)
end
return unless images.count == 0 || images.count > VpNode::MAX_IMAGES
middle_count = images.empty? ? (ProductImage.where.not(dhash1: nil).count / 2) : (images.count / 2)
median_query = build_median_query(image, node_id, middle_count, root); nil
median_score = ProductImage.connection.select_all(median_query).first['hd']
root = false
root = true
node = VpNode.find(node_id)
images = ProductImage.where(vp_node_id: node_id)
# find median distance based on ref_image's dhashs
image = node.ref_image
if node.ref_image.nil?
  image = if images.count.zero?
    ProductImage.where.not(dhash1: nil).offset(rand(ProductImage.where.not(dhash1: nil).count)).first
  else
    images.offset(rand(images.count)).first
  end
  node.update(ref_image: image)
  image.update(vp_node_id: nil)
end
return unless images.count == 0 || images.count > VpNode::MAX_IMAGES
middle_count = images.empty? ? (ProductImage.where.not(dhash1: nil).count / 2) : (images.count / 2)
median_query = build_median_query(image, node_id, middle_count, root); nil
median_query
median_score = ProductImage.connection.select_all(median_query).first['hd']
node.update(mu: median_score)
ancestry = node.ancestry.blank? ? "#{node.id}" : "#{node.ancestry}#{node.id}/"
lt_query = build_query(node_id, image, median_score, :left, root)
remove_duplicates(image, lt_query) # reassign listings images that are identical and delete duplicates
query = lt_query
dir = :left
lt_node = VpNode.create(parent_id: node.id, ancestry: ancestry, direction: VpNode.directions[dir])
query
ActiveRecord::Base.connection.execute(
  "UPDATE assets set vp_node_id=#{lt_node.id} WHERE assets.id in (#{query})"
)
query
query = "SELECT scores.id FROM ( SELECT id, HAMMINGDISTANCE( 6492880430487591291, 2025579254432177172, 10788155592114211145, 10128845577213746204, dhash1, dhash2, dhash3, dhash4 ) AS hd FROM assets WHERE dhash1 IS NOT NULL AND id != 7 ) AS scores WHERE scores.hd < 128 ORDER BY scores.hd ASC"
"UPDATE assets set vp_node_id=#{lt_node.id} WHERE assets.id in (#{query})"
ActiveRecord::Base.connection.execute(
  "UPDATE assets set vp_node_id=#{lt_node.id} WHERE assets.id in (#{query})"
)
lt_node.id
ProductImage.where(vp_node_id: 2).count
ProductImage.count
ActiveRecord::Base.connection_pool.with_connection do |conn|
  conn.execute(
    "UPDATE assets set vp_node_id=#{lt_node.id} WHERE assets.id in (#{query})"
  )
end
query
count_query = "SELECT COUNT(scores.id) FROM ( SELECT id, HAMMINGDISTANCE( 6492880430487591291, 2025579254432177172, 10788155592114211145, 10128845577213746204, dhash1, dhash2, dhash3, dhash4 ) AS hd FROM assets WHERE dhash1 IS NOT NULL AND id != 7 ) AS scores WHERE scores.hd < 128 ORDER BY scores.hd ASC"
ActiveRecord::Base.connection_pool.with_connection {|conn| connection.execute(count_query) }
ActiveRecord::Base.connection_pool.with_connection {|conn| conn.execute(count_query) }
_.first
ActiveRecord::Base.connection_pool.with_connection {|conn| conn.execute(count_query).first }
ActiveRecord::Base.connection_pool.with_connection {|conn| conn.execute(count_query).result }
count = ActiveRecord::Base.connection_pool.with_connection {|conn| conn.execute(count_query) }
count
count = ActiveRecord::Base.connection_pool.with_connection {|conn| conn.execute(count_query).try(:first).try(:first) }
count_query
query
ActiveRecord::Base.connection.execute("SELECT COUNT(#{query})")
ActiveRecord::Base.connection.execute("#{query}")
ActiveRecord::Base.connection.execute("#{query}").first
query
reload!
exit
reload!
job = BalanceVpNodeJob.new
cd job
root = true
VpNode.count
node_id = VpNode.first.id
image = node.ref_image
node = VpNode.find(node_id)
images = ProductImage.where(vp_node_id: node_id)
image = node.ref_image
middle_count = images.empty? ? (ProductImage.where.not(dhash1: nil).count / 2) : (images.count / 2)
median_query = build_median_query(image, node_id, middle_count, root); nil
median_query
median_score = ProductImage.connection.select_all(median_query).first['hd']
node.update(mu: median_score)
ancestry = node.ancestry.blank? ? "#{node.id}" : "#{node.ancestry}#{node.id}/"
dir = :left
where_clause = "scores.hd #{dir == :left ? :< : :>=} #{median_score}"
where_clause << " AND scores.id IN (SELECT id FROM assets WHERE vp_node_id = #{node_id})" unless root
where_clause
"SELECT
         scores.id
     FROM (
         SELECT
           id,
           HAMMINGDISTANCE(
             #{[img.dhash1, img.dhash2, img.dhash3, img.dhash4].join(', ')}, dhash1, dhash2, dhash3, dhash4
           ) AS hd
         FROM assets
         WHERE dhash1 IS NOT NULL AND id != #{img.id}
         ) AS scores
     WHERE #{where_clause}
     ORDER BY scores.hd ASC
     ".squish!
img = image
query =     "SELECT
         scores.id
     FROM (
         SELECT
           id,
           HAMMINGDISTANCE(
             #{[image.dhash1, image.dhash2, image.dhash3, image.dhash4].join(', ')}, dhash1, dhash2, dhash3, dhash4
           ) AS hd
         FROM assets
         WHERE dhash1 IS NOT NULL AND id != #{img.id}
         ) AS scores
     WHERE #{where_clause}
     ORDER BY scores.hd ASC
     ".squish!
query
ancestry
node
dir
ActiveRecord::Base.connection.execute("SELECT COUNT(assets.id) from assets WHERE assets.id in (#{query})").first.first
ProductImage.connection.with_connection {|conn| conn.execute(query).to_ary }
exit
cd job
image = VpNode.find(1).ref_image
dup_image_ids = []
dup_query = "SELECT
         scores.id
     FROM (
         SELECT
           id,
           HAMMINGDISTANCE(
             #{[image.dhash1, image.dhash2, image.dhash3, image.dhash4].join(', ')}, dhash1, dhash2, dhash3, dhash4
           ) AS hd
         FROM assets
         WHERE dhash1 IS NOT NULL AND id != #{image.id}
         ) AS scores
     WHERE scores.hd = 0
     ORDER BY scores.hd ASC
     ".squish!
dup_query
lt_images = ProductImage.connection.select_all(dup_query).to_ary; nil
lt_images
iamge
image
ProductImage.create(original_url: image.attachment.url)
image
iamge.attachment.url
image.attachment.url
image.save_attachment
image.attachment.url
image2 = ProductImage.create(original_url: image.attachment.url)
image2.save_attachment
image2
dup_image_ids = []
dup_query = "SELECT
         scores.id
     FROM (
         SELECT
           id,
           HAMMINGDISTANCE(
             #{[image.dhash1, image.dhash2, image.dhash3, image.dhash4].join(', ')}, dhash1, dhash2, dhash3, dhash4
           ) AS hd
         FROM assets
         WHERE dhash1 IS NOT NULL AND id != #{image.id}
         ) AS scores
     WHERE scores.hd = 0
     ".squish!
lt_images = ProductImage.connection.select_all(dup_query).to_ary; nil
lt_images
image.id
dup_images.map {|img| img['id'] }
dup_images = lt_images
dup_images.map {|img| img['id'] }
dup_images << {'id' => 7 }
ListingAsset.where(asset_id: dup_image_ids)
ListingAsset.where(asset_id: [7])
dup_images.map! {|img| img['id'] }
dup_images
ListingAsset.where(asset_id: dup_image_ids)
ListingAsset.where(asset_id: dup_images)
dup_images = ProductImage.connection.select_all(dup_query).to_ary; nil
dup_images
dup_images << {'id' => 7}
ListingAsset.where(asset_id: dup_images)
exit
cd VpNode
update_all(parent_id: nil, ref_image_id: nil)
ProductImage.update_all(vp_node_id: nil)
destroy_all
connection.execute("truncate table #{quoted_table_name}")
root_node = offset(rand(count)).first || create(ancestry: '/')
cd
cd BalanceVpNodeJob.new
node_id = 1
node = VpNode.find(node_id)
images = ProductImage.where(vp_node_id: node_id)
image = node.ref_image
image = if images.count.zero?
  ProductImage.where.not(dhash1: nil).offset(rand(ProductImage.where.not(dhash1: nil).count)).first
else
  images.offset(rand(images.count)).first
end
node.update(ref_image: image)
image.update(vp_node_id: nil)
middle_count = images.empty? ? (ProductImage.where.not(dhash1: nil).count / 2) : (images.count / 2)
median_query = build_median_query(image, node_id, middle_count, root); nil
root = false
median_query = build_median_query(image, node_id, middle_count, root); nil
median_query
root = true
median_query = build_median_query(image, node_id, middle_count, root); nil
median_query
median_score = ProductImage.connection.select_all(median_query).first['hd']
node.update(mu: median_score)
ancestry = node.ancestry.blank? ? "#{node.id}" : "#{node.ancestry}#{node.id}/"
lt_query = build_query(node_id, image, median_score, :left, root)
query = _
conn = ActiveRecord::Base.connection
count = conn.execute("SELECT COUNT(assets.id) from assets WHERE assets.id in (#{query})").try(:first).try(:first)
ProductImage.count
35 - 14
lt_node = VpNode.create(parent_id: node.id, ancestry: ancestry, direction: VpNode.directions[dir])
dif = :left
lt_node = VpNode.create(parent_id: node.id, ancestry: ancestry, direction: VpNode.directions[dir])
dir = :left
lt_node = VpNode.create(parent_id: node.id, ancestry: ancestry, direction: VpNode.directions[dir])
conn.execute("UPDATE assets set vp_node_id=#{lt_node.id} WHERE assets.id in (#{query})")
ProductImage.where(vp_node_id: 2)
ProductImage.where(vp_node_id: 2).count
rt_query = build_query(node_id, image, median_score, :right, root)
query = _
lt_query
count = conn.execute("SELECT COUNT(assets.id) from assets WHERE assets.id in (#{query})").try(:first).try(:first)
18 + 14
ProductImage.count
ProductImage.where(dhash1: nil).count
ProductImage.where(dhash1: nil).each(&:save_attachment)
img1 = ProductImage.where(dhash1: nil).first
img.destroy
img1.destroy
img1 = ProductImage.where(dhash1: nil).first
img1.save_attachment
img1
exit
VpNode.rebuild_tree
exit
Redis.current.flushall
cd VpNode
update_all(parent_id: nil, ref_image_id: nil)
ProductImage.update_all(vp_node_id: nil)
destroy_all
connection.execute("truncate table #{quoted_table_name}")
root_node = offset(rand(count)).first || create(ancestry: '/')
exit
cd BalanceVpNodeJob.new
node_id = 1
root = true
node = VpNode.find(node_id)
images = ProductImage.where(vp_node_id: node_id)
# find median distance based on ref_image's dhashs
image = node.ref_image
if node.ref_image.nil?
  image = if images.count.zero?
    ProductImage.where.not(dhash1: nil).offset(rand(ProductImage.where.not(dhash1: nil).count)).first
  else
    images.offset(rand(images.count)).first
  end
  node.update(ref_image: image)
  image.update(vp_node_id: nil)
end
return unless images.count == 0 || images.count > VpNode::MAX_IMAGES
middle_count = images.empty? ? (ProductImage.where.not(dhash1: nil).count / 2) : (images.count / 2)
median_query = build_median_query(image, node_id, middle_count, root); nil
median_score = ProductImage.connection.select_all(median_query).first['hd']
# update mu for this node
node.update(mu: median_score)
ancestry = node.ancestry.blank? ? "#{node.id}" : "#{node.ancestry}#{node.id}/"
VpNode.count
node
lt_query = build_child_query(node_id, image, median_score, :left, root)
query = lt_query
dir = :left
"UPDATE assets set vp_node_id=#{lt_node.id} WHERE assets.id in (SELECT id FROM (#{query})) AS tmp)"
lt_node = VpNode.create(parent_id: node.id, ancestry: ancestry, direction: VpNode.directions[dir])
"UPDATE assets set vp_node_id=#{lt_node.id} WHERE assets.id in (SELECT id FROM (#{query})) AS tmp)"
median_
median_score
"SELECT
         scores.id
     FROM (
         SELECT
           id,
           HAMMINGDISTANCE(
             #{[image.dhash1, image.dhash2, image.dhash3, image.dhash4].join(', ')}, dhash1, dhash2, dhash3, dhash4
           ) AS hd
         FROM assets as a
         WHERE dhash1 IS NOT NULL AND id != #{image.id}
         ) AS scores
     WHERE #{where_clause}
     ORDER BY scores.hd ASC
     ".squish!
where_clause = "scores.hd #{dir == :left ? :< : :>=} #{median_score}"
"SELECT
         scores.id
     FROM (
         SELECT
           id,
           HAMMINGDISTANCE(
             #{[image.dhash1, image.dhash2, image.dhash3, image.dhash4].join(', ')}, dhash1, dhash2, dhash3, dhash4
           ) AS hd
         FROM assets as a
         WHERE dhash1 IS NOT NULL AND id != #{image.id}
         ) AS scores
     WHERE #{where_clause}
     ORDER BY scores.hd ASC
     ".squish!
query = _
"UPDATE assets set vp_node_id=#{lt_node.id} WHERE assets.id in (SELECT tmp.id FROM (#{query})) AS tmp)"
conn
conn = ActiveRecord::Base.connection
conn.execute("UPDATE assets set vp_node_id=#{lt_node.id} WHERE assets.id in (SELECT tmp.id FROM (#{query})) AS tmp)")
where_clause = "scores.hd #{dir == :left ? :< : :>=} #{median_score}"
query =     "SELECT
         scores.id
     FROM (
         SELECT
           a1.id,
           HAMMINGDISTANCE(
             #{[image.dhash1, image.dhash2, image.dhash3, image.dhash4].join(', ')}, a1.dhash1, a1.dhash2, a1.dhash3, a1.dhash4
           ) AS hd
         FROM assets as a1
         WHERE a1.dhash1 IS NOT NULL AND a1.id != #{image.id}
         ) AS scores
     WHERE #{where_clause}
     ORDER BY scores.hd ASC
     ".squish!
query
"UPDATE assets set vp_node_id=#{lt_node.id} WHERE assets.id in (SELECT tmp.id FROM (#{query})) AS tmp)"
conn.execute("UPDATE assets set vp_node_id=#{lt_node.id} WHERE assets.id in (SELECT tmp.id FROM (#{query})) AS tmp)")
conn.execute("UPDATE assets set vp_node_id=#{lt_node.id} WHERE assets.id in (SELECT tmp.id FROM (#{query}) AS tmp)")
ProductImage.where(vp_node_id: lt_node.id)
ProductImage.where(vp_node_id: lt_node.id).count
exit
VpNode.rebuild_tree
node = VpNode.first
node.self_and_descendent
node.self_and_descendents
node = _.first
node_id, median_score, image = node.id, node.mu, node.ref_image
node.ref_image
image = node.ref_image
node.update(ref_image: image)
node.image
node.ref_image
node.images
node.product_images
exit
VpNode.rebuild_tree
VpNode.first
VpNode.rebuild_tree
VpNode.first
VpNode.first.self_and_descendents
exit
VpNode.all
node = VpNode.find 2
node.self_and_descendents
node.ancestry
reload!
node = VpNode.find 2
node.self_and_descendents
exit
VpNode.rebuild_tree
VpNode.count
ProductImage.count
exit
VpNode.rebuild_tree
VpNode.count
VpNode.all
node = VpNode.last
VpNode.product_images
node.product_images
node.product_images.count
exit
VpNode.rebuild_tree
VpNode.count
exit
VpNode.count
VpNode.rebuild_tree
exit
VpNode
VpNode.first
VpNode.all
node = VpNode.first
node_id = 1
node = VpNode.find(node_id)
images = ProductImage.connection.select_all(query).to_ary; nil
query = "SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE vp_node_id=#{node_id}"
images = ProductImage.connection.select_all(query).to_ary; nil
images.count
image = node.ref_image
images = ProductImage.connection.select_all(query).to_ary; nil
query = "SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE dhash1 NOT NULL"
images = ProductImage.connection.select_all(query).to_ary; nil
query = 'SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE dhash1 IS NOT NULL'
images = ProductImage.connection.select_all(query).to_ary; nil
images.count
image = node.ref_image
Redis.current.flushall
eixt
exit
VpNode.rebuild_tree
VpNode.count
VpNode.rebuild_tree
VpNode.count
ProductImage.pluck(:vp_node_id)
ProductImage.first
VpNode.first
ProductImage.pluck(:id, :vp_node_id)
ProductImage.sample
ProductImage.all.sample
image = _
VpNode.find_nearest_neighbors(image.attachment.url)
image.id
ProductImage.where(id: [2, 1]).map {|img| img.attachment.url}
exit
ProductImage.where(dhash1: nil)
VpNode.rebuild_tree
VpNode.all
node = VpNode.find(4)
node_id = 4
query = "SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE vp_node_id=#{node_id}"
images = ProductImage.connection.select_all(query).to_ary; nil
images.length
images
ProductImage.find 10
query = "SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE dhash1 IS NOT NULL AND vp_node_id=#{node_id}"
images = ProductImage.connection.select_all(query).to_ary; nil
images
Redis.current.flushall
Asset.find 10
exit
VpNode.rebuild_tree
VpNode.count
VpNode.all
image = ProductImage.all.sample
image = _
VpNode.find_nearest_neighbors(image.attachment.url)
image.attachment.url
image.save_attachment
VpNode.find_nearest_neighbors(image.attachment.url)
image
image = ProductImage.find 1
VpNode.find_nearest_neighbors(image.attachment.url)
exit
VpNode.rebuild_tree
VpNode.count
VpNode.all
VpNode.rebuild_tree
VpNode.count
VpNode.all
ProductImage.count
iamge = ProductImage.all.sample
image = _
ProductImage.create(original_url: image.attachment.url)
image
image = ProductImage.all.sample
ProductImage.create(original_url: image.attachment.url)
img = _
img.save_attachment
image
image.save_attachment
img.original_url = image.attachment.url
img.save
img
img.save_attachment
ProductImage.count
VpNode.rebuild_tree
ProductImage.count
img
image
DHasher.distance(img, image)
VpNode.rebuild_tree
ProductImage.count
VpNode.rebuild_tree
image
img
img.reload!
img.reload
image
img.ref_node
node = _
node.self_and_descendent_images
images = _
image = img
images_with_distances = []
images.count.times do |_idx|
  image2 = images.pop
  images_with_distances << { id: image2['id'], hd: DHasher.distance(image, image2) }
end
images_with_distances.sort_by! { |img| img[:hd] }; nil
images.to_a!
images = images.to_a
images
image.class
images.class
images_with_distances
images.count.times do |_idx|
  image2 = images.pop
  images_with_distances << { id: image2['id'], hd: DHasher.distance(image, image2) }
end
images_with_distances
node.self_and_descendent_images.count
images_with_distances.sort_by! { |img| img[:hd] }; nil
images_with_distances
mid = (images_with_distances.length - 1) / 2.0
median = (images_with_distances[mid.floor][:hd] + images_with_distances[mid.ceil][:hd]) / 2.0
mid
median
mid.floor
mid.ceil
image
images_with_distances
image.id
dup_image_ids = []
images
node
node = VpNode.find 14
exit
VpNode.rebuild_tree
ProductImage.count
image
img
image = ProductImage.last
node = image.ref_node
exit
VpNode.count
ProductImage.count
Redis.current.flushall
VpNode.rebuild_tree
ProductImage.count
image = ProductImage.last
image.ref_node
node = _
images = node.self_and_descendent_images.to_a
images_with_distances = []
images.count.times do |_idx|
  image2 = images.pop
  next if image2['id'] == image['id'] # Don't add the ref_image
  images_with_distances << { id: image2['id'], hd: DHasher.distance(image, image2) }
end
images_with_distances
image.id
images_with_distances.sort_by! { |img| img[:hd] }; nil
mid = (images_with_distances.length - 1) / 2.0
median = (images_with_distances[mid.floor][:hd] + images_with_distances[mid.ceil][:hd]) / 2.0
node.mu
node
node.children
VpNode.all
node.images
node.product_images
node.product_images.count
node.ref_image
exit
VpNode.rebuild_tree
ProductImage.count
image = ProductImage.last
image.vp_node
image.leaf_node
node = _
image = node.ref_image
node.images
node.product_images
node.children
images = node.product_images
dup_image_ids = []
node.ref_image
images = node.product_images.map {|img| { id: img.id, hd: DHasher.distance(image, img) } }
dup_image_ids = []
j    images.length.times do |idx|
  break if images.first[:hd] > 0
  dup_image = images.shift
  dup_image_ids << dup_image[:id]
end
images.length.times do |idx|
  break if images.first[:hd] > 0
  dup_image = images.shift
  dup_image_ids << dup_image[:id]
end
dup_image_ids
images
exit
VpNode.rebuild_tree
VpNode.count
ProductImage.count
VpNode.all
VpNode.rebuild_tree
VpNode.all
VpNode.rebuild_tree
VpNode.all
exit
VpNode.rebuild_tree
VpNode.count
VpNode.all
image = ProductImage.all.sample
image.attachment.url
image.save_attachment
image.attachment.url
VpNode.find_nearest_neighbors
VpNode.find_nearest_neighbors(image.attachment.url)
node = VpNode.find 15
node.ref_image
reload!
VpNode.find_nearest_neighbors(image.attachment.url)
node = node.find 10
node = VpNode.find 10
node.children
reload!
VpNode.find_nearest_neighbors(image.attachment.url)
reload!
VpNode.find_nearest_neighbors(image.attachment.url)
reload!
VpNode.find_nearest_neighbors(image.attachment.url)
VpNode.all
url = image.attachment.url
cd VpNode
url = image.attachment.url
url = "http://s3.amazonaws.com/rvx-rds-dev/development/images/uploads/000/000/019/original/open-uri20161115-47734-1uocjm0?1479241170"
existing_image = ProductImage.find_by(original_url: url)
hashes = DHasher.hash_from_path(url)
i = 0
hashes.each_with_object(Hashie::Mash.new) do |hash, obj|
  obj["dhash#{i+=1}"] = hash
end
image_obj = _
root = VpNode.root_node
tau = 45
nodes_to_visit = [root]
matches = []
nodes_to_visit.length > 0
root.left_child
node = nodes_to_visit.shift
distance = DHasher.distance(node.ref_image, image_obj)
distance < tau
node.is_leaf?
distance - tau <= node.mu
nodes_to_visit.append(node.left_child)
distance + tau >= node.mu
nodes_to_visit.append(node.right_child)
nodes_to_visit
node = nodes_to_visit.shift
distance = DHasher.distance(node.ref_image, image_obj)
distance < tau
node.is_leaf?
nodes_to_visit.append(node.left_child) if distance - tau <= node.mu
nodes_to_visit
nodes_to_visit.append(node.right_child) if distance + tau >= node.mu
nodes_to_visit
node = nodes_to_visit.shift
distance = DHasher.distance(node.ref_image, image_obj)
distance < tau
node.is_leaf?
nodes_to_visit.append(node.left_child) if distance - tau <= node.mu
distance + tau >= node.mu
nodes_to_visit.append(node.right_child) if distance + tau >= node.mu
node = nodes_to_visit.shift
distance = DHasher.distance(node.ref_image, image_obj)
node.is_leaf?
nodes_to_visit.append(node.left_child) if distance - tau <= node.mu
nodes_to_visit.append(node.right_child) if distance + tau >= node.mu
node = nodes_to_visit.shift
distance = DHasher.distance(node.ref_image, image_obj)
distance <= tau
matches << node.ref_image_id
node.is_leaf?
distance - tau <= node.mu
distance + tau >= node.mu
nodes_to_visit.append(node.left_child) if distance - tau <= node.mu
nodes_to_visit.append(node.right_child) if distance + tau >= node.mu
nodes_to_visit
node = nodes_to_visit.shift
distance = DHasher.distance(node.ref_image, image_obj)
node.is_leaf?
nodes_to_visit.append(node.left_child) if distance - tau <= node.mu
nodes_to_visit
node.left_child
node.right_child
reload!
exit
reload!
image
VpNode.find_nearest_neighbors(image.attachment.url)
exit
VpNode.rebuild_tree
VpNode.find_nearest_neighbors(image.attachment.url)
image = ProductImage.all.sample
VpNode.find_nearest_neighbors(image.attachment.url)
image.save_attachment
VpNode.find_nearest_neighbors(image.attachment.url)
reload!
VpNode.find_nearest_neighbors(image.attachment.url)
reload!
VpNode.find_nearest_neighbors(image.attachment.url)
reload!
VpNode.find_nearest_neighbors(image.attachment.url)
nodes = VpNode.all
nodes.select {|node| (node.left_child && node.right_child.nil?) || (node.left_child.nil? && node.right_child) }
node = Vp.node.find 5
node = VpNode.find 5
node.product_images
node.ref_image
exit
nodes = VpNode.all
nodes.select {|node| (node.left_child && node.right_child.nil?) || (node.left_child.nil? && node.right_child) }
nodes = VpNode.include(:children).all
nodes = VpNode.includes(:children).all
nodes.select {|node| (node.left_child && node.right_child.nil?) || (node.left_child.nil? && node.right_child) }
VpNode.rebuild_tree
nodes = VpNode.includes(:children).all
nodes.select {|node| (node.left_child && node.right_child.nil?) || (node.left_child.nil? && node.right_child) }
node = Node.find 3
node = VpNode.find 3
node.left_child
node.right_child
node.right_child.children
root = VpNode.root_node
root.children
root = true
query = 'SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE dhash1 IS NOT NULL'
root_node = VpNode.root_node
root_node.product_images
root_node.ref_image
VpNode.where.not(id: 1).destroy_all
ProductImage.find 16
image = _
image.ref_node
ProductImage.pluck(:vp_node_id)
ProductImage.update_all(vp_node_id: nil)
query = 'SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE dhash1 IS NOT NULL'
query << " AND vp_node_id=#{node_id}" unless root
query
images = ProductImage.connection.select_all(query).to_ary; nil
node_id = 1
node = VpNode.find(node_id)
image = node.ref_image
images_with_distances = []
images.count.times do |_idx|
  image2 = images.pop
  next if image2['id'] == image['id'] # Don't add the ref_image
  images_with_distances << { 'id' => image2['id'], 'hd' => DHasher.distance(image, image2) }
end
images_with_distances.sort_by! { |img| img[:hd] }; nil
reload!
exit
VpNode.rebuild_tree
nodes.select {|node| (node.left_child && node.right_child.nil?) || (node.left_child.nil? && node.right_child) }
nodes = VpNode.all
nodes.select {|node| (node.left_child && node.right_child.nil?) || (node.left_child.nil? && node.right_child) }
VpNode.all
node = VpNode.find 16
ProductImage.where(vp_node_id: 16).update_all(vp_node_id: 11)
node.ref_image
node.ref_image.update(vp_node_id: 11)
node.destroy
VpNode.all
node = VpNode.find 17
node.ref_image.update(vp_node_id: 15)
node.product_images.count
node.product_images.update_all(vp_node_id: 15)
node.destroy
node = VpNode.find 11
node.product_images.count
node.ref_image
node_id = 11
query = 'SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE dhash1 IS NOT NULL'
query << " AND vp_node_id=#{node_id}" unless root
root = false
query << " AND vp_node_id=#{node_id}" unless root
images = ProductImage.connection.select_all(query).to_ary; nil
images.count
node = VpNode.find(node_id)
image = node.ref_image
images_with_distances = []
images.count.times do |_idx|
  image2 = images.pop
  next if image2['id'] == image['id'] # Don't add the ref_image
  images_with_distances << { 'id' => image2['id'], 'hd' => DHasher.distance(image, image2) }
end
images_with_distances.sort_by! { |img| img['hd'] }; nil
images_with_distances
node.mu
mid = (images_with_distances.length - 1) / 2.0
median = (images_with_distances[mid.floor]['hd'] + images_with_distances[mid.ceil]['hd']) / 2.0
dup_image_ids = []
images.length.times do |idx|
  break if images.first['hd'] > 0
  dup_image = images.shift
  dup_image_ids << dup_image['id']
end
dup_image_ids
images
images_with_distances    images_with_distances.length.times do |idx|
  break if images.first['hd'] > 0
  dup_image = images.shift
  dup_image_ids << dup_image['id']
end
images_with_distances.length.times do |idx|
  break if images.first['hd'] > 0
  dup_image = images.shift
  dup_image_ids << dup_image['id']
end
images_with_distances.length.times do |idx|
  break if images.first['hd'] > 0
  dup_image = images.shift
  dup_image_ids << dup_image['id']
end
images_with_distances
images_with_distances.length
images_with_distances.length.times do |idx|
  break if images_with_distances.first['hd'] > 0
  dup_image = images_with_distances.shift
  dup_image_ids << dup_image['id']
end
images_with_distances
dup_image_ids
ListingAsset.where(asset_id: dup_image_ids).find_each do |listing_asset|
  begin
    listing_asset.update(asset_id: image['id'])
  rescue Mysql2::Error, ActiveRecord::RecordNotUnique => e
    raise e unless e.message.include?('Duplicate entry') || e.message.include?('Mysql2::Error')
  end
end
ProductImage.where(id: dup_image_ids).destroy_all
images.count == 0
images.count
images_with_distances.count == 0 || images_with_distances.count > VpNode::MAX_IMAGES
images.count == 0 || images.count > VpNode::MAX_IMAGES
reload!
VpNode.rebuild_tree
nodes = VpNode.all
nodes.select {|node| (node.left_child && node.right_child.nil?) || (node.left_child.nil? && node.right_child) }
reload!
VpNode.rebuild_tree
nodes = VpNode.all
nodes.select {|node| (node.left_child && node.right_child.nil?) || (node.left_child.nil? && node.right_child) }
image = ProductImage.all.sample
image.attachment.url
image.save_attachment
VpNode.find_nearest_neighbors(image.attachment.url)
image
exit
image = ProductImage.all.sample
ProductImage.create(original_url: image.attachment.url)
img = _
img.save_attachment
image
ProductImage.each do |image|
  image.save_attachment
end
ProductImage.find_each do |image|
  image.save_attachment
end
ProductImage.find_each do |image|
  next if image.attachment?
  image.save_attachment
end
ProductImage.last
ProductImage.last.destroy
ProductImage.find_each do |image|
  next if image.attachment?
  image.save_attachment
end
image = ProductImage.all.sample
ProductImage.create(original_url: image.attachment.url)
img = _
img.save_attachment
cd VpNode
update_all(parent_id: nil, ref_image_id: nil)
ProductImage.update_all(vp_node_id: nil)
destroy_all
connection.execute("truncate table #{quoted_table_name}")
root_node = offset(rand(count)).first || create(ancestry: '/')
exit
cd BalanceVpNodeJob.new
node_id = 1
root = true
query = 'SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE dhash1 IS NOT NULL'
query << " AND vp_node_id=#{node_id}" unless root
images = ProductImage.connection.select_all(query).to_ary; nil
node = VpNode.find(node_id)
image = node.ref_image
image = if images.count.zero?
  ProductImage.where.not(dhash1: nil).offset(rand(ProductImage.where.not(dhash1: nil).count)).first
else
  images.sample
end
images.count
node.update(ref_image_id: image['id'])
ProductImage.find(image['id']).update(vp_node_id: nil)
images_with_distances = []
images.count.times do |_idx|
  image2 = images.pop
  next if image2['id'] == image['id'] # Don't add the ref_image
  images_with_distances << images.merge!({ 'hd' => DHasher.distance(image, image2) })
end
images.count.times do |_idx|
  image2 = images.pop
  next if image2['id'] == image['id'] # Don't add the ref_image
  images_with_distances << image2.merge!({ 'hd' => DHasher.distance(image, image2) })
end
images_with_distances
images_with_distances.sort_by! { |img| img['hd'] }; nil
images_with_distances
dup_image_ids = []
images_with_distances.length.times do |idx|
  break if images_with_distances.first['hd'] > 0
  dup_image = images_with_distances.shift
  dup_image_ids << dup_image['id']
end
dup_image_ids
image.id
image
dup_image
images = ProductImage.connection.select_all(query).to_ary; nil
images.count
ProductImage.count
images
node.ref_image.id
images_with_distances = []
image['id']
images.count.times do |_idx|
  image2 = images.pop
  next if image2['id'] == image['id'] # Don't add the ref_image
  images_with_distances << image2.merge!({ 'hd' => DHasher.distance(image, image2) })
end
images_with_distances
images_with_distances.sort_by! { |img| img['hd'] }; nil
images_with_distances
images = ProductImage.connection.select_all(query).to_ary; nil
images_with_distances = []
images.count.times do |_idx|
  image2 = images.pop
  puts "#{image2['id']} -- #{image['id']}"
  next if image2['id'] == image['id'] # Don't add the ref_image
  images_with_distances << image2.merge!({ 'hd' => DHasher.distance(image, image2) })
end
cd BalanceVpNodeJob.new
node_id = 1; root = true;
query = 'SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE dhash1 IS NOT NULL'
query << " AND vp_node_id=#{node_id}" unless root
images = ProductImage.connection.select_all(query).to_ary; nil
images.first
node = VpNode.find(node_id)
image = node.ref_image
image['id']
images_with_distances = []
images.count.times do |_idx|
  image2 = images[_idx]
  puts "#{image2['id'].class} #{image2['id']} -- #{image['id'].class} #{image['id']}"
  next if Integer(image2['id']) == Integer(image['id']) # Don't add the ref_image
  images_with_distances << image2.merge!({ 'hd' => DHasher.distance(image, image2) })
end
images_with_distances.sort_by! { |img| img['hd'] }; nil
images_with_distances
images_with_distances = []
images.count.times do |_idx|
  image2 = images[_idx]
  puts "#{image2['id'].class} #{image2['id']} -- #{image['id'].class} #{image['id']}"
  puts "#{Integer(image2['id']) == Integer(image['id'])}"
  next if Integer(image2['id']) == Integer(image['id']) # Don't add the ref_image
  images_with_distances << image2.merge!({ 'hd' => DHasher.distance(image, image2) })
end
images_with_distances << image2.merge!({ 'hd' => DHasher.distance(image, image2) })
images_with_distances.sort_by! { |img| img['hd'] }; nil
images_with_distances
image
images_with_distances = []
images
images.map {|i| i['id'] }
ProductImage.last
images
image2 = images[24]
image2 = images[22]
image2 = images[21]
image
puts "#{image2['id'].class} #{image2['id']} -- #{image['id'].class} #{image['id']}"
image2['id'] == image['id']
images_with_distances = []
images.count.times do |_idx|
  image2 = images[_idx]
  puts "#{image2['id'].class} #{image2['id']} -- #{image['id'].class} #{image['id']}"
  puts "#{image2['id'] == image['id']}"
  # next  # Don't add the ref_image
  unless image['id'] == image2['id']
    images_with_distances << image2.merge!({ 'hd' => DHasher.distance(image, image2) })
  end
end
images_with_distances.sort_by! { |img| img['hd'] }; nil
images_with_distances
ProductImage.last
ProductImage.find(24)
image
image.attachment.url
ProductImage.create(original_url: image.attachment.url)
img = _
img.save_attachment
query = 'SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE dhash1 IS NOT NULL'
images = ProductImage.connection.select_all(query).to_ary; nil
node = VpNode.find(node_id)
image = node.ref_image
images_with_distances = []
images.count.times do |_idx|
  image2 = images[_idx]
  images_with_distances << image2.merge!({ 'hd' => DHasher.distance(image, image2) })
  unless image['id'] == image2['id']
    images_with_distances << image2.merge!({ 'hd' => DHasher.distance(image, image2) })
  end
end
images.size
images_with_distances
images_with_distances.sort_by! { |img| img['hd'] }; nil
images_with_distances
images.count
images_with_distances = []
images.count.times do |_idx|
  images.pop
  # image2 = images[_idx]
  # puts "#{image2['id'].class} #{image2['id']} -- #{image['id'].class} #{image['id']}"
  # puts "#{image2['id'] == image['id']}"
  # next if image['id'] == image2['id'] # Don't add the ref_image
  # images_with_distances << image2.merge!({ 'hd' => DHasher.distance(image, image2) })
  unless image['id'] == image2['id']
    images_with_distances << image2.merge!({ 'hd' => DHasher.distance(image, image2) })
  end
end
images
images_with_distances.sort_by! { |img| img['hd'] }; nil
images_with_distances
images = ProductImage.connection.select_all(query).to_ary; nil
images_with_distances = []
images.count.times do |_idx|
  image2 = images.pop
  # image2 = images[_idx]
  # puts "#{image2['id'].class} #{image2['id']} -- #{image['id'].class} #{image['id']}"
  # puts "#{image2['id'] == image['id']}"
  # next if image['id'] == image2['id'] # Don't add the ref_image
  # images_with_distances << image2.merge!({ 'hd' => DHasher.distance(image, image2) })
  unless image['id'] == image2['id']
    images_with_distances << image2.merge!({ 'hd' => DHasher.distance(image, image2) })
  end
end
images_with_distances.sort_by! { |img| img['hd'] }; nil
images_with_distances
dup_image_ids = []
images_with_distances.length.times do |idx|
  break if images_with_distances.first['hd'] > 0
  dup_image = images_with_distances.shift
  dup_image_ids << dup_image['id']
end
dup_image_ids
images_with_distances
listing_ids = ListingAsset.where(asset_id: dup_image_ids).ids
image
image.listings
Listings.count
Listing.count
Listing.first.images << ProductImage.last
listing_ids = ListingAsset.where(asset_id: dup_image_ids).ids
image.listing_ids
listing_ids = ListingAsset.where(asset_id: dup_image_ids).ids
ListingAsset.where(listing_id: listing_ids, asset_id: dup_image_ids)
ListingAsset.where(listing_id: listing_ids, asset_id: dup_image_ids).to_sql
listing_ids = ListingAsset.where(asset_id: dup_image_ids).ids
listing_ids = ListingAsset.where(asset_id: dup_image_ids).listing_ids
listing_ids = ListingAsset.where(asset_id: dup_image_ids).pluck(:listing_id)
listing_ids = ListingAsset.where(asset_id: dup_image_ids).uniq.pluck(:listing_id)
dup_image_ids
ListingAsset.where(listing_id: listing_ids, asset_id: dup_image_ids).delete_all
listing_ids.map! { |l_id| { listing_id: l_id, asset_id: image['id'] } }
ListingAsset.create(listing_ids)
ProductImage.where(id: dup_image_ids).delete_all
image.listing_ids
image.reload!
image = ProductImage.find 24
image.listing_ids
exit
reload!
VpNode.rebuild_tree
Redis.current.flushall
VpNode.all
VpNode.rebuild_tree
VpNode.count
VpNode.last
node = _
node.product_images
node.product_images.count
node.product_image_ids
node.ref_image_id
VpNode.last
VpNode.all
VpNode.rebuild_tree
VpNode.all
reload!
exit
reload!
exit
VpNode.rebuild_tree
reload!
VpNode.rebuild_tree
relaod!
reload!
VpNode.rebuild_tree
VpNode.all
VpNode.rebuild_tree
VpNode.all
VpNode.rebuild_tree
VpNode.all
VpNode.rebuild_tree
VpNode.all
VpNode.first.product_image_ids
VpNode.find_each(&:product_image_ids)
VpNode.all.each(&:product_image_ids)
VpNode.all.each{|node| puts node.product_image_ids}
leafs = VpNode.all.select {|n| n.is_leaf?}
leafs
n = Node.first
n = VpNode.first
n.self_and_descendent_images.count
ProductImage.count
image = ProductImage.all.sample
VpNode.find_nearest_neighbors(image.attachment.url)
VpNode.rebuild_tree
VpNode.destroy_all
cd VpNode
update_all(parent_id: nil, ref_image_id: nil)
ProductImage.update_all(vp_node_id: nil)
destroy_all
connection.execute("truncate table #{quoted_table_name}")
root_node = offset(rand(count)).first || create(ancestry: '/')
root_node.ref_image
root_node.ref_image = ProductImage.first
recursive_build(root_node.id, true, nil)
VpNode.all
VpNode.all.map {|n| [n.id, n.product_image_ids] }
VpNode.all.map {|n| [n.id, n.is_leaf?, n.product_image_ids] }
exit
node = VpNode.find 17
node.ref_images
node.ref_image
images = node.self_and_descendent_images
images = node.self_and_descendent_images.map {|i| i.slice(:id, :dash1, :dhash2, :dhash3, :dhash4) }
images = node.self_and_descendent_images.map {|i| i.slice(:id, :dhash1, :dhash2, :dhash3, :dhash4) }
images << node.ref_image.slice(:id, :dash1, :dhash2, :dhash3, :dhash4)
images << node.ref_image.slice(:id, :dhash1, :dhash2, :dhash3, :dhash4)
images
node
node_id = node.id
root = false
images = images
query = 'SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE dhash1 IS NOT NULL'
query << " AND vp_node_id=#{node_id}" unless root
images ||= ProductImage.connection.select_all(query).to_ary; nil
images
node = VpNode.find(node_id)
image = node.ref_image
node.children
images_with_distances = []
images.count.times do |_idx|
  image2 = images.pop
  unless image['id'] == image2['id']
    images_with_distances << image2.merge!({ 'hd' => DHasher.distance(image, image2) })
  end
end
images_with_distances.sort_by! { |img| img['hd'] }; nil
image
node
images_with_distances
images
images = [{"id"=>16, "dhash1"=>18005616263995355256, "dhash2"=>9550326316209433829, "dhash3"=>16702335939237846847, "dhash4"=>10308614039448741609},
  {"id"=>23, "dhash1"=>9771117939174860416, "dhash2"=>16556574733372480511, "dhash3"=>9287234394081132542, "dhash4"=>16992010188066119653},
  {"id"=>26, "dhash1"=>6148914418234759321, "dhash2"=>7206995719900510746, "dhash3"=>1890878214347634165, "dhash4"=>7880239745521181716},
  {"id"=>34, "dhash1"=>18446744069397431928, "dhash2"=>798567697792319352, "dhash3"=>3516453681316756991, "dhash4"=>5136150064968207243},
{"id"=>34, "dhash1"=>18446744069397431928, "dhash2"=>798567697792319352, "dhash3"=>3516453681316756991, "dhash4"=>5136150064968207243}]
images_with_distances = []
images
images.count.times do |_idx|
  # image2 = images.pop
  image2 = images[_idx]
  unless image['id'] == image2['id']
    images_with_distances << image2.merge!({ 'hd' => DHasher.distance(image, image2) })
  end
end
images_with_distances.sort_by! { |img| img['hd'] }; nil
images_with_distances
images
images.pop
images.count.times do |_idx|
  # image2 = images.pop
  image2 = images[_idx]
  unless image['id'] == image2['id']
    images_with_distances << image2.merge!({ 'hd' => DHasher.distance(image, image2) })
  end
end
images_with_distances.sort_by! { |img| img['hd'] }; nil
images_with_distances
images_with_distances = []
images.count.times do |_idx|
  # image2 = images.pop
  image2 = images[_idx]
  unless image['id'] == image2['id']
    images_with_distances << image2.merge!({ 'hd' => DHasher.distance(image, image2) })
  end
end
images_with_distances.sort_by! { |img| img['hd'] }; nil
images_with_distances
images_with_distances.count <= VpNode::MAX_IMAGES
reload!
exit
VpNode.rebuild_tree
VpNode.all
leafs = VpNode.all.select {|n| n.is_leaf?}
leafs.map {|n| [n.id, n.is_leaf?, n.product_image_ids]}
VpNode.all.map {|n| [n.id, n.is_leaf?, n.product_image_ids]}
image = ProductImage.all.sample
VpNode.find_nearest_neighbors(image.attachment.url)
image.id
exit
s = Scrape.last
s.listings.count
ProductImage.count
ProductImage.where(dhash1: nil).count
exit
VpNode.rebuild_tree(true)
VpNode.count
VpNode.last
ProductImage.all.sample
url = _.original_url
VpNode.find_nearest_neighbors(url)
ProductImage.all.sample
VpNode.find_nearest_neighbors()
VpNode.find_nearest_neighbors('https://images-na.ssl-images-amazon.com/images/I/51aqrvhvf%2BL.jpg')
VpNode.find_nearest_neighbors()
ProductImage.all.sample
VpNode.find_nearest_neighbors('https://images-na.ssl-images-amazon.com/images/I/510LdE4NAhL.jpg')
img = ProductImage.find 372
img.listings.count
l = img.listings.first
l.url
VpNode.find_nearest_neighbors('https://images-na.ssl-images-amazon.com/images/I/61Dt7iUfkmL._SL1000_.jpg')
ProductImage.where(id: [244, 370]).map {|i| i.attachment.url }
reload!
VpNode.find_nearest_neighbors('https://images-na.ssl-images-amazon.com/images/I/61Dt7iUfkmL._SL1000_.jpg')
reload!
VpNode.find_nearest_neighbors('https://images-na.ssl-images-amazon.com/images/I/61Dt7iUfkmL._SL1000_.jpg')
ProductImage.all.sample
ProductImage.all.sample.listing.first
ProductImage.all.sample.listings.first
ProductImage.all.sample.listings.first.url
VpNode.find_nearest_neighbors('https://images-na.ssl-images-amazon.com/images/I/41nhqyMJNaL.jpg')
ProductImage.all.sample.listings.first.url
l.images.count
l.images
l.images.map {|i| i.attachment.url}
l.url
exit
VpNode.rebuild_tree(true)
VpNode.count
VpNode.all
node = VpNode.find(2)
node.descendents.count
node.descendents
node = VpNode.find(3)
node.descendents.count
node.descendents
node.self_and_descendent_images.count
node.left_child
node.left_child.product_images.count
exit
Source.count
Source.first
Scrape.count
exit
Scrape.count
Scrape.last
Listing.count
Scrape.count
Scpape.first
Scrape.first
Scrape.last
Scrape.delete_all
exit
Product
Product.connection
Product
Asset
exit
Asset.count
exit
ListingUrl.count
exit
Product.count
exit
Asset.count
VpNode.count
models = %w(
activity_log
asset
bulk_category
bulk_category_source_category
category
category_listing
currency
download_file
download_request
identifier
listing
listing_asset
listing_identifier
listing_screenshot
listing_seller_screenshot
listing_url
manual_text
product
product_attribute_key
product_attribute_value
product_audit
product_image
product_listing
product_manual
product_manual_update_request
product_product_manual
product_update_request
scrape
scrape_listing_seller
scrape_source
seller
seller_asset
seller_image
seller_screenshot
source
source_category
source_type
vp_node
vp_tree
)
m = models.first
m.classify
models.map {|m| m.classify.constantize }
models
models.map! {|m| m.classify.constantize }
models
models.map(&:count)
models.pop
models.map(&:count)
exit
Listing.count
Product.count
VpNode.count
VpNode.last
node = _
node.children
cd VpNode
update_all(parent_id: nil, ref_image_id: nil)
ProductImage.update_all(vp_node_id: nil)
destroy_all
delete_all
exit
cd VpNode
delete_all
connection.execute("truncate table #{quoted_table_name}")
root_node = offset(rand(count)).first || create(ancestry: '/')
exit
root_node = node.first
root_node = VpNode.first.first
root_node = VpNode.first
VpNode.recursive_build(root_node.id, true, nil)
VpNode.count
exit
exit
VpNode.rebuild_tree(true)
VpNode.count
VpNode.last
node = _
node.reload
node.ref_image.attachment.url
node.ref_image.id
VpNode.count
VpNode.last
VpNode.count
VpNode.alst
VpNode.last
node = _
node.product_images.count
node.parent
node.product_images
node.product_images.count
exit
node = VpNode.find(3813)
p = node.parent
p.product_images.count
VpNode.count
ProductImage.where(vp_node_id: nil).count
ProductImage.where.not(dhash1: nil).where(vp_node_id: nil).count
VpNode.last
node = _
node.images
node.product_images
GC.start
root = VpNode.first
root.children
node = root.left_child
node.children
node = node.children.first
node.children
node = node.children.first
node.children
exit
exit
VpNode.rebuild_tree
VpNode.rebuild_tree(true)
exit
VpNode.first
Redis.current.flushall
VpNode.rebuild_tree(true)
VpNode.count
VpNode.pluck(:ancestry)
nodes = VpNode.where(ancestry: "/1/2/3/4/5/6/7/8/460/461/462/463/464/465/466/468/")
node = nodes.first
node.product_images
node.product_images.count
exit
nodes = VpNode.where(ancestry: "/1/2/3/4/5/6/7/8/460/461/462/463/464/465/466/468/")
node = nodes.first
node.product_images.count
VpNode.count
root = VpNode.first
root.children
VpNode.count
ProductImage.count
VpNode.count
10000 / 10
_ / 60
VpNode.count
ProductImage.count
exit
exit
VpNode.rebuild_tree(true)
VpNode.count
img = ProductImage.find 10617515
img.listings.count
img.listings.pluck(:url)
img.listings.pluck(:id)
img.id
VpNode.count
a = [1,2,3,4,54,56]
b = [4,5,6,56]
ids = Set.new
a.length.times { ids << a.pop }
ids
b.length.times { ids << b.pop }
b
a
ids
ids.uniq!
"#{ids}
"#{ids}"
"#{ids.to_s}"
"#{ids.to_a}"
a = [1,2,3,4,54,56]
b = [4,5,6,56]
b.length.times { a << b.pop }
a
a.uniq!
a.sort!
exit
exit
cd VpNode
update_all(parent_id: nil, ref_image_id: nil)
ProductImage.update_all(vp_node_id: nil)
delete_all
connection.execute("truncate table #{quoted_table_name}")
exit
VpNode.rebuild_tree(true)
VpNode.count
VpNode.first
exit
VpNode.first
exit
VpNode.count
ProductImage.count
VpNode.count
ProductImage.count
VpNode.count
ProductImage.count
VpNode.count
ProductImage.count
VpNode.last
root = VpNode.root
root = VpNode.root_node
root.childrent.first(10)
root.descendents.first(10)
root.descendents.first(13)
root.descendents.first(14)
root.descendents.first(16)
root.descendents.first(19)
root.descendents.first(10)
root.descendents.first(20)
root.descendents.first(2`)
root.descendents.first(21)
VpNode.count
ProductImage.count
VpNode.count
ProductImage.count
2698735 - )
2698735 - 2690054
before = 2698735
before - ProductImage.count
VpNode.count
before - ProductImage.count
ProductImage.count
VpNode.count
VpNode.first.children
VpNode.first.children.count
VpNode.children.last
node = VpNode.first.children.last
node.descendents.count
VpNode.count
node = VpNode.first.children.first
node.descendents.count
VpNode.first.children.last.children.count
VpNode.first.children.last.descendents.count
VpNode.first.children.first.descendents.count
VpNode.first.children.last.descendents.count
VpNode.count
exit
node = VpNode.find 732
node.children
node.product_images
p = node.parent
p.children
node.direction
node.self_and_descendent_images.count
VpNode.where(mu: nil)
node = VpNode.where(mu: nil).first
node.children
node.product_images
node.product_images.count
exit
node = VpNode.where(mu: nil).first
node.self_and_descendent_images.count
node
node.parent
exit
VpNode.count
exit
VpNode.rebuild_tree(true)
VpNode.count
exit
exit
VpNode.rebuild_tree(true)
VpNode.count
VpNode.first
eixt
exit
exit
VpNode.rebuild_tree(true)
exit
exit
VpNode.rebuild_tree(true)
VpNode.count
VpNode.first
VpNode.count
[].blankl?
[].blank?
[].empty?
''.empty?
VpNode.count
DateTime.now
(DateTime.now - VpNode.first.created_at)
(DateTime.now.to_i - VpNode.first.created_at.to_i)
(DateTime.now.to_i - VpNode.first.created_at.to_i) / 60
VpNode.count
140000 / _.to_f
VpNode.count / 140000.to_f
35 * 3
35 * 3 / 60
35 * 3 / 60.to_f
VpNode.count / 140000.to_f
VpNode.count
VpNode.first(25).map(&:ancestry)
VpNode.first(25).map{|n| [n.id, n.ancestry] }
leafs = VpNode.where(id: [16, 17, 19, 20, 21, 22])
n = leafs.first
n.product_images
n.product_images.count
Product.connection
n.product_images.count
exit
leafs = VpNode.where(id: [16, 17, 19, 20, 21, 22])
n = leafs.first
n.product_images.count
n.product_images.pluck(:id)
image_ids = _
image_ids
VpNode.count
VpNode.count / 140000.to_f
VpNode.count
(DateTime.now.to_i - VpNode.first.created_at.to_i) / 60
(DateTime.now.seconds - VpNode.first.created_at.to_i) / 60
now = DateTime.now
now.
now.second
now.seconds_since_midnight
now.seconds_since_midnight - VpNode.first.created_at.seconds_since_midnight_midnight
VpNode.first.created_at.class
VpNode.first.created_at.class.to_time
VpNode.first.created_at.class.to_timee
VpNode.first.created_at.class.to_time
DateTime(VpNode.first.created_at.class)
VpNode.count
ProductImage.count
image_ids
n
n.ref_image
VpNode.count
ProductImage.count
VpNode.count
n.ref_image
img = _
img.listings
VpNode.count
VpNode.first(25).map{|n| [n.id, n.ancestry] }
ids = VpNode.first(17).map(&:id)
VpNode.where(id: ids).map {|n| [n.id, n.product_images.count] }
exit
VpNode.count
VpNode.first
VpNode.select(:ref_image_id).mu(123)
ProductImage.where(id: VpNode.select(:ref_image_id).mu(123))
ProductImage.select(id: VpNode.select(:ref_image_id).where(mu: 123)).to_sql
ProductImage.select(id: VpNode.select(:ref_image_id).where(mu: 123))
_
_.first
ProductImage.select(id: VpNode.where(mu: 123).select(:ref_image_id)).to_sql
ProductImage.select(id: VpNode.where(mu: 123).select(:ref_image_id))
ProductImage.where(id: VpNode.select(:ref_image_id).where(mu: 123)).to_sql
ProductImage.where(id: VpNode.select(:ref_image_id).where(mu: 123)).count
VpNode.count
exit
exit
ids = VpNode.first(17).map(&:id)
VpNode.where(id: ids).map {|n| [n.id, n.product_images.count] }
exit
reload!
VpNode.find(2)
node = _
node
node.descendents.count
reload!
node = VpNode.find(2)
node.descendents.count
reload!
node.descendents.count
node = VpNode.find(2)
node.descendents.count
node.self_and_descendent_images.count
ProductImage.where.not(dhash1: nil).count
root = VpNode.first
root.self_and_descendent_images.count
Redis.current.flushall
exit
node = VpNode.create(ancestry: '/')
VpNode.rebuild_tree
Redis.current.flushall
exit
ProductImage.update_all(vp_node_id: nil)
connection.execute("truncate table #{quoted_table_name}")
VpNode.connection.execute("truncate table #{VpNode.quoted_table_name}")
VpNode.count
root_node = VpNode.create(ancestry: '/')
BalanceVpNodeJob.perform_async(root_node.id, true)
VpNode.count
arr = [1,2,32,3,4,5,6,6,3,2,2,5,24,13,11,2,4,6,]
Redis.current.flushall
arr1, arr2 = arr.partition {|el| el < 5 }
arr
arr1
arr2
exit
VpNode.rebuild_tree(true)
exit
ProductImage.where.not(dhash1: nil).first
image = ProductImage.find(3191597)
image.attachment.url
exit
image.attachment.url.gsub('/development/', '')
image.attachment.url.gsub('/development', '')
exit
VpNode.rebuild_tree
VpNode.count
Redis.current.flushall
exit
Redis.current.flushall
VpNode.rebuild_tree
VpNode.count
VpNode.first.count
VpNode.first.created_at
DateTime.parse(VpNode.first.created_at)
DateTime.parse(VpNode.first.created_at.to_s)
date = _
DateTime.now - date
VpNode.count
Time.now - VpNode.first.create_at
Time.now - VpNode.first.created_at
(Time.now - VpNode.first.created_at) / 60
VpNode.count
VpNode.first
node = _
VpNode.count
VpNode.count / ((Time.now - VpNode.first.created_at)) / 60)
VpNode.count / ((Time.now - VpNode.first.created_at) / 60)
Redis.current.flushall
exit
VpNode.rebuild_tree
VpNode.count
exit
Redis.current.flushall
exit
Product.first
exit
VpNode.rebuild_tree
VpNode.first
VpNode.count
VpNode.count
exit
node = VpNode.first
query = 'SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE dhash1 IS NOT NULL'
root = true
query << " AND vp_node_id=#{node_id}" unless root # Shouldn't happen in recursive since we keep passing images down
images ||= ProductImage.connection.select_all(query).to_ary; nil
images.length
nodes_to_build = [[node, images]]
node, images = nodes_to_build.shift
node
image = node.ref_image
image = root ? images.sample : images.pop # take last image? since it should be the furthest from the
node.attributes(
  ref_image_id: image['id'],
  dhash1: image['dhash1'],
  dhash2: image['dhash2'],
  dhash3: image['dhash3'],
  dhash4: image['dhash4']
)
node
node.attributes = {
  ref_image_id: image['id'],
  dhash1: image['dhash1'],
  dhash2: image['dhash2'],
  dhash3: image['dhash3'],
  dhash4: image['dhash4']
}
node
images_with_distances = []
images.count.times do |_idx|
  # image2 = images.pop
  image2 = images[_idx]
  unless image['id'] == image2['id']
    images_with_distances << image2.merge!({ 'hd' => DHasher.distance(image, image2) })
  end
end
images_with_distances.first
image.attachment.url
images_with_distances.last
images_with_distances.sort_by! { |img| img['hd'] }; nil
images_with_distances.first
images_with_distances.last
remove_duplicates(image, images_with_distances) if images_with_distances.first['hd'] == 0 # reassign listings images that are identical and delete duplicates
mid = (images_with_distances.length - 1) / 2.0
median = (images_with_distances[mid.floor]['hd'] + images_with_distances[mid.ceil]['hd']) / 2.0
images_with_distances.length
images_with_distances.length / 2
images_with_distances[_]['hd']
images_with_distances.count {|i| i['hd'] < 118 }
images_with_distances.count {|i| i['hd'] >= 118 }
images_with_distances.count {|i| i['hd'] <= 118 }
exit
exit
VpNode.count
VpNode.rebuild_tree
exit
reload!
VpNode.rebuild_tree
ProductImage.where.not(vp_node_id: nil).update_all(vp_node_id: nil)
exit
ProductImage.where.not(vp_node_id: nil).update_all(vp_node_id: nil)
connection.execute("truncate table #{quoted_table_name}")
cd VpNode
connection.execute("truncate table #{quoted_table_name}")
root_node = create(ancestry: '/')
ProductImage.where.not(dhash1: nil).serializable_hash(only: [:id, :dhash1, :dhash2, :dhash3, :dhash4])
exit
images = ProductImage.where.not(dhash1: nil).serializable_hash(only: [:id, :dhash1, :dhash2, :dhash3, :dhash4]); nil
query = 'SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE dhash1 IS NOT NULL'
images = ProductImage.connection.select_all(query).to_ary; nil
exit
xit
exit
cd VpNode
ProductImage.where.not(vp_node_id: nil).update_all(vp_node_id: nil)
connection.execute("truncate table #{quoted_table_name}")
root_node = create(ancestry: '/')
query = 'SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE dhash1 IS NOT NULL'
images = ProductImage.connection.select_all(query).to_ary; nil
nodes_to_balance = [[root_node, images]]
node, images = nodes_to_balance.shift
image = node.ref_image
image = images.sample
image.nil?
node.attributes = {
  ref_image_id: image['id'],
  dhash1: image['dhash1'],
  dhash2: image['dhash2'],
  dhash3: image['dhash3'],
  dhash4: image['dhash4']
}
images_with_distances = []
images.count.times do |_idx|
  image2 = images.pop
  unless image['id'] == image2['id']
    images_with_distances << image2.merge!({ 'hd' => DHasher.distance(image, image2) })
  end
end
images_with_distances.sort_by! { |img| img['hd'] }; nil
images_with_distances.first['hd']
mid = (images_with_distances.length - 1) / 2.0
median = (images_with_distances[mid.floor]['hd'] + images_with_distances[mid.ceil]['hd']) / 2.0
node.mu = median
node.save!
images_with_distances.length <= VpNode::MAX_IMAGES
images_with_distances.length
child_nodes = split_images(node, median, images_with_distances, root)
child_nodes = split_images(node, median, images_with_distances)
images_with_distances
child_nodes.each { |child| nodes_to_balance << child }
nodes_to_balance.length
node, images = nodes_to_balance.shift
images
image, node = nil
if image.nil?
  image = images.pop # take last image since it should be the furthest from the
end
node
node = Node.find 2
node = VpNode.find 2
node.attributes = {
  ref_image_id: image['id'],
  dhash1: image['dhash1'],
  dhash2: image['dhash2'],
  dhash3: image['dhash3'],
  dhash4: image['dhash4']
}
images_with_distances = []
images.count.times do |_idx|
  image2 = images.pop
  unless image['id'] == image2['id']
    images_with_distances << image2.merge!({ 'hd' => DHasher.distance(image, image2) })
  end
end
images_with_distances.sort_by! { |img| img['hd'] }; nil
images_with_distances.first['hd'] == 0
images_with_distances.first['hd']
mid = (images_with_distances.length - 1) / 2.0
median = (images_with_distances[mid.floor]['hd'] + images_with_distances[mid.ceil]['hd']) / 2.0
images_with_distances.last['hd']
median = (images_with_distances[mid.floor]['hd'] + images_with_distances[mid.ceil]['hd']) / 2.0
node.mu = median
node.save!
images_with_distances.length <= VpNode::MAX_IMAGES
child_nodes = split_images(node, median, images_with_distances)
child_nodes.each { |child| nodes_to_balance << child }
image, node = nil
node, images = nodes_to_balance.shift
exit
VpNode.rebuild_tree
VpNode.count
exit
exit
VpNode.rebuild_tree
VpNode.count
VpNode.pluck(:id, :ancestry)
VpNode.count
arr = [1,2,3,4]
arr.unshift(5)
arr
arr.unshift([6, 7])
arr.shift
arr.unshift(*[6, 7])
VpNode.count
exit
exit
VpNode.rebuild_tree
VpNode.count
VpNode.pluck(:id, :ancestry)
cd VpNode
ProductImage.where.not(vp_node_id: nil).update_all(vp_node_id: nil)
connection.execute("truncate table #{quoted_table_name}")
node = create(ancestry: '/')
query = 'SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE dhash1 IS NOT NULL'
images = ProductImage.connection.select_all(query).to_ary; nil
nodes_to_balance = [[node, images]]
image
nodes_to_balance
image = images.sample
image
nodes_to_balance.length > 0
node, images = nodes_to_balance.shift
image.nil?
image = images.pop if image.nil?
image
node.attributes = {
  ref_image_id: image['id'],
  dhash1: image['dhash1'],
  dhash2: image['dhash2'],
  dhash3: image['dhash3'],
  dhash4: image['dhash4']
}
images_with_distances = []
images.count.times do |_idx|
  image2 = images.pop
  unless image['id'] == image2['id']
    images_with_distances << image2.merge!({ 'hd' => DHasher.distance(image, image2) })
  end
end
images_with_distances.sort_by! { |img| img['hd'] }; nil
images_with_distances.count
images
images_with_distances.first
dup_image_ids = []
images_with_distances.length.times do |idx|
  break if images_with_distances.first['hd'] > 0
  dup_image = images_with_distances.shift
  dup_image_ids << dup_image['id']
end
dup_image_ids
listing_ids = ListingAsset.where(asset_id: dup_image_ids + [image['id']]).uniq.pluck(:listing_id)
ListingAsset.transaction do
  ListingAsset.where(listing_id: listing_ids, asset_id: dup_image_ids + [image['id']]).delete_all
  listing_ids.map! { |l_id| { listing_id: l_id, asset_id: image['id'] } }
  ListingAsset.create(listing_ids)
  ProductImage.where(id: dup_image_ids).delete_all
end
images_with_distances.count
images_with_distances.length
mid = (images_with_distances.length - 1) / 2.0
median = (images_with_distances[mid.floor]['hd'] + images_with_distances[mid.ceil]['hd']) / 2.0
node.mu = median
node.save!
images_with_distances.length <= VpNode::MAX_IMAGES
child_nodes = split_images(node, median, images_with_distances)
images_with_distances
child_nodes.map{|d| d[1].length}
nodes_to_balance
nodes_to_balance.unshift(*child_nodes)
nodes_to_balance
node
image, node = nil
node, images = nodes_to_balance.shift
nodes_to_balance.length
image = images.pop if image.nil?
node.attributes = {
  ref_image_id: image['id'],
  dhash1: image['dhash1'],
  dhash2: image['dhash2'],
  dhash3: image['dhash3'],
  dhash4: image['dhash4']
}
images_with_distances = []
images.count.times do |_idx|
  image2 = images.pop
  unless image['id'] == image2['id']
    images_with_distances << image2.merge!({ 'hd' => DHasher.distance(image, image2) })
  end
end
images_with_distances.sort_by! { |img| img['hd'] }; nil
images_with_distances.first
remove_duplicates(image, images_with_distances) if images_with_distances.first['hd'] == 0
images_with_distances.length
mid = (images_with_distances.length - 1) / 2.0
median = (images_with_distances[mid.floor]['hd'] + images_with_distances[mid.ceil]['hd']) / 2.0
node.mu = median
node.save!
images_with_distances.length <= VpNode::MAX_IMAGES
child_nodes = split_images(node, median, images_with_distances)
nodes_to_balance.unshift(*child_nodes)
nodes_to_balance.length
nodes_to_balance.map{|d| d[1].length}
image, node = nil
nodes_to_balance.length > 0
node, images = nodes_to_balance.shift
image = images.pop if image.nil?
node.attributes = {
  ref_image_id: image['id'],
  dhash1: image['dhash1'],
  dhash2: image['dhash2'],
  dhash3: image['dhash3'],
  dhash4: image['dhash4']
}
images_with_distances = []
images.count.times do |_idx|
  image2 = images.pop
  unless image['id'] == image2['id']
    images_with_distances << image2.merge!({ 'hd' => DHasher.distance(image, image2) })
  end
end
images_with_distances.sort_by! { |img| img['hd'] }; nil
images_with_distances.first['hd']
remove_duplicates(image, images_with_distances) if images_with_distances.first['hd'] == 0
mid = (images_with_distances.length - 1) / 2.0
median = (images_with_distances[mid.floor]['hd'] + images_with_distances[mid.ceil]['hd']) / 2.0
node.mu = median
node.save!
images_with_distances.length <= VpNode::MAX_IMAGES
child_nodes = split_images(node, median, images_with_distances)
nodes_to_balance.unshift(*child_nodes)
nodes_to_balance.length
nodes_to_balance.map{|d| d[1].length}
exit
exit
VpNode.rebuild_tree
VpNode.pluck(:id, :ancestry)
VpNode.count
VpNode.pluck(:id, :ancestry)
VpNode.find(30).product_images.count
VpNode.find(31).product_images.count
cd VpNode
ProductImage.where.not(vp_node_id: nil).update_all(vp_node_id: nil)
connection.execute("truncate table #{quoted_table_name}")
node = create(ancestry: '/')
query = 'SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE dhash1 IS NOT NULL'
images = ProductImage.connection.select_all(query).to_ary; nil
nodes_to_balance = [[node, images]]
image
images.count
image = images.sample
edit -t
exit
VpNode.rebuild_tree
reload!
exit
VpNode.pluck(:id, :ancestry)
VpNode.where(id: [30,31,32,33]).map {|n| n.product_images.count }
VpNode.count
(Time.now - VpNode.first.create_at) / 60
(Time.now - VpNode.first.created_at) / 60
VpNode.count / ((Time.now - VpNode.first.created_at) / 60)
140000 / 1000
140 / 60
VpNode.count
VpNode.count / ((Time.now - VpNode.first.created_at) / 60)
leafs = VpNode.where.not(id: VpNode.select(:parent_id))
VpNode.pluck(:parent_id)
VpNode.pluck(:id, :ancestry)
node = VpNode.find (31)
node.children
leafs = VpNode.where.not(id: VpNode.where.not(parent_id: nil).select(:parent_id)).count
leafs = VpNode.where.not(id: VpNode.where.not(parent_id: nil).select(:parent_id)).limit(60)
leafs.class
leafs.includes(:product_images).map {|l| [l.id, l.product_images.count]}
leafs.map {|l| [l.id, l.product_images.count]}
exit
leafs = VpNode.where.not(id: VpNode.where.not(parent_id: nil).select(:parent_id)).limit(60)
leafs.map {|l| [l.id, l.product_images.count]}
node = VpNode.find 3
node.children
VpNode.count
VpNode.find(100000)
VpNode.last
node = VpNode.find(66)
node.product_images
node = node.parent
node.descendent_images.count
node.descendent_images.ids
ids = _
ProductImage.where(id: ids).select_all.to_ary
node.descendent_images.to_sql
images = ProductImage.connection.exectue(node.descendent_images.to_sql).toary
exit
VpNode.pluck(:ancestry)
exit
VpNode.pluck(:ancestry)
node_id = 26
cd VpNode
node_id = 26
images = nil
node = find_by(id: node_id) || create(ancestry: '/')
node.self_and_descendent_images.to_sql
query = node.self_and_descendent_images.to_sql
images = ProductImage.connection.select_all(query).to_ary; nil
image.length
images.length
ancestry = node.ancestry.blank? ? "#{node.id}" : "#{node.ancestry}#{node.id}/"
node
left_child = VpNode.find_by(parent_id: node.id, ancestry: ancestry, direction: VpNode.directions[:left])
images = node.self_and_descendent_images
images_a = images.to_ary
images = node.self_and_descendent_images.select(:id, :dhash1, :dhash2, :dhash3, :dhash4); nil
images
node_id
node.self_and_descendent_images.update_all(vp_node_id: nil)
query = nil
query = node.self_and_descendent_images.to_sql
exit
cd VpNode
node_id = 26
node = find_by(id: node_id) || create(ancestry: '/')
ids = [7475051,
  7483913,
  7499057,
  7499813,
  7589411,
  7589631,
  7642783,
  7655248,
  7661590,
  7669186,
  7676519,
  7720664,
  7726715,
  7746821,
  7746825,
  7774661,
  7796105,
  7840525,
  7847226,
  7871240,
  7871277,
  7888254,
  7896413,
  7898028,
  7929817,
  7983970,
  8045545,
  8053220,
  8083629,
  8148118,
  8181607,
  8183042,
  8186207,
  8192214,
  8259160,
  8283055,
  8310678,
  8335363,
  8400536,
  8413931,
  8440046,
  8444157,
  8461603,
  8470604,
  8480764,
  8486578,
  8511529,
  8515830,
  8523056,
  8523057,
  8555885,
  8568568,
  8608381,
  8613347,
  8625228,
  8711730,
  8717375,
  8720406,
  8722244,
  8725225,
  8756524,
  8786099,
  8795917,
  8829503,
  8870942,
  8987185,
  8987240,
  9026252,
  9033542,
  9087530,
  9094810,
  9096297,
  9108636,
  9141610,
  9167945,
  9178220,
  9197963,
  9230814,
9259652]
ids
node.self_and_descendent_images
node.self_and_descendent_images.count
node.children
node.self_and_descendent_images.to_sql
query = "SELECT `assets`.* FROM `assets` WHERE `assets`.`type` IN ('ProductImage') AND (assets.id IN (SELECT `vp_nodes`.`ref_image_id` FROM `vp_nodes` WHERE (`vp_nodes`.`id` = 26 OR `vp_nodes`.`ancestry` LIKE '/1/2/4/6/8/10/12/14/16/18/20/22/24/26/%')) OR assets.vp_node_id IN (SELECT `vp_nodes`.`id` FROM `vp_nodes` WHERE (`vp_nodes`.`id` = 26 OR `vp_nodes`.`ancestry` LIKE '/1/2/4/6/8/10/12/14/16/18/20/22/24/26/28/')))"
images = ProductImage.connection.select_all(query).to_ary; nil
images.count
ids
q = "SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE id IN (#{ids.join(',')})"
images |= ProductImage.connection.select_all(query).to_ary
exit
cd VpNode
node_id = 26
node = find_by(id: node_id) || create(ancestry: '/')
query = nil
images = nil
query = node.self_and_descendent_images.to_sql
images = ProductImage.connection.select_all(query).to_ary; nil
q = "SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE id IN (#{ids.join(',')})"
ids.count
image_ids = [7475051,
  7483913,
  7499057,
  7499813,
  7589411,
  7589631,
  7642783,
  7655248,
  7661590,
  7669186,
  7676519,
  7720664,
  7726715,
  7746821,
  7746825,
  7774661,
  7796105,
  7840525,
  7847226,
  7871240,
  7871277,
  7888254,
  7896413,
  7898028,
  7929817,
  7983970,
  8045545,
  8053220,
  8083629,
  8148118,
  8181607,
  8183042,
  8186207,
  8192214,
  8259160,
  8283055,
  8310678,
  8335363,
  8400536,
  8413931,
  8440046,
  8444157,
  8461603,
  8470604,
  8480764,
  8486578,
  8511529,
  8515830,
  8523056,
  8523057,
  8555885,
  8568568,
  8608381,
  8613347,
  8625228,
  8711730,
  8717375,
  8720406,
  8722244,
  8725225,
  8756524,
  8786099,
  8795917,
  8829503,
  8870942,
  8987185,
  8987240,
  9026252,
  9033542,
  9087530,
  9094810,
  9096297,
  9108636,
  9141610,
  9167945,
  9178220,
  9197963,
  9230814,
9259652]
q = "SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE id IN (#{image_ids.join(',')})"
images |= ProductImage.connection.select_all(q).to_ary
images.count
query = node.self_and_descendent_images.to_sql
query.gsub('`assets`.*', 'id, dhash1, dhash2, dhash3, dhash4')
query.gsub!('`assets`.*', 'id, dhash1, dhash2, dhash3, dhash4')
images = ProductImage.connection.select_all(query).to_ary; nil
images |= ProductImage.connection.select_all(q).to_ary
images.first
images.last
nodes_to_balance = [[node, images]]
image = images.sample
image = node.ref_image
image = node.ref_image || images.sample
node, images = nodes_to_balance.shift
image = images.pop if image.nil?
node.attributes = {
  ref_image_id: image['id'],
  dhash1: image['dhash1'],
  dhash2: image['dhash2'],
  dhash3: image['dhash3'],
  dhash4: image['dhash4']
}
images_with_distances = []
images.count.times do |_idx|
  image2 = images.pop
  unless image['id'] == image2['id']
    images_with_distances << image2.merge!({ 'hd' => DHasher.distance(image, image2) })
  end
end
images_with_distances.sort_by! { |img| img['hd'] }; nil
remove_duplicates(image, images_with_distances) if images_with_distances.first['hd'] == 0
mid = (images_with_distances.length - 1) / 2.0
median = (images_with_distances[mid.floor]['hd'] + images_with_distances[mid.ceil]['hd']) / 2.0
node.mu
node.save!
images.firts
images.first
images_with_distances.first
image.id
image = node.ref_image || images.sample
image
image = node.ref_image || images.sample
ProductImage.find 11079573
ProductImage.find(11079573).attachment.url
image.attachment.url
img2 = ProductImage.find(11079573)
DHasher
DHasher.distance(image, img2)
Dhasher.hash_from_path
path = 
path = image.attachment.url.gsub('/development', '')
path2 = img2.attachment.url.gsub('/development', '')
Dhasher.hash_from_path(path)
::Dhasher.hash_from_path(path)
require 'd_hasher'
Dhasher.hash_from_path(path)
exit
path = image.attachment.url.gsub('/development', '')
exit
UserAccessLog
exit
ActivityLog
ActivityLog.connection
ActivityLog
exit
ActivityLog
ActivityLog.all
_.count
ActivityLog.last
ActivityLog.where(severity: 2).count
ActivityLog.delete_all
VpNode.rebuild_tree
exit
VpNode.rebuild_tree
exit
exit
VpNode.rebuild_tree
VpNode.count
VpNode.first
VpNode.find_by(id: nil)
exit
cd VpNode
node_id = nil
node = find_by(id: node_id) || create(ancestry: '/')
node_id
ProductImage.where.not(vp_node_id: nil).update_all(vp_node_id: nil)
connection.execute("truncate table #{quoted_table_name}")
node = create(ancestry: '/')
query = 'SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE dhash1 IS NOT NULL'
images = ProductImage.connection.select_all(query).to_ary; nil
image = node.ref_image || images.sample
node.ref_image
nodes_to_balance = [[node, images]]
edit -t
VpNode.count
ActivityLog.first
VpNode.count
VpNode.pluck(:id, :ancestry)
n1 = VpNode.first
n2 = Node.second
n2 = VpNode.second
distance = DHasher.distance(n1.slice(:dhash1, :dhash2, :dhash3, :dhash4), n2.slice(:dhash1, :dhash2, :dhash3, :dhash4))
VpNode.pluck(:id, :ancestry)
"/1/2/4/6/8/10/12/14/16/18/20/22/24/27/33/35/".length
ActivityLog.first
:tree_build_failure.to_sym
node = VpNode.find 39225
node.product_images.count
node.children.empty?
node.children.size == 0
node.    children.size.zero?
node = VpNode.root_node
node.children
VpNode.count
ActivityLog.first
VpNode.count
ActivityLog.first
VpNode.count
exit
node = VpNode.find 3
node.descendents.count
VpNode.find(2).descendents.count
VpNode.find(3).descendents.count
VpNode.find(12341).descendents.count
VpNode.find(3).descendents.count
node = VpNode.find 12341
node.self_and_descendent_images.count
edit -t
edit
exit
edit -t
edit
node.ancestry
edit
id = node.id
ancestry = node.ancestry
VpNode.arel_table[:id].eq(id).or(
  VpNode.arel_table[:ancestry].matches("#{ancestry}#{id}/%")
)[:id]
ProductImage.arel_table[:id].eq_any(node.self_and_descendents.select(:ref_image_id)).or(
  ProductImage.arel_table[:vp_node_id].eq_any(node.self_and_descendents.select(:id))
)
ProductImage.where(
  ProductImage.arel_table[:id].eq_any(node.self_and_descendents.select(:ref_image_id)).or(
    ProductImage.arel_table[:vp_node_id].eq_any(node.self_and_descendents.select(:id))
  )
)
table = VpNode.arel_table[:id].eq(id).or(
  VpNode.arel_table[:ancestry].matches("#{ancestry}#{id}/%")
)
table
table[:id]
nodes = Arel::Table.new(:vp_nodes)
nodes.project(nodes[:id])
nodes.project(nodes[:id]).to_sql
nodes.project(nodes[:id]).first
nodes.project(
  [:id]
).where(
  nodes[:id].eq(id).or(nodes[:ancestry].matches("#{ancestry}#{id}/%"))
)
ids = _
edit -t
nodes.project(
  [:ref_imageid]
).where(
  nodes[:ref_image_id].eq(id).or(nodes[:ancestry].matches("#{ancestry}#{id}/%"))
)
_.to_sql
ProductImage.where(
  ProductImage.arel_table[:id].eq_any(
    nodes.project(
      [:ref_image_id]
    ).where(query)
  ).or(
    ProductImage.arel_table[:vp_node_id].eq_any(
      nodes.project(
        [:id]
      ).where(query)
    )
  )
)
query = nodes[:ref_image_id].eq(id).or(nodes[:ancestry].matches("#{ancestry}#{id}/%"))
ProductImage.where(
  ProductImage.arel_table[:id].eq_any(
    nodes.project(
      [:ref_image_id]
    ).where(query)
  ).or(
    ProductImage.arel_table[:vp_node_id].eq_any(
      nodes.project(
        [:id]
      ).where(query)
    )
  )
)
nodes = Arel::Table.new(:vp_nodes)
nodes.class
nodes[:ref_image_id].eq(id).or(nodes[:ancestry].matches("#{ancestry}#{id}/%"))
_.to_sql
query = nodes[:id].eq(id).or(nodes[:ancestry].matches("#{ancestry}#{id}/%"))
query = nodes[:id].eq(id).or(nodes[:ancestry].matches("#{ancestry}#{id}/%")).to_sql
VpNode.arel_table
nodes = VpNode.arel_table
query = nodes[:id].eq(id).or(nodes[:ancestry].matches("#{ancestry}#{id}/%"))
query = nodes[:id].eq(id).or(nodes[:ancestry].matches("#{ancestry}#{id}/%")).to_sql
VpNode.arel_table[:id].eq(id).or(
  VpNode.arel_table[:ancestry].matches("#{ancestry}#{id}/%")
)
_.class
query = nodes[:id].eq(id).or(nodes[:ancestry].matches("#{ancestry}#{id}/%"))
query = nodes[:id].eq(id).or(nodes[:ancestry].matches("#{ancestry}#{id}/%")).to_sql
query = nodes.project([:id]).where(nodes[:id].eq(id).or(nodes[:ancestry].matches("#{ancestry}#{id}/%")).to_sql)
query = nodes.project([:id]).where(nodes[:id].eq(id).or(nodes[:ancestry].matches("#{ancestry}#{id}/%")))
query = nodes.project([:id]).where(nodes[:id].eq(id).or(nodes[:ancestry].matches("#{ancestry}#{id}/%"))).to_sql
nodes = VpNode.arel_table
nodes.project([:id]).where(
  nodes[:id].eq(id).or(
    VpNode.arel_table[:ancestry].matches("#{ancestry}#{id}/%")
  )
)
_.to_sql
VpNode.count
nodes[:id]
nodes[:id].to_sql
nodes
VpNode.arel_table[:id].eq(id).or(
  VpNode.arel_table[:ancestry].matches("#{ancestry}#{id}/%")
).to_sql
VpNode.count
ActivityLog.count
exit
ProductImage.arel_table
node = VpNode.find(12341)
reload!
node = VpNode.find(12341)
node.self_and_descendents.select(:ref_image_id).to_sql
Arel.sql(
  node.self_and_descendents.select(:ref_image_id).to_sql
)
ProductImage.arel_table[:id].in(
  Arel.sql(
    node.self_and_descendents.select(:ref_image_id).to_sql
  )
)
ProductImage.where(_)
_.count
ProductImage.where(
  ProductImage.arel_table[:id].in(
    Arel.sql(
      node.self_and_descendents.select(:ref_image_id).to_sql
    )
  ).or(
    Arel.sql(
      node.self_and_descendents.select(:id).to_sql
    )
  )
)
_.count
edit -t
exit
node = VpNode.find 12341
node.self_and_descendents.count
node.self_and_descendent_images.count
node.self_and_descendent_images.to_sql
ids = node.self_and_descendents.select(:id)
ProductImage.where(vp_node_id: ids)
ids = node.self_and_descendents.select(:ref_image_id)
ids = node.self_and_descendents.select(:id).pluck(:id)
ProductImage.where(vp_node_id: ids)
ids = node.self_and_descendents.select(:ref_image_id).pluck(:ref_image_id)
ProductImage.where(id: ids)
node.self_and_descendent_images.to_sql
ActivityLog.count
node.self_and_descendent_images.count
query = <<-SQL.gsub(/\s+/, '')
      (SELECT *
      FROM assets
      WHERE `assets`.`id` IN (SELECT `vp_nodes`.`ref_image_id`
                              FROM
                                `vp_nodes`
                              WHERE
                                (`vp_nodes`.`id` = 12341
                                 OR `vp_nodes`.`ancestry` LIKE
                                    '/1/2/4/6/9/7491/11436/11438/11441/11909/12143/12273/12336/12338/12341/%')))
      UNION ALL (SELECT *
      FROM assets
      WHERE `assets`.`vp_node_id` IN (SELECT `vp_nodes`.`id`
                              FROM
                                `vp_nodes`
                              WHERE
                                (`vp_nodes`.`id` = 12341
                                 OR `vp_nodes`.`ancestry` LIKE
                                    '/1/2/4/6/9/7491/11436/11438/11441/11909/12143/12273/12336/12338/12341/%')));
    SQL
query = <<-SQL.gsub(/\s+/, ' ')
      (SELECT *
      FROM assets
      WHERE `assets`.`id` IN (SELECT `vp_nodes`.`ref_image_id`
                              FROM
                                `vp_nodes`
                              WHERE
                                (`vp_nodes`.`id` = 12341
                                 OR `vp_nodes`.`ancestry` LIKE
                                    '/1/2/4/6/9/7491/11436/11438/11441/11909/12143/12273/12336/12338/12341/%')))
      UNION ALL (SELECT *
      FROM assets
      WHERE `assets`.`vp_node_id` IN (SELECT `vp_nodes`.`id`
                              FROM
                                `vp_nodes`
                              WHERE
                                (`vp_nodes`.`id` = 12341
                                 OR `vp_nodes`.`ancestry` LIKE
                                    '/1/2/4/6/9/7491/11436/11438/11441/11909/12143/12273/12336/12338/12341/%')));
    SQL
cd node
id
query = <<-SQL.gsub(/\s+/, ' ')
      (SELECT *
      FROM assets
      WHERE `assets`.`id` IN (SELECT `vp_nodes`.`ref_image_id`
                              FROM
                                `vp_nodes`
                              WHERE
                                (`vp_nodes`.`id` = #{self.id}
                                 OR `vp_nodes`.`ancestry` LIKE
                                    '#{self.ancestry}')))
UNION ALL (SELECT *
  FROM assets
  WHERE `assets`.`vp_node_id` IN (SELECT `vp_nodes`.`id`
                              FROM
                                `vp_nodes`
                              WHERE
                                (`vp_nodes`.`id` = #{self.id}
                                 OR `vp_nodes`.`ancestry` LIKE
                                    '#{self.ancestry}')));
    SQL
query
ProductImage.where(query).count
query
query = <<-SQL.gsub(/\s+/, ' ')
      (SELECT *
      FROM assets
      WHERE `assets`.`id` IN (SELECT `vp_nodes`.`ref_image_id`
                              FROM
                                `vp_nodes`
                              WHERE
                                (`vp_nodes`.`id` = #{self.id}
                                 OR `vp_nodes`.`ancestry` LIKE
                                    '#{self.ancestry}%')))
UNION ALL (SELECT *
  FROM assets
  WHERE `assets`.`vp_node_id` IN (SELECT `vp_nodes`.`id`
                              FROM
                                `vp_nodes`
                              WHERE
                                (`vp_nodes`.`id` = #{self.id}
                                 OR `vp_nodes`.`ancestry` LIKE
                                    '#{self.ancestry}%')));
    SQL
query
ProductImage.where(query)
_.count
ProductImage.connection.exec_query(query)
ProductImage.select(query)
ProductImage.connection.execute(query)
ProductImage.connection.execute(query).first
Arel.sql(query)
ProductImage.where(Arel.sql(query))
ProductImage.arel_table
ProductImage.arel_table.where(Arel.sql(query))
ProductImage.where(ProductImage.arel_table.where(Arel.sql(query)))
ProductImage.where(ProductImage.arel_table.where(Arel.sql(query))).first
query = <<-SQL.gsub(/\s+/, ' ')
SELECT `assets`.* from assets where id IN (SELECT id
FROM assets
WHERE `assets`.`id` IN (SELECT `vp_nodes`.`ref_image_id`
                        FROM
                          `vp_nodes`
                        WHERE
                          (`vp_nodes`.`id` = 12341
                           OR `vp_nodes`.`ancestry` LIKE
                              '/1/2/4/6/9/7491/11436/11438/11441/11909/12143/12273/12336/12338/12341/%')))
UNION ALL (SELECT *
FROM assets
WHERE `assets`.`vp_node_id` IN (SELECT `vp_nodes`.`id`
                        FROM
                          `vp_nodes`
                        WHERE
                          (`vp_nodes`.`id` = 12341
                           OR `vp_nodes`.`ancestry` LIKE
                              '/1/2/4/6/9/7491/11436/11438/11441/11909/12143/12273/12336/12338/12341/%')));
    SQL
ProductImage.where(query)
ProductImage.find_by_sql(query)
ProductImage.find_by_sql(query).class
ProductImage.select(query)
q1 = ProductImage.where(id: self_and_descendents.select(:ref_image_id))
xtei
i
xtei
qqqexit
exit
q1 = ProductImage.where(id: self_and_descendents.select(:ref_image_id)).select(:id).to_sql
node = VpNode.find 1234
exit
node = VpNode.find 1234
cd node
q1 = ProductImage.where(id: self_and_descendents.select(:ref_image_id)).select(:id).to_sql
q2 = ProductImage.where(vp_node_id: self_and_descendents.select(:id)).to_sql
id_column = "#{self.class.table_name}.id"
q1 = ProductImage.where(id: self_and_descendents.select(:ref_image_id)).select(:id).to_sql
q2 = ProductImage.where(vp_node_id: self_and_descendents.select(:id)).to_sql
sub_query = "#{q1} UNION ALL #{q2}"
ProductImage.where("assets.id IN (#{sub_query}")
sub_query = "#{q1} UNION #{q2}"
ProductImage.where("assets.id IN (#{sub_query}")
Asset.where("assets.id IN (#{sub_query}")
ProductImage.union
assoc = ProductImage.where();
Arel::Nodes::Union
q1 = ProductImage.where(id: self_and_descendents.select(:ref_image_id)).select(:id).to_sql
q2 = ProductImage.where(vp_node_id: self_and_descendents.select(:id)).select(:id).to_sql
q1 = ProductImage.where(id: self_and_descendents.select(:ref_image_id)).select(:id).to_sql
q2 = ProductImage.where(vp_node_id: self_and_descendents.select(:id)).select(:id).to_sql
sub_query = "#{q1} UNION #{q2}"
Asset.where("assets.id IN (#{sub_query}")
ProductImage.where("assets.id IN (#{q1} OR assets.id IN #{q2}")
images = _
images.first
self_and_descendents.count
self_and_descendents.select(:ref_image_id)
self_and_descendents.select(:id)
q1 = ProductImage.where(id: self_and_descendents.select(:ref_image_id)).select(:id).to_sql
ProductImage.where(_)
q1 = ProductImage.where(id: self_and_descendents.select(:ref_image_id)).select(:id).to_sql
q2 = ProductImage.where(vp_node_id: self_and_descendents.select(:id)).select(:id).to_sql
ProductImage.where(id: q1)
q1 = ProductImage.where(id: self_and_descendents.select(:ref_image_id)).select(:id)
ProductImage.where(id: q1)
q1 = ProductImage.where(id: self_and_descendents.select(:ref_image_id))
q1 = ProductImage.where(id: self_and_descendents.pluck(:ref_image_id))
exit
node = VpNode.find 12341
cd node
q1 = ProductImage.where(id: self_and_descendents.pluck(:ref_image_id))
q1 = ProductImage.where(id: self_and_descendents.pluck(:ref_image_id)); nil
q2 = ProductImage.where(vp_node_id: self_and_descendents.pluck(:id)); nil
ProductImage.where("assets.id IN (#{q1} OR assets.id IN #{q2}")
ProductImage.where(id: q1 | q2)
images = _
images.count
exit
url = 'http://assets.inhabitat.com/wp-content/blogs.dir/1/files/2015/10/Panasonic-HIT-module-lead.jpg'
VpNode.find_nearest_neighbors(url)
exit
xit
exit
VpNode.where.not(parent_id: nil).uniq.pluck(:parent_id)
VpNode.where.not(id: VpNode.where.not(parent_id: nil).select(:parent_id)).count
VpNode.where.not(id: VpNode.where.not(parent_id: nil).select(:parent_id))
ProductImage.find(9619197).attachment.url
nodes = VpNode.includes(:children); nil
node = nodes.shift
node = nodes.first
node.left_child
node.children
node.children.first
node.children.first.children.first
exit
node = VpNode.first
node.left_child
node.right_child
VpNode.order(ancestry: asc).limit(10)
VpNode.order(ancestry: :asc).limit(10)
image
node
"#{node.dhash1.to_s(2)}"
"#{node.dhash1.to_s(2)}#{node.dhash2.to_s(2)}#{node.dhash(3).to_s(2)}#{node.dhash4.to_s(2)}"
"#{node.dhash1.to_s(2)}#{node.dhash2.to_s(2)}#{node.dhash3.to_s(2)}#{node.dhash4.to_s(2)}"
"#{node.dhash1.to_s(2)}#{node.dhash2.to_s(2)}#{node.dhash3.to_s(2)}#{node.dhash4.to_s(2)}".to_i(2)
"#{node.dhash1.to_s(2)}#{node.dhash2.to_s(2)}#{node.dhash3.to_s(2)}#{node.dhash4.to_s(2)}".to_i
"#{node.dhash1.to_s(2)}#{node.dhash2.to_s(2)}#{node.dhash3.to_s(2)}#{node.dhash4.to_s(2)}".to_i(2)
num = "#{node.dhash1.to_s(2)}#{node.dhash2.to_s(2)}#{node.dhash3.to_s(2)}#{node.dhash4.to_s(2)}".to_i(2)
num.bytes
num.bit_length
node.dash1.bit_length
node.dhash1.bit_length
node.dhash2.bit_length
node.dhash4.bit_length
node.dhash3.bit_length
node2 = node.descendents.sample
num2 = "#{node2.dhash1.to_s(2)}#{node2.dhash2.to_s(2)}#{node2.dhash3.to_s(2)}#{node2.dhash4.to_s(2)}".to_i(2)
(num ^ num2).to_s(2).count('1')
edit -t
distance = 0
hashes1.length.times do |idx|
  distance += (hashes1[idx] ^ hashes2[idx]).to_s(2).count('1')
end
distance
VpNode.directions[:left]
arr = [1,2,3,4]
arr.push 5
arr.push(5,6,7]
arr.push(5,6,7)
node
node.children.where(direction: [1, 0])
url 
url = 'http://assets.inhabitat.com/wp-content/blogs.dir/1/files/2015/10/Panasonic-HIT-module-lead.jpg'
exit
cd VpNode
url = 'http://assets.inhabitat.com/wp-content/blogs.dir/1/files/2015/10/Panasonic-HIT-module-lead.jpg'
existing_image = ProductImage.find_by(original_url: url)
image_obj = if existing_image.present?
  Hashie::Mash.new(existing_image.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze))
else
  hashes = DHasher.hash_from_path(url)
  i = 0
  hashes.each_with_object(Hashie::Mash.new) do |hash, obj|
    obj["dhash#{i+=1}"] = hash
  end
end
image_obj
root = VpNode.root_node
tau = 45
nodes_to_visit = [root]
matches = []
nodes_to_visit.length > 0
node = nodes_to_visit.shift
distance = DHasher.distance(node.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze), image_obj)
distance <= tau
node.is_leaf?
dirs = []
distance < node.mu
dirs << VpNode.directions[:right]
distance < node.mu + tau
node.mu
distance
dirs << VpNode.directions[:left] if distance < node.mu + tau # partial overlap
nodes_to_visit.push(*node.children.where(direction: dirs))
nodes_to_visit
nodes_to_visit.class
node = nodes_to_visit.shift
distance = DHasher.distance(node.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze), image_obj)
dirs = []
if distance < node.mu
  # the object is in mu range of Vintage Point
  dirs << VpNode.directions[:left]
  dirs << VpNode.directions[:right] if distance >= node.mu - tau # partial overlap
else
  dirs << VpNode.directions[:right]
  dirs << VpNode.directions[:left] if distance < node.mu + tau # partial overlap
end
dirs
nodes_to_visit.push(*node.children.where(direction: dirs))
node = nodes_to_visit.shift
distance = DHasher.distance(node.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze), image_obj)
dirs = []
if distance < node.mu
  # the object is in mu range of Vintage Point
  dirs << VpNode.directions[:left]
  dirs << VpNode.directions[:right] if distance >= node.mu - tau # partial overlap
else
  dirs << VpNode.directions[:right]
  dirs << VpNode.directions[:left] if distance < node.mu + tau # partial overlap
end
dirs
nodes_to_visit.push(*node.children.where(direction: dirs))
node = nodes_to_visit.shift
distance = DHasher.distance(node.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze), image_obj)
dirs = []
if distance < node.mu
  # the object is in mu range of Vintage Point
  dirs << VpNode.directions[:left]
  dirs << VpNode.directions[:right] if distance >= node.mu - tau # partial overlap
else
  dirs << VpNode.directions[:right]
  dirs << VpNode.directions[:left] if distance < node.mu + tau # partial overlap
end
nodes_to_visit.push(*node.children.where(direction: dirs))
node = nodes_to_visit.shift
distance = DHasher.distance(node.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze), image_obj)
if distance <= tau
  matches << node.ref_image_id
end
dirs = []
if distance < node.mu
  # the object is in mu range of Vintage Point
  dirs << VpNode.directions[:left]
  dirs << VpNode.directions[:right] if distance >= node.mu - tau # partial overlap
else
  dirs << VpNode.directions[:right]
  dirs << VpNode.directions[:left] if distance < node.mu + tau # partial overlap
end
nodes_to_visit.push(*node.children.where(direction: dirs))
while nodes_to_visit.length > 0
  node = nodes_to_visit.shift
  distance = DHasher.distance(node.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze), image_obj)
  if distance <= tau
    matches << node.ref_image_id
  end
  # TODO: create cache column
  if node.is_leaf?
    node.product_images.each do |image|
      matches << image.id if (DHasher.distance(image, image_obj) < tau)
    end
    next
  end
  # TODO: Shrink Tau if matches.count >= number of desired results
  # and start evicting images based on distance from query image
  # nodes_to_visit.append(node.children.first) if distance - tau <= node.mu && node.children
  # nodes_to_visit.append(node.right_child) if distance + tau >= node.mu && node.right_child
  dirs = []
  if distance < node.mu
    # the object is in mu range of Vintage Point
    dirs << VpNode.directions[:left]
    dirs << VpNode.directions[:right] if distance >= node.mu - tau # partial overlap
  else
    gh                                     gh                                     gh                                     gh      sh(                       re                                              "M                                       
edit
exit
url = 'http://assets.inhabitat.com/wp-content/blogs.dir/1/files/2015/10/Panasonic-HIT-module-lead.jpg'
VpNode.find_nearest_neighbors(url)
require 'd_hasher'
DHasher.hash_from_path(
url = 'http://assets.inhabitat.com/wp-content/blogs.dir/1/files/2015/10/Panasonic-HIT-module-lead.jpg'
DHasher.hash_from_path(url)
exit
ProductImage.find 9344795
img = _
img.attachment.url
url = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/009/344/795/original/open-uri20160713-26532-1a8mtcc?1478752311'
VpNode.find_nearest_neighbors(url)
ProductImage.find(10850192).attachment.url
edit -t
ProductImage.where(id: ids).map {|i| i.attachment.url.gsub('/development', '') }
exit
ProductImage.find(10850192).attachment.url
url = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/010/850/192/original/open-uri20160721-32094-2gwq61?1478408704'
edit =t
find_img
edit
find_img url
1060 / 60
edit
ids
exit
def find_img(url)
exit
url = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/010/850/192/original/open-uri20160721-32094-2gwq61?1478408704'
VpNode.find_nearest_neighbors(url)
VpNode.count
137593 - 125453
_ / VpNode.size.to_f
_ / VpNode.count.to_f
exit
ActivityLog.count
reload!
exit
url = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/010/850/192/original/open-uri20160721-32094-2gwq61?1478408704'
VpNode.find_nearest_neighbors(url)
ids = _
ProductImage.where(id: ids).map {|i| i.attachment.url.gsub('/development', '') }
ProductImage.includes(listings: :sellers).where(id: ids).flat_map {|i| i.listings.map(&:seller_ids) }
sellers = _
sellers.flatten!
sellers.uniq!
Seller.where(id: sellers).pluck(:id, :name)
ProductImage.includes(listings: :sellers).where(id: ids).flat_map {|i| i.listings.map(&:url) }
exit
url = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/010/850/192/original/open-uri20160721-32094-2gwq61?1478408704'
VpNode.find_nearest_neighbors(url)
VpNode.where.not(id: VpNode.where.not(parent_id: nil).select(:parent_id)).count
VpNode.pluck(:mu)
edi t-t
edit -t
mus.map!(&:to_i)
mus.sum
VpNode.count
reload!
VpNode.find_nearest_neighbors(url)
reload!
VpNode.find_nearest_neighbors(url)
exit
url = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/010/850/192/original/open-uri20160721-32094-2gwq61?1478408704'
VpNode.find_nearest_neighbors(url)
exit
url = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/010/850/192/original/open-uri20160721-32094-2gwq61?1478408704'
cd VpNode
image_obj = if existing_image.present?
  Hashie::Mash.new(existing_image.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze))
else
  hashes = DHasher.hash_from_path(url)
  i = 0
  hashes.each_with_object(Hashie::Mash.new) do |hash, obj|
    obj["dhash#{i+=1}"] = hash
  end
end
existing_image = ProductImage.find_by(original_url: url)
url = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/010/850/192/original/open-uri20160721-32094-2gwq61?1478408704'
existing_image = ProductImage.find_by(original_url: url)
image_obj = if existing_image.present?
  Hashie::Mash.new(existing_image.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze))
else
  hashes = DHasher.hash_from_path(url)
  i = 0
  hashes.each_with_object(Hashie::Mash.new) do |hash, obj|
    obj["dhash#{i+=1}"] = hash
  end
end
root = VpNode.root_node
tau = 25
tau = 40
nodes_to_visit = [root]
matches = []
nodes_visited = 0
leaves_visited = 0
nodes_to_visit = { :10000 => [root] }
nodes_to_visit = { 10000 => [root] }
matches = []
nodes_visited = 0
leaves_visited = 0
nodes_to_visit.first
nodes_to_visit.sort!
nodes_to_visit.sort
nodes_to_visit
nodes_to_visit.keys.min
nodes_to_visit.length > 0
node = nodes_to_visit.shift
node = nodes_to_visit.shift[1]
node = node.last
max_dist = node.shift
node
max_dist = 1000
node = VpNode.first
dist = max_dist
prev_dist = dist
dist = nil
distance = DHasher.distance(node.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze), image_obj)
distance <= tau
dirs = []
distance < node.mu
dirs << VpNode.directions[:left] if distance + tau >= node.mu
dirs << VpNode.directions[:right] if distance - tau <= node.mu
dirs
distance
node.children.where(direction: dirs).each do |child|
  nodes_to_visit.unshift([distance, child])
end
nodes_to_visit =  [1000, root]
nodes_to_visit
prev_dist, node = nodes_to_visit.shift
node
nodes_to_visit
nodes_to_visit =  [[1000, root]]
prev_dist, node = nodes_to_visit.shift
tau = 45
matches = []
nodes_visited = 0
leaves_visited = 0
distance = DHasher.distance(node.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze), image_obj)
node.is_leaf?
dirs = []
if distance < node.mu
  # the object is in mu range of Vintage Point
  dirs << VpNode.directions[:right] if distance - tau <= node.mu # 119918 visited
  dirs << VpNode.directions[:left] if  distance + tau >= node.mu
  # dirs << VpNode.directions[:left]  # THIS SEARCHED 125453 nodes
  # dirs << VpNode.directions[:right] if distance - tau > node.mu - tau # partial overlap
else
  dirs << VpNode.directions[:left] if distance + tau >= node.mu
  dirs << VpNode.directions[:right] if distance - tau <= node.mu
  # dirs << VpNode.directions[:right]
  # dirs << VpNode.directions[:left] if distance <= node.mu + tau # partial overlap
end
node.children.where(direction: dirs).each do |child|
  nodes_to_visit.unshift([distance, child])
end
nodes_to_visit
nodes_to_visit.sort_by! { |pair| pair[0] }
prev_dist, node = nodes_to_visit.shift
distance = DHasher.distance(node.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze), image_obj)
distance <= tau
dirs = []
distance < node.mu
distance - tau <= node.mu
(distance - tau <= node.mu && distance < prev_dist)
dirs << VpNode.directions[:left] if distance + tau >= node.mu
dirs << VpNode.directions[:right] if (distance - tau <= node.mu && distance < prev_dist)
dirs
node.children.where(direction: dirs).each do |child|
  nodes_to_visit.unshift([distance, child])
end
nodes_to_visit.sort_by! { |pair| pair[0] }
prev_dist, node = nodes_to_visit.shift
distance = DHasher.distance(node.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze), image_obj)
distance < node.mu
distance + tau >= node.mu
(distance - tau <= node.mu && distance < prev_dist)
dirs << VpNode.directions[:left] if distance + tau >= node.mu
dirs << VpNode.directions[:right] if (distance - tau <= node.mu && distance < prev_dist)
dirs.uniq!
node.children.where(direction: dirs).each do |child|
  nodes_to_visit.unshift([distance, child])
end
nodes_to_visit.sort_by! { |pair| pair[0] }
exit
url = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/010/850/192/original/open-uri20160721-32094-2gwq61?1478408704'
VpNode.find_nearest_neighbors(url)
DHasher.hash_from_path(url)
hashes = _
i = 0
img_obj =                   hashes.each_with_object(Hashie::Mash.new) do |hash, obj|
  obj["dhash#{i+=1}"] = hash
end
img_obj
img = ProductImage.find 10168540
Dhasher.distance(img, img_obj)
DHasher.distance(img, img_obj)
img.attachment.url
exit
url = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/010/850/192/original/open-uri20160721-32094-2gwq61?1478408704'
VpNode.find_nearest_neighbors(url, 20)
exit
url = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/010/850/192/original/open-uri20160721-32094-2gwq61?1478408704'
VpNode.find_nearest_neighbors(url, 20)
cd VpNode
url = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/010/850/192/original/open-uri20160721-32094-2gwq61?1478408704'
existing_image = ProductImage.find_by(original_url: url)
image_obj = if existing_image.present?
  Hashie::Mash.new(existing_image.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze))
else
  hashes = DHasher.hash_from_path(url)
  i = 0
  hashes.each_with_object(Hashie::Mash.new) do |hash, obj|
    obj["dhash#{i+=1}"] = hash
  end
end
root = VpNode.root_node
nodes_to_visit =  [root]
neighbors = []
nodes_visited = 0
leaves_visited = 0
nodes_visited += 1
node = nodes_to_visit.shift
distance = DHasher.distance(node.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze), image_obj)
exit
url = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/010/850/192/original/open-uri20160721-32094-2gwq61?1478408704'
VpNode.find_nearest_neighbors(url, 20)
cd VpNode
url = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/010/850/192/original/open-uri20160721-32094-2gwq61?1478408704'
existing_image = ProductImage.find_by(original_url: url)
image_obj = if existing_image.present?
  Hashie::Mash.new(existing_image.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze))
else
  hashes = DHasher.hash_from_path(url)
  i = 0
  hashes.each_with_object(Hashie::Mash.new) do |hash, obj|
    obj["dhash#{i+=1}"] = hash
  end
end
root = VpNode.root_node
tau = 1000
nodes_to_visit =  [root]
neighbors = []
nodes_visited = 0
leaves_visited = 0
nodes_visited += 1
node = nodes_to_visit.shift
distance = DHasher.distance(node.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze), image_obj)
nodes_visited += 1
node = nodes_to_visit.shift
distance = DHasher.distance(node.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze), image_obj)
distance
node
kkk      nodes_visited += 1
nodes_visited += 1
nodes_visited = 0
leaves_visited = 0
nodes_visited += 1
node = nodes_to_visit.shift
node
node = VpNode.first
distance = DHasher.distance(node.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze), image_obj)
distance <= tau
neighbors << { image_id: node.ref_image_id, dist: distance }
neighbors
neighbors.sort_by! { |n| n[:dist] }
neighbors.pop if neighbors.length > k
k = 20
neighbors.pop if neighbors.length > k
tau = neighbors.last[:dist]
node.is_leaf?
dirs = []
distance < node.mu
distance >= node.mu - tau
exit
url = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/010/850/192/original/open-uri20160721-32094-2gwq61?1478408704'
VpNode.find_nearest_neighbors(url, 20)
arr = (1..20).to_a
arr.slice(0, 10)
arr.slice!(0, 10)
arr
arr.slice!(10, -1)
arr = (1..20).to_a
arr.slice!(10, -1)
arr
arr.slice!(9,)
arr
arr.slice!(9,-1)
arr
exit
url = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/010/850/192/original/open-uri20160721-32094-2gwq61?1478408704'
VpNode.find_nearest_neighbors(url, 20)
exit
url = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/010/850/192/original/open-uri20160721-32094-2gwq61?1478408704'
VpNode.find_nearest_neighbors(url, 20)
cd VpNode
url = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/010/850/192/original/open-uri20160721-32094-2gwq61?1478408704'
existing_image = ProductImage.find_by(original_url: url)
image_obj = if existing_image.present?
  Hashie::Mash.new(existing_image.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze))
else
  hashes = DHasher.hash_from_path(url)
  i = 0
  hashes.each_with_object(Hashie::Mash.new) do |hash, obj|
    obj["dhash#{i+=1}"] = hash
  end
end
root = VpNode.root_node
tau = 1000
nodes_to_visit =  [root]
neighbors = []
nodes_visited = 0
leaves_visited = 0
nodes_visited += 1
node = nodes_to_visit.shift
distance = DHasher.distance(node.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze), image_obj)
distance <= tau
neighbors << { image_id: node.ref_image_id, dist: distance }
neighbors.sort_by! { |n| n[:dist] }
neighbors.pop if neighbors.length > k
# shrink tau
tau = neighbors.last[:dist]
node.is_leaf?
if node.children.length == 0
node.children.length == 0
node.children
node.left_child
node.children
node.children.select {|c| c.direction == 0 }
node.children.select {|c| c.direction == 1 }
node.children
node.children.all.select {|c| c.direction == 1 }
node.children.first
node.children.first.direction
node.children.all.select {|c| c.direction == 'left' }
node.preload(:children)
node.load(:children)
node
distance < node.mu
distance >= node.mu - tau
nodes_to_visit << node.right_child if node.right_child
eixt
exit
node = VpNode.first
node.left_child.nil? && node.right_child.nil?
node.left_child
exit
url = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/010/850/192/original/open-uri20160721-32094-2gwq61?1478408704'
VpNode.find_nearest_neighbors(url, 20)
exit
VpNode.count
url = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/010/850/192/original/open-uri20160721-32094-2gwq61?1478408704'
VpNode.find_nearest_neighbors(url, 20)
exit
url = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/010/850/192/original/open-uri20160721-32094-2gwq61?1478408704'
existing_image = ProductImage.find_by(original_url: url)
image_obj = if existing_image.present?
  Hashie::Mash.new(existing_image.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze))
else
  hashes = DHasher.hash_from_path(url)
  i = 0
  hashes.each_with_object(Hashie::Mash.new) do |hash, obj|
    obj["dhash#{i+=1}"] = hash
  end
end
image_obj
image = VpNode.first
image1 = image_obj
image2 = image
distance = 0
hashes1 = image1.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze).values
hashes2 = image2.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze).values
hashes1.length.times do |idx|
  d = (hashes1[idx] ^ hashes2[idx]).to_s(2).count('1')
  distance += d**2
end
distance
Math.sqrt(distance)
distance1 = _
distance = 0
hashes1 = image1.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze).values
hashes2 = image2.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze).values
hashes1.length.times do |idx|
  d = (hashes1[idx] ^ hashes2[idx]).to_s(2).count('1')
  distance += d
end
distance
distance0 = 139
image3 = VpNode.where(ref_image_id = 10168540)
image3 = VpNode.where(ref_image_id: 10168540)
image3 = ProductImage.find(10168540)
hashes2 = image3.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze).values
hashes1.length.times do |idx|
  d = (hashes1[idx] ^ hashes2[idx]).to_s(2).count('1')
  distance += d
end
distance
image_obj
hashes1 = image_obj.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze).values
hashes2 = image3.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze).values
hashes1.length.times do |idx|
  d = (hashes1[idx] ^ hashes2[idx]).to_s(2).count('1')
  distance += d
end
distance
distance = 0
hashes1 = image_obj.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze).values
hashes2 = image3.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze).values
hashes1.length.times do |idx|
  d = (hashes1[idx] ^ hashes2[idx]).to_s(2).count('1')
  distance += d
end
distance
distance = 0
hashes1.length.times do |idx|
  d = (hashes1[idx] ^ hashes2[idx]).to_s(2).count('1')
  distance += d**2
end
Math.sqrt(distance)
distance3 = _
[distance0, distance1, distance3]
exit
cd VpNode
query = nil
images = nil
node = nil
node_id = nil
exit
VpNode.rebuild_tree
[139, 70.74602462329597, 13, 8.660254037844387]
139 - 13
70.74 - 8.6
exit
VpNode.count
VpNode.coutn
VpNode.count
VpNode.pluck(:mu)
VpNode.where(mu: 11)
node = VpNode.where(mu: 11).first
node.product_images
node.product_images.map {|i| i.attachment.url.gsub('/development', '' }
node.product_images.map {|i| i.attachment.url.gsub('/development', '') }
node = VpNode.where(mu: 2).first
node.product_images
[node.ref_image.attachment.url.gsub('/development', ''), node.product_images.map {|i| i.attachment.url.gsub('/development', '') }].flatten
img = node.ref_image
node.product_images.map do |i|
  hashes1 = img.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze).values
  hashes2 = i.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze).values
  distance = 0
  hashes1.length.times do |idx|
    d = (hashes1[idx] ^ hashes2[idx]).to_s(2).count('1')
    distance += d
  end
  distance
end
VpNode.count
Time.now - VpNode.first.created_at
_ / 60
Time.now - VpNode.first.created_at
_ / 60
VpNode.count
VpNode.count / (Time.now - Vp.created_at) / 60
VpNode.count / (Time.now - VpNode.first.created_at) / 60
VpNode.count / ((Time.now - VpNode.first.created_at) / 60)
VpNode.count
VpNode.count / 2
node = VpNode.find_by(mu: 8)
node.ref_image_attachment.url
node.ref_image.attachment.url
node.product_images.count
node.product_images.map {|i| i.attachment.url.gsub('/development', '') }
node = VpNode.find_by(mu: 3)
node.ref_image.attachment.url
node = VpNode.find_by(mu: 14)
node.ref_image.attachment.url.gsub('/development', '')
node.product_images.count
node.product_images.map {|i| i.attachment.url.gsub('/development', '') }
VpNode.count
edit -t
ProductImage.where(id: ids)
images = ProductImage.where(id: ids)
images.map {|i| i.attachment.url.gsub('/development', '') }
image = images.first
reload!
images = ProductImage.where(id: ids).to_a
image = images.first
images.map {|i| DHasher.distance(image, i) }
images.last
images.map {|i| i.attachment.url.gsub('/development', '') }
VpNode.count
node = VpNode.find 3
node.children
node = VpNode.where(mu: 2)
image = node.ref_image
node = node.first
image = node.ref_image
node.product_images.map {|i| DHasher.distance(image, i) }
VpNode.count
VpNode.first
VpNode.count
ProductImage.count
VpNode.count
exit
VpNode.count
ProductImage.first
image = _
ProductImage.where(image.slice('dhash1', 'dhash2', 'dhash3', 'dhash4')).count
ProductImage.where(image.slice('dhash1', 'dhash2', 'dhash3')).count
ProductImage.where(image.slice('dhash1', 'dhash2')).count
ProductImage.where(image.slice('dhash1')).count
ProductImage.where(image.slice()).count
ProductImage.count
exit
VpNode.count
exit
exit
VpNode.where(dhash1: 9128090395999590964)
url = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/010/850/192/original/open-uri20160721-32094-2gwq61?1478408704'
DHasher.hash_from_path(url)
ProductImage.find 9583641
existing_image = ProductImage.find_by(original_url: url)
image_obj = if existing_image.present?
  Hashie::Mash.new(existing_image.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze))
else
  hashes = DHasher.hash_from_path(url)
  i = 0
  hashes.each_with_object(Hashie::Mash.new) do |hash, obj|
    obj["dhash#{i+=1}"] = hash
  end
end
img = ProductImage.find 9583641
DHasher.distance(img, image_obj)
VpNode.find_nearest_neighbors(url, 15)
images = _
ids = images.map {|i| i[:image_id]}
exit
cd VpNode
url = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/010/850/192/original/open-uri20160721-32094-2gwq61?1478408704'
k = 20
existing_image = ProductImage.find_by(original_url: url)
image_obj = if existing_image.present?
  Hashie::Mash.new(existing_image.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze))
else
  hashes = DHasher.hash_from_path(url)
  i = 0
  hashes.each_with_object(Hashie::Mash.new) do |hash, obj|
    obj["dhash#{i+=1}"] = hash
  end
end
root = VpNode.root_node
tau = 10
nodes_to_visit =  [root]
neighbors = []
nodes_visited = 0
leaves_visited = 0
nodes_to_visit.length > 0
nodes_visited += 1
node = nodes_to_visit.shift
distance = DHasher.distance(node.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze), image_obj)
tau = 1000
distance <= tau
neighbors << { image_id: node.ref_image_id, dist: distance }
neighbors.sort_by! { |n| n[:dist] }
neighbors.pop if neighbors.length > k
k
tau = neighbors.last[:dist]
node.is_leaf?
distance < node.mu
node.mu
distance < node.mu + tau
nodes_to_visit.unshift(node.left_child) if node.left_child
distance >= node.mu - tau
neighbors
nodes_visited += 1
node = nodes_to_visit.shift
distance = DHasher.distance(node.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze), image_obj)
distance <= tau
tau
neighbors
neighbors << { image_id: node.ref_image_id, dist: distance }
neighbors.sort_by! { |n| n[:dist] }
neighbors.pop if neighbors.length > k
tau = neighbors.last[:dist]
distance < node.mu
distance < node.mu + tau
nodes_to_visit.unshift(node.left_child) if node.left_child
distance >= node.mu - tau
nodes_visited += 1
node = nodes_to_visit.shift
distance = DHasher.distance(node.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze), image_obj)
VpNode.first.mu
exit
VpNode.find_nearest_neighbors(url, 80)
url = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/010/850/192/original/open-uri20160721-32094-2gwq61?1478408704'
VpNode.find_nearest_neighbors(url, 80)
eit
VpNode.last
node = VpNode.find(2)
node.self_and_descendents
node.self_and_descendents.pluck(:id, :mu)
node.self_and_descendents.pluck(:id, :mu).map {|el| el[1].to_f}
node.self_and_descendents.pluck(:id, :mu).map {|el| [el[0], el[1].to_f] }
url = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/010/850/192/original/open-uri20160721-32094-2gwq61?1478408704'
VpNode.find_nearest_neighbors(url)
reload!
url = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/010/850/192/original/open-uri20160721-32094-2gwq61?1478408704'
VpNode.find_nearest_neighbors(url)
reload!
VpNode.find_nearest_neighbors(url)
reload!
VpNode.find_nearest_neighbors(url)
url
exit
url = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/010/850/192/original/open-uri20160721-32094-2gwq61?1478408704'
VpNode.find_nearest_neighbors(url)
ProductImage.find(10876722).attachment.url
reload!
VpNode.find_nearest_neighbors(url)
exit
image = ProductImage.first
image.vp_node
exit
image = ProductImage.first
image.vp_node
exit
image = ProductImage.first
image.vp_node
VpNode.first
node = 
_
node.mu2
node.mu3
node.vp1
node.vp2
node.vp3
node.left_child
node.left_child.to_sql
node.left_child
reload!
node = VpNode.first
node.left_child
reload!
node = VpNode.first
node.left_child_one
reload!
node = VpNode.first
node.parent
reload
reload!
node = VpNode.first
node.children
reload!
node = VpNode.first
node.child_ids
node.vp1
ProductImage.where.not(vp_node_id: nil).update_all(vp_node_id: nil)
VpNodeProductImage.connection.execute("truncate table #{VpNodeProductImage.quoted_table_name}")
VpNodeProductImage.first
cd VpNode
connection.execute("truncate table #{quoted_table_name}")
VpNode.delete_all
node = create(ancestry: '/')
query = 'SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE dhash1 IS NOT NULL'
images = ProductImage.connection.select_all(query).to_ary; nil
images.first
level = 1
nodes_to_balance = [[node, images, level]]; nil
vp1_image = images.sample
node, images, level = nodes_to_balance.shift; nil
level
vp1_image = images.sample
node.attributes = {
  vp1_id: vp1_image['id'],
  vp1_dhash1: vp1_image['dhash1'],
  vp1_dhash2: vp1_image['dhash2'],
  vp1_dhash3: vp1_image['dhash3'],
  vp1_dhash4: vp1_image['dhash4']
}
images_with_distances = []
images.count.times do |_idx|
  image2 = images.pop
  unless image['id'] == image2['id']
    images_with_distances << image2.merge!({ 'distance' => DHasher.distance(image, image2) })
  end
end
images_with_distances = []
images = ProductImage.connection.select_all(query).to_ary; nil
images_with_distances = []
images.count.times do |_idx|
  image2 = images.pop
  unless vp1_image['id'] == image2['id']
    images_with_distances << image2.merge!({ 'distance' => DHasher.distance(image, image2) })
  end
end
images.count.times do |_idx|
  image2 = images.pop
  unless vp1_image['id'] == image2['id']
    images_with_distances << image2.merge!({ 'distance' => DHasher.distance(vp1_image, image2) })
  end
end
images_with_distances.sort_by! { |img| img['distance'] }; nil
images_with_distances.first
vp1_image['id']
9437128
ProductImage.where(id: [9437128, 9437147]).map {|i| i.attachment.url.gsub('/development', '') }
exit
cd VpNode
query = nil
images = nil
node = nil
ProductImage.where.not(vp_node_id: nil).update_all(vp_node_id: nil)
VpNodeProductImage.connection.execute("truncate table #{VpNodeProductImage.quoted_table_name}")
# connection.execute("truncate table #{quoted_table_name}")
VpNode.delete_all
node = create(ancestry: '/')
query = 'SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE dhash1 IS NOT NULL'
images = ProductImage.connection.select_all(query).to_ary; nil
level = 1
nodes_to_balance = [[node, images, level]]; nil
node, images, level = nodes_to_balance.shift; nil
vp1_image = images.sample
node.attributes = {
  vp1_id: vp1_image['id'],
  vp1_dhash1: vp1_image['dhash1'],
  vp1_dhash2: vp1_image['dhash2'],
  vp1_dhash3: vp1_image['dhash3'],
  vp1_dhash4: vp1_image['dhash4']
}
images_with_distances = []
images.count.times do |_idx|
  image2 = images.pop
  unless vp1_image['id'] == image2['id']
    images_with_distances << image2.merge!({ 'distance' => DHasher.distance(vp1_image, image2) })
  end
end
images_with_distances.sort_by! { |img| img['distance'] }; nil
images_with_distances.first
ids = [vp1_image['id'], _['id']
]
ProductImage.where(id: ids).map {|i| i.attachment.url.gsub('/development', '') }
images_with_distances.first(10)
mid = (images_with_distances.length - 1) / 2.0
median = (images_with_distances[mid.floor]['distance'] + images_with_distances[mid.ceil]['distance']) / 2.0
mid = (images_with_distances.length - 1) / 2
images_with_distances[mid]['distance']
images_with_distances.last
mid = (images_with_distances.length - 1) / 2.0
median = (images_with_distances[mid.floor]['distance'] + images_with_distances[mid.ceil]['distance']) / 2.0
node.mu1 = median
node
node.save!
images_with_distances.length <= VpNode::MAX_IMAGES
level
level = 0
distance_with_images.first
images_with_distances
images_with_distances.map! do |image|
  image.merge!({'path' => [image['distance']})
end
images_with_distances.map! do |image|
  image.merge!({ 'path' => image['distance'] })
end
images_with_distances.map! do |image|
  image.merge!({ 'path' => [image['distance']] })
end; nil
images_with_distances.first
node.ancestry
exit
cd VpNode
exit
cd VpNode
ProductImage.where.not(vp_node_id: nil).update_all(vp_node_id: nil)
VpNodeProductImage.connection.execute("truncate table #{VpNodeProductImage.quoted_table_name}")
# connection.execute("truncate table #{quoted_table_name}")
VpNode.delete_all
node = create(ancestry: '/')
uery = 'SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE dhash1 IS NOT NULL'
images = ProductImage.connection.select_all(query).to_ary; nil
ProductImage.where.not(vp_node_id: nil).update_all(vp_node_id: nil)
VpNodeProductImage.connection.execute("truncate table #{VpNodeProductImage.quoted_table_name}")
# connection.execute("truncate table #{quoted_table_name}")
VpNode.delete_all
node = create(ancestry: '/')
uery = 'SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE dhash1 IS NOT NULL'
images = ProductImage.connection.select_all(query).to_ary; nil
ProductImage.where.not(vp_node_id: nil).update_all(vp_node_id: nil)
VpNodeProductImage.connection.execute("truncate table #{VpNodeProductImage.quoted_table_name}")
VpNode.delete_all
node = create(ancestry: '/')
query = 'SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE dhash1 IS NOT NULL'
images = ProductImage.connection.select_all(query).to_ary; nil
nodes_to_balance = [[node, images]]; nil
node, images = nodes_to_balance.shift; nil
vp1_image = images.sample
node.attributes = {
  vp1_id: vp1_image['id'],
  vp1_dhash1: vp1_image['dhash1'],
  vp1_dhash2: vp1_image['dhash2'],
  vp1_dhash3: vp1_image['dhash3'],
  vp1_dhash4: vp1_image['dhash4']
}
images_with_distances = calc_distances(images, vp1_image)
images_with_distances.first
remove_duplicates(vp1_image, images_with_distances) if images_with_distances.first['distance'] == 0
mid = (images_with_distances.length - 1) / 2.0
median1 = (images_with_distances[mid.floor]['distance'] + images_with_distances[mid.ceil]['distance']) / 2.0
node.mu1 = median1
left_images, right_images = split_images(median1, images_with_distances)
left_images.length
right_images.length
right_images.length - left_images.length
vp2_image = right_images.pop
node.attributes = {
  vp2_id: vp2_image['id'],
  vp2_dhash1: vp2_image['dhash1'],
  vp2_dhash2: vp2_image['dhash2'],
  vp2_dhash3: vp2_image['dhash3'],
  vp2_dhash4: vp2_image['dhash4']
}
node
left_images = calc_distances(left_images, vp2_image)
right_images = calc_distances(right_images, vp2_image)
right_images.first
remove_duplicates(vp1_image, right_images) if right_images.first['distance'] == 0
mid = (left_images.length - 1) / 2.0
median2 = (left_images[mid.floor]['distance'] + left_images[mid.ceil]['distance']) / 2.0
node.mu2 = median2
mid = (right_images.length - 1) / 2.0
median3 = (right_images[mid.floor]['distance'] + right_images[mid.ceil]['distance']) / 2.0
node.mu3 = median3
left_images1, right_images1 = split_images(median2, left_images)
left_images.length
left_images2, right_images2 = split_images(median3, right_images)
ancestry = "#{node.ancestry}#{node.id}/"
node.id
lt_child1 = VpNode.create(parent_id: node.id, ancestry: ancestry)
rt_child1 = VpNode.create(parent_id: node.id, ancestry: ancestry)
lt_child2 = VpNode.create(parent_id: node.id, ancestry: ancestry)
rt_child2 = VpNode.create(parent_id: node.id, ancestry: ancestry)
node.attributes = {
  lt_child1_id: lt_child1.id, rt_child1_id: rt_child1.id,
  lt_child2_id: lt_child2.id, rt_child2_id: rt_child2.id,
}
node.save!
child_nodes = [[lt_child1, left_images1], [rt_child1, right_images1],
[lt_child2, left_images2], [rt_child2, right_images2]]
nodes_to_balance.unshift(*child_nodes)
nodes_to_balance.length
edit -t
ActivityLog.count
exit
ActivityLog.first
ActivityLog.count
node = VpNode.find 131125
nodes_to_visit.count
exit
cd VpNode
edit -t
nodes_to_balance = [[node, images]]; nil
puts nodes_to_balance.length
node, images = nodes_to_balance.shift; nil
vp1_image = images.sample
node.attributes = {
  vp1_id: vp1_image['id'],
  vp1_dhash1: vp1_image['dhash1'],
  vp1_dhash2: vp1_image['dhash2'],
  vp1_dhash3: vp1_image['dhash3'],
  vp1_dhash4: vp1_image['dhash4']
}
dist_key = 'dist1'.freeze
images_with_distances = calc_distances(images, vp1_image, dist_key); nil
remove_duplicates(vp1_image, images_with_distances, dist_key) if images_with_distances.first[dist_key] == 0
mid = (images_with_distances.length - 1) / 2.0
median1 = (images_with_distances[mid.floor][dist_key] + images_with_distances[mid.ceil][dist_key]) / 2.0
node.mu1 = median1
images_with_distances.length <= VpNode::MAX_IMAGES + 1
left_images, right_images = split_images(median1, images_with_distances, dist_key)
vp2_image = right_images.pop
node.attributes = {
  vp2_id: vp2_image['id'],
  vp2_dhash1: vp2_image['dhash1'],
  vp2_dhash2: vp2_image['dhash2'],
  vp2_dhash3: vp2_image['dhash3'],
  vp2_dhash4: vp2_image['dhash4']
}
dist_key = 'dist2'.freeze
left_images = calc_distances(left_images, vp2_image, dist_key)
right_images = calc_distances(right_images, vp2_image, dist_key); nil
right_images.first
left_images.first
remove_duplicates(vp1_image, right_images, dist_key) if right_images.first[dist_key] == 0
mid = (left_images.length - 1) / 2.0
median2 = (left_images[mid.floor][dist_key] + left_images[mid.ceil][dist_key]) / 2.0
node.mu2 = median2
mid = (right_images.length - 1) / 2.0
median3 = (right_images[mid.floor][dist_key] + right_images[mid.ceil][dist_key]) / 2.0
node.mu3 = median3
left_images1, right_images1 = split_images(median2, left_images, dist_key)
left_images2, right_images2 = split_images(median3, right_images, dist_key)
left_images2.first
VpNode.count
ancestry = "#{node.ancestry}#{node.id}/"
lt_child1 = VpNode.create(parent_id: node.id, ancestry: ancestry)
rt_child1 = VpNode.create(parent_id: node.id, ancestry: ancestry)
lt_child2 = VpNode.create(parent_id: node.id, ancestry: ancestry)
rt_child2 = VpNode.create(parent_id: node.id, ancestry: ancestry)
node.attributes = {
  lt_child1_id: lt_child1.id, rt_child1_id: rt_child1.id,
  lt_child2_id: lt_child2.id, rt_child2_id: rt_child2.id,
}
node.save!
child_nodes = [[lt_child1, left_images1], [rt_child1, right_images1],
[lt_child2, left_images2], [rt_child2, right_images2]]
child_nodes.count
nodes_to_balance.unshift(*child_nodes)
edit -t
VpNode.last
VpNode.count
VpNode.last
VpNode.first 20
VpNode.pluck(:id, :ancestry)
node = VpNode.find(131158)
node.product_images
node.product_images.first
VpNodeProductImage.where(vp_node_id: node.id).count
exit
node = VpNode.find(131158)
node.product_images
VpNodeProductImage.count
VpNode.count
VpNode.first.created_at
(Time.now - VpNode.first.created_at) / 60
VpNode.count
VpNode.pluck(:id, :ancestry)
VpNode.pluck(:id, :ancestry).last 20
VpNode.first(4)
node2 = VpNode.find(131130)
node2.children
node2.children.count
VpNode.descendants.count
node2.ancestry
node2.parent
node = VpNode.first
node2.child_ids
node2.descendents.count
VpNode.count
VpNode.descendants.count
node.descendents.count
VpNode.descendants.count
node.descendents.count
VpNode.count
node.descendents.count
VpNode.count
node.descendents.last
node = _
node.product_images
ActivityLog.count
ActivityLog.all
Time.now
Time.now.utc
ProductImage.where(vp_node_id: 16453).count
VpNodeProductImage.count
exit
exit
cd VpNod
cd VpNode
ProductImage.where.not(vp_node_id: nil).update_all(vp_node_id: nil)
VpNodeProductImage.connection.execute("truncate table #{VpNodeProductImage.quoted_table_name}")
ActiveRecord::Base.connection.reset_pk_sequence!(table_name)
delete_all
VpNode.connection.reset_pk_sequence!(table_name)
VpNode.reset_primary_key
VpNode.first
node = create(ancestry: '/')
conn = ActiveRecord::Base.connection
conn.pk_and_sequence_for(table_name)
connection.sequence_name
count
delete_all
ActiveRecord::Base.connection.execute("ALTER TABLE "#{quoted_table_name}" AUTO_INCREMENT = 1")
ActiveRecord::Base.connection.execute("ALTER TABLE #{quoted_table_name} AUTO_INCREMENT = 1")
node = create(ancestry: '/')
exit
VpNode.rebuild_tree
VpNode.count
VpNodeProductImage.count
node = VpNode.find 2
node = node.descendents.last
node.is_leaf?
node.id
node = node.descendents.last
node = VpNode.find(2).descendents.last
node = VpNode.find(3461)
node.reload
node.product_images.count
VpNodeProductImage.count
node = VpNode.root_node
node.children.children.left
node.children.left
node.lt_child1
node.lt_child_one
node = node.lt_child1
node = node.lt_child_one
node = VpNode.find(30)
VpNode.pluck(:id, :ancestry)
VpNodeProductImage.count
ActivityLog.count
ActivityLog.last
exit
exit
VpNode.rebuild_tree
VpNode.count
ProductImage.where.not(vp_node_id: nil).count
exit
VpNode.count
VpNodeProductImage.count
VpNodeProductImage.first
exit
VpNode.rebuild_tree
exit
VpNode.count
VpNodeProductImage.first
VpNodeProductImage.first.attributes
node = VpNode.find(30)
node.ancestry
node.path
node.ancestry
node.ancestry.split('/')
node.ancestry.split('/').count
vpimg = VpNodeProductImage.first
vpimg.path.count
exit
VpNode.rebuild_tree
exit
VpNode.count
node = VpNode.find 30
node.is_leaf?
node.product_images.count
VpNodeProductImage.count
VpNodeProductImage.first
point = VpNodeProductImage.first
point.vp1_distance.to_f
point.vp2_distance.to_f
VpNode.count
VpNode.first.created_at
(Time.now - VpNode.first.created_at) / 60.0
VpNode.count / ((Time.now - VpNode.first.created_at) / 60.0)
140000 / 97
140000 / 97.0 / 60.0
VpNode.count / ((Time.now - VpNode.first.created_at) / 60.0)
140000 / _ / 60.0
VpNode.count
VpNodeProductImage.count
_ / ProductImage.count.to_f
VpNode.pluck(:id, :ancestry)
VpNodeProductImage.uniq.pluck(:vp_node_id).count
VpNodeProductImage.count
ActivityLog.last
VpNode.first.create_at
VpNode.first.created_at
VpNodeProductImage.count
VpNode.last
VpNode.count
(VpNode.count * 2)
(VpNode.count * 2) + VpNodeProductImage.count
((VpNode.count * 2) + VpNodeProductImage.count) / ProductImage.where.not(dhash1: nil).count.to_f
exit
exit
ActivityLog.delete_all
VpNode.rebuild_tree
VpNode.count
exit
exit
VpNode.rebuild_tree
exit
ActivityLog.count
VpNode.rebuild_tree
VpNode.count
VpNodeProductImage.count
VpNode.count
VpNodeProductImage.count
VpNode.count
VpNodeProductImage.count
((VpNode.count * 2) + VpNodeProductImage.count) / ProductImage.where.not(dhash1: nil).count.to_f
VpNodeProductImage.count
VpNode.count
((VpNode.count * 2) + VpNodeProductImage.count) / ProductImage.where.not(dhash1: nil).count.to_f
VpNode.count
VpNodeProductImage.count
((VpNode.count * 2) + VpNodeProductImage.count) / ProductImage.where.not(dhash1: nil).count.to_f
VpNodeProductImage.count
VpNode.count
((VpNode.count * 2) + VpNodeProductImage.count) / ProductImage.where.not(dhash1: nil).count.to_f
(Time.now - VpNode.first.created_at) / 60.0
45 * 4
180 / 60
180 - 45
_ / 60.0
VpNode.count
((VpNode.count * 2) + VpNodeProductImage.count) / ProductImage.where.not(dhash1: nil).count.to_f
VpNode.count
VpNodeProductImage.count
node = VpNode.find(30)
node.ancestry
p = 9 * 2
root = VpNode.root_node
node
node.vp_node_product_images.count
node.product_images
node.product_images.count
node
node.vp_node_product_images.first
node.vp_node_product_images.pluck(:asset_id, :path)
image_ids_with_paths = node.vp_node_product_images.pluck(:asset_id, :path)
image_ids_with_paths.each do |image_id, path|
  puts image_id
  puts path[0]
  puts path[1]
  puts '*' * 80
end
ActivityLog.count
image_ids_with_paths.each do |image_id, path|
  puts image_id
  puts path[level]
  puts path[level + 1]
  puts '*' * 80
end
leve = 0
level = 0
image_ids_with_paths.each do |image_id, path|
  puts image_id
  puts path[level]
  puts path[level + 1]
  puts '*' * 80
end
path
path = []
distance1
url = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/010/850/192/original/open-uri20160721-32094-2gwq61?1478408704'
edit -t
root = VpNode.root_node
level = 0
tau = 40
nodes_to_visit = [[root, level]]
neighbors = []
search_path = []
nodes_visited = 0
leaves_visited = 0
nodes_visited += 1
node, level = nodes_to_visit.shift
distance1 = DHasher.distance(node.slice('vp1_dhash1'.freeze, 'vp1_dhash2'.freeze, 'vp1_dhash3'.freeze, 'vp1_dhash4'.freeze), image_obj)
node.vp1
node.slice('vp1_dhash1'.freeze, 'vp1_dhash2'.freeze, 'vp1_dhash3'.freeze, 'vp1_dhash4'.freeze)
image_obj
node.slice('vp1_dhash1'.freeze, 'vp1_dhash2'.freeze, 'vp1_dhash3'.freeze, 'vp1_dhash4'.freeze)
node
node.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze, 'vp1_dhash1'.freeze, 'vp1_dhash2'.freeze, 'vp1_dhash3'.freeze, 'vp1_dhash4'.freeze).values
image1 = node.slice('vp1_dhash1'.freeze, 'vp1_dhash2'.freeze, 'vp1_dhash3'.freeze, 'vp1_dhash4'.freeze)
hashes1 = image1.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze, 'vp1_dhash1'.freeze, 'vp1_dhash2'.freeze, 'vp1_dhash3'.freeze, 'vp1_dhash4'.freeze).values
exit
exit
VpNode.last
(VpNode.last.created_at - VpNode.first.create_at) / 60.0
(VpNode.last.created_at - VpNode.first.created_at) / 60.0
((VpNodeProductImage.last.created_at - VpNode.first.created_at) / 60.0)
((VpNodeProductImage.last.created_at - VpNode.first.created_at) / 60.0) / 60
VpNodeProductImage.last
VpNodeProductImage.count
url = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/010/850/192/original/open-uri20160721-32094-2gwq61?1478408704'
image_obj = if existing_image.present?
  Hashie::Mash.new(existing_image.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze))
else
  hashes = DHasher.hash_from_path(url)
  i = 0
  hashes.each_with_object(Hashie::Mash.new) do |hash, obj|
    obj["dhash#{i+=1}"] = hash
  end
end
existing_image = ProductImage.find_by(original_url: url)
image_obj = if existing_image.present?
  Hashie::Mash.new(existing_image.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze))
else
  hashes = DHasher.hash_from_path(url)
  i = 0
  hashes.each_with_object(Hashie::Mash.new) do |hash, obj|
    obj["dhash#{i+=1}"] = hash
  end
end
root = VpNode.root_node
level = 0
tau = 40
nodes_to_visit = [[root, level]]
neighbors = []
search_path = []
nodes_visited = 0
leaves_visited = 0
nodes_visited += 1
node, level = nodes_to_visit.shift
distance1 = DHasher.distance(node.slice('vp1_dhash1'.freeze, 'vp1_dhash2'.freeze, 'vp1_dhash3'.freeze, 'vp1_dhash4'.freeze), image_obj)
distance2 = DHasher.distance(node.slice('vp2_dhash1'.freeze, 'vp2_dhash2'.freeze, 'vp2_dhash3'.freeze, 'vp2_dhash4'.freeze), image_obj)
vp1 = node.slice('vp1_dhash1'.freeze, 'vp1_dhash2'.freeze, 'vp1_dhash3'.freeze, 'vp1_dhash4'.freeze)
vp2 = node.slice('vp1_dhash1'.freeze, 'vp1_dhash2'.freeze, 'vp1_dhash3'.freeze, 'vp1_dhash4'.freeze)
vp1.keys
vp1.keys.each { |k| ages[k.gsub(/vp\d_/, '')] = vp1.delete(k)  }
vp1.keys.each { |k| vp1[k.gsub(/vp\d_/, '')] = vp1.delete(k)  }
vp1
reload!
exit
cd VpNode
url = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/010/850/192/original/open-uri20160721-32094-2gwq61?1478408704'
edit -t
nodes_visited += 1
node, level = nodes_to_visit.shift
vp1 = node.slice('vp1_dhash1'.freeze, 'vp1_dhash2'.freeze, 'vp1_dhash3'.freeze, 'vp1_dhash4'.freeze)
vp2 = node.slice('vp2_dhash1'.freeze, 'vp2_dhash2'.freeze, 'vp2_dhash3'.freeze, 'vp2_dhash4'.freeze)
vp1.keys.each { |k| vp1[k.gsub(/vp\d_/, '')] = vp1.delete(k)  }
vp2.keys.each { |k| vp2[k.gsub(/vp\d_/, '')] = vp2.delete(k)  }
vp1
distance1 = DHasher.distance(vp1, image_obj)
distance2 = DHasher.distance(vp2, image_obj)
search_path << distance1
search_path << distance2
distance1 <= tau
distance2 <= tau
node.is_leaf?
node
node.lt_child1
node.lt_child_one
require 'benchmark/ips'
Benchmark.ips do |x|
  x.report('Array#unshift') do
    array = []
    100_000.times { |i| array.unshift(i) }
  end
  x.report('Array#insert') do
    array = []
    100_000.times { |i| array.insert(0, i) }
  end
  x.compare!
end
edit -t
exit
edit
edit -t
exit
distance1
distance1 <= node.mu1
node.mu1
node.mu1.to_f
node.rt_child_two
nodes_to_visit
node
node.is_leaf?
distance1 + tau <= node.mu1
distance1 - tau >= node.mu1
tau
url
distance1
distance2
distance2 + tau
node.mu1.to_f
distance1 - tau
distance1 + tau <= node.mu1
exit
reload!
url = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/010/850/192/original/open-uri20160721-32094-2gwq61?1478408704'
VpNode.find_nearest_neighbors(url)
exit
cd VpNode
url = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/010/850/192/original/open-uri20160721-32094-2gwq61?1478408704'
existing_image = ProductImage.find_by(original_url: url)
image_obj = if existing_image.present?
  Hashie::Mash.new(existing_image.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze))
else
  hashes = DHasher.hash_from_path(url)
  i = 0
  hashes.each_with_object(Hashie::Mash.new) do |hash, obj|
    obj["dhash#{i+=1}"] = hash
  end
end
root = VpNode.root_node
level = 0
tau = 40
nodes_to_visit = [[root, level]]
neighbors = []
search_path = []
nodes_visited = 0
leaves_visited = 0
nodes_visited += 1
node, level = nodes_to_visit.shift
vp1 = node.slice('vp1_dhash1'.freeze, 'vp1_dhash2'.freeze, 'vp1_dhash3'.freeze, 'vp1_dhash4'.freeze)
vp2 = node.slice('vp2_dhash1'.freeze, 'vp2_dhash2'.freeze, 'vp2_dhash3'.freeze, 'vp2_dhash4'.freeze)
vp1.keys.each { |k| vp1[k.gsub(/vp\d_/, '')] = vp1.delete(k)  }
vp2.keys.each { |k| vp2[k.gsub(/vp\d_/, '')] = vp2.delete(k)  }
distance1 = DHasher.distance(vp1, image_obj)
distance2 = DHasher.distance(vp2, image_obj)
1 <= 1.0
1 < 1.0
url
image_obj
nodes_to_visit
root = VpNode.root_node
level = 0
tau = 45
nodes_to_visit = [[root, level]]
neighbors = []
search_path = []
nodes_visited = 0
leaves_visited = 0
nodes_visited += 1
node, level = nodes_to_visit.shift
vp1 = node.slice('vp1_dhash1'.freeze, 'vp1_dhash2'.freeze, 'vp1_dhash3'.freeze, 'vp1_dhash4'.freeze)
vp2 = node.slice('vp2_dhash1'.freeze, 'vp2_dhash2'.freeze, 'vp2_dhash3'.freeze, 'vp2_dhash4'.freeze)
vp1.keys.each { |k| vp1[k.gsub(/vp\d_/, '')] = vp1.delete(k)  }
vp2.keys.each { |k| vp2[k.gsub(/vp\d_/, '')] = vp2.delete(k)  }
distance1 = DHasher.distance(vp1, image_obj)
distance2 = DHasher.distance(vp2, image_obj)
search_path << distance1
search_path << distance2
distance1 + tau <= node.mu1
distance1 - tau >= node.mu1
tau = 100
distance1 + tau <= node.mu1
distance1 - tau >= node.mu1
exit
cd VpNode
ProductImage.where.not(vp_node_id: nil).count
ProductImage.where.not(vp_node_id: nil).update_all(vp_node_id: nil)
VpNodeProductImage.connection.execute("truncate table #{VpNodeProductImage.quoted_table_name}")
VpNode.delete_all
exit
ex;it
exit
ProductImage.where.not(vp_node_id: nil).update_all(vp_node_id: nil)
VpNodeProductImage.connection.execute("truncate table #{VpNodeProductImage.quoted_table_name}")
VpNode.delete_all
connection.execute("ALTER TABLE #{quoted_table_name} AUTO_INCREMENT = 1") # reset id count to 1
cd VpNode
connection.execute("ALTER TABLE #{quoted_table_name} AUTO_INCREMENT = 1") # reset id count to 1
node = create(ancestry: '/')
query = 'SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE dhash1 IS NOT NULL'
images = ProductImage.connection.select_all(query).to_ary; nil
nodes_to_balance = [[node, images]]; nil
puts nodes_to_balance.length
node, images = nodes_to_balance.shift; nil
vp1_image = images.sample
node.attributes = {
  vp1_id: vp1_image['id'],
  vp1_dhash1: vp1_image['dhash1'],
  vp1_dhash2: vp1_image['dhash2'],
  vp1_dhash3: vp1_image['dhash3'],
  vp1_dhash4: vp1_image['dhash4']
}
dist_key = 'dist1'.freeze
images_with_distances = calc_distances(images, vp1_image, dist_key); nil
remove_duplicates(vp1_image, images_with_distances, dist_key) if images_with_distances.first[dist_key] == 0
mid = (images_with_distances.length - 1) / 2.0
median1 = (images_with_distances[mid.floor][dist_key] + images_with_distances[mid.ceil][dist_key]) / 2.0
node.mu1 = median1
left_images, right_images = split_images(median1, images_with_distances, dist_key); nil
vp2_image = right_images.sample
node.attributes = {
  vp2_id: vp2_image['id'],
  vp2_dhash1: vp2_image['dhash1'],
  vp2_dhash2: vp2_image['dhash2'],
  vp2_dhash3: vp2_image['dhash3'],
  vp2_dhash4: vp2_image['dhash4']
}
dist_key = 'dist2'.freeze
left_images = calc_distances(left_images, vp2_image, dist_key); nil
right_images = calc_distances(right_images, vp2_image, dist_key); nil
remove_duplicates(vp1_image, right_images, dist_key) if right_images.first[dist_key] == 0
right_images.first
right_images.first(10)
left_images.first(20)
remove_duplicates(vp2_image, right_images, dist_key) if right_images.first[dist_key] == 0
mid = (left_images.length - 1) / 2.0
median2 = (left_images[mid.floor][dist_key] + left_images[mid.ceil][dist_key]) / 2.0
node.mu2 = median2
mid = (right_images.length - 1) / 2.0
median3 = (right_images[mid.floor][dist_key] + right_images[mid.ceil][dist_key]) / 2.0
node.mu3 = median3
left_images.last
left_images1, right_images1 = split_images(median2, left_images, dist_key); nil
left_images2, right_images2 = split_images(median3, right_images, dist_key); nil
ancestry = "#{node.ancestry}#{node.id}/"
lt_child1 = VpNode.create(parent_id: node.id, ancestry: ancestry)
rt_child1 = VpNode.create(parent_id: node.id, ancestry: ancestry)
lt_child2 = VpNode.create(parent_id: node.id, ancestry: ancestry)
rt_child2 = VpNode.create(parent_id: node.id, ancestry: ancestry)
node.attributes = {
  lt_child1_id: lt_child1.id, rt_child1_id: rt_child1.id,
  lt_child2_id: lt_child2.id, rt_child2_id: rt_child2.id,
}
node.save!
child_nodes = [[lt_child1, left_images1], [rt_child1, right_images1],
[lt_child2, left_images2], [rt_child2, right_images2]]; nil
nodes_to_balance.unshift(*child_nodes); nil
nodes_to_balance.count
edit -t
VpNode.count
VpNode.pluck(:id, :ancestry)
node = VpNode.find 30
node.is_root?
node.is_leaf?
VpNodeProductImage.count
node.product_images.count
node = VpNode.find(31)
node.product_images.count
VpNode.count
((VpNode.count * 2) + VpNodeProductImage.count) / ProductImage.where.not(dhash1: nil).count.to_f
VpNodeProductImage.count
ActivityLog.last
exit
VpNodeProductImage.last
exit
ActivityLog.count
exit
VpNode.rebuild_tree
ActivityLog.count
ActivityLog.connection.execute("truncate table #{ActivityLog.quoted_table_name}")
VpNode.count
ActivityLog.coung
ActivityLog.count
ActivityLog.last
ActivityLog.count
((VpNode.count * 2) + VpNodeProductImage.count) / ProductImage.where.not(dhash1: nil).count.to_f
exit
exit
ActivityLog.connection.execute("truncate table #{ActivityLog.quoted_table_name}")
VpNode.rebuild_tree
ActivityLog.count
VpNode.count
ActivityLog.count
ActivityLog.last
ActivityLog.connection.execute("truncate table #{ActivityLog.quoted_table_name}")
reload!
exit
VpNode.rebuild_tree
ActivityLog.count
VpNode.count
ActivityLog.connection.execute("truncate table #{ActivityLog.quoted_table_name}")
exit
VpNode.rebuild_tree
exit
VpNode.rebuild_tree
ActivityLog.count
VpNode.count
ActivityLog.count
VpNode.count
ActivityLog.count
VpNode.count
ActivityLog.count
url = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/010/850/192/original/open-uri20160721-32094-2gwq61?1478408704'
VpNode.find_nearest_neighbors(url)
reload!
url = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/010/850/192/original/open-uri20160721-32094-2gwq61?1478408704'
VpNode.find_nearest_neighbors(url)
exit
reload!
url = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/010/850/192/original/open-uri20160721-32094-2gwq61?1478408704'
VpNode.find_nearest_neighbors(url)
exit
image1 = ProductImage.first
image2 = ProductImage.second
DHasher.distance(image1, image2)
VpNode.rebuild_tree
exit
VpNode.count
ActivityLog.count
VpNode.count
ActivityLog.count
VpNode.count
VpNode.first
node = _
node.mu1
node.mu1.to_f
node.mu2.to_f
node.mu3.to_f
VpNode.count
((VpNode.count * 2) + VpNodeProductImage.count) / ProductImage.where.not(dhash1: nil).count.to_f
VpNode.count
cd VpNode
url = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/010/850/192/original/open-uri20160721-32094-2gwq61?1478408704'
existing_image = ProductImage.find_by(original_url: url)
image_obj = if existing_image.present?
  Hashie::Mash.new(existing_image.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze))
else
  hashes = DHasher.hash_from_path(url)
  i = 0
  hashes.each_with_object(Hashie::Mash.new) do |hash, obj|
    obj["dhash#{i+=1}"] = hash
  end
end
root = VpNode.root_node
level = 0
tau = 10
nodes_to_visit = [[root, level]]
tau = 10
nodes_to_visit = [[root, level]]
neighbors = []
search_path = []
nodes_visited = 0
leaves_visited = 0
nodes_to_visit.length > 0
nodes_visited += 1
node, level = nodes_to_visit.shift
vp1 = node.slice('vp1_dhash1'.freeze, 'vp1_dhash2'.freeze, 'vp1_dhash3'.freeze, 'vp1_dhash4'.freeze)
vp2 = node.slice('vp2_dhash1'.freeze, 'vp2_dhash2'.freeze, 'vp2_dhash3'.freeze, 'vp2_dhash4'.freeze)
vp1.keys.each { |k| vp1[k.gsub(/vp\d_/, '')] = vp1.delete(k) }
vp2.keys.each { |k| vp2[k.gsub(/vp\d_/, '')] = vp2.delete(k) }
distance1 = DHasher.distance(vp1, image_obj)
distance2 = DHasher.distance(vp2, image_obj)
distance1 + tau <= node.mu1
distance1 - tau >= node.mu1
distance1
distance2
node.mu1
node.mu1.to_f
tau = 5
distance1 + tau <= node.mu1
distance1 - tau >= node.mu1
distance2 + tau <= node.mu2
distance2 - tau >= node.mu2
distance2
tau 3
tau = 3
distance1 - tau >= node.mu1
distance1
node.mu1
node.mu1.to_f
distance1 - tau <= node.mu1
distance1 + tau >= node.mu1
node.mu1
node.mu1.to_f
tau
tau = 10
distance1 + tau >= node.mu1
distance1 - tau <= node.mu1
exit
reload!
exit
url = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/010/850/192/original/open-uri20160721-32094-2gwq61?1478408704'
VpNode.find_nearest_neighbors(url)
reload!
VpNode.find_nearest_neighbors(url)
exit
reload!
url = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/010/850/192/original/open-uri20160721-32094-2gwq61?1478408704'
VpNode.find_nearest_neighbors(url)
cd VpNode
url = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/010/850/192/original/open-uri20160721-32094-2gwq61?1478408704'
existing_image = ProductImage.find_by(original_url: url)
image_obj = if existing_image.present?
  Hashie::Mash.new(existing_image.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze))
else
  hashes = DHasher.hash_from_path(url)
  i = 0
  hashes.each_with_object(Hashie::Mash.new) do |hash, obj|
    obj["dhash#{i+=1}"] = hash
  end
end
VpNodeProductImage.where(asset_id: 7799786)
VpNodeProductImage.where(asset_id: 7799786).count
VpNodeProductImage.where(asset_id: 7799786).first
vpimg = _
path = vp_img.path
path = vpimg.path
path.lenth
path.length
vpimg.node
vpimg.vp_node
d_arr = []
image_obj
node = vpimg.vp_node
node.product_images.count
node.product_images.each do |img|
  d_arr << DHasher.distance(img, image_obj)
end
d_arr
exit
reload!
VpNode.find_nearest_neighbors(url)
reload!
VpNode.find_nearest_neighbors(url)
reload!
VpNode.find_nearest_neighbors(url)
exit
exit
reload!
exit
cd VpNode
ProductImage.where.not(vp_node_id: nil).update_all(vp_node_id: nil)
VpNodeProductImage.connection.execute("truncate table #{VpNodeProductImage.quoted_table_name}")
VpNode.delete_all
connection.execute("ALTER TABLE #{quoted_table_name} AUTO_INCREMENT = 1") # reset id count to 1
query = 'SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE dhash1 IS NOT NULL'
images = ProductImage.connection.select_all(query).to_ary; nil
images.length
nodes_to_balance = [[node, images]]; nil
node = create(ancestry: '/')
nodes_to_balance = [[node, images]]; nil
nodes_to_balance.length > 0
vp1_image, vp2_image = choose_vantage_points(images)
images.length
images.first
images.sort_by { |i| i['dhash1'] }
max_dist = 0
vp1, vp2 = nil
images.length
_**2
edit -t
max_dist = 0
vp1, vp2 = nil
edit -t
ProductImage.where.not(dhash1: nil).count
exit
VpNode.rebuild_tree
VpNode.count
VpNode.all.sample
node = _
node.vp1
img = _
url = img.attachment.url.gsub('/development', '')
url = "http://s3.amazonaws.com/rvx-rds-dev/images/uploads/007/567/588/original/data?1478083823"
VpNode.find_nearest_neighbors(url)
reload!
VpNode.find_nearest_neighbors(url)
exit
exit
edit -t
cd VpNode
edit t
edit -t
query = 'SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE dhash1 IS NOT NULL LIMIT 200000'
images = ProductImage.connection.select_all(query).to_ary.sample(10000); nil
reload!
exit
reload!
url
url = "http://s3.amazonaws.com/rvx-rds-dev/images/uploads/007/567/588/original/data?1478083823"
url = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/009/344/795/original/open-uri20160713-26532-1a8mtcc?1478752311'
VpNode.find_nearest_neighbors(url)
VpNode.count
VpNode.rebuild_tree
VpNode.find_nearest_neighbors(url)
VpNode.last
node = _
reload!
VpNode.rebuild_tree
url = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/009/344/795/original/open-uri20160713-26532-1a8mtcc?1478752311'
VpNode.find_nearest_neighbors(url)
reload!
VpNode.find_nearest_neighbors(url)
url
results = [{:image_id=>8639335, :dist=>10},
  {:image_id=>8639335, :dist=>10},
{:image_id=>8639335, :dist=>10}]
results.map! {|el| el[:image_id] }
ProductImage.where(id: results).map {|i| i.attachment.url.gsub('/development', '') }
reload!
VpNode.find_nearest_neighbors(url)
reload!
VpNode.find_nearest_neighbors(url)
exit
url = 'https://s3.amazonaws.com/rvx-rds-dev/images/uploads/009/344/795/original/open-uri20160713-26532-1a8mtcc?1478752311'
VpNode.find_nearest_neighbors(url)
VpNode.count
reload!
VpNode.find_nearest_neighbors(url)
VpNode.count
VpNode.first
node = _
node.mu1
node.mu1.to_f
node.vp1
exit
Redis.current.flushall
exit
image = ProductImage.first
hashes1 = image1.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze).values
hashes1 = image.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze).values
DHasher.hash_from_path(image.attachment.url.gsub('/development'.freeze, ''.freeze))
exit
image1 = ProductImage.first
image2 = ProductImage.first(1000).sample
path1 = image1.attachment.url.gsub('/development'.freeze, ''.freeze)
path2 = image2.attachment.url.gsub('/development'.freeze, ''.freeze)
dhashes = {}
DHasher.hash_from_path(path1).each_with_index do |hash, idx|
  dhashes["dhash#{idx+1}"] = hash
end
hashes1 = _
dhashes = {}
DHasher.hash_from_path(path2).each_with_index do |hash, idx|
  dhashes["dhash#{idx+1}"] = hash
end
hashes2 = _
distance = 0
hashes1.length.times do |idx|
  distance = (hashes1[idx] ^ hashes2[idx]).to_s(2).count('1')
end
distance
path1
path2
reload!
dhashes1 = {}
DHasher.hash_from_path(path1).each_with_index do |hash, idx|
  dhashes1["dhash#{idx+1}"] = hash
end
exit
image1 = ProductImage.first
image2 = ProductImage.create(original_url: image.attachment.url)
ProductImage.where.not(dhash1: nil).last(20000).sample
image2 = _
path1 = image1.attachment.url.gsub('/development'.freeze, ''.freeze)
path2 = image2.attachment.url.gsub('/development'.freeze, ''.freeze)
dhashes1 = {}
DHasher.hash_from_path(path1).each_with_index do |hash, idx|
  dhashes1["dhash#{idx+1}"] = hash
end
dhashes2 = {}
DHasher.hash_from_path(path1).each_with_index do |hash, idx|
  dhashes2["dhash#{idx+1}"] = hash
end
hashes1.length.times do |idx|
  distance = (hashes1[idx] ^ hashes2[idx]).to_s(2).count('1')
end
distance
distance = 0
hashes1.length.times do |idx|
  distance = (hashes1[idx] ^ hashes2[idx]).to_s(2).count('1')
end
distance
distance = 0
image1
hashes1
dhashes1.length.times do |idx|
  distance = (dhashes1[idx] ^ dhashes2[idx]).to_s(2).count('1')
end
dhashes
dhashes1
dhashes2
dhashes1.length
distance
dhashes1.length.times do |idx|
  distance += (dhashes1[idx] ^ dhashes2[idx]).to_s(2).count('1')
end
hashes1 = dhashes1.values
hashes2 = dhashes2.values
hashes1.length.times do |idx|
  distance += (hashes1[idx] ^ hashes2[idx]).to_s(2).count('1')
end
distance
idx = 0
hashes1[idx] ^ hashes2[idx]
dhashes2 = {}
DHasher.hash_from_path(path2).each_with_index do |hash, idx|
  dhashes2["dhash#{idx+1}"] = hash
end
dhashes2
hashes2 = dhashes2.values
hashes1
hashes1.length.times do |idx|
  distance += (hashes1[idx] ^ hashes2[idx]).to_s(2).count('1')
end
distance
exit
Redis.current.flushall
exit
image1 = ProductImage.first
image2 = ProductImage.where.not(dhash1: nil).last(20000).sample
hashes1 = image1.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze).values
hashes2 = image2.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze).values
distance = 0
hashes1.length.times do |idx|
  distance += (hashes1[idx] ^ hashes2[idx]).to_s(2).count('1')
end
distance
exit
Redis.current.flushall
exit
Redis.current.flushall
exit
Redis.current.flushall
exit
ProductImage.first
Redis.current.flushall
ProductImage.first
DHasher.distance(ProductImage.first, ProductImage.second)
image1 = ProductImage.first
image2 = ProductImage.second
hashes1 = image1.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze).values
hashes2 = image2.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze).values
id1 = 3559052
id2 = 3559061
image1 = ProductImage.find(id1)
image2 = ProductImage.find(id2)
distance = 0
hashes1 = image1.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze).values
hashes2 = image2.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze).values
hashes1.length.times do |idx|
  distance += (hashes1[idx] ^ hashes2[idx]).to_s(2).count('1')
end
distance
ProductImage.where.not(dhash1: nil).last
last = _
last.id - 9819360
last.id
last.id - 10106374
last.id
(47.0 + 27.0) / 2.0
37 / 20
37.0 / 20
20 / 37.0
2371126 * 0.54054054
(2371126 * 0.54054054) / 60 / 60
(2371126 * 0.54054054) / 60 / 60 / 24
exit
Redis.current.flushall
edit -t
exit
edit
Redis.current.flushall
edit -t
edit
exit
edit
ProductImage.where("id > 7477643").count
ProductImage.where("id > 7477643").update_all(dhash1: nil, dhash2: nil, dhash3: nil, dhash4: nil)
ProductImage.where.not(id: nil).count
ProductImage.where.not(dhash1: nil).count
exit
reload!
ProductImage.first
image1 = ProductImage.first
image2 = ProductImage.second
DHasher.distance(image1, image2)
image1.id
image2
image1, image2 = ProductImage.where(id: [3321679, 3324775])
DHasher.distance(image1, image2)
price = { iso_code: 'JPY', currency: '$' }
edit -t
currency
price.delete(:iso_code)
edit -t
price = {}
edit -t
reload!
edit -t
exit
cd VpNode
ProductImage.where.not(vp_node_id: nil).update_all(vp_node_id: nil)
VpNodeProductImage.connection.execute("truncate table #{VpNodeProductImage.quoted_table_name}")
VpNode.delete_all
connection.execute("ALTER TABLE #{quoted_table_name} AUTO_INCREMENT = 1") # reset id count to 1
node = create(ancestry: '/')
query = 'SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE dhash1 IS NOT NULL LIMIT 200000'
query = 'SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE dhash1 IS NOT NULL'
images = ProductImage.connection.select_all(query).to_ary; nil
images.count
nodes_to_balance = [[node, images]]; nil
puts nodes_to_balance.length
node, images = nodes_to_balance.shift; nil
vp1_image = images.sample
node.attributes = {
  vp1_id: vp1_image['id'],
  vp1_dhash1: vp1_image['dhash1'],
  vp1_dhash2: vp1_image['dhash2'],
  vp1_dhash3: vp1_image['dhash3'],
  vp1_dhash4: vp1_image['dhash4']
}
dist_key = 'dist1'.freeze
images_with_distances = []
images.count.times do |_idx|
  image2 = images.pop
  unless vp1_image['id'] == image2['id']
    distance = DHasher.distance(vp1_image, image2)
    image2['path'] ||= []
    image2['path'] << distance
    images_with_distances << image2.merge!({ dist_key => distance })
  end
end
images_with_distances.sort_by! { |img| img[dist_key] }; nil
images_with_distances.first
images_with_distances.last
remove_duplicates(vp1_image, images_with_distances, dist_key) if images_with_distances.first[dist_key] == 0
mid = (images_with_distances.length - 1) / 2.0
median1 = (images_with_distances[mid.floor][dist_key] + images_with_distances[mid.ceil][dist_key]) / 2.0
node.mu1 = median1
images_with_distances.length <= VpNode::MAX_IMAGES + 1
dist_key
images_with_distances.select {|i| i[dist_key] <= median1 }.count
images_with_distances.select {|i| i[dist_key] > median1 }.count
left_images = []
right_images = []
images_with_distances.select {|i| i[dist_key] >= median1 }.count
images_with_distances.select {|i| i[dist_key] <= median1 }.count
images_with_distances.count
mid = (images_with_distances.length - 1) / 2.0
images_with_distances.length
images_with_distances[mid.floor][dist_key]
images_with_distances[mid.ceil][dist_key]
19999 / 2
images_with_distances[9999][dist_key]
images_with_distances[9999 - 1][dist_key]
images_with_distances[9999 - 2][dist_key]
images_with_distances[9999 - 3][dist_key]
exit
VpNode.rebuild_tree
VpNode.count
ActivityLog.count
ActivityLog.last
VpNode.count
exit
ActivityLog.delete_all
exit
VpNode.rebuild_tree
VpNode.count
ActivityLog.count
VpNode.count
ProductImage.where.not(dhash1: nil).sample
arr = [1,2,3,4,5,6,7,8,9]
els = arr.slice!(0..4)
els
arr
exit
exit
cd VpNode
ProductImage.where.not(vp_node_id: nil).update_all(vp_node_id: nil)
VpNodeProductImage.connection.execute("truncate table #{VpNodeProductImage.quoted_table_name}")
VpNode.delete_all
connection.execute("ALTER TABLE #{quoted_table_name} AUTO_INCREMENT = 1") # reset id count to 1
node = create(ancestry: '/')
query = 'SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE dhash1 IS NOT NULL'
images = ProductImage.connection.select_all(query).to_ary; nil
images.length
nodes_to_balance = [[node, images]]; nil
puts nodes_to_balance.length
node, images = nodes_to_balance.shift; nil
vp1_image = images.sample
node.attributes = {
  vp1_id: vp1_image['id'],
  vp1_dhash1: vp1_image['dhash1'],
  vp1_dhash2: vp1_image['dhash2'],
  vp1_dhash3: vp1_image['dhash3'],
  vp1_dhash4: vp1_image['dhash4']
}
dist_key = 'dist1'.freeze
images_with_distances = calc_distances(images, vp1_image, dist_key); nil
remove_duplicates(vp1_image, images_with_distances, dist_key) if images_with_distances.first[dist_key] == 0
mid = (images_with_distances.length - 1) / 2.0
images_with_distances
images_with_distances.length
images_with_distances.first
median1 = (images_with_distances[mid.floor][dist_key] + images_with_distances[mid.ceil][dist_key]) / 2.0
node.mu1 = median1
images_with_distances.length
left_images = images_with_distances.slice(0..mid)
left_images.length
images_with_distances.length
_ - 9899
left_images = images_with_distances.slice!(0..mid)
left_images.length
images_with_distances.length
left_images.last
images_with_distances.first
exit
VpNode.rebuild_tree
ActivityLog.count
VpNode.rebuild_tree
ActivityLog.count
ProductImage.where.not(dhash1: nil).sample
url = 'http://i.ebayimg.com/images/g/M08AAOSwbdpWVnjT/s-l1600.jpg'
exit
url = 'http://i.ebayimg.com/images/g/M08AAOSwbdpWVnjT/s-l1600.jpg'
VpNode.find_nearest_neighbors(url)
VpNode.count
root = VpNode.first
neighbors.uniq_by {|n| n[:image_id] }
edit -t
neighbors.uniq_by {|n| n[:image_id] }
neighbors.uniq_by! {|n| n[:image_id] }
neighbors.uniq! {|n| n[:image_id] }
reload!
VpNode.find_nearest_neighbors(url)
VpNode.count
45 / 341
45 / 341.0
exit
VpNode.rebuild_tree
url = 'http://i.ebayimg.com/images/g/M08AAOSwbdpWVnjT/s-l1600.jpg'
VpNode.find_nearest_neighbors(url)
VpNode.count
122 / 1365.0
exit
VpNode.count
.09 * 10000
0.09 * 10000
exit
ProductImage.where.not(dhash1: nil).count
exit
VpNode.rebuild_tree
exit
cd VpNode
ProductImage.where.not(vp_node_id: nil).update_all(vp_node_id: nil)
VpNodeProductImage.connection.execute("truncate table #{VpNodeProductImage.quoted_table_name}")
VpNode.delete_all
connection.execute("ALTER TABLE #{quoted_table_name} AUTO_INCREMENT = 1") # reset id count to 1
node = create(ancestry: '/')
query = 'SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE dhash1 IS NOT NULL LIMIT 20000'
images = ProductImage.connection.select_all(query).to_ary; nil
images.length
exit
VpNode.rebuild_tree
VpNode.count
VpNode.rebuild_tree
exit
VpNode.rebuild_tree
VpNode.count
url = 'http://i.ebayimg.com/images/g/M08AAOSwbdpWVnjT/s-l1600.jpg'
VpNode.find_nearest_neighbors(url)
VpNode.all.sample
img = ProductImage.where(id: 8494441)
url = img.original_url
img = img.first
url = img.original_url
VpNode.find_nearest_neighbors(url)
ids = _.map {|i| i[:image_id] }
imgs = ProductImage.where(id: ids)
imgs.map {|i| i.attachment.url }
imgs.map {|i| i.attachment.url.gsub('/development', '') }
VpNode.count
reload!
VpNode.find_nearest_neighbors(url)
606 / VpNode.count.to_f
VpNode.rebuild_tree
VpNode.count
reload!
exit
VpNode.count
VpNode.rebuild_tree
VpNode.count
url
url = 'http://s3.amazonaws.com/rvx-rds-dev/images/uploads/008/195/237/original/open-uri20160709-734-1nlf33p?1478690946'
VpNode.find_nearest_neighbors(url)
reload!
VpNode.find_nearest_neighbors(url)
VpNode.all.select! {|n| n.is_leaf? }
VpNode.all.to_a.select! {|n| n.is_leaf? }
leafs = _
leafs.length
VpNode.count
results = _
images = results.map {|i| ProductImage.find(id: i[:image_id] }
images = results.map {|i| ProductImage.find(id: i[:image_id]) }
ids = results.map {|i| i[:image_id] }
images = ProductImage.where(id: ids)
images.map {|i| i.attachment.url.gusb('/attachment', '') }
images.map {|i| i.attachment.url.gsub('/attachment', '') }
images.map {|i| i.attachment.url.gsub('/development', '') }
results
ProductImage.find(8468124).attachment.url
reload!
VpNode.find_nearest_neighbors(url)
1271.0 / VpNode.count
901.0 / 5461
reload!
VpNode.rebuild_tree
VpNode.count
VpNode.find_nearest_neighbors(url)
316.0 / VpNode.count
url = ProductImage.find(8195237).attachment.url
VpNode.find_nearest_neighbors(url)
url = ProductImage.find(8195237).attachment.url.gsub('/development', '')
VpNode.find_nearest_neighbors(url)
url = ProductImage.find(8195237).original_url
VpNode.find_nearest_neighbors(url)
url = 'http://s3.amazonaws.com/rvx-rds-dev/images/uploads/008/195/237/original/open-uri20160709-734-1nlf33p?1478690946'
VpNode.find_nearest_neighbors(url)
results = [{:image_id=>8195237, :dist=>12},
  {:image_id=>8469716, :dist=>12},
  {:image_id=>8494441, :dist=>11},
{:image_id=>9278925, :dist=>12}]
results
ids = results.map(&:[], :image_id)
ids = results.map {|i| i[:image_id] }
edit -t
VpNode.find_nearest_neighbors(url)
ProductImage.find(8469716)
image = -
img = ProductImage.find(8469716)
img.attachment.url
img.save_attachment
img.attachment.url
img.reload
image = img
path = img.attachment.url
dhashes = {}
DHasher.hash_from_path(path).each_with_index do |hash, idx|
  dhashes["dhash#{idx+1}"] = hash
end
image.update(dhashes)
VpNode.find_nearest_neighbors(url)
VpNode.first
VpNode.where(vp1_id: image.id)
node = _.first
image.dhash1
node.update(vp1_dhash1: image.dhash1, vp1_dhash2: image.dhash2, vp1_dhash3: image.dhash3, vp1_dhash4: image.dhash4)
VpNode.find_nearest_neighbors(url)
reload!
VpNode.find_nearest_neighbors(url)
VpNode.count
203.0 / 1365
def search(url)
  start_time = Time.now
  results = VpNode.find_nearest_neighbors(url)
  end_time = Time.now
  puts "ELAPSED TIME #{end_time - start_time}"
  results
end
search url
ProductImage.where('id < ?', 9278925).count
ProductImage.where('id < ? AND dhash1 IS NOT NULL', 9278925).count
query = 'SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE dhash1 IS NOT NULL LIMIT 50000'
images = ProductImage.connection.select_all(query).to_ary; nil
images.length
images.last
ProductImage.last
images.last
query = 'SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE dhash1 IS NOT NULL ORDER BY id DESC LIMIT 50000'
images = ProductImage.connection.select_all(query).to_ary; nil
images.length
images.last
images.first
images.last
query = 'SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE dhash1 IS NOT NULL ORDER BY id ASC LIMIT 20000'
images = ProductImage.connection.select_all(query).to_ary; nil
query = 'SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE dhash1 IS NOT NULL ORDER BY id ASC LIMIT 50000'
images = ProductImage.connection.select_all(query).to_ary; nil
images.last['id']
ProductImage.where('id < ?', _).count
ProductImage.where('id < ? AND dhash1 IS NOT NULL', 7508819).count
ProductImage.where('id <= ? AND dhash1 IS NOT NULL', 7508819).count
reload!
VpNode.rebuild_tree
url
VpNode.find_nearest_neighbors(url)
ids
query = 'SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE dhash1 IS NOT NULL OR id IN (8195237, 8469716, 8494441, 9278925) ORDER BY id DESC LIMIT 25000'
images = ProductImage.connection.select_all(query).to_ary; nil
images.count
query = 'SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE id IN (8195237, 8469716, 8494441, 9278925) ORDER BY id DESC LIMIT 25000'
images = ProductImage.connection.select_all(query).to_ary; nil
images.count
query = 'SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE id IN (8195237, 8469716, 8494441, 9278925) ORDER BY id DESC LIMIT 25000'
images = ProductImage.connection.select_all(query).to_ary; nil
images.length
query = 'SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE dhash1 IS NOT NULL OR id IN (8195237, 8469716, 8494441, 9278925) ORDER BY id DESC LIMIT 25000'
images = ProductImage.connection.select_all(query).to_ary; nil
images.length
exit
VpNode.rebuild_tree
VpNode.find_nearest_neighbors(url)
url = 'http://s3.amazonaws.com/rvx-rds-dev/images/uploads/008/195/237/original/open-uri20160709-734-1nlf33p?1478690946'
VpNode.find_nearest_neighbors(url)
images
query = 'SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE dhash1 IS NOT NULL OR id IN (8195237, 8469716, 8494441, 9278925) ORDER BY id DESC LIMIT 25000'
ids = [8195237, 8469716, 8494441, 9278925]
VpNode.where(vp1_id: ids)
VpNode.where(vp2_id: ids)
VpNodeProductImage.where(asset_id: ids)
query = 'SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE dhash1 IS NOT NULL OR id IN (8195237, 8469716, 8494441, 9278925) ORDER BY id DESC LIMIT 25004'
images = ProductImage.connection.select_all(query).to_ary; nil
images.count
images.last
ids
images.first
query = 'SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE dhash1 IS NOT NULL ORDER BY id ASC LIMIT 25000'
images = ProductImage.connection.select_all(query).to_ary; nil
images.length
query = 'SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE id IN (8195237, 8469716, 8494441, 9278925)'
images |= ProductImage.connection.select_all(query).to_ary; nil
images.length
relaod!
reload!
VpNode.rebuild_tree
VpNode.count
url
VpNode.find_nearest_neighbors(url)
def search(url)
  start_time = Time.now
  VpNode.find_nearest_neighbors(url)
  end_time = Time.now
  puts "time: #{end_time - start_time}"
def search(url)
  start_time = Time.now
  results = VpNode.find_nearest_neighbors(url)
  end_time = Time.now
  puts "time: #{end_time - start_time}"
  results
end
search(url)
reload!
search(url)
root = VpNode.first
root.mu1
root.mu1.to_f
reload!
search(url)
VpNode.count
reload!
search(url)
VpNode.count
reload!
VpNode.rebuild_tree
search(url)
VpNode.count
ProductImage.find LEAFS VISITED: 68
NODES VISITED: 129
time: 3.488149
=> [{:image_id=>8195237, :dist=>0}, {:image_id=>8469716, :dist=>3}]
[46] pry(main)> VpNode.count
(1.3ms)  SELECT COUNT(*) FROM `vp_nodes`
=> 5461
ProductImage.find 9278925
ProductImage.find 8494441
reload!
search(url)
reload!
search(url)
reload!
search(url)
VpNode.rebuild_tree
search(url)
reload!
VpNode.rebuild_tree
ActivityLog.count
ActivityLog.last
ActivityLog.delete_all
ActivityLog.last
ActivityLog.count
search(url)
VpNode.count
cd VpNode
cd find_nearest_neighbors
define_method find_nearest_neighbors
define_method find_nearest_neighbors(url)
show-method
exit
reload!
VpNode.rebuild_tree
VpNode.count
root = VpNode.first
search(url)
reload!
VpNode.count
reload!
VpNode.rebuild_tree
VpNode.count
search(url)
reload!
VpNode.rebuild_tree
ActivityLog.count
VpNode.count
search(url)
VpNode.count
ActivityLog.count
reload!
search(url)
reload!
VpNode.count
reload!
VpNode.rebuild_tree
VpNode.count
ActivityLog.count
VpNode.count
VpNode.count
search(url)
relaod!
reload!
VpNode.rebuild_tree
ActivityLog.count
ActivityLog.all
ActivityLog.count
VpNode.count
ActivityLog.count
VpNode.count
ActivityLog.count
VpNode.count
ActivityLog.count
ActivityLog.all
ActivityLog.count
VpNode.count
ActivityLog.count
VpNode.count
ProductImage.where.not(dhash1: nil).count
search(url)
ProductImage.find LEAFS VISITED: 532
NODES VISITED: 1013
time: 30.484074
=> [{:image_id=>8195237, :dist=>0}, {:image_id=>8494441, :dist=>1}, {:image_id=>8469716, :dist=>3}]
image = ProductImage.find(8195237)
reload!
NODES VISITED: 1013
search(url)
ActivityLog.all
reload!
node = VpNode.where(id: 85053)
ProductImage.where(vp_node_id: 85053)
node.vp_1
node.vp1
node.vp1_id
node = node.first
node.vp1_id
node.vp1
node.vp2
VpNodeProductImage
VpNodeProductImage.where(node_id: node.id)
VpNodeProductImage.where(vp_node_id: node.id)
VpNodeProductImage.where(vp_node_id: node.id).count
q      vp1 = node.slice('vp1_dhash1'.freeze, 'vp1_dhash2'.freeze, 'vp1_dhash3'.freeze, 'vp1_dhash4'.freeze)
vp1 = node.slice('vp1_dhash1'.freeze, 'vp1_dhash2'.freeze, 'vp1_dhash3'.freeze, 'vp1_dhash4'.freeze)
vp2 = node.slice('vp2_dhash1'.freeze, 'vp2_dhash2'.freeze, 'vp2_dhash3'.freeze, 'vp2_dhash4'.freeze)
vp1.keys.each { |k| vp1[k.gsub(/vp\d_/, '')] = vp1.delete(k) }
vp2.keys.each { |k| vp2[k.gsub(/vp\d_/, '')] = vp2.delete(k) }
distance1 = DHasher.distance(vp1, image_obj)
existing_image = ProductImage.find_by(original_url: url)
url
url = "http://s3.amazonaws.com/rvx-rds-dev/images/uploads/008/195/237/original/open-uri20160709-734-1nlf33p?1478690946"
existing_image = ProductImage.find_by(original_url: url)
image_obj = if existing_image.present?
  Hashie::Mash.new(existing_image.slice('dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze))
else
  hashes = DHasher.hash_from_path(url)
  i = 0
  hashes.each_with_object(Hashie::Mash.new) do |hash, obj|
    obj["dhash#{i+=1}"] = hash
  end
end
node.path_from_root
node.lt_child
node.is_leaf?
distance1 = DHasher.distance(vp1, image_obj)
distance2 = DHasher.distance(vp2, image_obj)
neighbors = []
node.vp_node_product_images.count
node.vp_node_product_images.includes(:product_image).select(:id, :asset_id, :path).first
node_img = _
path = node_img.path
p = path.length
d1 = path[level]
level = 17
d1 = path[level]
d2 = path[level + 1]
level = 16
d1 = path[level]
d2 = path[level + 1]
image = node_img.product_image
(distance1 - tau <= d1 && d1 <= distance2 + tau) && (distance2 - tau <= d2 && d2 <= distance2 + tau)
tau = 5
(distance1 - tau <= d1 && d1 <= distance2 + tau) && (distance2 - tau <= d2 && d2 <= distance2 + tau)
node.vp_node_product_images.includes(:product_image).select(:id, :asset_id, :path).find_each do |node_img|
  path = node_img.path
  p = [path.length, search_path.length].min
  d1 = path[level]
  d2 = path[level + 1]
  image = node_img.product_image
  if (distance1 - tau <= d1 && d1 <= distance2 + tau) && (distance2 - tau <= d2 && d2 <= distance2 + tau)
    p.times do |idx|
      next if path[idx].nil?
      if (search_path[idx] - tau <= path[idx]) && (path[idx] <= search_path[idx] + tau)
        d = DHasher.distance(image, image_obj)
        neighbors << { image_id: image.id, dist: d } if d <= tau
      end
    end
  end
end
search_path = path
node.vp_node_product_images.includes(:product_image).select(:id, :asset_id, :path).find_each do |node_img|
  path = node_img.path
  p = [path.length, search_path.length].min
  d1 = path[level]
  d2 = path[level + 1]
  image = node_img.product_image
  if (distance1 - tau <= d1 && d1 <= distance2 + tau) && (distance2 - tau <= d2 && d2 <= distance2 + tau)
    p.times do |idx|
      next if path[idx].nil?
      if (search_path[idx] - tau <= path[idx]) && (path[idx] <= search_path[idx] + tau)
        d = DHasher.distance(image, image_obj)
        neighbors << { image_id: image.id, dist: d } if d <= tau
      end
    end
  end
end
neighbors
VpNodeProductImage.where(path: nil)
VpNodeProductImage.where(asset_id: nil)
VpNodeProductImage.where(vp_node_id: nil)
node.where(vp1_id: nil)
VpNode.where(vp1_id: nil)
VpNode.where(vp1_id: nil).count
VpNode.where(vp2_id: nil).count
node.v1
node
node = VpNode.where(vp2_id: nil).first
node.vp1
node.vp2
reload!
(node.v1.nil? || node.v2.nil)
(node.v1.nil? || node.v2.nil)
(node.vp1.nil? || node.vp2.nil?)
reload!
search(url)
edit -t
PRoductImage.where(id: ids).map {|i| i.attachment.url.gsub('/development', '') }
ProductImage.where(id: ids).map {|i| i.attachment.url.gsub('/development', '') }
ProductImage.where(id: ids).map {|i| [i.id, i.attachment.url.gsub('/development', '')] }
edit -t
ProductImage.where(id: ids).map {|i| [i.id, scores[i.id], i.attachment.url.gsub('/development', '')] }
results = _
results.sort_by! {|i| i[1] }
ProductImage.find(7796017).attachment.url.gsub('/development', '')
scores = [[8195237, 0, "http://s3.amazonaws.com/rvx-rds-dev/images/uploads/008/195/237/original/open-uri20160709-734-1nlf33p?1478690946"],
  [8494441, 1, "http://s3.amazonaws.com/rvx-rds-dev/images/uploads/008/494/441/original/open-uri20161104-11943-m4vz3y?1478248186"],
  [8469716, 3, "http://s3.amazonaws.com/rvx-rds-dev/images/uploads/008/469/716/original/open-uri20161129-18809-z9p914?1480443831"],
  [8567689, 38, "http://s3.amazonaws.com/rvx-rds-dev/images/uploads/008/567/689/original/open-uri20160712-5773-1cbstmw?1478860930"],
  [8781331, 38, "http://s3.amazonaws.com/rvx-rds-dev/images/uploads/008/781/331/original/open-uri20160712-32347-18qv1k6?1478879170"],
  [8468124, 39, "http://s3.amazonaws.com/rvx-rds-dev/images/uploads/008/468/124/original/open-uri20160710-26506-msns4v?1478710563"],
[8508441, 39, "http://s3.amazonaws.com/rvx-rds-dev/images/uploads/008/508/441/original/open-uri20161104-11943-9m7p98?1478248343"]]
relaod!
reload!
search(url)
exit
exit
path1 = 'http://s3.amazonaws.com/rvx-rds-dev/images/uploads/008/195/237/original/open-uri20160709-734-1nlf33p?1478690946'
path2 = 'http://s3.amazonaws.com/rvx-rds-dev/images/uploads/008/494/441/original/open-uri20161104-11943-m4vz3y?1478248186'
path3 = 'http://s3.amazonaws.com/rvx-rds-dev/images/uploads/008/508/441/original/open-uri20161104-11943-9m7p98?1478248343'
hashes1 = DHasher.hash_from_path(path1)
require 'd_hasher'
hashes1 = DHasher.hash_from_path(path1)
hashes2 = DHasher.hash_from_path(path2)
hashes3 = DHasher.hash_from_path(path3)
distance = 0
hashes1.length.times do |idx|
  distance += (hashes1[idx] ^ hashes2[idx]).to_s(2).count('1')
end
distance
distance = 0
hashes1.length.times do |idx|
  distance += (hashes1[idx] ^ hashes3[idx]).to_s(2).count('1')
end
distance
sql = ActiveRecord::Base.connection()
sql.execute "SET autocommit=0"
sql.transaction do
  sql.execute("DROP TABLE IF EXISTS assets_new")
  sql.execute("CREATE TABLE assets_new LIKE assets")
  add_column :assets_new, :vp1_dhash5, 'bigint unsigned'
  add_column :assets_new, :vp1_dhash6, 'bigint unsigned'
  add_column :assets_new, :vp1_dhash7, 'bigint unsigned'
  add_column :assets_new, :vp1_dhash8, 'bigint unsigned'
  add_column :assets_new, :vp1_dhash9, 'bigint unsigned'
  add_column :assets_new, :vp1_dhash10, 'bigint unsigned'
  add_column :assets_new, :vp1_dhash11, 'bigint unsigned'
  add_column :assets_new, :vp1_dhash12, 'bigint unsigned'
  add_column :assets_new, :vp1_dhash13, 'bigint unsigned'
  add_column :assets_new, :vp1_dhash14, 'bigint unsigned'
  add_column :assets_new, :vp1_dhash15, 'bigint unsigned'
  add_column :assets_new, :vp1_dhash16, 'bigint unsigned'
  sql.execute('INSERT INTO assets_new SELECT *, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL FROM assets')
  rename_table :assets, :assets_old
  rename_table :assets_new, :assets
  sql.execute("DROP TABLE IF EXISTS assets_old")
end
exit
VpNodeProductImage.connection.execute("truncate table #{VpNodeProductImage.quoted_table_name}")
ProductImage.where.not(vp_node_id: nil).update_all(vp_node_id: nil)
VpNode.delete_all
VpNode.connection.execute("ALTER TABLE #{VpNode.quoted_table_name} AUTO_INCREMENT = 1") # reset id count to 1
exit
sql =     sql = ActiveRecord::Base.connection()
sql = ActiveRecord::Base.connection()
sql.execute("INSERT INTO assets_new SELECT * FROM vp_nodes LIMIT 1")
exit
sql.execute("SELECT * FROM vp_nodes")
sql.execute "SET autocommit=0"
sql = ActiveRecord::Base.connection()
sql.execute("SELECT * FROM vp_nodes LIMIT 1")
sql.execute("SELECT * FROM vp_nodes LIMIT 1").to_ary
result = sql.execute("SELECT * FROM vp_nodes LIMIT 1")
result.to_hash
result.to_a
result.class
result.fields
migration = ActiveRecord::Migration.new
cd migration
sql = ActiveRecord::Base.connection()
sql.execute("DROP TABLE IF EXISTS vp_nodes_new")
sql.execute("CREATE TABLE vp_nodes_new LIKE vp_nodes")
add_column :vp_nodes_new, :vp1_dhash5, 'bigint unsigned'
result = sql.execute("SELECT * FROM vp_nodes_new LIMIT 1")
result.fields
edit -t
result = sql.execute("SELECT * FROM vp_nodes_new LIMIT 1")
result.fields
result.fields.count
sql.execute("SELECT * FROM vp_nodes")
sql.execute("SELECT * FROM vp_nodes").fields.count
sql.execute("SELECT * FROM vp_nodes").fields.class
sql.execute("SELECT * FROM vp_nodes").fields.concat('NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL'.split(',')).count
sql.execute("SELECT * FROM vp_nodes").fields
result.fields
add_column :vp_nodes_new, :vp2_dhash5, 'bigint unsigned'
result.fields
result = sql.execute("SELECT * FROM vp_nodes_new LIMIT 1")
result.fields
edit -t
result = sql.execute("SELECT * FROM vp_nodes_new LIMIT 1")
result.fields
result.fields.count
sql.execute("SELECT * FROM vp_nodes").fields
sql.execute("SELECT * FROM vp_nodes").fields.concat('NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL'.split(','))
sql.execute("SELECT * FROM vp_nodes").fields.concat('NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL'.split(',')).count
sql.execute "SET autocommit=0"
sql.transaction do
  sql.execute("DROP TABLE IF EXISTS vp_nodes_new")
  sql.execute("CREATE TABLE vp_nodes_new LIKE vp_nodes")
  add_column :vp_nodes_new, :vp1_dhash5, 'bigint unsigned'
  add_column :vp_nodes_new, :vp1_dhash6, 'bigint unsigned'
  add_column :vp_nodes_new, :vp1_dhash7, 'bigint unsigned'
  add_column :vp_nodes_new, :vp1_dhash8, 'bigint unsigned'
  add_column :vp_nodes_new, :vp1_dhash9, 'bigint unsigned'
  add_column :vp_nodes_new, :vp1_dhash10, 'bigint unsigned'
  add_column :vp_nodes_new, :vp1_dhash11, 'bigint unsigned'
  add_column :vp_nodes_new, :vp1_dhash12, 'bigint unsigned'
  add_column :vp_nodes_new, :vp1_dhash13, 'bigint unsigned'
  add_column :vp_nodes_new, :vp1_dhash14, 'bigint unsigned'
  add_column :vp_nodes_new, :vp1_dhash15, 'bigint unsigned'
  add_column :vp_nodes_new, :vp1_dhash16, 'bigint unsigned'
  add_column :vp_nodes_new, :vp2_dhash5, 'bigint unsigned'
  add_column :vp_nodes_new, :vp2_dhash6, 'bigint unsigned'
  add_column :vp_nodes_new, :vp2_dhash7, 'bigint unsigned'
  add_column :vp_nodes_new, :vp2_dhash8, 'bigint unsigned'
  add_column :vp_nodes_new, :vp2_dhash9, 'bigint unsigned'
  add_column :vp_nodes_new, :vp2_dhash10, 'bigint unsigned'
  add_column :vp_nodes_new, :vp2_dhash11, 'bigint unsigned'
  add_column :vp_nodes_new, :vp2_dhash12, 'bigint unsigned'
  add_column :vp_nodes_new, :vp2_dhash13, 'bigint unsigned'
  add_column :vp_nodes_new, :vp2_dhash14, 'bigint unsigned'
  add_column :vp_nodes_new, :vp2_dhash15, 'bigint unsigned'
  add_column :vp_nodes_new, :vp2_dhash16, 'bigint unsigned'
  sql.execute("INSERT INTO vp_nodes_new SELECT *, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL FROM vp_nodes")
  rename_table :vp_nodes, :vp_nodes_old
  rename_table :vp_nodes_new, :vp_nodes
  sql.execute("DROP TABLE IF EXISTS vp_nodes_old")
end
exit
VpNode.first
VpNode.columns
VpNode.column_names
exit
VpNode.column_names
exit
edit -t
cd ActiveRecord::Migration.new
drop_table :assets_new
exit
reload!
require 'd_hasher'
ProductImage.first
img1 = _
img2 = ProductImage.second
DHasher.distance(img1, img2)
fields ||= ['dhash1'.freeze, 'dhash2'.freeze, 'dhash3'.freeze, 'dhash4'.freeze]
fields
hashes1 = obj1.slice(*fields).values
hashes1 = img1.slice(*fields).values
path = img1.attachment.url
path = img1.attachment.url.gsub('/development', '')
dhashes = {}
DHasher.hash_from_path(path).each_with_index do |hash, idx|
  dhashes["dhash#{idx+1}"] = hash
end
j      image.update(dhashes)
image.update(dhashes)
img1.update(dhashes)
reload!
img1.update(dhashes)
img1 = _
ProductImage.first(10).each do |image|
  path = image.attachment? ? image.attachment.url.gsub('/development'.freeze, ''.freeze) : image.original_url
  begin
    dhashes = {}
    DHasher.hash_from_path(path).each_with_index do |hash, idx|
      dhashes["dhash#{idx+1}"] = hash
    end
    image.update(dhashes)
  rescue Magick::ImageMagickError => e
    Rails.logger.error("Image dhash generation failed for ProductImage id : #{image.id}")
    Rails.logger.error(e.message)
  end
end
img1 = ProductImage.first
ProductImage.first(10).map do |image|
  dist = DHasher.distance(img1, image)
  { id: image.id, dist: dist }
end
[1..16]
fields = (1..16).to_a
files.map! {|i| "dhash#{i}
files.map! {|i| "dhash#{i}" }
fields.map! {|i| "dhash#{i}" }
fields
ProductImage.first(10).map do |image|
  dist = DHasher.distance(img1, image, fields)
  { id: image.id, dist: dist }
end
[{:id=>3191597, :dist=>0},
  {:id=>3191598, :dist=>126},
  {:id=>3191599, :dist=>119},
  {:id=>3234735, :dist=>117},
  {:id=>3234736, :dist=>108},
  {:id=>3234737, :dist=>110},
  {:id=>3234738, :dist=>131},
  {:id=>3234739, :dist=>113},
  {:id=>3234748, :dist=>138},
{:id=>3234749, :dist=>107}]
img_dhash_fields = (1..16).to_a.map { |i| dhash_fields << "dhash#{i}" }
img_dhash_fields = (1..16).to_a.map { |i| "dhash#{i}" }
img_dhash_fields
img1
gg IMG_DHASH_FIELDS = (1..16).to_a.map { |i| "dhash#{i}" }
IMG_DHASH_FIELDS = (1..16).to_a.map { |i| "dhash#{i}" }
img1.slice(*IMG_DHASH_FIELDS)
vp1_image = img
vp1_image = img1
IMG_DHASH_FIELDS = (1..16).to_a.map { |i| "dhash#{i}" }
VP1_DHASH_FIELDS = (1..16).to_a.map { |i| "vp1_dhash#{i}" }
VP2_DHASH_FIELDS = (1..16).to_a.map { |i| "vp2_dhash#{i}" }
attrs = { vp1_id: vp1_image['id'] }
VP1_DHASH_FIELDS.length.times do |idx|
  attrs[VP1_DHASH_FIELDS[idx]] = vp1_image[IMG_DHASH_FIELDS[idx]]
end
attrs
exit
edit -t
exit
edit -t
ProductImage.where.not(dhash1: nil).first(1000).last
edit -t
ProductImage.first(1000)
ProductImage.where.not(dhash1: nil).first(1000) do |image|
  path = image.attachment? ? image.attachment.url.gsub('/development', '') : image.original_url
  dhashes = {}
  DHasher.hash_from_path(path).each_with_index do |hash, idx|
    dhashes["dhash#{idx+1}"] = hash
  end
  image.update(dhashes)
end
edit -t
ProductImage.where.not(dhash1: nil).first(1000).each do |image|
  # image = ProductImage.find(id)
  path = image.attachment? ? image.attachment.url.gsub('/development', '') : image.original_url
  # begin
  dhashes = {}
  puts image.id
  DHasher.hash_from_path(path).each_with_index do |hash, idx|
    dhashes["dhash#{idx+1}"] = hash
  end
  image.update(dhashes)
  # rescue Magick::ImageMagickError => e
  #   Rails.logger.error("Image dhash generation failed for ProductImage id : #{image.id}")
  #   Rails.logger.error(e.message)
  # end
end
exit
reload!
VpNode.rebuild_tree
reload!
VpNode.rebuild_tree
VpNode.count
ProductImage.first(1000).sample
img = _
img.attachment.url
ProductImage.first(1000).sample
img = _
img.attachment.url
url = 'http://s3.amazonaws.com/rvx-rds-dev/images/uploads/003/271/323/original/data?1478071424'
VpNode.find_nearest_neighbors(url)
reload!
VpNode.find_nearest_neighbors(url)
exit
VpNode.find_nearest_neighbors(url)
url = 'http://s3.amazonaws.com/rvx-rds-dev/images/uploads/003/271/323/original/data?1478071424'
VpNode.find_nearest_neighbors(url)
DHasher.hash_from_path(url)
url
cd VpNode
url = "http://s3.amazonaws.com/rvx-rds-dev/images/uploads/008/195/237/original/open-uri20160709-734-1nlf33p?1478690946"
existing_image = ProductImage.find_by(original_url: url)
image_obj = if existing_image.present?
  Hashie::Mash.new(existing_image.slice(*IMG_DHASH_FIELDS))
else
  hashes = DHasher.hash_from_path(url)
  i = 0
  hashes.each_with_object(Hashie::Mash.new) do |hash, obj|
    obj["dhash#{i+=1}"] = hash
  end
end
root = VpNode.root_node
VpNode.pluck(:vp1_dhash5)
VpNode.first
exit
reload!
VpNode.rebuild_tree
VpNode.first
cd VpNode
ProductImage.where.not(vp_node_id: nil).update_all(vp_node_id: nil)
VpNodeProductImage.connection.execute("truncate table #{VpNodeProductImage.quoted_table_name}")
VpNode.delete_all
VpNode.connection.execute("ALTER TABLE #{VpNode.quoted_table_name} AUTO_INCREMENT = 1") # reset id count to 1
node = create(ancestry: '/')
query = 'SELECT id, dhash1, dhash2, dhash3, dhash4 FROM assets WHERE dhash5 IS NOT NULL'
images = ProductImage.connection.select_all(query).to_ary; nil
query = "SELECT id, #{IMG_DHASH_FIELDS.join(', ')} FROM assets WHERE dhash5 IS NOT NULL"
exit
reload!
VpNode.rebuild_tree
url = "http://s3.amazonaws.com/rvx-rds-dev/images/uploads/008/195/237/original/open-uri20160709-734-1nlf33p?1478690946"
VpNode.find_nearest_neighbors(url)
reload!
VpNode.find_nearest_neighbors(url)
reload!
VpNode.find_nearest_neighbors(url)
url
url = ProductImage.first(1000).sample.attachment.url.gsub('/development', '')
VpNode.find_nearest_neighbors(url)
reload!
VpNode.find_nearest_neighbors(url)
VpNode.count
relaod!
reload!
VpNode.count
VpNode.find_nearest_neighbors(url)
reload!
VpNode.find_nearest_neighbors(url)
ids = [
  8195237,
  8494441,
  8469716,
  8567689,
  8781331,
  8468124,
  8508441,
  7796017,
  7796025,
  7795995
]
ProductImage.where(id: ids).each do |image|
  # image = ProductImage.find(id)
  path = image.attachment? ? image.attachment.url.gsub('/development', '') : image.original_url
  # begin
  dhashes = {}
  puts image.id
  DHasher.hash_from_path(path).each_with_index do |hash, idx|
    dhashes["dhash#{idx+1}"] = hash
  end
  image.update(dhashes)
  # rescue Magick::ImageMagickError => e
  #   Rails.logger.error("Image dhash generation failed for ProductImage id : #{image.id}")
  #   Rails.logger.error(e.message)
  # end
end
ProductImage.where(id: ids).each do |image|
  puts image.id
  # image = ProductImage.find(id)
  path = image.attachment? ? image.attachment.url.gsub('/development', '') : image.original_url
  # begin
  dhashes = {}
  puts image.id
  DHasher.hash_from_path(path).each_with_index do |hash, idx|
    dhashes["dhash#{idx+1}"] = hash
  end
  image.update(dhashes)
  # rescue Magick::ImageMagickError => e
  #   Rails.logger.error("Image dhash generation failed for ProductImage id : #{image.id}")
  #   Rails.logger.error(e.message)
  # end
end
ProductImage.where(id: ids).each do |image|
  puts image.id
  # image = ProductImage.find(id)
  path = image.attachment? ? image.attachment.url : image.original_url
  path.gsub!('/development', '') unless image.id == 8469716
  # begin
  dhashes = {}
  puts image.id
  DHasher.hash_from_path(path).each_with_index do |hash, idx|
    dhashes["dhash#{idx+1}"] = hash
  end
  image.update(dhashes)
  # rescue Magick::ImageMagickError => e
  #   Rails.logger.error("Image dhash generation failed for ProductImage id : #{image.id}")
  #   Rails.logger.error(e.message)
  # end
end
reload!
VpNode.rebuild_tree
url = "http://s3.amazonaws.com/rvx-rds-dev/images/uploads/008/195/237/original/open-uri20160709-734-1nlf33p?1478690946"
VpNode.find_nearest_neighbors(url)
relaod!
reload!
VpNode.find_nearest_neighbors(url)
reload!
VpNode.find_nearest_neighbors(url)
reload!
VpNode.find_nearest_neighbors(url)
VpNode.count
exit
image = ProductImage.first
path = image.attachment? ? image.attachment.url : image.original_url
path.gsub!('/development', '')
dhashes = {}
DHasher.hash_from_path(path).each_with_index do |hash, idx|
  dhashes["dhash#{idx+1}"] = hash
end
image.attributes = dhashes
image.save
image.attributes
exit
image = ProductImage.where.not(dhash5: nil).count
image = ProductImage.where.not(dhash5: nil).sample
image1
image1 = image
image2 = ProductImage.where.not(dhash5: nil).sample
image1.id
image2.id
path1 = image1.attachment.url.gsub('/development', '')
path2 = image2.attachment.url.gsub('/development', '')
iids = [
  8195237,
  8494441,
  8469716,
  8567689,
  8781331,
  8468124,
  8508441,
  7796017,
  7796025,
  7795995
]
image1, image2 = ProductImage.where(id: ids).first(2)
ids = iids
image1, image2 = ProductImage.where(id: ids).first(2)
path1 = image1.attachment.url.gsub('/development', '')
image1, image2 = ProductImage.where(id: ids).last(2)
path1 = image1.attachment.url.gsub('/development', '')
path2 = image2.attachment.url.gsub('/development', '')
image1 = ProductImage.find 8195237
image2 = ProductImage.find 8508441
path2 = image2.attachment.url.gsub('/development', '')
path1 = image1.attachment.url.gsub('/development', '')
DHasher.distance(image1, image1, VpNode::IMG_DHASH_FIELDS)
DHasher.distance(image1, image2, VpNode::IMG_DHASH_FIELDS)
[image1, image2].each do |image|
  path = image.attachment? ? image.attachment.url.gsub('/development', '') : image.original_url
  dhashes = {}
  DHasher.hash_from_path(path).each_with_index do |hash, idx|
    dhashes["dhash#{idx+1}"] = hash
  end
  image.update(dhashes)
end
DHasher.distance(image1, image2, VpNode::IMG_DHASH_FIELDS)
ProductImage.where.not(dhash5: nil).count
ProductImage.where.not(dhash5: nil).each do |image
ProductImage.where.not(dhash5: nil).each do |image|
  path = image.attachment? ? image.attachment.url.gsub('/development', '') : image.original_url
  begin
    dhashes = {}
    DHasher.hash_from_path(path).each_with_index do |hash, idx|
      dhashes["dhash#{idx+1}"] = hash
    end
    image.update(dhashes)
  rescue Magick::ImageMagickError => e
    # in case of bad attachment url, try re-saving attachment
    if image.attachment?
      path = image.attachment.url.gsub('rvx-rds-dev', 'rvx-rds')
      image.attachment = path
      dhashes = {}
      DHasher.hash_from_path(path).each_with_index do |hash, idx|
        dhashes["dhash#{idx+1}"] = hash
      end
      image.attributes = dhashes
      image.save
    end
  rescue NoMethodError => e
    # Catch errors when file is below 10kb and returns StringIO
    if e.message.include?("undefined method `path'")
      Rails.logger.error("Image dhash generation failed for ProductImage id : #{image.id} -- Image too small")
    else
edit -t
exit
url = "http://s3.amazonaws.com/rvx-rds-dev/images/uploads/008/469/716/original/open-uri20161129-18809-z9p914?1480443831"
image = Magick::Image.read(path).first
path = url
image = Magick::Image.read(path).first
path = "http://s3.amazonaws.com/rvx-rds-dev/images/uploads/008/195/237/original/open-uri20160709-734-1nlf33p?1478690946"
image = Magick::Image.read(path).first
image = image.quantize(256, Magick::GRAYColorspace)
image
image.write('tmp/grayscale.jpg')
image.trim!
image.write('tmp/grayscale.jpg')
image.resize(36, 36)
image.resize!(36, 36)
image.write('tmp/grayscale.jpg')
image.destroy!
image = Magick::Image.read(path).first
image.write('tmp/grayscale.jpg')
image = image.quantize(256, Magick::RGBColorspace, Magick::NoDitherMethod, 8)
image.write('tmp/grayscale2.jpg')
image.destroy!
image = Magick::Image.read(path).first
image = image.quantize(256, Magick::RGBColorspace, Magick::NoDitherMethod, 8)
image.geometry
image.
image.columns
image.rows
block_count=32
x_size = image.columns / block_count
y_size = image.rows / block_count
500 / 32.0
x_size = image.columns / block_count
y_size = image.rows / block_count
image.rows / 15
image.rows / 16
32 * 15
32 * 16
img = ProductImage.first
img.dhash4.bit_length
block_count=62
x_size = image.columns / block_count
y_size = image.rows / block_count
8 * 62
slice = image.excerpt(0, 0, 8, 8)
pix = slice.scale(1, 1)
avg_color_hex = pix.to_color(pix.pixel_color(0,0))
pix.pixel_color(0,0)
pix.pixel_color(0,0).to_f
pix.pixel_color(0,0)
pix.pixel_color(0,0).hash
slice2 = image.excerpt(8, 0, 8, 8)
pix = slice2.scale(1, 1)
pix.pixel_color(0,0).hash
image.hash
pix.pixel_color(0,0)
pix
pix.pixel_color(0,0)
pix    avg_color_hex = pix.to_color(pix.pixel_color(0,0))
avg_color_hex = pix.to_color(pix.pixel_color(0,0))
pix.total_colors
image.total_colors
image.pixel_color(0, 0)
pixel = pix.pixel_color(0,0)
pixel.to_hsla
image = image.quantize(256, Magick::RGBColorspace, Magick::NoDitherMethod)
image = image.quantize(256, Magick::GRAYColorspace, Magick::NoDitherMethod)
image.destroy!
exit
url = "http://s3.amazonaws.com/rvx-rds-dev/images/uploads/008/469/716/original/open-uri20161129-18809-z9p914?1480443831"
path = url
image = Magick::Image.read(path).first
path = 'http://s3.amazonaws.com/rvx-rds-dev/images/uploads/008/508/441/original/open-uri20161104-11943-9m7p98?1478248343'
image = Magick::Image.read(path).first
image = image.quantize(256, Magick::GRAYColorspace, Magick::NoDitherMethod)
image.signature
image.columns
bits=62
width = image.columns
even_x = width % bits == 0
even_y = height % bits == 0
height = image.rows
even_y = height % bits == 0
(0...32).to_a
Array.new(32)
Array.new(32, 0)
Array.new(32, Array.new(32, 0))
arr = _
arr.length
arr[0].length
block_width = width.to_f / bit_length
bit_length=62
width = image.columns
height = image.rows
blocks = Array.new(bit_length, Array.new(bit_length, 0))
block_width = width.to_f / bit_length
block_height = height.to_f / bit_length
image.write('tmp/grey.png')
pix
pixel
iamge
image
slice
slice = image.excerpt(0, 0, 8, 8)
pix = slice.scale(1, 1)
pixel = pix.pixel_color(0,0)
pixel.intensity
image1 = image.scale(1,1)
image1.pixel_color(0, 0).intensity
image
image.destroy
image.destroy!
image1.destroy!
bit_length=64
image = Magick::Image.read(path).first
image = image.quantize(256, Magick::GRAYColorspace, Magick::NoDitherMethod)
x_size = image.columns / bit_length
x_size = image.columns / bit_length.to_f
y_size = image.rows / bit_length.to_f
500 % 64
64 * 8
image.resize!(512, 512)
block_width = image.columns / bit_length.to_f
block_height = image.rows / bit_length.to_f
image.destroy!
width = 500
(0...width).each do |i|
  puts i
end
block_height
image = Magick::Image.read(path).first
image = image.quantize(256, Magick::GRAYColorspace, Magick::NoDitherMethod)
width = image.columns
height = image.rows
block_width = width / bit_length.to_f
block_height = height / bit_length.to_f
y = 0
blocks = Array.new(bit_length, Array.new(bit_length, 0))
block_height
1 % 7.8125
y
block_top = block_bottom = y / block_height
block_top
block_top = block_bottom = Integer(y / block_height)
block_top
(y + 1) % block_height
1.23.divmod 1
-1.23.divmod 1
-1.23 % 1
1.23.modulo(1)
-1.23.modulo(1)
1.23.frac
BigDecimal(1.23).frac
BigDecimal.new(1.23).frac
BigDecimal.new('1.23').frac
(num - num.to_i).abs
num = 1.23
(num - num.to_i).abs
num = -1.23
(num - num.to_i).abs
(num - num.to_i)
(x + 1) % block_width
(y + 1) % block_width
(2 + 1) % block_width
(3 + 1) % block_width
(54 + 1) % block_width
(52 + 1) % block_width
(55 + 1) % block_width
(0 + 1) % block_width
y / block_height
block
block_height
y
y = 24
y / block_height
y_frac = (num = y / block_height) && (num - num.to_i)
y_int = y.to_i
(num = y / block_height
num = y / block_height
1.072 - 1
(1.072 - 1).class
y_frac = (num = y / block_height) && (num - num.to_i)
y_frac
block_top = block_bottom = Integer(y / block_height)
block_bottom = Integer(-(-y / block_height)) # int(math.ceil(float(y) / block_height))
pixel
pixel.red
pixel.blue
image.destroy!
image = Magick::Image.read(path).first
image = image.quantize(256, Magick::RGBColorspace, Magick::NoDitherMethod, 8)
pixel = image.pixel_color(0, 0)
pixel.red
pixel.blue
pixel.greenn
pixel.green
pixel = image.pixel_color(250, 250)
pixel.green
pixel.red
pixel.blue
pixel.intensity
pixel.red + pixel.green + pixel.blue
pixel.red + pixel.green + pixel.blue + pixel.yellow
path
image
image.destroy!
kyj    original = Magick::Image.read(path).first
original = Magick::Image.read(path).first
image = original.quantize(256, Magick::RGBColorspace, Magick::NoDitherMethod, 8)
original.destroy!
width = image.columns
height = image.rows
block_width = width / bit_length.to_f
block_height = height / bit_length.to_f
block_width = width / bit_length.to_f
block_height = height / bit_length.to_f
jj    blocks = Array.new(bit_length, Array.new(bit_length, 0))
blocks = Array.new(bit_length, Array.new(bit_length, 0))
block_top, block_left, block_right, block_bottom, value, weight_left, weight_right = nil
(0...height).each do |y|
  if height % bit_length == 0
    # don't bother dividing y, if the size evenly divides by bits
    block_top = block_bottom = Integer(y / block_height)
    weight_top, weight_bottom = 1, 0
  else
    y_frac = (num = y / block_height) && (num - num.to_i)
    y_int = y.to_i
    weight_top = (1 - y_frac)
    weight_bottom = (y_frac)
    # y_int will be 0 on bottom/right borders and on block boundaries
    if y_int > 0 or (y + 1) == height
      block_top = block_bottom = Integer(y / block_height)
    else
      block_top = Integer(y / block_height)
      block_bottom = Integer(-(-y / block_height))
    end
  end
  (0...width).each do |x|
    value = total_value(image.pixel_color(x, y))
    if width % bit_length == 0
      # don't bother dividing x, if the size evenly divides by bits
      block_left = block_right = Integer(x / block_width)
      weight_left, weight_right = 1, 0
    else
edit -t
blocks
gst
exit
Product.count
product = Product.first
exit
product = Product.first
product.brand
reload!
exit
product = Product.first
product.brand
exit
product = Product.first
product.brand
product.brand = 'Sony'
exit
product = Product.first
product.brand
product.brand = 'Sony'
Brand.count
Product.join(:brand).all
Product.joins(:brand).all
product
product.save
product.save!
exit
product = Product.first
product.brand
product.brand = 'Sony'
product.brand = 'sony'
Product.count
Brand.count
product.brand = 'Sony'
product.save
product.save!
relaod!
reoad!
reload!
product = Product.first
product.brand = 'Sony'
product.save!
exit
product = Product.first
product.brand
product.brand = 'Asus'
product.brand
product.save!
exit
product = Product.first
product.brand = 'Asus'
product.valid?
product.save
exit
product = Product.first
product.brand = 'Asus'
product.save
Brand.count
b = Brand.first
b.products
product.brand
product.brand_id
Product.pluck(:brand_id)
Product.pluck(:brand_id).reject(&:nil?)
product.brand = 'Sony'
product.save
product.brand_id
exit
Product.pluck(:brand)
Product.pluck(:brand).uniq
Product.pluck(:brand).uniq.sort
Product.pluck(:brand).uniq.sort.count
Product.pluck(:brand).uniq.sort.map(&:downcase).count
Product.pluck(:brand).uniq.sort.map(&:downcase)
brand = Brand.first
brand.products
brand.products.count
Brand.count
Brand.map {|b| b.products.count }
Brand.all.map {|b| b.products.count }
Brand.all.map {|b| [b.id, b.name, b.products.count] }
Brand.all.map {|b| [b.id, b.name, b.products.count] }.sort_by {|arr| arr[2] }
counts = _
counts
Brand.all.map {|b| [b.id, b.name, b.products.count] }.sort_by {|arr| -arr[2] }
counts = _
cd Product.first
self[:brand] = name
name
self[:brand] = 'Sony'
brand_changed?
self[:brand] = 'Brand1'
brand_changed?
brand_id_changed?
exit
product = Product.first
product.brand
product.brand_id
product.brands
product.brand = 'Sony 2'
product.brand_id
product.brand
product = Product.first
product.brand
cd product
brand = 'Sony 2'
brand_changed? || brand_id_changed
brand_changed? || brand_id_changed?
self.brand
self.brand_changed? || self.brand_id_changed?
brand
new_brand = Brand.find_or_create_by(name: 'Sony 2')
self[:brand_id] = new_brand.id
self.brand_changed? || self.brand_id_changed?
self[:brand_id] = new_brand.id
exit
product = Product.first
product.brand
product.brand = 'Sony 2'
product.save
product
product.brand
product.brand_id = 1
product.save
product
product.brand
reload!
product = Product.first
product.brand
product.brand = 'Sony 2'
product.save
product
product.brand
product.brand_id = 1
product.sve
product.save
proudct
product
Brand.count
Brand.group(:name).count
Brand.group(:name).count.to_sql
Brand.group(:name).count
Brand.group(:name)
Brand.group.count(:name)
Brand.group(:name)
2 ** 62
(2 ** 62 - 1).object_id
exit
p = Product.first
p.brand
p.brand = 'Sony 2'
p.save
p
p.brand
exit
p = FactoryGirl.create(:product, brand_id: 1)
p
p = Product.initialize
p = Product.new
p.brand_changed?
p.brand_id_changed?
reload!
p = FactoryGirl.create(:product, brand_id: 1)
exit
p = FactoryGirl.create(:product, brand_id: 1)
p = Product.new(brand_id: 1)
p.save
p.save!
p.entity_id = 1
Entity.first
p.save!
p.model = 'xxx123'
p.save!
p.brand_id_changed? || p.brand.nil?
p.brand_id
new_brand = Brand.find_or_create_by(id: p[:brand_id])
p.brand = new_brand.name
p.save!
relaod!
exit
p = FactoryGirl.create(:product, brand_id: 1)
p = Product.new(brand_id: 1)
p = FactoryGirl.create(:product, brand_id: 1)
Country.find_by(name: 'Country2')
Country.last
Country.find_by(name: 'Country2').destroy
p = FactoryGirl.create(:product, brand_id: 1)
Brand.al
Brand.all
Brand.last(2).map(&:destroy)
p = FactoryGirl.create(:product, brand_id: 1)
p = FactoryGirl.build(:product, brand_id: 1)
p.valid?
p.errors
p.brand = 'Sony 2'
p.valid?
p.save
exit
Brand.insert <<-SQL.squish!
      INSERT INTO brands(name) VALUES(SELECT DISTINCT(products.brand) from products)
    SQL
<<-SQL.squish!
      INSERT INTO brands(name) VALUES(SELECT DISTINCT(products.brand) from products)
    SQL
Brand.execute <<-SQL.squish!
      INSERT INTO brands(name) VALUES(SELECT DISTINCT(products.brand) from products)
    SQL
query = <<-SQL.squish!
      INSERT INTO brands(name) VALUES(SELECT DISTINCT(products.brand) from products)
    SQL
query
Brand.connection.execute query
INSERT INTO brands(name) VALUES((SELECT DISTINCT(products.brand) from products))
query = <<-SQL.squish!
INSERT INTO brands(name) VALUES((SELECT DISTINCT(products.brand) from products))
SQL
Brand.connection.execute query
Brand.execute <<-SQL.squish!
      INSERT INTO brands(name) SELECT DISTINCT(products.brand) from products
    SQL
Brand.connection.execute <<-SQL.squish!
      INSERT INTO brands(name) SELECT DISTINCT(products.brand) from products
    SQL
Brand.count
exit
Brand.count
Brand.first
Product.first
Product.first.update(brand: 'Sony')
Product.first
Brand.count
Brand.first.products.count
Brand.all.map {|b| [b.id, b.name, b.products.count] }
p = Product.first
p.brand = 'Sony 2'
p.save
p
exit
<<-SQL.squish!
      INSERT INTO brands(name) SELECT DISTINCT(unknown_products.brand) FROM unknown_products WHERE unknown_products.brand NOT IN (SELECT DISTINCT(brands.name))
    SQL
query = _
Brand.connection.execute <<-SQL.squish!
      INSERT INTO brands(name) SELECT DISTINCT(unknown_products.brand) FROM unknown_products WHERE unknown_products.brand NOT IN (SELECT DISTINCT(brands.name))
    SQL
exit
Brand.count
exit
Brand.count
exit
Brand.count
Product.count
Brand.count
RvxSignal.count
RvxSignal.pluck(:brand)
rri = RoyaltyReport.first
rri.product
rri.fuz_brand
rri = RoyaltyReportItem.first
rri.fuz_brand
rri.product
rri.product.brand = 'Asus'
rri.product.save
rri.product
Brand.first
p = Product.first
p.save!
p.save
exit
p = Product.first
p.save
p
Brand.find(207)
p.brand = 'Asus'
p.save
p
p.save!
p = Product.first
p.brand
p.brand_id
Brand.find(207)
p.brand_id = 207
p.save
p
p.reload
p.brand = 'Sony 2'
p.save
p
p.brand = 'Asus'
p.save
p
rri = RoyaltyReportItem.first
rri.product
rri.product.brand = 'Sony'
rri.product
rri.product.save
p = Product.first
rri.product
rri
p
p.brand = 'Asus'
Brand.count
reload!
exit
b = Brand.first
b.save!
b
b.update!
b.update
b.set_clean_name
b
clean_name = b.name.gsub(/[^0-9a-z ]/, '')
clean_name = b.name.gsub(/[^0-9A-z ]/, '')
clean_name.squish!
clean_name.upcase!
self.clean_name = clean_name
reload!
b = Brand.first
b.set_clean_name
b
b.save
Brand.find_each do |b|
  b.set_clean_name
  b.save
end
Brand.pluck(:clean_name)
Brand.pluck(:clean_name).count
Brand.pluck(:clean_name).uniq.count
b = Brand.second
b = Brand.first
b = Brand.second
b = Brand.find 2
b.set_clean_name
b.save
b
Brand.find_each { |b| b.set_clean_name! }
Brand.find_each { |b| puts b.id; b.set_clean_name! }
b = Brand.first
b.set_clean_name!
reload!
b = Brand.first
b.name
b.name.gsub(/[^0-9A-z ]/, '')
clean_name.squish!
clean_name = b.name.gsub(/[^0-9A-z ]/, '')
clean_name.squish!
clean_name.upcase!
b.clean_name = clean_name
b.save
reload!
Brand.find_each { |b| puts b.id; b.set_clean_name! }
Brand.count
Brand.pluck(:clean_name)
Brand.pluck(:clean_name).count
Brand.pluck(:clean_name).uniq.count
exit
b = Brand.find 512
b.products
exit
Product.where(brand: nil)
Product.where(brand: [nil, ''])
Brand.where(name: nil)(
Brand.where(name: nil)
b = Brand.first
b.products
b.royalty_report_items
exit
Attachment.count
Attachment.first
rr = Attachment.first
rr.attachment.url
exit
Contract.first
c = _
c.technologies.count
t = c.technologies.first
t.products
p = t.products.first
c.contract_num
p.brand
p.model
t
p.application_types
Entity
Entity.first
EntityType.count
EntityType.pluck(:id, :name)
type = EntityType.find 18
type.entities
entity = type.entities.first
entity.name
Attachment.last
Attachment.last.attachment.url
file = Attachment.last
file.dl_attachment_url
file.dl_attachment_path
file.direct_upload_url
Attachment.last
Attachment.direct_upload_url
Attachment.last.direct_upload_url
exiet
exit
Brand.first
exit
DeveloperKey
DeveloperKey.last
key = _
key.access_id
key.secret
Brand.includes(
  products: [:entity,
    :product_category,
    :application_types,
    :manufacturer,
    :royalty_paid_by_entity,
    :product_status,
    :mfg_models,
  :product_technologies => [:technology_key, :technology]]
)
b = _.first
b.products.first
p = _
p.royalty_paid_by_entity
p.royalty_report_items
query =     Brand.includes(
  products: [
    :entity,
    :royalty_report_itmes,
    :product_category,
    :application_types,
    :manufacturer,
    :royalty_paid_by_entity,
    :product_status,
    :product_technologies => [:technology_key, :technology]
  ]
)
entity_ids = [1]
query.count
@query.where!(products: { entity_id: entity_ids })
query.where!(products: { entity_id: entity_ids })
query.count
Brand.includes(
  products: [
    :entity,
    :royalty_report_items,
    :product_category,
    :application_types,
    :manufacturer,
    :royalty_paid_by_entity,
    :product_status,
    :product_technologies => [:technology_key, :technology]
  ]
).where(products: { entity_id: entity_ids })
Brand.first
b = Brand.first
b.clean_name
b.set_clean_name!
b
exit
b = Brand.first
b.products_count
p = Product.first
p.brand
p.brand_model
p.brand_id
reload!
p = Product.first
p.brand_id
p.brand_model
exit
cloud
Cloud.current
exit
b = Brand.first
b.products.first
p = _
p.brand_model
b1 = Brand.second
b1 = Brand.find 2
b2 = _
b2.products.count
b2.products.first
b2.products.first.name
b2.products.destroy_all
b2.destroy
b2 = Product.first(2).last
b2 = Brand.first(2).last
b2.products
b
b.products_count
b2.products.each {|p| p.update(brand_id: 1) }
b2.products.count
b.products.count
exit
Brand.includes(:products).where(name: 'Sony').count
Brand.includes(:products).where(name: 'Sony').products.count
b = Brand.includes(:products).where(name: 'Sony')
b.products.size
b = Brand.first
b.products.size
b.products.count
p
p = Product.first
p['brand''
p['brand']
@brand_field = :brand
p.call(@brand_field)
p.send(@brand_field)
exit
p = Product.first
p.brand
b = Brand.last
p.brand_id = b.id
p.save
reload!
p
p.reload
p.brand_id = b.id
p.save
exit
p = Product.first
b = Brand.last
p
p.brand_id = b.id
p.save
cd p
self.brand_id
self.send(@brand_field)
exit
p = Product.first
b = Brand.last
p.brand_id = b.id
p.save
p
b
b.products.first
p.brand
p.reload
p.brand_id = 1
p.save!
p
cd p
brand_col = :brand
self.brand_id.nil? && self[brand_col].nil?
self.brand_changed?
(self.brand_id.nil? && self[brand_col].present?)
self.brand_id_changed?
self.send(:"#{brand_col}_changed?")
self.brand_id_changed? || (self.brand_id.nil? && self[brand_col].nil?)
b
b = Brand.last
self.brand_id = b.id
self.brand_id_changed? || (self.brand_id.nil? && self[brand_col].nil?)
new_brand = Brand.find_or_create_by(id: self[:brand_id])
self[brand_col] = new_brand.name
self
self.save
exit
p
exit
p = Product.first
b = Brand.first
p.brand_id = 1
p.save
p
p.reload
reload!
p
p = Product.first
p.update_brand_fields
p
reload!
p = Product.first
p.brand = 'Sony'
p.save
p
p.brand_id
p.brand_id = b.id
b
b = Brand.last
p.brand_id = b.id
p.save
p
p.brand_id = 1
p.save
reload!
p = Product.first
b = Brand.last
p.brand_id = b.id
p.save
p
p.brand_model
UsageReportItem
UsageReportItem.last
exit
UsageReport.count
UsageReportItem.count
UsageReportItem.uniq.pluck(:product_id)
UsageReportItem.uniq.pluck(:product_id).count
edit -t
edit
exit
edit
query.count
Proudct.first
prod = Product.first
prod.brand_id
prod.brand_model
exit
b = Brand.first
b.products.size
RoyalyReport.first
RoyaltyReportItem.first
rri = _
rri.brand
p = Product.first
p.brand_model
rri.brand
rri.fuz_brand
b = rri.brand
b.royalty_report_items.count
b.usage_report_items.count
reload!
b
b.reload
b = Brand.find b.id
b.usage_report_items.count
b.usage_report_items
exit
b = Brand.find_by name: 'Asus'
b.products.size
b.usage_report_items.size
b.products
p
b
cd b
send(
send("#{'product'}s")
send("#{'product'}s").count
send("#{'product'}s").size
exit
cd Product
exit
cd Project
self.constants
self::AVAILABLE_CHANNELS
exit
event_type = 'jonahtan
'
event_type = 'jonahtan'
<<-RUBY
          # prepend: true is needed because after callback are triggered in reverse order
          after_commit :after_#{event_type}_for_notifications, prepend: true,
                        on: :#{event_type}, unless: :disable_event_creation
        RUBY
str= 'str'
str.to_sym!
exit
UsageReportItem.pluck(:brand).count
UsageReportItem.pluck(:brand).uniq.count
UsageReportItem.uniq.pluck(:brand)
UsageReportItem.uniq.pluck(:brand).count
USAGE_REPORTING_OFFSET
USAGE_REPORTING_TYPE
uri = UsageReportItem.first
uri.product.brand
uri.brand
uri.brand_id
uri.product.brand_id
exit
"belongs_to :#{association_name}, class_name: 'Brand', foreign_key: 'brand_id', counter_cache: true"
association_name = :brand_model
"belongs_to :#{association_name}, class_name: 'Brand', foreign_key: 'brand_id', counter_cache: true"
exit
p = Product.first
p.brand
p.assoc_brand
p.brand_id = 1
p.save
p.brand
p
p.brand = 'ASUS'
p.save
p
p.assoc_brand
exit
SchemaMigration
exit
b = Brand.first
b.products.size
b.royalty_report_items.size
b.usage_report_items.size
b.unknown_products.size
b.product_ids
exit
rri = RoyaltyReportItems.first
rri = RoyaltyReportItem.first
rri.brand
rri.fuz_brand
rri.product
exit
Incident.first
Incident.first.products
p = _.first
p.incidents
p.incidents.to_sql
p.id
Brand.includes()
Brand.includes(nil)
Brand
exit
BrandsTable.new(current_user).with_filters(params).as_json
current_user = User.first
params = {}
BrandsTable.new(current_user).with_filters(params).as_json
reload!
BrandsTable.new(current_user).with_filters(params).as_json
exit
Brand.where()
Brand.includes(nil).where()
edit -t
Brand.where().class
ActiveRecord_Relation
exit
current_user = User.first
params = CGI.parse('sort_key=products_count&sort_order=desc')
params.class
params = Hashie::Mash.new(CGI.parse('sort_key=products_count&sort_order=desc'))
params.class
current_user
BrandsTable.new(current_user).with_filters(params).as_json
BrandsTable.new(current_user).with_filters({}).as_json
current_user
Brand.count
exit
current_user = User.first
params = Hashie::Mash.new(CGI.parse('sort_key=products_count&sort_order=desc'))
BrandsTable.new(current_user).with_filters(params).as_json
params
Brand.count
Brand.count / 50.0
exit
1
b = Brand.first
b = Brand.where("name like '%OEM%').last
b = Brand.where("name like '%OEM%'").last
Brand.pluck(:clean_name).uniq.count
Brand.all.each(&:set_clean_name!)
Brand.pluck(:clean_name).uniq.count
RdsProduct.count
RvxSignal.count
RvxSignal
RvxSignal.pluck(:brand)
RvxSignal.pluck(:brand).uniq
rvx_brands = RvxSignal.pluck(:brand).uniq
rvx_brands.reject! do |b|
edit
rvx_brands
rvx_brands.reject!(&:blank)
rvx_brands.reject!(&:blank?)
rvx_brands.count
rvx_brands
rvx_brands.uniq!
rvx_brands.count
rvx_brands
b = rvx_brands[2]
b.gsub(/-/, ''))
b.gsub(/-/, '')
edit
rvx_brands
rvx_brands.reject!(&:blank?)
rvx_brands = RvxSignal.pluck(:brand).reject(&:blank?).uniq
edit -t
rvx_brands[2]
b = _
b.gsub(/(\.|\?|\"|\,|\-)+/, '')
edit -t
rvx_brands
rvx_brands.reject!(&:blank?)
edit -t
rvx_brands.count
'11111'.match(/[A-z]/)
'11111a'.match(/[A-z]/)
!'11111'.match(/[A-z]/)
!'11111a'.match(/[A-z]/)
rvx_brands.reject! {|b| !b.match(/[A-z]/) }
rvx_brands.count
rvx_brands.uniq.count
rvx_brands
"4GB DIGITAL VOICE RECORDER MQU300 USB MEMORY SPY HIDDEN 144HRS AUTO SENSITIVITY CONTROL".match(/\s/).size
"4GB DIGITAL VOICE RECORDER MQU300 USB MEMORY SPY HIDDEN 144HRS AUTO SENSITIVITY CONTROL".match(/\s/)
"4GB DIGITAL VOICE RECORDER MQU300 USB MEMORY SPY HIDDEN 144HRS AUTO SENSITIVITY CONTROL".split(' ').size
rvx_brands.reject! {|b| b.split(' ').size > 5 }
rvx_brands.count
rvx_brands
Product.where(brand_id: 1)
brand_ids = [1]
RvxSignal.where(brand: Brand.where(id: brand_ids).select(:name)).count
RvxSignal.where(brand: Brand.where(id: brand_ids).select(:name)).first
RvxSignal.where(brand: Brand.where(id: brand_ids).select(:name)).first.brand
exit
Brand.first
Brand.where(brand_type: RDS_BRAND).count
Brand.where(brand_type: CONFIRMED_BRAND).count
exit
1
b = Brand.first
b.clean_name
b.clean_name = 'oem'
b.save
b.products.first
b.name
exit
p = Product.first
cd p
royalty_report_items.size
self.class.BRAND_DEPENDENTS
self.class::BRAND_DEPENDENTS
exit
p = Product.first
cd p
self.class::BRAND_DEPENDENTS.empty?
self.class::BRAND_DEPENDENTS
edit
self.send(dependent_class).update_all(values)
self.class::BRAND_DEPENDENTS.each_pair do |dependent_class, fields|
  values = {}
  fields.each {|field| values[field] = self[field]}
  puts values
  puts self.send(dependent_class).count
  self.send(dependent_class).update_all(values)
end
exit
p = Product.first
p.update(brand_id: 1)
exit
p = Product.first
p.update(brand_id: 1)
p.usage_report_items.first
Brand.first
p.brand_id = 148
p.save
p.usage_report_items.first
uri = _
uri.product_brand
uri.brand
Product.reflect_on_all_associations.map { |assoc| assoc.name}
field = { fuz_brand: :brand }
field.keys
field.first
key, value = field
key
value
exit
p = Product.first
p.royalty_report_items.pluck(:brand_id)
p.royalty_report_items.pluck(:brand)
p.royalty_report_items.pluck(:fuz_brand)
rri = FactoryGirl.create(:fuzzy_report_item)
rri.product
p
rri = FactoryGirl.create(:royalty_report_item, product: p)
rri.brand
rri.fuz_brand
p.brand
p.brand_id = 1
p.save
rr.brand
rri.brand
rri.brand_id
rri.product_id
rri.reload
rri.brand
rri.brand_id
rri.fuz_brand
uri = FactoryGirl.create(:usage_report_item, product: p)
uri = FactoryGirl.create(:usage_report_item, product_id: p.id)
uri.product
uri.brand
uri.brand_id
uri.product
uri.reload
exit
uri = UsageReportItem.first
uri.product
p = Product.first
p.active
p = Product.find 1111
p = Product.unscoped.find 1111
p.active
p.deleted_at
exit
uri = UsageReportItem.first
uri.product
exit
uri = UsageReportItem.first
uri.product
Product.uscope(where: :active)
Product.unscope(where: :active)
Product.unscope(where: :active).find(1111)
Product.unscoped(where: :active).find(1111)
Product.unscoped(:active).find(1111)
Product.unscope(where: :active).find(1111)
Product.unscope(where: :active).where(id: 1111)
Product.unscope(where: :active).where(id: 1111).unscope(:where)
Product.find(1111).unscope(:where)
Product.find(1111).unscope(:active)
Product.find(1111).unscope(where: :active)
reload!
uri = UsageReportItem.first
uri.product
reload!
uri = UsageReportItem.first
uri.product
p = Product.first
p.product_category
p.product_category = Entity.first
reload!
uri = UsageReportItem.first
uri.product
uri.product = Product.first
uri.product
uri.active
uri.save
uri.product_brand
uri.brand_id = uri.product.brand_id
uri.save
uri.product_brand
reload!
uri.product_id
uri.product_id = 1111
uri.save
uri.brand
uri.product = Product.unscoped.find(1111)
uri.brand
uri.save
uri.product
uri.brand
uri.product.brand
reload!
uri = UsageReportItem.first
uri.product
uri.product = Product.first
uri.brand
uri.brand_id
uri.save
uri.brand
uri.product
p = Product.unscoped.find(1111)
p.brand
p.brand_record
uri.product = p
uri.brand
uri.brand_id
uri.save
p.brand_record
p.reload
p.brand_record
reload!
uri = UsageReportItem.first
uri.product
reload!
uri = UsageReportItem.first
uri.product
reload!
uri = UsageReportItem.first
uri.product
reload!
uri = UsageReportItem.first
uri.product
Product.find(1111)
Product.find(1111).unscope(where: :active)
p = Product.find(1)
p = Product.select(:id, :brand, :brand_id).find(1)
new_product_id = 1111
new_product = Product.select(:id, :brand_id, :brand).find(new_product_id)
new_product = Product.select(:id, :brand_id, :brand).find(1)
exit
uri = FactoryGirl.create(:usage_report_item, product: product)
p = Product.first
uri = FactoryGirl.create(:usage_report_item, product: p)
uri
uri.product
cd uri
show-method
show-source product=
exit
reload!
uri = FactoryGirl.create(:usage_report_item, product: p)
p
uri = UsageReportItem.first
cd uri
show-source product=
exit
reload!
uri = UsageReportItem.first
cd uri
show-source
exit
cexit
exit
uri = UsageReportItem.first
cd uri
show-source product
uri
product_id
uri.association(:product).unscoped.reader(*args)
association(:product).unscoped.reader(*args)
association(:product).unscoped
association(:product).unscope
association(:product)
association(:product).reader(1111)
association(:product).reader(1111).unscoped
association(:product).reader(1111).unscope
association(:product).reader(1111)
association(:product).reader(1)
association(:product).reader)
association(:product).reader()
show-source product
exit
Brand.reflect_on_association
Brand.reflect_on_association(:product)
Brand.reflect_on_association
b = Brand.first
b.reflect_on_association
exit
uri = UsageReportItem.includes(:product).first
uri.product
Scrape.pluck(:id)
Scrape.pluck(:source_id)
exit
Scrape.count
Scrape.unscoped
s = Scrape.unscoped.map{|s| s.products.count }
exit
reload!
s = Scrape.unscoped.map{|s| s.products.count }
exit
s = Scrape.all.map{|s| [s.id, s.products.count] }
s = Scrape.find 83
exit
Listing.count
Listing.first
reload!
Scrape.find 84
s = Scrape.find 83
s.listings.count
Scrape.pluck(:id)
s = Scrape.last
s.listings.count
s.listings.unscoped(where: :scrap_id).count
s.listings.unscoped.count
reload!
s = Scrape.last
reload!
s = Scrape.last
s.listings.count
s.listings.first
reload!
s = Scrape.last
s.listings.count
reload!
s = Scrape.last
s.listings.count
s.listings << Listing.first
Listing.first
l = _
l.scrape_id
s.id
l.relaod
l.reload
l.scrape
l = Listing.first
l.scrape_id
reload!
uri = UsageReportItem.first
p = Product.last
uri.update(product: p)
uri.product
p
uri.reload
uri.product
uri.product_id
uri.product
p.id
p.brand
uri.brand
reload!
uri = UsageReportItem.first
uri.product
p = Product.last
uri.update(product: p)
uri
uri.product
reload!
uri = UsageReportItem.first
p = Product.first
uri.update(product: p)
uri.brand
uri.product
exitexit
exit
exit
uri = UsageReportItem.first
uri.product
reload!
uri.product
uri = UsageReportItem.first
uri.product
reload!
uri = UsageReportItem.first
uri.product
reload!
uri = UsageReportItem.first
uri.product
Product.paranoi_column
Product.paranoi_column?
Product.paranoia_column?
Product.paranoia_column
Product.with_deleted.find(1111)
exit
uri = UsageReportItem.first
uri.product
exit
uri = UsageReportItem.first
uri.product
uri = UsageReportItem.includes:product).first
uri = UsageReportItem.includes(:product).first
uri.product
uri.product.update(brand_id: 1)
uri.brand
uri
uri.reload
uri
uri.product
uri.brand_model
uri.brand_record
exit
RevenueByCompanyTable.new(User.first).page(1).per(10).range((Date.today - 6.months).to_s, (Date.today + 6.months).to_s).as_json
RevenueByCompanyTable.new(User.first).page(1).per(10).as_json
RevenueByCompanyTable.new(User.first).page(1).as_json
RevenueByCompanyTable.new(User.first).as_json
RevenueByCompanyTable.new(User.first)
RevenueByCompanyTable.new(User.first).first
RevenueByCompanyTable.new(User.first)
RevenueByCompanyTable.new(User.first).as_json
reload!
RevenueByCompanyTable.new(User.first).as_json
exit
uri = UsageReportItem.first
uri.product
reload!
uri = UsageReportItem.first
uri.product
uri.product.unscoped
uri.product.unscope
Product.unscope(where: :active).where(id: 1111).limit(1)
Product.unscoped.where(id: 1111).limit(1)
Product.unscoped.where(id: 1111).limit(1).first
exit
uri = UsageReportItem.first
uri.product
exit
uri = UsageReportItem.first
uri.product
uri = UsageReportItem.include(:product).first
uri = UsageReportItem.includes(:product).first
uri.product
exit
uri = UsageReportItem.includes(:product).first
uri.product
uri = UsageReportItem.first
uri.product
reload!
Product.with_deleted.find(1111)
exit
uri = UsageReportItem.includes(:product).first
uri.product
exit
rri = RoyaltyReportItem.first
rri.brand
rri.brand_record
exit
Brand
m = AddCounterCacheFieldsToBrand.new
m = ActiveRecord::Migration
change_table :brands do |t|
  t.integer :products_count, default: 0
  t.integer :unknown_products_count, default: 0
  t.integer :royalty_report_items_count, default: 0
  t.integer :usage_report_items_count, default: 0
end
cd m
change_table :brands do |t|
  t.integer :products_count, default: 0
  t.integer :unknown_products_count, default: 0
  t.integer :royalty_report_items_count, default: 0
  t.integer :usage_report_items_count, default: 0
end
def data(model)
  execute <<-SQL.squish!
        UPDATE brands
           SET #{model}_count = (SELECT count(1)
                                   FROM #{model}
                                  WHERE #{model}.brand_id = brands.id)
    SQL
end
MODELS = %w(products unknown_products royalty_report_items usage_report_items)
MODELS.each do |model|
  dir.up { data(model) }
end
MODELS.each do {|model| data(model) }
reversible do |dir|
  MODELS.each do |model|
    dir.up { data(model) }
  end
end
exit
Apartment::Tenant.switch 'synopsys'
Apartment::Tenant.switch! 'synopsys'
m = ActiveRecord::Migration
cd m
def data(model)
  execute <<-SQL.squish!
        UPDATE brands
           SET #{model}_count = (SELECT count(1)
                                   FROM #{model}
                                  WHERE #{model}.brand_id = brands.id)
    SQL
end
MODELS = %w(products unknown_products royalty_report_items usage_report_items)
change_table :brands do |t|
  t.integer :products_count, default: 0
  t.integer :unknown_products_count, default: 0
  t.integer :royalty_report_items_count, default: 0
  t.integer :usage_report_items_count, default: 0
end
reversible do |dir|
  MODELS.each do |model|
    dir.up { data(model) }
  end
end
exit
Brand
exit
uri = UsageReportItem.includes(:product).first
uri.product
exit
rri = RoyaltyReportItem.first
p1_id = rri.product_id
exit
rri = RoyaltyReportItem.first
p = rri.product
rri.brand
p.id
Prouduct.find 2
Product.find 2
p2 = _
p1 = Product.find 1
rri.product = p2
rr.product_id
rri.product_id
rri.rebrand_col = rri.class::CACHE_COLUMN
brand_col = rri.class::CACHE_COLUMN
rri.respond_to?(:product_id) && rri.product_id_changed?
p = Product.unscoped.includes(:brand).where(id: rri.product_id).first
p = Product.unscoped.where(id: rri.product_id).first
attrs = {brand_id: p.brand_id, brand: p.brand}
rri.attributes = attrs
rri
rri.save
rri
p
p.brand
attrs = {brand_id: p.brand_id, brand: p.brand}
rri
rri.attributes = attrs
rri
rri.product
rri.save
rri[brand_id] = p.brand_id
rri[brand_col]  = p.brand
rri.save
exit
rri = RoyaltyReportItem.first
p1 = rri.product
p2 = Product.second
p2 = Product.find 2
brand_col = rri.class::CACHE_COLUMN
rri.respond_to?(:product_id) && rri.product_id_changed?
p = Product.unscoped.where(id: rri.product_id).first
rri['brand_id'] = p.brand_id
rri[brand_col]  = p.brand
rri
rri.product_id = 2
rri.respond_to?(:product_id) && rri.product_id_changed?
rri[brand_col]  = p.brand
p = Product.unscoped.where(id: rri.product_id).first
rri['brand_id'] = p.brand_id
rri[brand_col]  = p.brand
rri.valid?
rri.save
rri.product
b = rri.brand_record
rri.product_id = 1
p = Product.unscoped.where(id: rri.product_id).first
rri['brand_id'] = p.brand_id
rri[brand_col]  = p.brand
rri.save
b.reload
p
p.brand?
p.brand_id?
reload!
p
p = Product.first
b = Brand.first
b.products << p
b.products_count
b.reload
b.products_count
exit
Cloud.current
Cloud.current.enable_usage_reporting?
Cloud.current.enable_illuminate?
cloud = Cloud.current
cloud.enable_illuminate?
BrandsTable.to_s
cd BrandsTable
self
self.class_name
self.class_name.gsub(/Table/, '')
self.class_name.gsub(/sTable/, '')
self.class_name.gsub(/sTable/, '').downcase
name = self.class_name.gsub(/sTable/, '').downcase
name << '_id'
name = self.class_name.downcase
name = self.class_name.downcase.gsub('stable', '_id')
name = self.class_name.downcase.gsub(/stable\z/, '_id')
'StableBrandsTable'.downcase.gsub(/stable\z/, '_id')
'StableBrandsTable'.snakeize.gsub(/stable\z/, '_id')
'StableBrandsTable'.snakeize.gsub(/stable\z/, '_id')
'StableBrandsTable'.underscore.gsub(/stable\z/, '_id')
'StableBrandsTable'.underscore.gsub(/s_table\z/, '_id')
h = { key: 'something' }
h.merge!({key: nil})
h
exit
query = "SELECT id, name FROM brands"
brands = ActiveRecord::Base.connection.select_all(query).to_ary; nil
brands.count
brands.first
brands.second
brands.third
brands.length
brands.class
query = "SELECT id, name as clean_name FROM brands"
brands = ActiveRecord::Base.connection.select_all(query).to_ary; nil
brands.third
def clean_name(brand)
  brand['clean_name'].gsub!(/(\.|\?|\"|\,)+/, '')
  brand['clean_name'].squish!
  brand['clean_name'].upcase!
end
brands.map! { |brand| clean_name(brand) }
''.squish!
str = ''
str.gsub!(/(\.|\?|\"|\,)+/, '')
str
query = "SELECT id, name as clean_name FROM brands"
brands = ActiveRecord::Base.connection.select_all(query).to_ary; nil
brands.each { |brand| clean_name(brand) }
brands
m = ActiveRecord::Migration.new
cd m
query = "SELECT id, name as clean_name FROM brands"
brands = ActiveRecord::Base.connection.select_all(query).to_ary; nil
brands.each { |brand| clean_name(brand) }
brands.each do |brand|
  execute <<-SQL.squish!
        UPDATE brands
        SET clean_name = #{brand['clean_name']}
        WHERE id = #{brand['id']}
      SQL
    end
brands
brands.each do |brand|
  next if brand.blank?
  execute <<-SQL.squish!
        UPDATE brands
        SET clean_name = #{brand['clean_name']}
        WHERE id = #{brand['id']}
      SQL
    end
brand = brands.first
<<-SQL.squish!
        UPDATE brands
        SET clean_name = #{brand['clean_name']}
        WHERE id = #{brand['id']}
      SQL
brands.each do |brand|
  next if brand['clean_name'].blank?
  execute <<-SQL.squish!
        UPDATE brands
        SET clean_name = #{brand['clean_name']}
        WHERE id = #{brand['id']}
      SQL
    end
brands.each do |brand|
  next if brand['clean_name'].blank?
  execute <<-SQL.squish!
        UPDATE brands
        SET clean_name = '#{brand['clean_name']}'
        WHERE id = #{brand['id']}
      SQL
    end
b = Brand.find(134)
brands.each do |brand|
  execute <<-SQL.squish!
        UPDATE brands
        SET clean_name = '#{brand['clean_name']}'
        WHERE id = #{brand['id']}
      SQL
    end
exit
b = Brand.first
b.products.count
b.products.size
Product.pluck(:brand_id)
exit
uri = UsageReportItem.first
p1 = Product.find 1
uri.update(product: p1)
uri.brand
uri.brand_id
uri.product = Product.unscoped.find(1111)
uri.save
uri.brand
uri.brand_id
uri.product_id = 1111
uri.save
uri.product
uri.brand
p1.brand
p1.id
p2 = Product.second
p2 = Product.find 2
uri.product = p2
uri.save
uri.brand
uri.brand_id
exit
def sum(product_type, field)
  "sum(ifnull(incident_#{product_type}_products.#{field}, 0))"
end
edit
exit
Listing
Listing.first
l = _
Listing.column_names
Listing.count
LisitingUrl.count
ListingUrl.count
ListingUrl
exit
rri = RoyaltyReportItem.first
rri.attributes = nil
rri.save
new_attributes = nil
!new_attributes.respond_to?(:stringify_keys)
{}.blank?
new_attributes = {|
new_attributes = {}
!new_attributes.respond_to?(:stringify_keys)
exit
brands = Brand.all
brands.class
exit
Product.pluck(:brand).uniq.count
exit
"A'CLASS ; A'CLASS CO"
str = _
ActiveRecord::Base.sanitize(str)
b = Brand.first
b.clean_name
b.reload
Brand.where("clean_name like '%'%'")
Brand.where("clean_name like '%\'%'")
Brand.where("clean_name like '%\\'%'")
Brand.where("clean_name like '%\\"%'")
b = Brand.first
b = Brand.find 134
Brand.where(name: '')
Brand.pluck(:name).sort
Product.column
Product.columns
Product.columns.all
Product.columns.map {|col| "#{col.name}: #{col.type}"}
def column_def(model)
  str = model.columns.map {|col| "#{col.name}: #{col.type}"}.join("\n")
  str
end
column_def(Product)
p _
puts _
puts column_def(UnknownProduct)
puts column_def(Technology)
puts column_def(Brand)
def column_def(model)
  str = model.columns.map {|col| "#{col.name}: #{col.type}"}.join("\n")
  str
end
column_def(Listing)
puts _
ListingUr
ListingUrl
puts column_def(ListingUrl)
ListingUrl.pluck(:url)
s = Scrape.last
exit
s = Scrape.last
s.listings.count
s.listings.uniq.pluck(:listing_url_id).count
s.listings.pluck(:listing_url_id).uniq.count
puts column_def(Cloud)
exit
Listing.count
Product.count
Figaro.env
Figaro.env.mailer_sender
reload!
Figaro.env.mailer_sender
exit
Figaro.env.mailer_sender
gst
exit
Listing
Listing.pluck(:listing_url_id).uniq.count
ListingUrl.count
technologies = Technology.all
technologies.pluck(:strong_keywords)
skws = technologies.pluck(:strong_keywords)
wkws = technologies.pluck(:weak_keywords)
wkws.reject!(&:blank?)
skws.reject!(&:blank?)
skws.uniq!
wkws.uniq!
keywords = skws | wkws
keywords.map! {|kw| kw.split(',').map(&:squish!)}
keywords
keywords.flatten!
keywords.uniq!
keywords
exit
query = "SELECT id as technology_id, strong_keywords, weak_keywords FROM technologies"
technologies = ActiveRecord::Base.connection.select_all(query).to_ary; nil
technologies
technologies.reject! {|t| t['strong_keywords'].blank? && t['strong_keywords'].blank?}
technologies
t = technologies.last
kw = t.delete('strong_keywords')
kw
t
t['strong_keywords'] = t['strong_keywords'].split(',')
t['strong_keywords'] = kw
t['strong_keywords'] = t['strong_keywords'].split(',')
t = technologies[2]
t['strong_keywords'] = t['strong_keywords'].split(',')
t
t.map!(&:squish!)
t['strong_keywords'].map!(&:squish!)
strong_keywords = []
technologies.each do |t|
  t['strong_keywords'] = t['strong_keywords'].split(',')
  t['strong_keywords'].map!(&:squish!)
  t['strong_keywords'].uniq!
  t['strong_keywords'].count.times do |keyword|
    strong_keywords << { technology_id: t['technology_id'], keyword: t['strong_keywords'].pop }
  end
end
t
query = "SELECT id as technology_id, strong_keywords, weak_keywords FROM technologies"
technologies = ActiveRecord::Base.connection.select_all(query).to_ary; nil
technologies.reject! { |t| t['strong_keywords'].blank? && t['strong_keywords'].blank? }
strong_keywords = []
technologies.each do |t|
  t['strong_keywords'] = t['strong_keywords'].split(',')
  t['strong_keywords'].map!(&:squish!)
  t['strong_keywords'].uniq!
  t['strong_keywords'].count.times do |keyword|
    strong_keywords << { technology_id: t['technology_id'], keyword: t['strong_keywords'].pop }
  end
end
strong_keywords
query = "SELECT id as technology_id, strong_keywords, weak_keywords FROM technologies"
technologies = ActiveRecord::Base.connection.select_all(query).to_ary; nil
technologies.reject! { |t| t['strong_keywords'].blank? && t['strong_keywords'].blank? }
strong_keywords = []
weak_keywords = []
def extract_keywords(strong_keywords, key, t)
  t[key] = t[key].split(',')
  t[key].map!(&:squish!)
  t[key].uniq!
  t[key].count.times do |keyword|
    strong_keywords << { technology_id: t['technology_id'], keyword: t[key].pop }
  end
end
technologies.each do |t|
  extract_keywords(strong_keywords, 'strong_keywords', t)
  extract_keywords(weak, 'weak_keywords', t)
end
query = "SELECT id as technology_id, strong_keywords, weak_keywords FROM technologies"
technologies = ActiveRecord::Base.connection.select_all(query).to_ary; nil
technologies.reject! { |t| t['strong_keywords'].blank? && t['strong_keywords'].blank? }
strong_keywords = []
weak_keywords = []
def extract_keywords(strong_keywords, key, t)
  t[key] = t[key].split(',')
  t[key].map!(&:squish!)
  t[key].uniq!
  t[key].count.times do |keyword|
    strong_keywords << { technology_id: t['technology_id'], keyword: t[key].pop }
  end
end
technologies.each do |t|
  extract_keywords(strong_keywords, 'strong_keywords', t)
  extract_keywords(weak_keyword, 'weak_keywords', t)
end
query = "SELECT id as technology_id, strong_keywords, weak_keywords FROM technologies"
technologies = ActiveRecord::Base.connection.select_all(query).to_ary; nil
technologies.reject! { |t| t['strong_keywords'].blank? && t['strong_keywords'].blank? }
strong_keywords = []
weak_keywords = []
def extract_keywords(strong_keywords, key, t)
  t[key] = t[key].split(',')
  t[key].map!(&:squish!)
  t[key].uniq!
  t[key].count.times do |keyword|
    strong_keywords << { technology_id: t['technology_id'], keyword: t[key].pop }
  end
end
technologies.each do |t|
  extract_keywords(strong_keywords, 'strong_keywords', t)
  extract_keywords(weak_keywords, 'weak_keywords', t)
end
query = "SELECT id as technology_id, strong_keywords, weak_keywords FROM technologies"
technologies = ActiveRecord::Base.connection.select_all(query).to_ary; nil
technologies.reject! { |t| t['strong_keywords'].blank? && t['strong_keywords'].blank? }
strong_keywords = []
weak_keywords = []
def extract_keywords(strong_keywords, key, t)
  return if t[key].blank?
  t[key] = t[key].split(',')
  t[key].map!(&:squish!)
  t[key].uniq!
  t[key].count.times do |keyword|
    strong_keywords << { technology_id: t['technology_id'], keyword: t[key].pop }
  end
end
technologies.each do |t|
  extract_keywords(strong_keywords, 'strong_keywords', t)
  extract_keywords(weak_keywords, 'weak_keywords', t)
end
strong_keywords
weak_keywords
ActiveRecord::Base.sanitize(weak_keywords.map{|kw| kw[:keyword] } )
ActiveRecord::Base.sanitize(weak_keywords.map{|kw| kw[:keyword] }.join(', ') )
weak_keywords.map{|kw| kw[:keyword] }.join(', ')
weak_keywords.map{|kw| "(#{kw[:keyword] })" }
weak_keywords.map{|kw| "(#{kw[:keyword] })" }.join(', ')
keywords = strong_keywords
values = keywords.map{|kw| "(#{kw[:keyword] })" }.join(', ')
query = "SELECT id as technology_id, strong_keywords, weak_keywords FROM technologies"
technologies = ActiveRecord::Base.connection.select_all(query).to_ary; nil
technologies.reject! { |t| t['strong_keywords'].blank? && t['weak_keywords'].blank? }
technologies
query = "SELECT id as technology_id, strong_keywords, weak_keywords FROM technologies"
technologies = ActiveRecord::Base.connection.select_all(query).to_ary; nil
technologies.reject! { |t| t['strong_keywords'].blank? && t['weak_keywords'].blank? }
keywords = Set.new
tech = technologies.first
keys = ['strong_keywords', 'weak_keywords']
keys.first
tech[key].blank?
key = keys.first
tech[key].blank?
tech[key] = tech[key].split(',')
tech[key].map!(&:squish!)
tech[key].uniq!
keywords.class
keywords.merge(tech[key])
edit -t
query = "SELECT id as technology_id, strong_keywords, weak_keywords FROM technologies"
technologies = ActiveRecord::Base.connection.select_all(query).to_ary; nil
technologies.reject! { |t| t['strong_keywords'].blank? && t['weak_keywords'].blank? }
keywords = Set.new
technologies.length.times do
  keywords.merge(extract_keywords(technologies.pop, 'strong_keywords', 'weak_keywords'))
end
exit
adsf
technologies
t = technologies.first
keyword = extract_keywords(t, 'strong_keywords', 'weak_keywords')
keywords
keywords.merge(keyword)
technologies.length.times do
  keywords.merge(extract_keywords(technologies.pop, 'strong_keywords', 'weak_keywords'))
end
def extract_keywords(tech, *keys)
  keywords = Set.new
  keys.each do |key|
    next if tech[key].blank?
    tech[key] = tech[key].split(',')
    tech[key].map!(&:squish!)
    tech[key].uniq!
    tech[key].count.times do
      keywords << tech[key].pop
    end
  end
  keywords
end
technologies.length.times do
  keywords.merge(extract_keywords(technologies.pop, 'strong_keywords', 'weak_keywords'))
end
keyword
keywords
query = "SELECT id as technology_id, strong_keywords, weak_keywords FROM technologies"
technologies = ActiveRecord::Base.connection.select_all(query).to_ary; nil
technologies.reject! { |t| t['strong_keywords'].blank? && t['weak_keywords'].blank? }
keywords = Set.new
technologies.length.times do
  keywords.merge(extract_keywords(technologies.pop, 'strong_keywords', 'weak_keywords'))
end
keywords
Technology.all
keywords
keywords.join(', ')
keywords.to_a
keywords.to_a.join(', ')
values = keywords.to_a.map{|kw| "(#{kw})" }.join(', ')
exit
Keyword.count
m = ActiveRecord::Migration.new
cd m
create_table :keywords, force: true do |t|
  t.string :name, null: false
  t.boolean :active, default: true
  t.datetime :deleted_at
  t.timestamps
  t.index [:name, :active], unique: true
end
Keyword.count
query = "SELECT id as technology_id, strong_keywords, weak_keywords FROM technologies"
technologies = ActiveRecord::Base.connection.select_all(query).to_ary; nil
technologies.reject! { |t| t['strong_keywords'].blank? && t['weak_keywords'].blank? }
keywords = Set.new
technologies.length.times do
  keywords.merge(extract_keywords(technologies.pop, 'strong_keywords', 'weak_keywords'))
end
def extract_keywords(tech, *keys)
  keywords = Set.new
  keys.each do |key|
    next if tech[key].blank?
    tech[key] = tech[key].split(',')
    tech[key].map!(&:squish!)
    tech[key].uniq!
    tech[key].count.times do
      keywords << tech[key].pop
    end
  end
  keywords
end
def insert_keywords(keywords)
  values = keywords.to_a.map{|kw| "(#{kw})" }.join(', ')
  execute <<-SQL.squish!
      INSERT IGNORE INTO keywords(name) VALUES #{values}
    SQL
end
edit -t
keywords
values = keywords.to_a.map{|kw| "(#{kw})" }.join(', ')
<<-SQL.squish!
      INSERT IGNORE INTO keywords(name) VALUES #{values}
    SQL
execute <<-SQL.squish!
      INSERT IGNORE INTO keywords(name) VALUES #{values}
    SQL
values = keywords.to_a.map{|kw| "(#{sanitize(kw)})" }.join(', ')
values = keywords.to_a.map{|kw| "(#{ActiveRecord::Base.sanitize(kw)})" }.join(', ')
execute <<-SQL.squish!
      INSERT IGNORE INTO keywords(name) VALUES #{values}
    SQL
Keyword.count
Keyword.first
Keyword.delete_all
Keyword.count
Keyword.unscoped.count
Keyword.unscoped
edit -t
query = "SELECT id as technology_id, strong_keywords, weak_keywords FROM technologies"
technologies = ActiveRecord::Base.connection.select_all(query).to_ary; nil
technologies.reject! { |t| t['strong_keywords'].blank? && t['weak_keywords'].blank? }
keywords = Set.new
technologies.length.times do
  keywords.merge(extract_keywords(technologies.pop, 'strong_keywords', 'weak_keywords'))
end
insert_keywords(keywords)
Keywords.count
Keyword.count
Keyword.first
query = "SELECT id as technology_id, strong_keywords, weak_keywords FROM technologies"
technologies = ActiveRecord::Base.connection.select_all(query).to_ary; nil
technologies.reject! { |t| t['strong_keywords'].blank? && t['weak_keywords'].blank? }
technologies
exit
Keyword.unscoped.delete_all
exit
query = "SELECT id as technology_id, strong_keywords, weak_keywords FROM technologies"
technologies = ActiveRecord::Base.connection.select_all(query).to_ary; nil
technologies.reject! { |t| t['strong_keywords'].blank? && t['strong_keywords'].blank? }
strong_keywords = []
weak_keywords = []
edit -t
technologies.each do |t|
  extract_keywords(strong_keywords, 'strong_keywords', t)
  extract_keywords(weak_keywords, 'weak_keywords', t)
end
weak_keywords
strong_keywords
query = "SELECT id as keyword_id, name FROM keywords"
keywords = ActiveRecord::Base.connection.select_all(query).to_ary; nil
technologies.reject! { |t| t['strong_keywords'].blank? && t['strong_keywords'].blank? }
query = "SELECT id as technology_id, strong_keywords, weak_keywords FROM technologies"
technologies = ActiveRecord::Base.connection.select_all(query).to_ary; nil
technologies.reject! { |t| t['strong_keywords'].blank? && t['strong_keywords'].blank? }
strong_keywords = []
weak_keywords = []
def extract_keywords(keywords, key, t)
  return if t[key].blank?
  t[key] = t[key].split(',')
  t[key].map!(&:squish!)
  t[key].uniq!
  t[key].count.times do
    keywords << { technology_id: t['technology_id'], keyword: t[key].shift }
  end
end
technologies.each do |t|
  extract_keywords(strong_keywords, 'strong_keywords', t)
  extract_keywords(weak_keywords, 'weak_keywords', t)
end
strong_keywords
weak_keywords
query = "SELECT id as keyword_id, name FROM keywords"
keywords = ActiveRecord::Base.connection.select_all(query).to_ary; nil
weak_keywords
skw = strong_keywords.first
keyword = keywords.find {|kw| kw['name'] == skw['keyword']}
keyword
keywords
skw
keyword = keywords.find {|kw| kw['name'] == skw[:keyword]}
strong_keywords.map! do |skw|
  keyword = keywords.find { |kw| kw['name'] == skw[:keyword] }
  { technology_id: skw[:technology_id], keyword_id: kw['keyword_id'] }
end
strong_keywords.map! do |skw|
  keyword = keywords.find { |kw| kw['name'] == skw[:keyword] }
  keyword.present? ? { technology_id: skw[:technology_id], keyword_id: keyword['keyword_id'] } : nil
end
weak_keywords
edit -t
query = "SELECT id as technology_id, strong_keywords, weak_keywords FROM technologies"
technologies = ActiveRecord::Base.connection.select_all(query).to_ary; nil
technologies.reject! { |t| t['strong_keywords'].blank? && t['strong_keywords'].blank? }
technology_keywords = []
technologies.each do |tech|
  extract_keywords(technology_keywords, tech)
end
technology_keywords
technolog
technologies
tech = _.first
key, strength = [{ 'strong_keywords': 0, 'weak_keywords': 1 }].first
def extract_keywords(tech_keywords, tech)
  [{ 'strong_keywords': 0}, {'weak_keywords': 1 }].each do |key, strength|
    next if tech[key].blank?
    tech[key] = tech[key].split(',')
    tech[key].map!(&:squish!)
    tech[key].uniq!
    tech[key].count.times do
      tech_keywords << { 'technology_id': tech['technology_id'], 'keyword': tech[key].shift, 'keyword_strength': strength }
    end
  end
end
technologies.each do |tech|
  extract_keywords(technology_keywords, tech)
end
technology_keywords
key, strength = [{ 'strong_keywords': 0}, {'weak_keywords': 1 }].first
key
strength
key, strength = [['strong_keywords', 0], ['weak_keywords', 1 ]].first
key
strength
edit -t
technologies.each do |tech|
  extract_keywords(technology_keywords, tech)
end
technology_keywords
technologies
query = "SELECT id as technology_id, strong_keywords, weak_keywords FROM technologies"
technologies = ActiveRecord::Base.connection.select_all(query).to_ary; nil
technologies.reject! { |t| t['strong_keywords'].blank? && t['strong_keywords'].blank? }
technology_keywords = []
technologies.length.times do
  extract_keywords(technology_keywords, technologies.shift)
end
query = "SELECT id as keyword_id, name FROM keywords"
keywords = ActiveRecord::Base.connection.select_all(query).to_ary; nil
map_technology_keywords(keywords, technology_keywords)
edit -t
query = "SELECT id as technology_id, strong_keywords, weak_keywords FROM technologies"
technologies = ActiveRecord::Base.connection.select_all(query).to_ary; nil
technologies.reject! { |t| t['strong_keywords'].blank? && t['strong_keywords'].blank? }
technology_keywords = []
technologies.length.times do
  extract_keywords(technology_keywords, technologies.shift)
end
query = "SELECT id as keyword_id, name FROM keywords"
keywords = ActiveRecord::Base.connection.select_all(query).to_ary; nil
map_technology_keywords(keywords, technology_keywords)
technology_keywords
edit -t
query = 'SELECT id as technology_id, strong_keywords, weak_keywords FROM technologies'
technologies = query_records(query)
technologies.reject! { |t| t[:strong_keywords].blank? && t[:weak_keywords].blank? }
technologies
technology_keywords = []
technologies.length.times do
  extract_keywords(technology_keywords, technologies.shift)
end
technologies
technology_keywords
query = 'SELECT id as keyword_id, name FROM keywords'
keywords = query_records(query)
map_technology_keywords(keywords, technology_keywords)
technology_keywords
edit -t
map_technology_keywords(keywords, technology_keywords)
va  values = technology_keywords.map{|tkw| "(#{tkw[:technology_id]}, #{tkw[:keyword_id]}, #{tkw[:keyword_strength]})" }.join(', ')
values = technology_keywords.map{|tkw| "(#{tkw[:technology_id]}, #{tkw[:keyword_id]}, #{tkw[:keyword_strength]})" }.join(', ')
edit -t
query = 'SELECT id as technology_id, strong_keywords, weak_keywords FROM technologies'
technologies = query_records(query)
technologies.reject! { |t| t[:strong_keywords].blank? && t[:weak_keywords].blank? }
technology_keywords = []
technologies.length.times do
  extract_keywords(technology_keywords, technologies.shift)
end
query = 'SELECT id as keyword_id, name FROM keywords'
keywords = query_records(query)
map_technology_keywords(keywords, technology_keywords)
values = technology_keywords.map{|tkw| "(#{tkw[:technology_id]}, #{tkw[:keyword_id]}, #{tkw[:keyword_strength]})" }.join(', ')
execute <<-SQL.squish!
    INSERT IGNORE INTO technology_keywords(technology_id, keyword_id, keyword_strength) values #{values}
  SQL
values = technology_keywords.map! { |tkw| "(#{tkw[:technology_id]}, #{tkw[:keyword_id]}, #{tkw[:keyword_strength]})" }
technology_keywords.map! { |tkw| tkw[:technology_id] && tkw[:keyword_id] && tkw[:keyword_strength] }
def populate_join_table(technology_keywords)
  technology_keywords.select! { |tkw| tkw[:technology_id] && tkw[:keyword_id] && tkw[:keyword_strength] }
  values = technology_keywords.map { |tkw| "(#{tkw[:technology_id]}, #{tkw[:keyword_id]}, #{tkw[:keyword_strength]})" }.join(', ')
  execute <<-SQL.squish!
      INSERT IGNORE INTO technology_keywords(technology_id, keyword_id, keyword_strength) values #{values}
    SQL
end
cd ActiveRecordMigration.new
m =  ActiveRecord::Migration.new
cd m
edit -t
create_table :technology_keywords, force: true do |t|
  t.references :technology, index: true
  t.references :keyword, index: true
  t.integer :keyword_strength
  t.boolean :active, default: true
  t.datetime :deleted_at
  t.timestamps
  t.index :active
  t.index :keyword_strength
end
query = 'SELECT id as technology_id, strong_keywords, weak_keywords FROM technologies'
technologies = query_records(query)
technologies.reject! { |t| t[:strong_keywords].blank? && t[:weak_keywords].blank? }
technology_keywords = []
technologies.length.times do
  extract_keywords(technology_keywords, technologies.shift)
end
technology_keywords
query = 'SELECT id as keyword_id, name FROM keywords'
keywords = query_records(query)
map_technology_keywords(keywords, technology_keywords)
keywords
values = technology_keywords.select { |tkw| tkw[:technology_id] && tkw[:keyword_id] && tkw[:keyword_strength] }
values = values.map { |tkw| "(#{tkw[:technology_id]}, #{tkw[:keyword_id]}, #{tkw[:keyword_strength]})" }.join(', ')
<<-SQL.squish!
      INSERT IGNORE INTO technology_keywords(technology_id, keyword_id, keyword_strength) values #{values}
    SQL
execute <<-SQL.squish!
      INSERT IGNORE INTO technology_keywords(technology_id, keyword_id, keyword_strength) VALUES #{values}
    SQL
exit
t = Technology.first
t.keywords
exit
Keyword.strong
Keyword.strong.first
reload!
Keyword.strong.first
Keyword.strong.count
Keyword.weak
reload!
Keyword.weak
Keyword.strong
Brand.first
exit
Product.joins(:brand).to_sql
Product.joins(:brand_records).to_sql
Product.joins(:brand_record).to_sql
Product.joins(:brand_record).select('products.created_at')
Product.joins(:brand_record).select('products.created_at').uniq('products.brand_id')
Product.joins(:brand_record).select('products.created_at').uniq('products.brand_id').to_sql
Product.joins(:brand_record).select('products.created_at').uniq(:brand_id).to_sql
Product.joins(:brand_record).select(:id, :brand_id, :created_at).uniq(:brand_id).to_sql
Product.joins(:brand_record).select(:id, :brand_id, :created_at).uniq(:brand_id)
Product.joins(:brand_record).select(:id, :brand_id, :created_at).uniq(:brand_id).first
Product.joins(:brand_record).select('products.id', :brand_id, 'products.created_at').uniq(:brand_id).first
Product.joins(:brand_record).select('products.id', :brand_id, 'products.created_at').uniq(:brand_id)
Product.joins(:brand_record).select('products.id', :brand_id, 'products.created_at').group(:brand_id)
Product.joins(:brand_record).select('products.id', :brand_id, 'products.created_at').group(:brand_id).to_sql
Product.joins(:brand_record).select(:brand_id, 'products.created_at').group(:brand_id).to_sql
eaxit
exit
m = ActiveRecord::Migration.new
cd m
execute <<-SQL.squish!
        UPDATE brands, (SELECT brand_id, products.created_at FROM `products` INNER JOIN `brands` ON `brands`.`id` = `products`.`brand_id` AND `brands`.`active` = 1 WHERE `products`.`active` = 1 GROUP BY brand_id) as products
          SET  brands.created_at = products.created_at, brands.updated_at = products.created_at
          WHERE brands.id = products.brand_id
        SQL
Brand.first
b = _
b.products.first
b.products.first.created_at == b.created_at
ActiveRecord::Base.connection.execute(<<-SQL.squish!)
        UPDATE brands, (SELECT brand_id, products.created_at FROM `products` INNER JOIN `brands` ON `brands`.`id` = `products`.`brand_id` AND `brands`.`active` = 1 WHERE `products`.`active` = 1 GROUP BY brand_id) as products
          SET  brands.created_at = products.created_at, brands.updated_at = products.created_at
          WHERE brands.id = products.brand_id
        SQL
exit
query = "SELECT id as technology_id, strong_keywords, weak_keywords, created_at FROM technologies"
technologies = ActiveRecord::Base.connection.select_all(query).to_ary; nil
technologies.reject! { |t| t['strong_keywords'].blank? && t['weak_keywords'].blank? }
key = 'strong_keywords'
tech[key] = tech[key].split(',')
tech = technologies.first
tech[key] = tech[key].split(',')
keywords = Set.new
keywords << [tech[key].shift, tech['created_at']]
keywords.pop
keywords.delete_if? {|k| k[0].nil? }
keywords.delete_if {|k| k[0].nil? }
keywords << ['HDMI', tech['created_at']]
values = keywords.to_a.map{|kw| "(#{ActiveRecord::Base.sanitize(kw)})" }.join(', ')
values = keywords.to_a.map{|kw, created_at, updated_at| "(#{ActiveRecord::Base.sanitize(kw)}, #{created_at}, #{updated_at})" }.join(', ')
keywords << ['HDMI', tech['created_at'], tech['created_at']
keywords << ['HDMI', tech['created_at'], tech['created_at']]
keywords = []
keywords = Set.new
keywords << ['HDMI', tech['created_at'], tech['created_at']]
values = keywords.to_a.map{|kw, created_at, updated_at| "(#{ActiveRecord::Base.sanitize(kw)}, #{created_at}, #{updated_at})" }.join(', ')
values = keywords.to_a.map{|kw, created_at| "(#{ActiveRecord::Base.sanitize(kw)}, #{created_at}, #{created_at})" }.join(', ')
m = ActiveRecord::Migration.new
cd m
edit
create_table :keywords, force: true do |t|
  t.string :name, null: false
  t.boolean :active, default: true
  t.datetime :deleted_at
  t.timestamps
  t.index [:name, :active], unique: true
end
Keyword.first
query = "SELECT id as technology_id, strong_keywords, weak_keywords, created_at FROM technologies"
technologies = ActiveRecord::Base.connection.select_all(query).to_ary; nil
technologies.reject! { |t| t['strong_keywords'].blank? && t['weak_keywords'].blank? }
return if technologies.empty?
keywords = Set.new
technologies.length.times do
  keywords.merge(extract_keywords(technologies.shift, 'strong_keywords', 'weak_keywords'))
end
keywords
insert_keywords(keywords) unless keywords.blank?
values = keywords.to_a.map{|kw, created_at| "(#{ActiveRecord::Base.sanitize(kw)}, #{created_at}, #{created_at})" }.join(', ')
execute <<-SQL.squish!
      INSERT IGNORE INTO keywords(name, created_at, updated_at) VALUES #{values}
    SQL
values = keywords.to_a.map do |kw, created_at|
  "(#{ActiveRecord::Base.sanitize(kw)}, #{ActiveRecord::Base.sanitize(created_at)}, #{ActiveRecord::Base.sanitize(created_at)})"
end.join(', ')
execute <<-SQL.squish!
      INSERT IGNORE INTO keywords(name, created_at, updated_at) VALUES #{values}
    SQL
Keyword.first
edit -t
create_table :keywords, force: true do |t|
  t.string :name, null: false
  t.boolean :active, default: true
  t.datetime :deleted_at
  t.timestamps
  t.index [:name, :active], unique: true
end
Keyword.count
data
Keyword.count
Keyword.first
exit
b = Brand.first
b = Brand.find 2
Product.join(:mfg_models).first
p = Product.joins(:mfg_models).first
exit
p = Product.joins(:mfg_models).first
p.mfg_models
RdsProduct
brand, model = p.brand, p.model
mfg_model = p.mfg_models.last.name
rds_p1 = RdsProduct.create(brand: brand, model: model)
reload!
rds_p1 = RdsProduct.create(brand: brand, model: model)
exit
p = Product.joins(:mfg_models).first
brand, model = p.brand, p.model
mfg_model = p.mfg_models.last.name
rds_p1 = RdsProduct.create(brand: brand, model: model)
reload!
rds_p1 = RdsProduct.create(brand: brand, model: model)
p.reload
rds_p1
RdsProduct.count
rds_p1.brand
p
p.rds_products.size
b = p.brand_record
b.rds_products_count
rds_p2 = RdsProduct.create(brand: brand, model: mfg_model)
products = Product.includes(:mfg_models).
where('products.brand = :brand AND (products.model :model OR mfg_models.name = :model)',
  brand: self.brand, model: self.model
).first
product = Product.includes(:mfg_models).
where('products.brand = :brand AND (products.model :model OR mfg_models.name = :model)',
  # brand: self.brand, model: self.model
  brand: brand, model: model
).first
self.product = product
product = Product.includes(:mfg_models).
where('products.brand = :brand AND (products.model :model OR mfg_models.name = :model)',
  # brand: self.brand, model: self.model
  brand: brand, model: model
).first
product = Product.includes(:mfg_models).
where('products.brand = :brand AND (products.model :model OR mfg_models.name = :model)',
  # brand: self.brand, model: self.model
  brand: brand, model: model
).first
product = Product.includes(:mfg_models).
where('products.brand = :brand AND (products.model :model OR mfg_models.name = :model)',
  # brand: self.brand, model: self.model
  brand: brand, model: model
).to_sql
Product.joins(:mfg_models).to_sql
product = Product.eager_load(:mfg_models).
where('products.brand = :brand AND (products.model :model OR mfg_models.name = :model)',
  # brand: self.brand, model: self.model
  brand: brand, model: model
).to_sql
product = Product.eager_load(:mfg_models).
where('products.brand = :brand AND (products.model :model OR mfg_models.name = :model)',
  # brand: self.brand, model: self.model
  brand: brand, model: model
).first
product = Product.eager_load(:mfg_models).
where('products.brand = :brand AND (products.model :model OR mfg_models.name = :model)',
  # brand: self.brand, model: self.model
  brand: brand, model: model
).references(:mfg_models).first
product = Product.eager_load(:mfg_models).
where('products.brand = :brand AND (products.model = :model OR mfg_models.name = :model)',
  # brand: self.brand, model: self.model
  brand: brand, model: model
).references(:mfg_models).first
p.model
product = Product.eager_load(:mfg_models).
where('products.brand = :brand AND (products.model = :model OR mfg_models.name = :model)',
  # brand: self.brand, model: self.model
  brand: brand, model: mfg_model
).references(:mfg_models).first
mfg_model
p.mfg_model
p.model
edit -t
exit
brand, model = p.brand, p.model
p = Product.joins(:mfg_models).first
brand, model = p.brand, p.model
mfg_model = p.mfg_models.last.name
rds_p1 = RdsProduct.create(brand: brand, model: model)
rds_p1.product
rds_p2 = RdsProduct.create(brand: brand, model: mfg_model)
rds_p2.product
MfgModel.count
exit
m = ActiveRecord::Migration.new
cd m
@tables = {
  products: [],
  entities: [],
  royalty_report_items: []
}
attr_accessor :tables
tables = {
  products: [],
  entities: [],
  royalty_report_items: []
}
tables
tables.first
tables.each do |table, columns|
  for i in 1..10
    unless column_exists? table, "custom#{i}_text"
      columns << "t.string, custom#{i}_text"
    end
    unless column_exists? table, "custom#{i}_categorical"
      columns << "t.string, custom#{i}_categorical"
    end
    unless column_exists? table, "custom#{i}_boolean"
      columns << "t.boolean, custom#{i}_boolean"
    end
    unless column_exists? table, "custom#{i}_date"
      columns << "t.datetime, custom#{i}_datetime"
    end
    unless column_exists? table, "custom#{i}_numeric"
      columns << "t.decimal, custom#{i}_numeric, precision: 15, scale: 2"
    end
  end
end
tables.each do |table, columns|
  for i in 1..10
    unless column_exists? table, "custom#{i}_text"
      columns << "t.string, custom#{i}_text"
    end
    unless column_exists? table, "custom#{i}_categorical"
      columns << "t.string, custom#{i}_categorical"
    end
    unless column_exists? table, "custom#{i}_boolean"
      columns << "t.boolean, custom#{i}_boolean"
    end
    unless column_exists? table, "custom#{i}_date"
      columns << "t.datetime, custom#{i}_datetime"
    end
    unless column_exists? table, "custom#{i}_numeric"
      columns << "t.decimal, custom#{i}_numeric, precision: 15, scale: 2"
    end
  end
end
tables
tables.each do |table, columns|
  for i in 1..10
    # unless column_exists? table, "custom#{i}_text"
    columns << "t.string, custom#{i}_text"
    # end
    # unless column_exists? table, "custom#{i}_categorical"
    columns << "t.string, custom#{i}_categorical"
    # end
    # unless column_exists? table, "custom#{i}_boolean"
    columns << "t.boolean, custom#{i}_boolean"
    # end
    # unless column_exists? table, "custom#{i}_date"
    columns << "t.datetime, custom#{i}_datetime"
    # end
    # unless column_exists? table, "custom#{i}_numeric"
    columns << "t.decimal, custom#{i}_numeric, precision: 15, scale: 2"
    # end
  end
end
tables
table, columns = tables.first
table
columns
col = columns.first
columns.join("\n")
method_definition = "change_table :#{table}, bulk: true do |t| \n"
method_definition << columns.join("\n")
method_definition << "\nend"
puts method_definition
table_definition = nil
tables.each do |table, columns|
  next if columns.empty?
  table_definition = "change_table :#{table}, bulk: true do |t| \n"
  table_definition << columns.join("\n")
  table_definition << "\nend"
  self.class.class_eval(<<-RUBY)
        def update_#{table}
          #{table_definition}
        end
      RUBY
end
table, columns = tables.first
table_definition = "change_table :#{table}, bulk: true do |t| \n"
table_definition << columns.join("\n")
table_definition << "\nend"
self.class.class_eval(<<-RUBY)
        def update_#{table}
          #{table_definition}
        end
      RUBY
edit -t
table_definition = "change_table :#{table}, bulk: true do |t| \n"
table_definition << columns.join(";\n")
table_definition << "\nend"
class_eval(<<-RUBY)
        def update_#{table}
          #{table_definition}
        end
      RUBY
m.class
class
self.class
exit
m = CreateCustomFieldConfigs.new
require '/Users/jonathan/rvx-spa/db/migrate/20161209195616_create_custom_field_configs.rb'
m = CreateCustomFieldConfigs.new
tables = {
  products: [],
  entities: [],
  royalty_report_items: []
}
tables.each do |table, columns|
  for i in 1..10
    # unless column_exists? table, "custom#{i}_text"
    columns << "t.string, custom#{i}_text"
    # end
    # unless column_exists? table, "custom#{i}_categorical"
    columns << "t.string, custom#{i}_categorical"
    # end
    # unless column_exists? table, "custom#{i}_boolean"
    columns << "t.boolean, custom#{i}_boolean"
    # end
    # unless column_exists? table, "custom#{i}_date"
    columns << "t.datetime, custom#{i}_datetime"
    # end
    # unless column_exists? table, "custom#{i}_numeric"
    columns << "t.decimal, custom#{i}_numeric, precision: 15, scale: 2"
    # end
  end
end
jj    table_definition = nil
table_definition = nil
table, columns = tables.first
table_definition = "change_table :#{table}, bulk: true do |t| \n"
table_definition
table_definition << columns.join(";\n")
table_definition << "\n"
table_definition << "end"
puts table_definition
table_definition = "change_table :#{table}, bulk: true do |t| \n"
table_definition << columns.join("\n")
table_definition << "\n"
table_definition << "end"
puts _
tables = {
  products: [],
  entities: [],
  royalty_report_items: []
}
table_definition = nil
tables.each do |table, columns|
  for i in 1..10
    # unless column_exists? table, "custom#{i}_text"
    columns << "t.string custom#{i}_text"
    # end
    # unless column_exists? table, "custom#{i}_categorical"
    columns << "t.string custom#{i}_categorical"
    # end
    # unless column_exists? table, "custom#{i}_boolean"
    columns << "t.boolean custom#{i}_boolean"
    # end
    # unless column_exists? table, "custom#{i}_date"
    columns << "t.datetime custom#{i}_datetime"
    # end
    # unless column_exists? table, "custom#{i}_numeric"
    columns << "t.decimal custom#{i}_numeric, precision: 15, scale: 2"
    # end
  end
end
table, columns = tables.first
table_definition = nil
table_definition = "change_table :#{table}, bulk: true do |t| \n"
table_definition << columns.join("\n")
table_definition << "\n"
table_definition << "end"
CreateCustomFieldConfigs.class_eval(<<-RUBY)
        def update_#{table}
          #{table_definition}
        end
      RUBY
update_products
m = CreateCustomFieldConfigs.new
m.update_products
exit
require '/Users/jonathan/rvx-spa/db/migrate/20161209195616_create_custom_field_configs.rb'
m =  CreateCustomFieldConfigs.new
cd m
tables
exit
require '/Users/jonathan/rvx-spa/db/migrate/20161209195616_create_custom_field_configs.rb'
m =  CreateCustomFieldConfigs.new
cd m
table_exists? :custom_field_configs
if table_exists? :custom_field_configs
  drop_table :custom_field_configs
end
tables = {
  products: [],
  entities: [],
  royalty_report_items: []
}
tables.each do |table, _columns|
  for i in 1..10
    if column_exists? table, "custom#{i}_text"
      remove_column table, "custom#{i}_text", :string
    end
    if column_exists? table, "custom#{i}_categorical"
      remove_column table, "custom#{i}_categorical", :string
    end
    if column_exists? table, "custom#{i}_boolean"
      remove_column table, "custom#{i}_boolean", :boolean
    end
    if column_exists? table, "custom#{i}_date"
      remove_column table, "custom#{i}_date", :datetime
    end
    if column_exists? table, "custom#{i}_numeric"
      remove_column table, "custom#{i}_numeric", :decimal, precision: 15, scale: 2
    end
  end
end
tables = {
  products: [],
  entities: [],
  royalty_report_items: []
}
edit -t
edit
check_columns(tables)
table, columns = tables.first
table_definition = nil
table_definition = "change_table :#{table}, bulk: true do |t| \n"
table_definition << columns.join("\n")
table_definition << "\n"
table_definition << "end"
self.class_eval(<<-RUBY)
        def update_#{table}
      #{table_definition}
        end
      RUBY
self.send("update_#{table}")
show-source update_products
source update_products
definition
table_definition
table_definition = "self.change_table :#{table}, bulk: true do |t| \n"
table_definition << columns.join("\n")
table_definition << "\n"
table_definition << "end"
self.class_eval(<<-RUBY)
        def update_#{table}
      #{table_definition}
        end
      RUBY
self.send("update_#{table}")
tables = {
  products: [],
  entities: [],
  royalty_report_items: []
}
tables.each do |table, columns|
  for i in 1..10
    unless column_exists? table, "custom#{i}_text"
      columns << "t.string :custom#{i}_text"
    end
    unless column_exists? table, "custom#{i}_categorical"
      columns << "t.string :custom#{i}_categorical"
    end
    unless column_exists? table, "custom#{i}_boolean"
      columns << "t.boolean :custom#{i}_boolean"
    end
    unless column_exists? table, "custom#{i}_date"
      columns << "t.datetime :custom#{i}_datetime"
    end
    unless column_exists? table, "custom#{i}_numeric"
      columns << "t.decimal :custom#{i}_numeric, precision: 15, scale: 2"
    end
  end
end
table, columns = tables.first
table_definition = nil
table_definition = "change_table :#{table}, bulk: true do |t| \n"
table_definition << columns.join("\n")
table_definition << "\n"
table_definition << "end"
self.class_eval(<<-RUBY)
        def update_#{table}
      #{table_definition}
        end
      RUBY
update_products
exit
require '/Users/jonathan/rvx-spa/db/migrate/20161209195616_create_custom_field_configs.rb'
m = CustomMigration.new
cd m
self.is_a? ActiveRecord::Migration
tables = {
  products: [],
  entities: [],
  royalty_report_items: []
}
tables.each do |table, columns|
  for i in 1..10
    unless column_exists? table, "custom#{i}_text"
      # columns << "t.string :custom#{i}_text"
      columns << { type: :string, name: ":custom#{i}_text" }
    end
    unless column_exists? table, "custom#{i}_categorical"
      # columns << "t.string :custom#{i}_categorical"
      columns << { type: :string, name: ":custom#{i}_categorical" }
    end
    unless column_exists? table, "custom#{i}_boolean"
      # columns << "t.boolean :custom#{i}_boolean"
      columns << { type: :boolean, name: ":custom#{i}_boolean" }
    end
    unless column_exists? table, "custom#{i}_date"
      # columns << "t.datetime :custom#{i}_date"
      columns << { type: :datetime, name: ":custom#{i}_date" }
    end
    unless column_exists? table, "custom#{i}_numeric"
      # columns << "t.decimal :custom#{i}_numeric, precision: 15, scale: 2"
edit -t
new_columns = []
dup_columns = []
table, columns = tables.first
new_columns = []
dup_columns = []
columns.length.times do
  column = columns.shift
  if column_exists? table, column
    dup_columns << column
  else
    new_columns << column
  end
end
new_columns
column = columns.first
dup_columns
tables[table] = new_columns
table, columns = tables.first
column = columns.first
table
if column_exists? table, column
lcolumn[:name]
column[:name]
_.to_sym
tables = {
  products: [],
  entities: [],
  royalty_report_items: []
}
tables.each do |table, columns|
  for i in 1..10
    # unless column_exists? table, "custom#{i}_text"
    #   # columns << "t.string :custom#{i}_text"
    columns << { type: :string, name: "custom#{i}_text" }
    # end
    # unless column_exists? table, "custom#{i}_categorical"
    # columns << "t.string :custom#{i}_categorical"
    columns << { type: :string, name: "custom#{i}_categorical" }
    # end
    # unless column_exists? table, "custom#{i}_boolean"
    # columns << "t.boolean :custom#{i}_boolean"
    columns << { type: :boolean, name: "custom#{i}_boolean" }
    # end
    # unless column_exists? table, "custom#{i}_date"
    # columns << "t.datetime :custom#{i}_date"
    columns << { type: :datetime, name: "custom#{i}_date" }
    # end
    # unless column_exists? table, "custom#{i}_numeric"
    # columns << "t.decimal :custom#{i}_numeric, precision: 15, scale: 2"
    columns << { type: :decimal, name: "custom#{i}_numeric", options: { pre          columns <2           columns << { type:
edit -t
new_columns = []
dup_columns = []
table, columns = tables.first
table
edit -t
table, columns = tables.first
column = columns.first
column_exists? table, column[:name]
new_columns = []
dup_columns = []
columns.length.times do
  column = columns.shift
  if column_exists? table, column[:name]
    dup_columns << column
  else
    new_columns << column
  end
end
new_columns
dup_columns
dup_columns.map{|c| c[:name]}
dup_columns.map{|c| c[:name]}.join(', ')
"-- #{table} already has #{dup_columns.map{|c| c[:name]}.join(', ')}"
"-- `#{table}` table already has #{dup_columns.map{|c| "`c[:name]`"}.join(', ')}"
"-- `#{table}` table already has #{dup_columns.map{|c| "`#{c[:name]}`"}.join(', ')}"
"-- WARN: `#{table}` table already has #{dup_columns.map{|c| "`#{c[:name]}`"}.join(', ')}"
edit -t
columns
tables[table] = dup_columns
column
column[:options]
col_as_str = "t.#{dir == :up ? nil : 'remove_'}column :#{column[:name]}, :#{column[:type]}"
dir = :up
col_as_str = "t.#{dir == :up ? nil : 'remove_'}column :#{column[:name]}, :#{column[:type]}"
column[:options].each do |key, value|
  col_as_str << ", #{key}: #{value}"
column[:options].each do |key, value|
  col_as_str << ", #{key}: #{value}"
end
col_as_str
dir = :down
columns
table
tables
tables.first
table, columns = tables.first
dir = :up
table_definition = "change_table :#{table}, bulk: true do |t| \n"
edit -t
table_definition = "change_table :#{table}, bulk: true do |t| \n"
table_definition << columns.map { |col| column_string(col, dir) }.join("\n")
table_definition << "\n"
table_definition << "end"
edit -t
table_definition = "change_table :#{table}, bulk: true do |t| \n"
table_definition << columns.map { |col| column_string(col, dir) }.join("\n")
table_definition << "\n"
table_definition << "end"
self.class_eval(<<-RUBY)
        def update_#{table}
      #{table_definition}
        end
      RUBY
source update_products
exit
require '/Users/jonathan/rvx-spa/db/migrate/20161209195616_create_custom_field_configs.rb'
m = CreateCustomFieldConfigs.new
cd m
unless table_exists? :custom_field_configs
  create_table :custom_field_configs do |t|
    t.string :label, null: false
    t.string :mapped_field, null: false
    t.string :field_type, null: false
    t.text :options
    t.boolean :internal_only, null: false, default: false
    t.boolean :required, null: false, default: false
    t.string :model_name, null: false
  end
end
tables = custom_field_table_cols
edit -t
up
Apartment::Tenant.switch! 'synopsys'
m
up
Apartment::Tenant.switch! 'rvx-spa'
up
exit
Product.column_names
require '/Users/jonathan/rvx-spa/db/migrate/20161209195616_create_custom_field_configs.rb'
m = CreateCustomFieldConfigs.new
up
cd m
up
self.up
ActiveRecord::Migration.check_pending!
cd ..
m
m.migrate(:up)
m.migrate(:down)
require '/Users/jonathan/rvx-spa/db/migrate/20161209195616_create_custom_field_configs.rb'
load '/Users/jonathan/rvx-spa/db/migrate/20161209195616_create_custom_field_configs.rb'
m 
m = CreateCustomFieldConfigs.new
m.migrate(:down)
load '/Users/jonathan/rvx-spa/db/migrate/20161209195616_create_custom_field_configs.rb'
m = CreateCustomFieldConfigs.new
m.migrate(:down)
m.migrate(:up)
exit
Product.column_names
require '/Users/jonathan/rvx-spa/db/migrate/20161209195616_create_custom_field_configs.rb'
m = CreateCustomFieldConfigs.new
load '/Users/jonathan/rvx-spa/db/migrate/20161209195616_create_custom_field_configs.rb'
m = CreateCustomFieldConfigs.new
m.migrate(:down)
Apartment::Tenant.switch! 'rvx-spa'
m = CreateCustomFieldConfigs.new
m.migrate(:down)
Apartment::Tenant.switch! 'synopsys'
m = CreateCustomFieldConfigs.new
m.migrate(:down)
columns = [:name, :product_id]
"#{columns}"
edit -t
dir ||= :up
tables.each do |table, columns|
  new_columns = []
  old_columns = []
  columns.length.times do
    column = columns.shift
    if (column[:type].to_s == 'index'.freeze) && index_exists?(table, (column[:column] || column[:name]))
      old_columns << column
    elsif column_exists? table, column[:name]
      old_columns << column
    else
      new_columns << column
    end
  end
  tables[table] = dir == :up ? new_columns : old_columns
tables
edit -t
m = ActiveRecord::Migration.new
cd m
edit -t
dir ||= :up
edit -t
tables[:products]
table, columns = tables.first
columns
column = columns.last
table_definition = "change_table :#{table}, bulk: true do |t| \n"
edit -t
column_string(column, dir)
column
col_as_str = "t.#{dir == :up ? "#{column[:type]}" : 'remove_index'.freeze}"
col_as_str << " #{'column: '.freeze if (dir == :up)}#{column[:column]}" unless column[:column].blank?
col_as_str << ", name: #{column[:name]}" unless column[:name].blank?
column
column[:options] = { length: { custom_text: 1 } }
column
column[:options].each do |key, value|
  col_as_str << ", #{key}: #{value}"
end
col_as_str
exit
rds_p1 = RdsProduct.create(brand: brand, model: model)
p = Product.joins(:mfg_models).first
brand, model = p.brand, p.model
mfg_model = p.mfg_models.last.name
rds_p1 = RdsProduct.create(brand: brand, model: model)
reload!
p = Product.joins(:mfg_models).first
reload!
p = Product.joins(:mfg_models).first
brand, model = p.brand, p.model
mfg_model = p.mfg_models.last.name
rds_p1 = RdsProduct.create(brand: brand, model: model)
rds_p1.products
rds_p1.products.count
rds_p1.products
rds_p1.products.count
rds_p1.save
products
cd rds_p1
products = Product.eager_load(:mfg_models).
where('products.brand = :brand AND (products.model = :model OR mfg_models.name = :model)',
  brand: self.brand, model: self.model
).references(:mfg_models).select('products.id')
products
edit -t
).references(:mfg_models).pluck('products.id')
products = Product.eager_load(:mfg_models).
where('products.brand = :brand AND (products.model = :model OR mfg_models.name = :model)',
  brand: self.brand, model: self.model
).references(:mfg_models).pluck('products.id')
product_ids = Product.eager_load(:mfg_models).
where('products.brand = :brand AND (products.model = :model OR mfg_models.name = :model)',
  brand: self.brand, model: self.model
).references(:mfg_models).pluck('products.id')
).references(:mfg_models).uniq.pluck('products.id')
product_ids = Product.eager_load(:mfg_models).
where('products.brand = :brand AND (products.model = :model OR mfg_models.name = :model)',
  brand: self.brand, model: self.model
).references(:mfg_models).uniq.pluck('products.id')
product_ids = Product.eager_load(:mfg_models).
where('products.brand = :brand AND (products.model = :model OR mfg_models.name = :model)',
  brand: self.brand, model: self.model
).references(:mfg_models).pluck('products.id').uniq
product_ids.map! { |p| "(#{p.id}, #{self.id})" }
product_ids.map! { |p_id| "(#{p_id}, #{self.id})" }
rds_p1
self.id
product_ids.map! { |p_id| "(#{p_id}, #{self.id})" }
product_ids = ["(598, 8)"]
values = product_ids.join(',')
ProductRdsProduct.connection_pool.with_connection do |conn|
  conn.execute(<<-SQL.squish!)
        INSERT IGNORE INTO product_rds_products(product_id, rds_product_id) VALUES #{values}
      SQL
end
ProductRdsProduct.connection_pool.with_connection do |conn|
  conn.execute(<<-SQL.squish!)
        INSERT IGNORE INTO product_rds_products(product_id, rds_product_id) VALUES #{values}
      SQL
end
ProductRdsProduct.connection_pool.with_connection do |conn|
  conn.execute(<<-SQL.squish!)
        INSERT IGNORE INTO product_rds_products(product_id, rds_product_id) VALUES #{values}
      SQL
end
ProductRdsProduct.connection_pool.with_connection do |conn|
  conn.execute(<<-SQL.squish!)
        INSERT IGNORE INTO product_rds_products(product_id, rds_product_id) VALUES #{values}
      SQL
end
ProductRdsProduct.connection_pool.with_connection do |conn|
  conn.execute(<<-SQL.squish!)
        INSERT IGNORE INTO product_rds_products(product_id, rds_product_id) VALUES #{values}
      SQL
end
ProductRdsProduct.connection_pool.with_connection do |conn|
  conn.execute(<<-SQL.squish!)
        INSERT IGNORE INTO product_rds_products(product_id, rds_product_id) VALUES #{values}
      SQL
end
ProductRdsProduct.connection_pool.with_connection do |conn|
  conn.execute(<<-SQL.squish!)
        INSERT IGNORE INTO product_rds_products(product_id, rds_product_id) VALUES #{values}
      SQL
end
ProductRdsProduct.connection_pool.with_connection do |conn|
  conn.execute(<<-SQL.squish!)
        INSERT IGNORE INTO product_rds_products(product_id, rds_product_id) VALUES #{values}
      SQL
end
ProductRdsProduct.connection_pool.with_connection do |conn|
  conn.execute(<<-SQL.squish!)
        INSERT INTO product_rds_products(product_id, rds_product_id) VALUES #{values}
      SQL
end
product_ids = Product.eager_load(:mfg_models).
where('products.brand = :brand AND (products.model = :model OR mfg_models.name = :model)',
  brand: self.brand, model: self.model
).references(:mfg_models).uniq.pluck('products.id')
products
self.products
self.products.first
self.products.first.mfg_models
mfg_model = self.products.first.mfg_models.last
exit
reload!
mfg_model
brand
rds_p2 = RdsProduct.create(brand: brand, model: mfg_model)
rds_p2.product_rds_products
reload!
rds_p2 = RdsProduct.create(brand: brand, model: mfg_model)
rds_p2.save 
reload!
rds_p2.reload
rds_p2.id
rds_p2
rds_p2 = RdsProduct.last
rds_p2.save 
rds_p2.product_rds_products
RdsProduct.really_delete_all
RdsProduct.really_delete_all!
RdsProduct.really_destroy_all!
RdsProduct.all.map(&:really_destroy!)
reload!
brand
model
mfg_model = MfgModel.last
mfg_model.products
mfg_model = MfgModel.limit(10).last
mfg_model.products
brand2, model2 = '3M', mfg_model.name
rds_p1 = RdsProduct.create(brand: brand, model: model)
ProductRdsProduct.all
reload!
rds_p1.really_destroy!
rds_p1 = RdsProduct.create(brand: brand, model: model)
reload!
rds_p1 = RdsProduct.create(brand: brand, model: model)
brand
reload!
RdsProduct.first.destroy!
RdsProduct.count
rds_p1 = RdsProduct.create(brand: brand, model: model)
brand2
rds_p2 = RdsProduct.create(brand: brand2, model: model2)
rds_p2.products
ProductRdsProduct.connection_pool.with_connection do |conn|
  puts conn.quoted_table_name
end
ProductRdsProduct.quoted_table_name
reload!
RdsProduct.all.map(&:really_destroy!)
RdsProduct.all
rds_p2 = RdsProduct.create(brand: brand2, model: model2)
relaod!
reload!
rds_p2 = RdsProduct.create(brand: brand2, model: model2)
ProductRdsProduct.all
rds_p2.save
ProductRdsProduct.all
Product.joins(:mfg_models).to_sql
edit -t
join = <<-SQL.squish!
      SELECT 
        mfg_models.name
      FROM
      mfg_models 
          INNER JOIN
      product_mfg_models ON product_mfg_models.product_id = mfg_models.id 
          INNER JOIN
      products ON products.id = product_mfg_models.mfg_model_id AND products.id = :product_id
    SQL
edit -t
rds_p1.products
rds_p1.save
rds_p1.products
rds_p2.products
p1 = rds_p2.products.first
p2 = _
rds_roduct_ids = RdsProduct.where(query, brand: p2.brand, model: p2.model, product_id: p2.id)
rds_roduct_ids = RdsProduct.where(query, brand: p2.brand, model: p2.model, product_id: p2.id).to_sql
join = <<-SQL.squish!
      SELECT 
        mfg_models.name
      FROM mfg_models 
          INNER JOIN product_mfg_models 
            ON product_mfg_models.mfg_model_id = mfg_models.id 
          INNER JOIN
      products ON products.id = product_mfg_models.mfg_model_id AND products.id = :product_id
    SQL
query = <<-SQL.squish!
      rds_products.brand = :brand AND (rds_products.model = :model OR 
        rds_products.model IN (#{join})
    SQL
rds_roduct_ids = RdsProduct.where(query, brand: p2.brand, model: p2.model, product_id: p2.id)
p2.id
edit -t
rds_roduct_ids = RdsProduct.where(query, p2.brand, p2.model, p2.id)
rds_roduct_ids = RdsProduct.unscoped.where(query, p2.brand, p2.model, p2.id)
edit -t
rds_roduct_ids = RdsProduct.unscoped.where(query, p2.brand, p2.model, p2.id)
edit -t
rds_roduct_ids = RdsProduct.unscoped.where(query, p2.brand, p2.model, p2.id)
reload1
reload!
p2.save
Product.create(brand: 'newman co', model: 'alphabet')
p = _
p.valid?
p.errors
Entity.first
EntityType
EntityType.all
EntityType.find(18).entities
entity = EntityType.find(18).entities.first
Product.create(brand: 'newman co', model: 'alphabet', entity: entity)
reload!
Product.create(brand: 'newman co', model: 'alphabet', entity: entity)
entity = EntityType.find(18).entities.first
Product.create(brand: 'newman co', model: 'alphabet', entity: entity)
reload!
Product.create(brand: 'newman co', model: 'alphabet', entity: entity)
entity = EntityType.find(18).entities.first
Product.create(brand: 'newman co', model: 'alphabet', entity: entity)
reload!
entity = EntityType.find(18).entities.first
Product.create(brand: 'newman co', model: 'alphabet', entity: entity)
p = _
p.rds_products
p.really_destroy!
measure
def measure
edit -t
measure do 
  Product.create(brand: 'newman co', model: 'alphabet', entity: entity)
end
Product.last
Product.last.really_destroy!
Product.last
Product.last.destroy
Product.last
measure do 
  Product.create(brand: 'newman co', model: 'alphabet', entity: entity)
end
Product.last.destroy
reload!
measure do end
measure do 
  Product.create(brand: 'newman co', model: 'alphabet', entity: entity)
end
entity = EntityType.find(18).entities.first
measure do 
  Product.create(brand: 'newman co', model: 'alphabet', entity: entity)
end
Product.last.destroy
measure do 
  Product.create(brand: 'newman co', model: 'alphabet', entity: entity)
end
Product.last.destroy
measure(true) do
  Product.create(brand: 'newman co', model: 'alphabet', entity: entity)
end
cd Product.last
RdsProduct.where(model: [self.model, mfg_models.pluck(:name)].flatten!).to_sql
mfg_models.create(name: 'abcs')
RdsProduct.where(model: [self.model, mfg_models.pluck(:name)].flatten!).to_sql
self.rds_products << RdsProduct.where(model: [self.model, mfg_models.pluck(:name)].flatten!)
cd
RdsProduct.count
Product.first
p = _
p.rds_product
p.rds_products
p.save
RdsProduct.create(brand: self.brand, model: self.model)
cd p
RdsProduct.create(brand: self.brand, model: self.model)
self.rds_products << RdsProduct.where(model: [self.model, mfg_models.pluck(:name)].flatten!)
MfgModel.joins(:products).to_sql
MfgModel.joins(:products).select(:name).to_sql
MfgModel.joins(:products).select(:name).references(:mfg_models).to_sql
MfgModel.joins(:products).select('DISTINCT(mfg_models.name)').to_sql
MfgModel.joins(:products).select('DISTINCT(mfg_models.name)')
MfgModel.joins(:products).select('DISTINCT(mfg_models.name)').to_sql
join = MfgModel.joins(:products).select('DISTINCT(mfg_models.name)').to_sql << ' AND products.id = :product_id'
query = <<-SQL.squish!
      rds_products.brand = :brand AND (rds_products.model = :model OR
        rds_products.model IN (#{join}))
    SQL
rds_product_ids = RdsProduct.unscoped.where(query, brand: self.brand, model: self.model, product_id: self.id)
.uniq.pluck('rds_products.id')
query
join = MfgModel.joins(:products).select('DISTINCT(mfg_models.name)').to_sql << ' AND products.id = :product_id'
query = <<-SQL.squish!
      rds_products.brand = :brand AND (rds_products.model = :model OR
        rds_products.model IN (#{join}))
    SQL
query
rds_product_ids = RdsProduct.where(query, brand: self.brand, model: self.model, product_id: self.id)
.uniq.pluck('rds_products.id')
rds_product_ids = RdsProduct.where(query, brand: self.brand, model: self.model, product_id: self.id).to_sql
rds_product_ids = RdsProduct.where(query, brand: self.brand, model: self.model, product_id: self.id)
rds_product_ids = RdsProduct.where(query, brand: self.brand, model: self.model, product_id: self.id).uniq.pluck('rds_products.id')
rds_product_ids = RdsProduct.where(
  query, brand: self.brand, model: self.model, product_id: self.id
).uniq.pluck('rds_products.id')
join = MfgModel.joins(:products).select('DISTINCT(mfg_models.name)').to_sql << ' AND products.id = :product_id'
query = <<-SQL.squish!
      rds_products.brand = :brand AND (rds_products.model = :model OR
        rds_products.model IN (#{join}))
    SQL
ProductRdsProduct.connection_pool.with_connection do |conn|
  time = conn.quote(DateTime.now)
  table = ProductRdsProduct.quoted_table_name
  rds_product_ids.map! do |rds_id|
    "(#{self.id}, #{rds_id}, #{time}, #{time})"
  end
  values = rds_product_ids.join(',')
  conn.execute(
    "INSERT IGNORE INTO #{table} (product_id, rds_product_id, created_at, updated_at) VALUES #{values}"
  )
end
rds_products
self.rds_products
self.rds_products.unscoped
exit
technologies = Technology.includes(:keywords); nil
keywords = technology.keywords.strong
technology = technologies.first
keywords = technology.keywords.strong
reload!
technology = Technology.first
technology.strong_keywords
reload!
technology = Technology.first
technology.strong_keywords
reload!
technology = Technology.first
technology.weak_keywords
technologies = Technology.includes(:strong_keywords); nil
technologies.first
keywords = technology.keywords
technologies = Technology.includes(:strong_keywords); nil
technologies.first
keywords = technology.strong_keywords
technology = Technology.includes(:strong_keywords).first
keywords = technology.strong_keywords
technologies = Technology.includes(:strong_keywords).load; nil
technology = technologies.first
technology.strong_keywords
include SafeTableChange
exit
technologies = Technology.joins(:strong_keywords)
technologies.first.strong_keywords
technologies = Technology.joins(:strong_keywords).load; nil
technologies.first
technologies.first.strong_keywords
technologies = Technology.includes(:strong_keywords).load; nil
technologies.first.strong_keywords
"/#{:listings}"
search_params = Hashie::Mash.new({keywords: 'hello', type: :listings }
search_params = Hashie::Mash.new({keywords: 'hello', type: :listings })
Figaro.env.rds_api_url + "/#{search_params[:type]}"
edit -t
exit
edit
def block_method(&block)
end
block_method
exit
t = Technology.first
t.strong_keywords
royalty_products = RoyaltyReport.includes(:products).flat_map(&:products).uniq(&:id).map { |p| Hashie::Mash.new(brand: p.brand, model: p.model) }
royalty_products = RoyaltyReport.joins(:products).flat_map(&:products).uniq(&:id).map { |p| Hashie::Mash.new(brand: p.brand, model: p.model) }
exit
Product.join(:royalty_reports).to_sql
Product.joins(:royalty_reports).to_sql
Product.joins(:royalty_reports).select(:brand, :model).distinct('products.id').to_sql
Product.joins(:royalty_reports).select(:brand, :model).distinct('products.id')
Product.joins(:royalty_reports).select(products: [:brand, :model]).distinct('products.id')
Product.joins(:royalty_reports).select('products.brand', 'products.model).distinct('products.id')
Product.joins(:royalty_reports).select('products.brand', 'products.model').distinct('products.id')
Product.joins(:royalty_reports).select('products.brand', 'products.model').distinct('products.id').count
RoyaltyReport.includes(:products).flat_map(&:products).uniq(&:id).map { |p| Hashie::Mash.new(brand: p.brand, model: p.model) }.count
Product.joins(:royalty_reports).select('products.brand', 'products.model').distinct('products.id').count
Product.joins(:royalty_reports).select('products.brand', 'products.model').distinct('products.id')
Product.connection.select_all(Product.joins(:royalty_reports).select('products.brand', 'products.model').distinct('products.id').to_sql)
Product.connection.select_all(Product.joins(:royalty_reports).select('products.brand', 'products.model').distinct('products.id').to_sql).to_ary
Product.connection.select_all(Product.joins(:royalty_reports).select('products.brand', 'products.model').distinct('products.id').to_sql).map {|p| Hashie::Mash.new(brand: p.brand, model: p.model) }
Product.connection.select_all(Product.joins(:royalty_reports).select('products.brand', 'products.model').distinct('products.id').to_sql).map {|p| Hashie::Mash.new(p) }
royalty_products = Product.connection.with_connection do |conn|
  conn.select_all(Product.joins(:royalty_reports).select('products.brand', 'products.model').distinct('products.id').to_sql).map {|p| Hashie::Mash.new(p) }
end
royalty_products = Product.connection_pool.with_connection do |conn|
  conn.select_all(Product.joins(:royalty_reports).select('products.brand', 'products.model').distinct('products.id').to_sql).map {|p| Hashie::Mash.new(p) }
end
royalty_products = Product.connection_pool.with_connection do |conn|
  conn.select_all(Product.joins(:royalty_reports).select('products.brand', 'products.model').distinct('products.id').to_sql)
end
royalty_products.map! {|p| Hashie::Mash.new(p) }
query = Product.joins(:royalty_reports).select('products.brand', 'products.model').distinct('products.id').to_sql
royalty_products = Product.connection_pool.with_connection do |conn|
  conn.select_all(query)
end
edit -t
royalty_products = RoyaltyReport.includes(:products).flat_map(&:products).uniq(&:id).map {|p| Hashie::Mash.new(brand: p.brand, model: p.model)}
edit -t
measure(true) do
  royalty_products = RoyaltyReport.includes(:products).flat_map(&:products).uniq(&:id).map { |p| Hashie::Mash.new(brand: p.brand, model: p.model) }
end
edit -t
exit
query = Product.joins(:royalty_reports).select('products.brand', 'products.model').distinct('products.id').to_sql
royalty_products = Product.connection_pool.with_connection do |conn|
  conn.select_all(query)
end
RvxSignalGenerator.find 3
RvxSignal.first
RvxSignal.first.txn_id
txn_id = Digest::MD5.hexdigest "#{date_now}#{Cloud.current.id.to_s}"
date_now = DateTime.now.to_s
txn_id = Digest::MD5.hexdigest "#{date_now}#{Cloud.current.id.to_s}"
date_now = DateTime.now.to_s
txn_id = Digest::MD5.hexdigest "#{date_now}#{Cloud.current.id.to_s}"
signal_generator = RvxSignalGenerator.find(3)
emits, skips = 0, 0
technologies = Technology.includes(:strong_keywords)
tecnology.firsttechnology.first
tecnology = technology.first
tecnology = technologies.first
keyword = technology.strong_keywords.first
technology = technologies.first
keyword = technology.strong_keywords.first
cd RvxSignalGenerator::UnreportedProduct.new
require '/Users/jonathan/rvx-spa/app/workers/rvx_signal_generators/10000_unreported_product.rb
require '/Users/jonathan/rvx-spa/app/workers/rvx_signal_generators/10000_unreported_product.rb'
cd RvxSignalGenerator::UnreportedProduct.new
cd RvxSignalGenerators::UnreportedProduct.new
cache=RvxSharedCache.new(Cloud.current_name)
date_now = DateTime.now.to_s
txn_id = Digest::MD5.hexdigest "#{date_now}#{Cloud.current.id.to_s}"
signal_generator = RvxSignalGenerator.find(3)
cache
emits, skips = 0, 0
technologies = Technology.includes(:strong_keywords)
technology = technologies.first # delete
technologies[1]
technologies[2]
technologies[3]
technologies[4]
technologies.last
technologies[-3]
technologies[-4]
technologies[-5]
technologies[-6]
technologies[-7]
technology = technologies.first # delete
keyword = technology.strong_keywords.first
keyword = technology.strong_keywords.first.name
Keyword.all
Keyword.all.map(&:name)
cache=RvxSharedCache.new(Cloud.current_name)
date_now = DateTime.now.to_s
txn_id = Digest::MD5.hexdigest "#{date_now}#{Cloud.current.id.to_s}"
signal_generator = RvxSignalGenerator.find(3)
edit -t
technology = technologies.first # delete
keyword = technology.strong_keywords.first.name
exit
add_cloud_rds_config({keywords: 'HDMI'}).merge(type: :products)
Cloud.current
exit
cd RvxSignalGenerators::UnreportedProduct.new
cache=RvxSharedCache.new(Cloud.current_name)
date_now = DateTime.now.to_s
txn_id = Digest::MD5.hexdigest "#{date_now}#{Cloud.current.id.to_s}"
signal_generator = RvxSignalGenerator.find(3)
Figaro.env.rds_api_url
edit -t
technology = technologies.first # delete
keyword = 'everclean'
edit -t
RvxSignal.last
new_date = RvxSignal.last.created_at
RvxSignal.last(10)
response = RestClient.get(Figaro.env.rds_api_url + "/#{search_params[:type]}",
  params: search_params.reject { |k| k == :type },
:content_type => :json, :accept => :json)
search_params = {keywords: 'everclean'}
response = RestClient.get(Figaro.env.rds_api_url + "/#{search_params[:type]}",
  params: search_params.reject { |k| k == :type },
:content_type => :json, :accept => :json)
search_params = {type: :products, keywords: 'everclean'}
response = RestClient.get(Figaro.env.rds_api_url + "/#{search_params[:type]}",
  params: search_params.reject { |k| k == :type },
:content_type => :json, :accept => :json)
ro = Oj.load(response)
total = ro[:pagination][:total]
per_page = ro[:pagination][:per_page]
version = ro[:version]
results = ro[search_params[:type].to_sym]
rds_products = results
p = rds_products.first
p.brand
query = Product.joins(:royalty_reports).select('products.brand', 'products.model').distinct('products.id').to_sql
royalty_products = Product.connection_pool.with_connection do |conn|
  conn.select_all(query)
end
royalty_products.first
royalty_products.to_h
royalty_products.to_hash
royalty_products.to_json
royalty_products.to_hash
royalty_products = royalty_products.to_hash
royalty_products.class
h = royalty_products.first
royalty_products.to_ary
royalty_products = royalty_products.to_ary
h = royalty_products.first
h.symbolize_keys!
royalty_products.map!(&:stringify_keys)
royalty_products.map!(&:symbolize_keys!)
edit
query = Product.joins(:royalty_reports).select('products.brand', 'products.model').distinct('products.id').to_sql
royalty_products = Product.connection_pool.with_connection do |conn|
  conn.select_all(query)
end
royalty_products.map!(&:symbolize_keys!)
query = Product.joins(:royalty_reports).select('products.brand', 'products.model').distinct('products.id').to_sql
royalty_products = Product.connection_pool.with_connection do |conn|
  conn.select_all(query)
end
measure do
  royalty_products.map! { |p| Hashie::Mash.new(p) }
end
query = Product.joins(:royalty_reports).select('products.brand', 'products.model').distinct('products.id').to_sql
royalty_products = Product.connection_pool.with_connection do |conn|
  conn.select_all(query)
end
edit -t
edit
[].empty?
[].blank?
nil.empty?
nil.blank?
[].present
[].present?
search_params
search_params.except(:type)
exit
cd RvxSignalGenerators::UnreportedProduct.new
cache=RvxSharedCache.new(Cloud.current_name)
date_now = DateTime.now.to_s
txn_id = Digest::MD5.hexdigest "#{date_now}#{Cloud.current.id.to_s}"
signal_generator = RvxSignalGenerator.find(3)
emits, skips = 0, 0
technologies = Technology.includes(:strong_keywords).load; nil # delete load; nil
technology = technologies.first # delete
exit
cd RvxSignalGenerators::UnreportedProduct.new
exit
cd RvxSignalGenerators::UnreportedProduct.new
require "json"
require "benchmark"
def measure(no_gc=false, &block)
  if no_gc
    GC.disable
  else
    # collect memory allocated during library loading
    # and our own code before the measurement
    GC.enable
    GC.start
  end
  memory_before = `ps -o rss= -p #{Process.pid}`.to_i/1024
  gc_stat_before = GC.stat
  time = Benchmark.realtime do
    yield
  end
  puts ObjectSpace.count_objects
  unless no_gc
    GC.start(full_mark: true, immediate_sweep: true, immediate_mark: true)
  end
  puts ObjectSpace.count_objects
  gc_stat_after = GC.stat
  memory_after = `ps -o rss= -p #{Process.pid}`.to_i/1024
  puts({
      RUBY_VERSION => {
        gc: no_gc ? 'disabled' : 'enabled',
        time: time.round(2),
        gc_count: gc_stat_after[:count] - gc_stat_before[:count],
        memory: "%d MB" % (memory_after - memory_before)
      }
  }.to_json)
end
edit -t
exit
cd RvxSignalGenerators::UnreportedProduct.new
edit -t
RvxSignal.count
exit
cd RvxSignalGenerators::UnreportedProduct.new
edit -t
RdsProduct
RdsListing
cloud = Cloud.current
cloud.rds_config
edit -t
RvxSignal.count
edit
edit -t
edit
reload!
exit
cd RvxSignalGenerators::UnreportedProduct.new
edit -t
search_params = {keywords: 'HDMI', type: :products}
response = RestClient.get(Figaro.env.rds_api_url + "/#{search_params[:type]}",
  params: search_params.except(:type),
:content_type => :json, :accept => :json)
response = Hashie::Mash.new(Oj.load(response))
total = response[:pagination][:total]
per_page = response[:pagination][:per_page]
version = response[:version]
results = response.delete([search_params[:type]])
results = response.[search_params[:type]]
results = response[search_params[:type]]
results = response.delete([search_params[:type].to_s])
results
response
type
results = response.delete(search_params[:type].to_s)
exit
search_params = {keywords: 'HDMI', type: :products}
response = RestClient.get(Figaro.env.rds_api_url + "/#{search_params[:type]}",
  params: search_params.except(:type),
:content_type => :json, :accept => :json)
response = Hashie::Mash.new(Oj.load(response))
total = response[:pagination][:total]
per_page = response[:pagination][:per_page]
version = response[:version]
results = response.delete(search_params[:type])
results
respon
response
cd RvxSignalGenerators::UnreportedProduct.new
edit -t
exit
exit
p = Product.joins(:royalty_report_items).first
rp = p
rds_reported_product = rp
reported_product = Product.includes(:royalty_report_items).find_by(brand: rds_reported_product.brand, model: rds_reported_product.model)
reported_product.technologies
technology = _.first
reported_product.royalty_report_items.map(&:technology_id).compact.uniq.include?(technology.id)
reported_product.royalty_report_item.map(&:technology_id)
reported_product.royalty_report_items.map(&:technology_id)
technology.id
reported_product.royalty_report_items.map(&:technology_id)
reported_product = Product.includes(royalty_report_items: :technology).find_by(brand: rds_reported_product.brand, model: rds_reported_product.model)
p
p.technology_ids
p.royalty_report_items.includes(:technology).select('technology.id')
p.royalty_report_items.includes(:technology).select('technologids.id')
p.royalty_report_items.includes(:technology).select('technologies.id')
Technology.joins(products: [:royalty_report_items])
RoyaltyReportItem.joins(:products).where(product_id: reported_product.id)
RoyaltyReportItem.joins(:products).where(product_id: reported_product.id).count
RoyaltyReportItem.joins(:product).where(product_id: reported_product.id).count
RoyaltyReportItem.joins(:product).where(product_id: reported_product.id).uniq.pluck(:technology_id)
RoyaltyReportItem.joins(:product).where(product_id: reported_product.id, technology_id: technology.id)
RoyaltyReportItem.joins(:product).where(product_id: reported_product.id, technology_id: technology.id)      RoyaltyReportItem.joins(:product).where(product_id: reported_product.id, technology_id: technology.id).size > 0
RoyaltyReportItem.joins(:product).where(product_id: reported_product.id, technology_id: technology.id).size > 0
cd RvxSignalGenerators::UnlicensedTechnology.new
txn_id = Digest::MD5.hexdigest "#{date_now}#{Cloud.current.id.to_s}"
date_now = DateTime.now.to_s
signal_generator = RvxSignalGenerator.find(4)
cache=RvxSharedCache.new(Cloud.current_name)
txn_id = Digest::MD5.hexdigest "#{date_now}#{Cloud.current.id.to_s}"
emits, skips = 0, 0
technologies = Technology.includes(:strong_keywords)
query = Product.joins(:royalty_reports).select('products.brand', 'products.model').distinct('products.id').to_sql
royalty_products = Product.connection_pool.with_connection do |conn|
  conn.select_all(query)
end
royalty_products.map! { |p| Hashie::Mash.new(p) }
technology = technologies[1]
edit -t
RvxSignal.count
exit
RvxSignal.count
exit
RvxSignal.count
signal_gen_id = 4
date_now = DateTime.now.to_s
txn_id = Digest::MD5.hexdigest "#{date_now}#{Cloud.current.id.to_s}"
cache=RvxSharedCache.new(Cloud.current_name)
reload!
signal_gen_id = 4
txn_id = Digest::MD5.hexdigest "#{DateTime.now.to_s}#{Cloud.current.id.to_s}"
cache=RvxSharedCache.new(Cloud.current_name)
RvxSignalGenerators::UnlicensedTechnology.new.perform(signal_gen_id, txn_id, cache)
RvxSignalGenerators::UnlicensedTechnology
exit
RvxSignalGenerators::UnlicensedTechnology
signal_gen_id = 4
txn_id = Digest::MD5.hexdigest "#{DateTime.now.to_s}#{Cloud.current.id.to_s}"
cache=RvxSharedCache.new(Cloud.current_name)
RvxSignalGenerators::UnlicensedTechnology.new.perform(signal_gen_id, txn_id, cache)
Time.now
start = _
RvxSignal.last.created_at
RvxSignal.count
RvxSignal.last
RvxSignal.last.data_hash
Techology.includes(:strong_keywords).map {|sk| sk.name }
Technology.includes(:strong_keywords).map {|sk| sk.name }
Technology.last
Technology.includes(:strong_keywords).flat_map {|t| t.strong_keywords.map(&:name) }
RvxSignal.count
RvxSignal.last.data_hash
technologies = Technology.includes(strong_keywords: :name)
RvxSignal.last.data_hash
[].select!{|v| v.present? }
arr = []
arr.select!{|v| v.present? }
arr
arr = [1, 2, nil]
arr.select!{|v| v.present? }
arr
RvxSignal.last.data_hash
RvxSignal.count
Time.now
DateTime.parse('2016-12-18 18:29:46 -0800')
DateTime.parse('2016-12-18 19:04:39 -0800') - DateTime.parse('2016-12-18 18:29:46 -0800')
Date.parse('2016-12-18 19:04:39 -0800') - Date.parse('2016-12-18 18:29:46 -0800')
Date.new('2016-12-18 19:04:39 -0800') - Date.parse('2016-12-18 18:29:46 -0800')
Date.new('2016-12-18 19:04:39 -0800') - Date.new('2016-12-18 18:29:46 -0800')
Date.new('2016-12-18 19:04:39 -0800') - DateTime.new('2016-12-18 18:29:46 -0800')
DateTime.new('2016-12-18 19:04:39 -0800') - DateTime.new('2016-12-18 18:29:46 -0800')
DateTime.parse('2016-12-18 18:29:46 -0800')
date = 
d1 = DateTime.parse('2016-12-18 18:29:46 -0800')
d1.class
d1.to_date
d1.to_i
reported_product = Product.includes(:royalty_report_items).find_by(brand: rds_reported_product.brand, model: rds_reported_product.model)
exit
IS_VALID_QUERY = <<-SQL.squish!.freeze
    (
      (
        contracts.contract_effective_date IS NULL
        AND
        contracts.reporting_effective_date IS NULL
      )
      OR
      contracts.contract_effective_date <= :end_date
      OR
      contracts.reporting_effective_date <= :end_date
    )
    AND
    (
      (
        contracts.contract_end_date IS NULL
        AND
        contracts.reporting_end_date IS NULL
      )
      OR
      contracts.contract_end_date >= :start_date
      OR
      contracts.reporting_end_date >= :start_date
    )
    AND
    (
      contracts.termination_date IS NULL
      OR
      contracts.termination_date >= :start_date
    )
  SQL
IS_VALID_QUERY
rri = RoyaltyReportItem.first
reported_item = rri.product
product_entity = reported_product.entity
reported_product = rri.product
product_entity = reported_product.entity
contracted_technologies = product_entity.contracts.contract_effective_range(Date.today, Date.today).flat_map(&:technologies)
contracted_technologies = product_entity.contracts.includes(:technologies).contract_effective_range(Date.today, Date.today).flat_map(&:technologies)
Technology.merge(product_entity.contracts.contract_effective_range(Date.today, Date.today))
Technology.where(contract: product_entity.contracts.contract_effective_range(Date.today, Date.today))
Contract.contract_effective_range(Date.today, Date.today)
Contract.contract_effective_range(Date.today, Date.today).class
Technology.where(contract: Contracts.joins(:entities).where('entities.id': product_entity.id).contract_effective_range(Date.today, Date.today)
Technology.where(contract: Contracts.joins(:entities).where('entities.id': product_entity.id).contract_effective_range(Date.today, Date.today))
Technology.where(contract: Contract.joins(:entities).where('entities.id': product_entity.id).contract_effective_range(Date.today, Date.today))
Contract.joins(:entities).where('entities.id': product_entity.id).contract_effective_range(Date.today, Date.today)
Contract.joins(:entities).where('entities.id': product_entity.id).contract_effective_range(Date.today, Date.today)                      Technology.all.merge(Contract.joins(:entities).where('entities.id': product_entity.id).contract_effective_range(Date.today, Date.today))
Technology.all.merge(Contract.joins(:entities).where('entities.id': product_entity.id).contract_effective_range(Date.today, Date.today))
Technology.joins(:contracts).where('contracts.id': Contract.joins(:entities).where('entities.id': product_entity.id).contract_effective_range(Date.today, Date.today).pluck(:id))
product_entity.contracts.includes(:technologies).contract_effective_range(Date.today, Date.today).flat_map(&:technologies)
valid_technologies = Technology.joins(:contracts).where('contracts.id': Contract.joins(:entities).where('entities.id': product_entity.id).contract_effective_range(Date.today, Date.today).pluck(:id)).pluck(:id)
Technology = Technology.first
valid_technologies = Technology.joins(:contracts)
.where(
'contracts.id': Contract.joins(:entities)
.where('entities.id': product_entity.id)
.contract_effective_range(Date.today, Date.today)
.pluck(:id)
).where(id: technology.id)
edit -t
valid_technologies = Technology.joins(:contracts).
where(
  'contracts.id': Contract.joins(:entities).
  where('entities.id': product_entity.id).
  contract_effective_range(Date.today, Date.today).
  pluck(:id)
).where(id: technology.id)
edit -t
Technology.join(:contracts)
Technology.jois(:contracts)
Technology.joins(:contracts)
exit
edit -t
product_entity = RoyaltyReportItem.first.product.entity
technology = Technology.first
valid_technologies = Technology.joins(:contracts).
where(
  'contracts.id': Contract.joins(:entities).
  where('entities.id': product_entity.id).
  contract_effective_range(Date.today, Date.today).
  pluck(:id)
).where(id: technology.id)
valid_technologies = Technology.joins(:contracts).
where(
  'contracts.id': Contract.joins(:entities).
  where('entities.id': product_entity.id).
  contract_effective_range(Date.today, Date.today).
  pluck(:id)
).where(id: technology.id).present?
).where(id: technology.id).present?.to_sql
).where(id: technology.id).size > 0
Date.today
Date.today.5.years.ago
5.years.ago
Technology.joins(:contracts).
where(
  'contracts.id': Contract.joins(:entities).
  where('entities.id': product_entity.id).
  contract_effective_range(5.years.ago, Date.today).
  pluck(:id)
).where('technologies.id': technology.id)
Technology.joins(:contracts).
where(
  'contracts.id': Contract.joins(:entities).
  where('entities.id': product_entity.id).
  contract_effective_range(Date.today, Date.today).
  pluck(:id)
).where('technologies.id': technology.id)
Technology.joins(:contracts).
where(
  'contracts.id': Contract.joins(:entities).
  where('entities.id': product_entity.id).
  contract_effective_range(Date.today, Date.today).
  pluck(:id)
).where('technologies.id': technology.id).count > 0
Technology.joins(:contracts).
where(
  'contracts.id': Contract.joins(:entities).
  where('entities.id': product_entity.id).
  contract_effective_range(5.years.ago, Date.today).
  pluck(:id)
).where('technologies.id': technology.id).count > 0
Technology.joins(:contracts).
where(
  'contracts.id': Contract.joins(:entities).
  where('entities.id': product_entity.id).
  contract_effective_range(5.years.ago, Date.today).
  uniq.pluck(:id)
).where('technologies.id': technology.id).count > 0
Technology.joins(:contracts).
where(
  'contracts.id': Contract.joins(:entities).
  where('entities.id': product_entity.id).
  contract_effective_range(Date.today, Date.today).
  uniq.pluck(:id)
).where('technologies.id': technology.id).count > 0
edit -t
edit
edit -t
edit
reload!
exit
technology = Technology.first
product_entity = RoyaltyReportItem.first.product.entity
edit -t
xxxxq::q!edt
xit
exit
technology = Technology.first
product_entity = RoyaltyReportItem.first.product.entity
product_entity.technologies
technology = Technology.find 14
Technology.joins(:contracts).
where(
  'contracts.id': Contract.joins(:entities).
  where('entities.id': product_entity.id).
  contract_effective_range(5.years.ago, Date.today).
  select('DISTINCT entities.id')
).where('technologies.id': technology.id)
Technology.joins(:contracts).
where(
  'contracts.id': Contract.joins(:entities).
  where('entities.id': product_entity.id).
  contract_effective_range(5.years.ago, Date.today).
  select('DISTINCT(contracts.id)')
).where('technologies.id': technology.id).count > 0
edit -t
technology = Technology.first
edit -t
exit
txn_id = Digest::MD5.hexdigest "#{DateTime.now.to_s}#{Cloud.current.id.to_s}"
signal_generator_id = 7
cache=RvxSharedCache.new(Cloud.current_name)
RvxSignalGenerators::ReportedProductUnlicensedTech
def fire_signals
  start_time = DateTime.now
exit
  signal_count = RvxSignal.count
  RvxSignalGenerators::ReportedProductUnlicensedTech.new.perform(signal_generator_id, txn_id, cache)
def fire_signals
  start_time = DateTime.now
  signal_count = RvxSignal.count
  signal_generator_id = 7
  txn_id = Digest::MD5.hexdigest "#{DateTime.now.to_s}#{Cloud.current.id.to_s}"
  cache=RvxSharedCache.new(Cloud.current_name)
  RvxSignalGenerators::ReportedProductUnlicensedTech.new.perform(signal_generator_id, txn_id, cache)
  end_time = DateTime.now
  end_count = RvxSignal.count
  puts "Time = #{end_time - start_time}, created = #{end_count - signal_count}"
end
fire_signals
RvxSignal.count
Contract.joins(:entities).
where('entities.id': product_entity.id).
contract_effective_range(Date.today, Date.today).
select('DISTINCT(contracts.id)').to_sql
edit -t
product_entity = RoyaltyReportItem.first.product.entity
technology = Technology.first
edit -t
RvxSignal.count
RvxSignal.last.data_hash
Technology.last
RvxSignal.last.data_hash
RvxSignal.count
RvxSignal.last.data_hash
Time.now - 10.seconds.ago
DateTime.now - DateTime.now
exit
exit
date = Date.today.prev_month(3)
3.months.ago
3.months.ago.class
date = Date.today.prev_month(3).class
date = Date.today.prev_month(3)
edit -t
generator = RvxSignalGenerator.find(signal_generator_id)
stats = {
  emitted: 0,
  skipped: 0
}
edit -t
reporting_entity_ids
(date.prev_year(3)..date)
(date.prev_year(3)..date).to_a
(date.prev_year(3)..date).to_a.count
(date.prev_year(3)..date).select { |d| [1, 4, 7, 10].include?(d.month) && d.day == 1 }.count
(date.prev_year(3)..date).select { |d| [1, 4, 7, 10].include?(d.month) && d.day == 1 }
ReportingPeriodService.quarters_from_range(date.prev_year(3), data)
ReportingPeriodService.quarters_from_range(date.prev_year(3), date)
ReportingPeriodService.quarters_from_range(date.prev_year(3), date).count
date = Date.today.prev_month(3)
start_date, end_date = date.prev_year(3), date
start_date.to_date.beginning_of_financial_quarter
end_date.to_date.end_of_financial_quarter
dates = [Tue, 01 Oct 2013,
  Wed, 01 Jan 2014,
Tue, 01 Apr 2014,
Tue, 01 Jul 2014,
Wed, 01 Oct 2014,
Thu, 01 Jan 2015,
Wed, 01 Apr 2015,
Wed, 01 Jul 2015,
Thu, 01 Oct 2015,
Fri, 01 Jan 2016,
Fri, 01 Apr 2016,
Fri, 01 Jul 2016]
dates = (date.prev_year(3)..date).select { |d| [1, 4, 7, 10].include?(d.month) && d.day == 1 }
qdate = dates.first
entity_id = reporting_entity_ids.first
reports = RoyaltyReport.includes(:royalty_report_items)
edit -t
reports.map { |rr| convert_to_hash(rr).merge({ report: rr }) }
reports.map { |rr| convert_to_hash(rr).merge!({ report: rr }) }
edit -t
reports
reports.map { |rr| convert_to_hash(rr).merge({ report: rr }) }
prev_report
edit -t
reports = reports.map { |rr| convert_to_hash(rr).merge({ report: rr }) }
reports.first
reports.second
reports.class
entity_id
qdate
last_2_reports(entity_id, qdate)
edit -t
edit
edit -t
last_2_reports(entity_id, qdate)
edit -t
report
reports
exit
emits = 0
emits += 1
emits
exit
RvxSignalGenerators::UnreportedProduct.new.perform(3
RvxSignalGenerators::UnreportedProduct.new.perform(3, Digest::MD5.hexdigest "#{DateTime.now.to_s}#{Cloud.current.id.to_s}"
RvxSignalGenerators::UnreportedProduct.new.perform(3, Digest::MD5.hexdigest("#{DateTime.now.to_s}#{Cloud.current.id.to_s}")
RvxSignalGenerators::UnreportedProduct.new.perform(3, Digest::MD5.hexdigest("#{DateTime.now.to_s}#{Cloud.current.id.to_s}"), RvxSharedCache.new(Cloud.current_name))
edit -t
obj = TestClass.new
obj.my_var
obj.method1
obj.my_var
edit
edit -t
obj = TestClass.new
obj.my_var
obj.method1
obj.my_var
obj.method2
obj.my_var
obj.method2
obj.my_var
RvxSignalGenerators::UnreportedProduct.new.perform(3, Digest::MD5.hexdigest("#{DateTime.now.to_s}#{Cloud.current.id.to_s}"), RvxSharedCache.new(Cloud.current_name))
exit
RvxSignalGenerators::UnreportedProduct.new.perform(3, Digest::MD5.hexdigest("#{DateTime.now.to_s}#{Cloud.current.id.to_s}"), RvxSharedCache.new(Cloud.current_name))
exit
RvxSignalGenerators::UnreportedProduct.new.perform(3, Digest::MD5.hexdigest("#{DateTime.now.to_s}#{Cloud.current.id.to_s}"), RvxSharedCache.new(Cloud.current_name))
RvxSignalGenerators::UnlicensedTechnology.new.perform(3, Digest::MD5.hexdigest("#{DateTime.now.to_s}#{Cloud.current.id.to_s}"), RvxSharedCache.new(Cloud.current_name))
RvxSignal.last
RvxSignalGenerators::ReportedProductUnlicensedTech.new.perform(3, Digest::MD5.hexdigest("#{DateTime.now.to_s}#{Cloud.current.id.to_s}"), RvxSharedCache.new(Cloud.current_name))
exit
RvxSignalGenerators::ReportedProductUnlicensedTech.new.perform(3, Digest::MD5.hexdigest("#{DateTime.now.to_s}#{Cloud.current.id.to_s}"), RvxSharedCache.new(Cloud.current_name))
RvxSignalGenerators::UnlicensedTechnology.new.perform(4, Digest::MD5.hexdigest("#{DateTime.now.to_s}#{Cloud.current.id.to_s}"), RvxSharedCache.new(Cloud.current_name))
RvxSignalGenerator.find(4)
signal_generator_id, txn_id, cache =
4, Digest::MD5.hexdigest("#{DateTime.now.to_s}#{Cloud.current.id.to_s}"), RvxSharedCache.new(Cloud.current_name
signal_generator_id, txn_id, cache =
4, Digest::MD5.hexdigest("#{DateTime.now.to_s}#{Cloud.current.id.to_s}"), RvxSharedCache.new(Cloud.current_name)
signal_generator.present?
signal_generator = RvxSignalGenerator.find(signal_generator_id)
technologies = Technology.includes(:strong_keywords)
Technology.count
exit
Technology.count
RvxSignalGenerators::UnlicensedTechnology.new.perform(4, Digest::MD5.hexdigest("#{DateTime.now.to_s}#{Cloud.current.id.to_s}"), RvxSharedCache.new(Cloud.current_name))
exit
RvxSignalGenerators::UnlicensedTechnology.new.perform(4, Digest::MD5.hexdigest("#{DateTime.now.to_s}#{Cloud.current.id.to_s}"), RvxSharedCache.new(Cloud.current_name))
exit
RvxSignalGenerators::UnlicensedTechnology.new.perform(4, Digest::MD5.hexdigest("#{DateTime.now.to_s}#{Cloud.current.id.to_s}"), RvxSharedCache.new(Cloud.current_name))
RdsProduct
RdsListing
RdsProduct
exit
RvxSignalGenerators::UnreportedProduct.new.perform(3, Digest::MD5.hexdigest("#{DateTime.now.to_s}#{Cloud.current.id.to_s}"), RvxSharedCache.new(Cloud.current_name))
RvxSignalGenerators::ReportedProductUnlicensedTech.new.perform(7, Digest::MD5.hexdigest("#{DateTime.now.to_s}#{Cloud.current.id.to_s}"), RvxSharedCache.new(Cloud.current_name))
search_params = {type: :products, keywords: 'hdmi' }
response = RestClient.get Figaro.env.rds_api_url + "/#{params[:type]}", params: params, :content_type => :json, :accept => :json
params = search_params
response = RestClient.get Figaro.env.rds_api_url + "/#{params[:type]}", params: params, :content_type => :json, :accept => :json
response =     Oj.load(response)
response.code
response[:code]
response = RestClient.get Figaro.env.rds_api_url + "/#{params[:type]}", params: params, :content_type => :json, :accept => :json
response.code
exit
RvxSignalGenerators::ReportedProductUnlicensedTech.new.perform(7, Digest::MD5.hexdigest("#{DateTime.now.to_s}#{Cloud.current.id.to_s}"), RvxSharedCache.new(Cloud.current_name))
edit -t
measure(true) do
  cache = RvxSharedCache.new(Cloud.current_name)
  RvxSignal.select(:id, :sig).find_in_batches do |sbatch|
    sbatch.each do |sigobj|
      cache.add_to_set('existing_signals', sigobj[:sig])
    end
  end
end
RvxSignal.count
_ / 1000.0
sigs = RvxSignal.connection.select_all('SELECT sig FROM rvx_signals').to_ary; nil
sigs.count
sigs.first
sigs.map! {|r| r['sig'] }
RvxSignal.last
exit
edit -t
cache = RvxSharedCache.new(Cloud.current_name)
cache.clear_cache!('existing_sigals'.freeze)
measure do
  sigs = RvxSignal.connection.select_all('SELECT sig FROM rvx_signals').to_ary; nil
  sigs.map! { |r| r['sig'] }
  cache.add_to_set('existing_signals'.freeze, sigs)
end
cache.smembers('existing_signals'.freeze
cache.smembers('existing_signals'.freeze)
cache.fetch('existing_signals'.freeze)
cache.connection
cache.send(:connection)
cache.send(:connection).smemebers('existing_signals')
cache.send(:connection).smembers('existing_signals')
cache
full_key = "#{Rails.env}:shared_cache"
full_key = "#{Rails.env}:shared_cache:demo:existing_signals"
cache.send(:connection).smembers(full_key)
cache.send(:connection).smembers(full_key).count
cache.clear_cache!('existing_sigals'.freeze)
cache.send(:connection).smembers(full_key).count
cache.clear_cache!('existing_signals'.freeze)
cache.send(:connection).smembers(full_key).count
cache = RvxSharedCache.new(Cloud.current_name)
cache.clear_cache!('existing_signals'.freeze)
measure do
  sigs = RvxSignal.connection.select_all('SELECT sig FROM rvx_signals').to_ary; nil
  sigs.map! { |r| r['sig'] }
  cache.add_to_set('existing_signals'.freeze, sigs)
end
cache = RvxSharedCache.new(Cloud.current_name)
cache.clear_cache!('existing_signals'.freeze)
measure do
  RvxSignal.select(:id, :sig).find_in_batches do |sbatch|
    sbatch.each do |sigobj|
      cache.add_to_set('existing_signals'.freeze, sigobj[:sig])
    end
  end
end
exit
edit -t
cache = RvxSharedCache.new(Cloud.current_name)
cache.clear_cache!('existing_signals')
measure do
  RvxSignal.select(:id, :sig).find_in_batches do |sbatch|
    sbatch.each do |sigobj|
      cache.add_to_set('existing_signals', sigobj[:sig])
    end
  end
end
cache = RvxSharedCache.new(Cloud.current_name)
cache.clear_cache!('existing_signals')
measure do
  RvxSignal.select(:id, :sig).find_in_batches do |sbatch|
    sbatch.each do |sigobj|
      cache.add_to_set('existing_signals'.freeze, sigobj[:sig])
    end
  end
end
exit
edit -t
cache = RvxSharedCache.new(Cloud.current_name)
cache.clear_cache!('existing_signals'.freeze)
measure do
  RvxSignal.select(:id, :sig).find_in_batches do |sbatch|
    sbatch.each do |sigobj|
      cache.add_to_set('existing_signals'.freeze, sigobj[:sig])
    end
  end
end
cache = RvxSharedCache.new(Cloud.current_name)
cache.clear_cache!('existing_signals'.freeze)
measure do
  sigs = RvxSignal.connection.select_all('SELECT sig FROM rvx_signals').to_ary; nil
  sigs.map! { |r| r['sig'] }
  cache.add_to_set('existing_signals'.freeze, sigs)
end
cache = RvxSharedCache.new(Cloud.current_name)
cache.clear_cache!('existing_signals'.freeze)
measure do
  sigs = RvxSignal.pluck(:sig)
  # sigs.map! { |r| r['sig'] }
  cache.add_to_set('existing_signals'.freeze, sigs)
end
cache = RvxSharedCache.new(Cloud.current_name)
cache.clear_cache!('existing_signals'.freeze)
measure do
  sigs = RvxSignal.pluck(:sig)
  # sigs.map! { |r| r['sig'] }
  cache.add_to_set('existing_signals'.freeze, sigs)
end
cache = RvxSharedCache.new(Cloud.current_name)
cache.clear_cache!('existing_signals'.freeze)
measure do
  sigs = RvxSignal.connection.select_all('SELECT sig FROM rvx_signals').to_ary; nil
  sigs.map! { |r| r['sig'] }
  cache.add_to_set('existing_signals'.freeze, sigs)
end
RvxSignal.count
sigs = RvxSignal.connection.select_rows('SELECT sig FROM rvx_signals'); nil
sigs
cache = RvxSharedCache.new(Cloud.current_name)
# cache.clear_cache!('existing_signals'.freeze)
measure do
  # sigs = RvxSignal.connection.select_all('SELECT sig FROM rvx_signals').to_ary; nil
  # sigs.map! { |r| r['sig'] }
  sigs = RvxSignal.connection.select_rows('SELECT sig FROM rvx_signals'); nil
  sigs.flatten!
  cache.add_to_set('existing_signals'.freeze, sigs)
end
measure do
  cache = RvxSharedCache.new(Cloud.current_name)
  sigs = RvxSignal.connection.select_rows('SELECT sig FROM rvx_signals'); nil
  sigs.flatten!
  cache.add_to_set('existing_signals'.freeze, sigs)
end
measure do
  cache = RvxSharedCache.new(Cloud.current_name)
  sigs = RvxSignal.connection.select_rows('SELECT sig FROM rvx_signals'); nil
  sigs.flatten!
  cache.add_to_set('existing_signals'.freeze, sigs)
end
exit
edit -t
measure(true) do
  # cache = RvxSharedCache.new(Cloud.current_name)
  # RvxSignal.select(:id, :sig).find_in_batches do |sbatch|
  #   sbatch.each do |sigobj|
  #     cache.add_to_set('existing_signals', sigobj[:sig])
  #   end
  # end
  cache = RvxSharedCache.new(Cloud.current_name)
  sbatch = RvxSignal.select(:id, :sig).first(1000)
  sbatch.each do |sigobj|
    cache.add_to_set('existing_signals', sigobj[:sig])
  end
end
measure do
  # cache = RvxSharedCache.new(Cloud.current_name)
  # RvxSignal.select(:id, :sig).find_in_batches do |sbatch|
  #   sbatch.each do |sigobj|
  #     cache.add_to_set('existing_signals', sigobj[:sig])
  #   end
  # end
  cache = RvxSharedCache.new(Cloud.current_name)
  sbatch = RvxSignal.select(:id, :sig).first(1000)
  sbatch.each do |sigobj|
    cache.add_to_set('existing_signals', sigobj[:sig])
  end
end
sigs = RvxSignal.connection.select_rows('SELECT sig FROM `rvx_signals` ORDER BY `rvx_signals`.`id` ASC LIMIT 1000'); nil
sigs
measure(true) do
  cache = RvxSharedCache.new(Cloud.current_name)
  sigs = RvxSignal.connection.select_rows('SELECT sig FROM `rvx_signals` ORDER BY `rvx_signals`.`id` ASC LIMIT 1000'); nil
  sigs.flatten!
  cache.add_to_set('existing_signals'.freeze, sigs)
end
RvxSignal.last
RvxSignal.last.id
RvxSignal.count
sigs = RvxSignal.connection.select_rows('SELECT sig FROM `rvx_signals` ORDER BY `rvx_signals`.`id` ASC LIMIT 300000, 100000'); nil
sigs.count
sigs = RvxSignal.connection.select_rows('SELECT sig FROM `rvx_signals` LIMIT 300000, 100000'); nil
sigs.count
signal_count = RvxSignal.count
offset = 0
sigs = RvxSignal.connection.select_rows("SELECT sig FROM `rvx_signals` ORDER BY `rvx_signals`.`id` ASC LIMIT #{offset}, 100000"); nil
exit
RvxSignal.first.id
signal_count = RvxSignal.count
offset = 0
sigs = RvxSignal.connection.select_rows("SELECT sig FROM `rvx_signals` LIMIT #{offset}, 1"); nil
sigs
sigs = RvxSignal.connection.select_rows("SELECT id, sig FROM `rvx_signals` LIMIT #{offset}, 1"); nil
sigs
RvxSignal.last
RvxSignal.first
offset = 0
signal_count = RvxSignal.count
offset = 0
batch_size = 100000
sigs = []
while offset < signal_count
  sigs << RvxSignal.connection.select_rows("SELECT sig FROM `rvx_signals` LIMIT #{offset}, #{batch_size}"); nil
  offset += batch_size
  # sigs.flatten!
  # cache.add_to_set('existing_signals'.freeze, sigs)
end
sigs.flattent!
sigs.flatten!
sigs.uni!
sigs.uniq!
sigs.count
RvxSignal.count
edit -t
cache = RvxSharedCache.new(Cloud.current_name)
cache.clear_cache!('existing_signals'.freeze)
measure(true) do
  signal_count = RvxSignal.count
  offset = 0
  batch_size = 100000
  while offset < signal_count
    sigs = RvxSignal.connection.select_rows("SELECT sig FROM `rvx_signals` LIMIT #{offset}, #{batch_size}"); nil
    offset += batch_size
    sigs.flatten!
    cache.add_to_set('existing_signals'.freeze, sigs)
  end
end
cache = RvxSharedCache.new(Cloud.current_name)
cache.clear_cache!('existing_signals'.freeze)
measure(false) do
  signal_count = RvxSignal.count
  offset = 0
  batch_size = 100000
  while offset < signal_count
    sigs = RvxSignal.connection.select_rows("SELECT sig FROM `rvx_signals` LIMIT #{offset}, #{batch_size}"); nil
    offset += batch_size
    sigs.flatten!
    cache.add_to_set('existing_signals'.freeze, sigs)
  end
end
cache
cache.smembers('existing_signals'.freeze)
cache.send(:full_key, 'existing_signals')
full_key = _
cache.send(:connection).smembers(full_key).count
offset = RvxSignal.count
batch_size = 100000
sigs = RvxSignal.connection.select_rows("SELECT sig FROM `rvx_signals` LIMIT #{offset}, #{batch_size}"); nil
sigs
signal_count = RvxSignal.count
def measure(no_gc=false, &block)
  if no_gc
    GC.disable
  else
    # collect memory allocated during library loading
    # and our own code before the measurement
    GC.enable
    GC.start
  end
  memory_before = `ps -o rss= -p #{Process.pid}`.to_i/1024
  gc_stat_before = GC.stat
  time = Benchmark.realtime do
    yield
  end
  puts ObjectSpace.count_objects
  unless no_gc
    GC.start(full_mark: true, immediate_sweep: true, immediate_mark: true)
  end
  puts ObjectSpace.count_objects
  gc_stat_after = GC.stat
  memory_after = `ps -o rss= -p #{Process.pid}`.to_i/1024
  puts({
      RUBY_VERSION => {
        gc: no_gc ? 'disabled' : 'enabled',
        time: time.round(2),
        gc_count: gc_stat_after[:count] - gc_stat_before[:count],
        memory: "%d MB" % (memory_after - memory_before)
      }
  }.to_json)
end
full_key
cache.send(:connection).scard(full_key)
cache = RvxSharedCache.new(Cloud.current_name)
cache.clear_cache!('existing_signals'.freeze)
measure(false) do
  offset = 0
  batch_size = 100000
  loop do
    sigs = RvxSignal.connection.select_rows("SELECT sig FROM `rvx_signals` LIMIT #{offset}, #{batch_size}"); nil
    offset += batch_size
    sigs.flatten!
    cache.add_to_set('existing_signals'.freeze, sigs)
    break if sigs.length < batch_size
  end
end
full_key = "development:shared_cache:demo:existing_signals"
cache.send(:connection).scard(full_key)
cache = RvxSharedCache.new(Cloud.current_name)
cache.clear_cache!('existing_signals'.freeze)
measure(true) do
  offset = 0
  batch_size = 100000
  loop do
    sigs = RvxSignal.connection.select_rows("SELECT sig FROM `rvx_signals` LIMIT #{offset}, #{batch_size}"); nil
    offset += batch_size
    sigs.flatten!
    cache.add_to_set('existing_signals'.freeze, sigs)
    break if sigs.length < batch_size
  end
end
cache = RvxSharedCache.new(Cloud.current_name)
cache.clear_cache!('existing_signals'.freeze)
measure(true) do
  sigs = RvxSignal.connection.select_rows('SELECT sig FROM rvx_signals'); nil
  sigs.flatten!
  cache.add_to_set('existing_signals'.freeze, sigs)
end
cache = RvxSharedCache.new(Cloud.current_name)
cache.clear_cache!('existing_signals'.freeze)
edit -t
exit
edit -t
cache = RvxSharedCache.new(Cloud.current_name)
cache.clear_cache!('existing_signals')
measure do
  sigs = RvxSignal.connection.select_rows('SELECT sig FROM rvx_signals'); nil
  sigs.flatten!
  cache.add_to_set('existing_signals', sigs)
end
{:TOTAL=>2342068, :FREE=>70, :T_OBJECT=>58829, :T_CLASS=>17860, :T_MODULE=>2882, :T_FLOAT=>10, :T_STRING=>1270246, :T_REGEXP=>4367, :T_ARRAY=>742364, :T_HASH=>51586, :T_STRUCT=>4975, :T_BIGNUM=>11, :T_FILE=>10, :T_DATA=>115659, :T_MATCH=>6, :T_COMPLEX=>1, :T_RATIONAL=>61, :T_SYMBOL=>7888, :T_NODE=>59220, :T_ICLASS=>6023}
{:TOTAL=>2342068, :FREE=>1492297, :T_OBJECT=>58827, :T_CLASS=>17860, :T_MODULE=>2882, :T_FLOAT=>10, :T_STRING=>367001, :T_REGEXP=>4367, :T_ARRAY=>153630, :T_HASH=>51587, :T_STRUCT=>4975, :T_BIGNUM=>11, :T_FILE=>10, :T_DATA=>115421, :T_MATCH=>5, :T_COMPLEX=>1, :T_RATIONAL=>61, :T_SYMBOL=>7888, :T_NODE=>59212, :T_ICLASS=>6023}
{"2.2.5":{"gc":"enabled","time":1.5,"gc_count":3,"memory":"61 MB"}}
cache = RvxSharedCache.new(Cloud.current_name)
cache.clear_cache!('existing_signals')
measure do
  sigs = RvxSignal.connection.select_rows('SELECT sig FROM rvx_signals'); nil
  sigs.flatten!
  cache.add_to_set('existing_signals', sigs)
end
full_key = "development:shared_cache:demo:existing_signals"
cache.send(:connection).scard(full_key)
RvxSignal.count
exit
RvxSignalGeneratorBatchWorker.new.perform_async
RvxSignalGeneratorBatchWorker.perform_async
RvxSignalGeneratorBatchWorker.perform_async({})
include CloudHelpers
each_cloud do |cloud|
  puts cloud
  puts cloud.enable_signal_generators
end
cloud = _.first
cloud.enable_signal_generators
Cloud
RvxSignalGeneratorBatchWorker.perform_async({})
Clould.all.map(&:enable_signal_generators)
Cloud.all.map(&:enable_signal_generators)
Cloud.update_all(enable_signal_generators: true)
exit
RvxSignal.count
SignalGeneratorLog.last
Redis.current
Redis.current.flushall
exit
SignalGeneratorLog.last
SignalGeneratorLog.last(2)
SignalGeneratorLog.last(3)
SignalGeneratorLog.last(4)
SignalGeneratorLog.last(5)
sig_gens = RvxSignalGenerator.
select('rvx_signal_generators.id', :signal_generator_num, :class_name, cloud_signal_generators: [:active]).
includes(:cloud_signal_generator).
where("cloud_signal_generators.active = 1")
RvxSignalGenerator.count
CloudSignalGenerator.first
CloudSignalGenerator.pluck(:active)
CloudSignalGenerator.each { |c| c.update(active: true)}
CloudSignalGenerator.all.each { |c| c.update(active: true)}
Redis.current.flushall
SignalGeneratorLog.last
RvxSignalGenerator.last
contracts = Contract.where("TIMESTAMPDIFF(MONTH, contract_effective_date, ?) >= ?", Date.today, self.duration_months)
contracts = Contract.where("TIMESTAMPDIFF(MONTH, contract_effective_date, ?) >= ?", Date.today, 6)
c = contracts.first
total_missing_technologies = []
entities = {}
ct = c.contract_technologies.first
ct.entity.present?
ct.technology
exit
[1,2,3,4,5].each do |i|
  my_var ||= 10
txn_id = Digest::MD5.hexdigest "#{DateTime.now.to_s}#{Cloud.current.id.to_s}" 
exit
cache=RvxSharedCache.new(Cloud.current_name)
signal_generator_id = 3
signal_generator = RvxSignalGenerator.find(signal_generator_id)
txn_id = "4335231ee3e9fba2e5af11ab507b6952"
emits, skips = 0, 0
technologies = Technology.all
technology = technologies.first
technology.strong_keywords.nil?
strong_keywords = parse_keywords(technology.strong_keywords)
cd RvxSignalGenerators::UnlicensedTechnology.new
cache=RvxSharedCache.new(Cloud.current_name)
signal_generator_id = 3
signal_generator = RvxSignalGenerator.find(signal_generator_id)
txn_id = "4335231ee3e9fba2e5af11ab507b6952"
emits, skips = 0, 0
technologies = Technology.all
technology = technologies.first
strong_keywords = parse_keywords(technology.strong_keywords)
edit -t
royalty_products = nil
edit -t
exit
cache = RvxSharedCache.new(Cloud.current_name)
offset = 0
batch_size = 300000
sigs = RvxSignal.connection.select_rows("SELECT sig FROM `rvx_signals` LIMIT #{offset}, #{batch_size}"); nil
sigs = RvxSignal.select("SELECT sig FROM `rvx_signals` LIMIT #{offset}, #{batch_size}"); nil
sigs
sigs = RvxSignal.select("sig FROM `rvx_signals` LIMIT #{offset}, #{batch_size}"); nil
sigs
sigs = RvxSignal.select("sig").limit("LIMIT #{offset}, #{batch_size}"); nil
sigs
sigs.first
sigs.first.class
sigs.count
sigs
sigs.first
sigs.all
sigs.load
sigs = RvxSignal.connection.select_rows("SELECT sig FROM `rvx_signals` LIMIT #{offset}, #{batch_size}"); nil
sigs.count
sigs = RvxSignal.connection.select_rows("SELECT sig FROM `rvx_signals`"); nil
sigs.count
Apartment::Tenant.switch! 'synopsys'
sigs = RvxSignal.connection.select_rows("SELECT sig FROM `rvx_signals`"); nil
sigs.count
exit
source_id = 2
Source.find 2
source_name = 'alius'
redis = Redis::Namespace.new "#{Rails.env}_#{source_name}"
edit -t
output_message = "Starting scrape for source: #{source_name}"
test_process = 'falase
test_process = 'false'
pool = ConnectionPool.new { redis }
item = { 'class' => CategoryJob, 'args' => [source_id, source_category_ids, max_pages, max_per_page, max_products] }
source_category_ids = SourceCategory.where(source_id: source_id).test.pluck(:id) if test_process == 'true'
item = { 'class' => CategoryJob, 'args' => [source_id, source_category_ids, max_pages, max_per_page, max_products] }
item = { 'class' => CategoryJob, 'args' => [source_id, source_category_ids, nil, nil, nil] }
source_id = 2; source_category_ids = nil; max_pages = nil; max_per_page = nil; max_products = nil
source = Source.find(source_id)
return if Scrape.where(running: true, source_id: source_id).any?
Scrape.where(running: true, source_id: source_id).any?
reported_product = rri.product
rri = RoyaltyReportItem.first
reported_product = rri.product
technology = rri.technology
RoyaltyReportItem.joins(:product).where(product_id: reported_product.id, technology_id: technology.id).size > 0
RoyaltyReportItem.joins(:product).where(product_id: reported_product.id, technology_id: technology.id).any?
product
reported_product.entity
product_entity = +
product_entity = reported_product.entity
Technology.joins(:contracts).
where(
  'contracts.id': Contract.joins(:entities).
  where('entities.id': product_entity.id).
  contract_effective_range(5.years.ago, Date.today).
  select('DISTINCT(contracts.id)')
).where('technologies.id': technology.id).any?
edit -t
edit
technology = _.first
technology.active
technology.active?
technology.projects
technology.projections
edit
exit
scrape = Scrape.create source: source, running: true
ScrapeSource.create(scrape: scrape, source: source)
Integer(nil)
nil.to_i
Sidekiq.redis do |redis|
  redis.set("scrape:#{scrape.id}:max_pages", max_pages) if max_pages
  redis.set("scrape:#{scrape.id}:max_per_page", max_per_page) if max_per_page
  redis.set("scrape:#{scrape.id}:max_products", max_products) if max_products
end
max_pages = nil
max_per_page = nil
max_products = nil
Sidekiq.redis do |r|
max_pages = Integer(r.get("scrape:#{scrape_id}:max_pages")) rescue nil
max_per_page = Integer(r.get("scrape:#{scrape_id}:max_per_page")) rescue nil
max_products = Integer(r.get("scrape:#{scrape_id}:max_products")) rescue nil
end
max_pages
source_categories = source_category_ids.present? ?
source.source_categories.where(id: source_category_ids) : source.source_categories.active
source_cat = source_categories.first
scrape_id, source_category_id, category_url = scrape.id, source_cat.id, source_cat.parse_url
args = [117, 1872, "https://www.alibaba.com/catalogs/products/CID100005071/1"]
cd AlibabaProductListingJob.new
args = [117, 1872, "https://www.alibaba.com/catalogs/products/CID100005071/1"]
scrape_id, source_category_id, category_url = args
retry_time = 0
visit_page(URI.escape(category_url, '|'), load_images: @load_img, cookie_url: @cookie_url)
@session.save_and_open_screenshot
has_error?(scrape_id, source_category_id, category_url, retry_time)
incrementally_scroll_page unless @no_scroll
next_link = @session.first(:next_link)
next_link.nil? || !valid_within_batch?
next_link.nil?
category_url = next_link_url next_link
next_url = next_link_url
@session.all(:product)
urls = []
@session.all(:product).each_with_index do |product, i|
  break if (max_per_page && i >= max_per_page)
  urls << url = get_product_url(product)
  # if url.present?
  #   get_detail_product(scrape_id, source_category_id, url)
  #   if max_products
  #     num_products = Sidekiq.redis { |r| r.incr("scrape:#{scrape_id}:num_products") }
  #     break if num_products >= max_products
  #   end
  # else
  #   log_error(scrape_id, source_category_id, product)
  # end
end
@session.all(:product).each_with_index do |product, i|
  # break if (max_per_page && i >= max_per_page)
  urls << url = get_product_url(product)
  # if url.present?
  #   get_detail_product(scrape_id, source_category_id, url)
  #   if max_products
  #     num_products = Sidekiq.redis { |r| r.incr("scrape:#{scrape_id}:num_products") }
  #     break if num_products >= max_products
  #   end
  # else
  #   log_error(scrape_id, source_category_id, product)
  # end
end
urls
next_button = @session.first(:next_link)
redirected?(category_url)
@session.save_and_open_screenshot
@load_img = false
visit_page(URI.escape(category_url, '|'), load_images: @load_img, cookie_url: @cookie_url)
@session.save_and_open_screenshot
exit
Redis.current.flushall
exit
Scrape.update_all(running: false)
exit
source_name = alius
source_name = 'alius'
redis = Redis::Namespace.new "#{Rails.env}_#{source_name}"
key = "cookies:*"
redis.get key
redis.get ':cookies:*'
redis.get ':cookies'
redis = Redis.new 
key = "#{Rails.env}_#{source_name}:cookies*"
redis.del key
key = "cookies:*"
redis = Redis::Namespace.new "#{Rails.env}_#{source_name}"
key = "cookies:*"
redis.keys(key).each do |key|
  redis.del key
end
redis.keys(key)
exit
redis = Redis::Namespace.new "#{Rails.env}_#{source_name}"
source_name = alius
source_name = 'alius'
redis = Redis::Namespace.new "#{Rails.env}_#{source_name}"
redis.keys('cookies:*')
redis.keys('cookies:*').count
proxies_file = "#{Rails.root}/proxy_list.txt"
keys = redis.keys('cookies:*')
key = keys.first
redis.get key
val = _
unzip_val = ActiveSupport::Gzip.decompress(val)
cookies = _
user_agent = cookies.delete('user_agent'.freeze)
user_agent
cookies['cookies']
unzip_val
Oj.load(unzip_val)
cookies = _
user_agent = cookies.delete('user_agent'.freeze)
s = Scrape.last
s.listings.count
s.reload!
s = Scrape.last(2).first
s.listings.count
url = "https://www.alibaba.com/product-detail/sale-direct-china-cheap-price-156_60565127145.html"
cd AlibabaProductDetailJob.new
url = "https://www.alibaba.com/product-detail/sale-direct-china-cheap-price-156_60565127145.html"
detail_url = get_detail_url(url)
url_md5 = Digest::MD5.hexdigest(detail_url)
listing = Listing.find_by(scrape_id: scrape_id, url: detail_url)
listing_url = ListingUrl.find_or_create_by(url_md5: url_md5, url: detail_url)
listing = Listing.find_by(scrape_id: scrape_id, url: detail_url)
Scrape.lis
Scrape.last
scrape_id = 118
listing = Listing.find_by(scrape_id: scrape_id, url: detail_url)
visit_page detail_url, cookie_url: @cookie_url
@session
@session.driver.clear_cookies
@session.visit(@cookie_url)
@session.visit('http://www.alibaba.com')
exit
s = Scrape.last
s1, s2 = Scrape.last(2)
s1.listings.count
s2.listings.count
exit
cd AlibabaProductDetailJob.new
url = "https://www.alibaba.com/product-detail/100w-mono-semi-flexible-solar-panel_60518126160.html"
scrape_id = 119
detail_url = get_detail_url(url)
url_md5 = Digest::MD5.hexdigest(detail_url)
listing_url = ListingUrl.find_or_create_by(url_md5: url_md5, url: detail_url)
listing = Listing.find_by(scrape_id: scrape_id, url: detail_url)
options = {cookie_url: @cookie_url}
opts = _
options = {
  load_images: true,
  proxy: (ENV['SOCKS_PROXY'].present? ? ENV['SOCKS_PROXY'] : pop_proxy),
}.merge!(opts)
proxy = options[:proxy]
COOKIE_JAR
COOKIE_JAR.keys(':*')
COOKIE_JAR.keys
COOKIE_JAR
key = COOKIE_JAR.keys.first
COOKIE_JAR.fetch(key)
COOKIE_JAR.fetch('98.159.236.188:60000')
cookie = _
cookie['JSESSIONID']
cookies = COOKIE_JAR.fetch('98.159.236.188:60000')
cookes.find {|c| c['name'] == 'JSESSIONID' }
cookies.find {|c| c['name'] == 'JSESSIONID' }
cookies.first
cookies.delete('user-agent')
cookies.first
cookies.delete('user_agent')
cookies.first
cookies
cookies['cookies'].find {|c| c['name'] == 'JSESSIONID' }
exit
Listing.count
exit
Redis.current.flushall
exit
Listing.count
exit
Listing.count
exit
